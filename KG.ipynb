{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:22.320215Z",
     "start_time": "2026-02-18T14:27:20.582385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "files = glob.glob(\"Scientific_Novelty_Detection/Triplets/**/*.csv\", recursive=True)\n",
    "\n",
    "print(\"Files found:\", files)\n",
    "\n",
    "if len(files) == 0:\n",
    "    print(\"No files found. Check path.\")\n",
    "else:\n",
    "    dfs = [pd.read_csv(f) for f in files]\n",
    "    scind = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"Loaded rows:\", len(scind))"
   ],
   "id": "fa7723ade27d2c83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: ['Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\Dia_Blogs_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\MT_Blogs_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\QA_Blogs_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\SA_Blogs_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\Sum_Blogs_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\Dia2021_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\MT2021_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\QA2021_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\SA2021_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\Sum2021_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\Dia_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\MT_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\NLI_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\Par_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\QA_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\SA_triplets.csv', 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\Sum_triplets.csv']\n",
      "Loaded rows: 238088\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:22.374279Z",
     "start_time": "2026-02-18T14:27:22.366295Z"
    }
   },
   "cell_type": "code",
   "source": "files",
   "id": "b156cc0f801a508c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\Dia_Blogs_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\MT_Blogs_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\QA_Blogs_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\SA_Blogs_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Blogs\\\\Sum_Blogs_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\Dia2021_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\MT2021_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\QA2021_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\SA2021_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\Novel_Papers\\\\Sum2021_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\Dia_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\MT_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\NLI_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\Par_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\QA_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\SA_triplets.csv',\n",
       " 'Scientific_Novelty_Detection/Triplets\\\\SKG\\\\Sum_triplets.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:23.105435Z",
     "start_time": "2026-02-18T14:27:23.098233Z"
    }
   },
   "cell_type": "code",
   "source": "scind.columns",
   "id": "c069197b3279758f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic', 'paper_ID', 'sentence_ID', 'info-unit', 'sub', 'pred', 'obj',\n",
       "       'triplets', 'pred_weights'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:23.162257Z",
     "start_time": "2026-02-18T14:27:23.150921Z"
    }
   },
   "cell_type": "code",
   "source": "scind.head()",
   "id": "153d1a14f2fcbbb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   topic  paper_ID  sentence_ID          info-unit                    sub  \\\n",
       "0  Blogs         0           23  ablation-analysis       hyper-parameters   \n",
       "1  Blogs         0           23  ablation-analysis  more powerful decoder   \n",
       "2  Blogs         0           23  ablation-analysis      ablation analysis   \n",
       "3  Blogs         0           59  ablation-analysis         tuned decoding   \n",
       "4  Blogs         0           59  ablation-analysis              ssa score   \n",
       "\n",
       "              pred                            obj  \\\n",
       "0  discovered that          more powerful decoder   \n",
       "1           key to  higher conversational quality   \n",
       "2           tuning               hyper-parameters   \n",
       "3         advances                      ssa score   \n",
       "4               to                           79 %   \n",
       "\n",
       "                                            triplets  pred_weights  \n",
       "0  hyper-parameters discovered that more powerful...      0.709181  \n",
       "1  more powerful decoder key to higher conversati...      0.592724  \n",
       "2          ablation analysis tuning hyper-parameters      0.820923  \n",
       "3                  tuned decoding advances ssa score      0.693804  \n",
       "4                                  ssa score to 79 %      0.526003  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>paper_ID</th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>info-unit</th>\n",
       "      <th>sub</th>\n",
       "      <th>pred</th>\n",
       "      <th>obj</th>\n",
       "      <th>triplets</th>\n",
       "      <th>pred_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>ablation-analysis</td>\n",
       "      <td>hyper-parameters</td>\n",
       "      <td>discovered that</td>\n",
       "      <td>more powerful decoder</td>\n",
       "      <td>hyper-parameters discovered that more powerful...</td>\n",
       "      <td>0.709181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>ablation-analysis</td>\n",
       "      <td>more powerful decoder</td>\n",
       "      <td>key to</td>\n",
       "      <td>higher conversational quality</td>\n",
       "      <td>more powerful decoder key to higher conversati...</td>\n",
       "      <td>0.592724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>ablation-analysis</td>\n",
       "      <td>ablation analysis</td>\n",
       "      <td>tuning</td>\n",
       "      <td>hyper-parameters</td>\n",
       "      <td>ablation analysis tuning hyper-parameters</td>\n",
       "      <td>0.820923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>ablation-analysis</td>\n",
       "      <td>tuned decoding</td>\n",
       "      <td>advances</td>\n",
       "      <td>ssa score</td>\n",
       "      <td>tuned decoding advances ssa score</td>\n",
       "      <td>0.693804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>ablation-analysis</td>\n",
       "      <td>ssa score</td>\n",
       "      <td>to</td>\n",
       "      <td>79 %</td>\n",
       "      <td>ssa score to 79 %</td>\n",
       "      <td>0.526003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:23.879293Z",
     "start_time": "2026-02-18T14:27:23.297002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files = glob.glob(\"Scientific_Novelty_Detection/Triplets/**/*.csv\", recursive=True)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    if \"SKG\" in f:\n",
    "        df[\"split\"] = \"SKG\"\n",
    "    elif \"Novel_Papers\" in f:\n",
    "        df[\"split\"] = \"NOVEL\"\n",
    "    elif \"Blogs\" in f:\n",
    "        df[\"split\"] = \"BLOG\"\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "scind = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Total rows:\", len(scind))\n",
    "print(scind[\"split\"].value_counts())"
   ],
   "id": "6429500945c99f07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 238088\n",
      "SKG      187198\n",
      "NOVEL     42359\n",
      "BLOG       8531\n",
      "Name: split, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:23.925570Z",
     "start_time": "2026-02-18T14:27:23.914497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(scind[\"paper_ID\"].head())\n",
    "print(scind[\"paper_ID\"].unique()[:20])"
   ],
   "id": "41efb1e016034be1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: paper_ID, dtype: int64\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:24.019402Z",
     "start_time": "2026-02-18T14:27:24.011279Z"
    }
   },
   "cell_type": "code",
   "source": "print(scind[\"paper_ID\"].unique()[:15])",
   "id": "9fc0bcef1ea94214",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:24.137539Z",
     "start_time": "2026-02-18T14:27:24.132484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.listdir())"
   ],
   "id": "efe8689632b0fdbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.idea', 'KG.ipynb', 'Scientific_Novelty_Detection']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:24.226780Z",
     "start_time": "2026-02-18T14:27:24.222474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(len(os.listdir(\"Scientific_Novelty_Detection/PDF_Papers\")))\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/PDF_Papers\")[:10])"
   ],
   "id": "f42f6ed27fe82d01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['Blogs', 'Novel_Papers_2021', 'SKG']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:24.372050Z",
     "start_time": "2026-02-18T14:27:24.366036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(\"SKG PDFs:\")\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/PDF_Papers/SKG\")[:15])\n",
    "\n",
    "print(\"\\nNovel PDFs:\")\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/PDF_Papers/Novel_Papers_2021\")[:15])\n",
    "\n",
    "print(\"\\nBlog PDFs:\")\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/PDF_Papers/Blogs\")[:15])"
   ],
   "id": "35daf16024e93494",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKG PDFs:\n",
      "['Dia', 'MT', 'NER', 'NLI', 'Par', 'Pos', 'QA', 'SA', 'Sum']\n",
      "\n",
      "Novel PDFs:\n",
      "['Dia2021', 'MT2021', 'QA2021', 'SA2021', 'Sum2021']\n",
      "\n",
      "Blog PDFs:\n",
      "['Dia', 'MT', 'QA', 'SA', 'Sum']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:24.446764Z",
     "start_time": "2026-02-18T14:27:24.435084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(\"Example SKG Dia folder:\")\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/PDF_Papers/SKG/Dia\")[:15])"
   ],
   "id": "6efbac4c9df0cb9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example SKG Dia folder:\n",
      "['10.IIT-UHH at SemEval-2017 Task 3 Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification.pdf', '10.PLATO Pre-trained Dialogue Generation Model with Discrete Latent Variable.pdf', '10.Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks.pdf', '10.The Impact of Interpretation Problems on Tutorial Dialogue.pdf', '100.Learning the Information Status of Noun Phrases in Spoken Dialogues.pdf', '103.Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model.pdf', '104.Towards an Automatic Turing Test Learning to Evaluate Dialogue Responses.pdf', '107.Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System.pdf', '11.A Statistical Spoken Dialogue System using Complex User Goals and Value Directed Compression.pdf', '11.Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-Adaptive Spoken Dialogue System.pdf', '11.Representing Movie Characters in Dialogues.pdf', '11.Safe In-vehicle Dialogue Using Learned Predictions of User Utterances.pdf', '11.Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network.pdf', '111.Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems.pdf', '113.Feudal Reinforcement Learning for Dialogue Management in Large Domains.pdf']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:24.674839Z",
     "start_time": "2026-02-18T14:27:24.666833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Example Novel Dia2021 folder:\")\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/PDF_Papers/Novel_Papers_2021/Dia2021\")[:15])"
   ],
   "id": "eeb0af82d12ed8ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Novel Dia2021 folder:\n",
      "['112.Preview, Attend and Review Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking.pdf', '114.Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images.pdf', '123.Fine-grained Post-training for Improving Retrieval-based Dialogue Systems.pdf', '125.Adding Chit-Chat to Enhance Task-Oriented Dialogues.pdf', '14.Saying No is An Art Contextualized Fallback Responses for Unanswerable Dialogue Queries.pdf', '141.DIALKI Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization.pdf', '144.Contextual Rephrase Detection for Reducing Friction in Dialogue Systems.pdf', '164.Reference-Centric Models for Grounded Collaborative Dialogue.pdf', '169.Neural Path Hunter Reducing Hallucination in Dialogue Systems via Path Grounding.pdf', '170.Thinking Clearly, Talking Fast Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems.pdf', '172.Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration.pdf', '173.CoLV A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation.pdf', '174.A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation.pdf', '175.Intention Reasoning Network for Multi-Domain End-to-end Task-Oriented Dialogue.pdf', '176.More is Better Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge.pdf']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:24.776404Z",
     "start_time": "2026-02-18T14:27:24.719125Z"
    }
   },
   "cell_type": "code",
   "source": "print(scind.groupby([\"split\", \"topic\"])[\"paper_ID\"].nunique())",
   "id": "fffc562052554389",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split  topic        \n",
      "BLOG   Blogs              20\n",
      "       summarization      25\n",
      "NOVEL  translation       302\n",
      "SKG    Blogs              28\n",
      "       translation      1449\n",
      "Name: paper_ID, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:24.970487Z",
     "start_time": "2026-02-18T14:27:24.806255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"BLOG paper_ID min/max:\",\n",
    "      scind[scind[\"split\"]==\"BLOG\"][\"paper_ID\"].min(),\n",
    "      scind[scind[\"split\"]==\"BLOG\"][\"paper_ID\"].max())\n",
    "\n",
    "print(\"SKG paper_ID min/max:\",\n",
    "      scind[scind[\"split\"]==\"SKG\"][\"paper_ID\"].min(),\n",
    "      scind[scind[\"split\"]==\"SKG\"][\"paper_ID\"].max())\n",
    "\n",
    "print(\"NOVEL paper_ID min/max:\",\n",
    "      scind[scind[\"split\"]==\"NOVEL\"][\"paper_ID\"].min(),\n",
    "      scind[scind[\"split\"]==\"NOVEL\"][\"paper_ID\"].max())"
   ],
   "id": "53ec1d7a2dfd8487",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOG paper_ID min/max: 0 24\n",
      "SKG paper_ID min/max: 0 1475\n",
      "NOVEL paper_ID min/max: 0 306\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:25.003811Z",
     "start_time": "2026-02-18T14:27:24.996841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(\"SKG total PDFs:\",\n",
    "      sum([len(files) for _,_,files in os.walk(\"PDF_Papers/SKG\")]))\n",
    "\n",
    "print(\"NOVEL total PDFs:\",\n",
    "      sum([len(files) for _,_,files in os.walk(\"PDF_Papers/Novel_Papers_2021\")]))\n",
    "\n",
    "print(\"BLOG total PDFs:\",\n",
    "      sum([len(files) for _,_,files in os.walk(\"PDF_Papers/Blogs\")]))"
   ],
   "id": "1e721c398b3acdce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKG total PDFs: 0\n",
      "NOVEL total PDFs: 0\n",
      "BLOG total PDFs: 0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:25.632863Z",
     "start_time": "2026-02-18T14:27:25.024913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "records = []\n",
    "\n",
    "files = glob.glob(\"Scientific_Novelty_Detection/Triplets/**/*.csv\", recursive=True)\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    # Determine split\n",
    "    if \"SKG\" in f:\n",
    "        split = \"SKG\"\n",
    "    elif \"Novel_Papers\" in f:\n",
    "        split = \"NOVEL\"\n",
    "    elif \"Blogs\" in f:\n",
    "        split = \"BLOG\"\n",
    "\n",
    "    # Extract domain from filename\n",
    "    filename = os.path.basename(f)\n",
    "    domain = filename.replace(\"_triplets.csv\", \"\")\n",
    "\n",
    "    df[\"split\"] = split\n",
    "    df[\"domain_folder\"] = domain\n",
    "    df[\"source_file\"] = f\n",
    "\n",
    "    records.append(df)\n",
    "\n",
    "scind = pd.concat(records, ignore_index=True)\n",
    "\n",
    "print(\"Total rows:\", len(scind))"
   ],
   "id": "dd800568712a8d8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 238088\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:25.877064Z",
     "start_time": "2026-02-18T14:27:25.661318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "mapping_records = []\n",
    "\n",
    "for (split, domain), group in scind.groupby([\"split\", \"domain_folder\"]):\n",
    "\n",
    "    # Determine correct PDF folder path\n",
    "    if split == \"SKG\":\n",
    "        pdf_path = f\"Scientific_Novelty_Detection/PDF_Papers/SKG/{domain}\" ## *.pdf\n",
    "    elif split == \"NOVEL\":\n",
    "        pdf_path = f\"Scientific_Novelty_Detection/PDF_Papers/Novel_Papers_2021/{domain}\" ## *.pdf\n",
    "    elif split == \"BLOG\":\n",
    "        pdf_path = f\"Scientific_Novelty_Detection/PDF_Papers/Blogs/{domain}\" ## *.pdf\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(\"Missing folder:\", pdf_path)\n",
    "        continue\n",
    "\n",
    "    # Unique paper IDs\n",
    "    paper_ids = sorted(group[\"paper_ID\"].unique())\n",
    "\n",
    "    # PDF files\n",
    "    pdf_files = sorted([f for f in os.listdir(pdf_path) if f.endswith(\".pdf\")])\n",
    "\n",
    "    if len(paper_ids) > len(pdf_files):\n",
    "        print(\"Mismatch:\", split, domain)\n",
    "        continue\n",
    "\n",
    "    # Map index-wise\n",
    "    for pid, pdf in zip(paper_ids, pdf_files):\n",
    "        title = pdf.replace(\".pdf\", \"\")\n",
    "        title = re.sub(r\"^\\d+\\.\", \"\", title)\n",
    "\n",
    "        mapping_records.append({\n",
    "            \"split\": split,\n",
    "            \"domain_folder\": domain,\n",
    "            \"paper_ID\": pid,\n",
    "            \"title\": title\n",
    "        })\n",
    "\n",
    "paper_mapping = pd.DataFrame(mapping_records)\n",
    "\n",
    "print(\"Mapped papers:\", len(paper_mapping))"
   ],
   "id": "5d52ef2ef84e18f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing folder: Scientific_Novelty_Detection/PDF_Papers/Blogs/Dia_Blogs\n",
      "Missing folder: Scientific_Novelty_Detection/PDF_Papers/Blogs/MT_Blogs\n",
      "Missing folder: Scientific_Novelty_Detection/PDF_Papers/Blogs/QA_Blogs\n",
      "Missing folder: Scientific_Novelty_Detection/PDF_Papers/Blogs/SA_Blogs\n",
      "Missing folder: Scientific_Novelty_Detection/PDF_Papers/Blogs/Sum_Blogs\n",
      "Mapped papers: 3881\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:25.952123Z",
     "start_time": "2026-02-18T14:27:25.940375Z"
    }
   },
   "cell_type": "code",
   "source": "paper_mapping",
   "id": "af658fe7dd2214ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      split domain_folder  paper_ID  \\\n",
       "0     NOVEL       Dia2021         0   \n",
       "1     NOVEL       Dia2021         1   \n",
       "2     NOVEL       Dia2021         2   \n",
       "3     NOVEL       Dia2021         3   \n",
       "4     NOVEL       Dia2021         4   \n",
       "...     ...           ...       ...   \n",
       "3876    SKG           Sum       216   \n",
       "3877    SKG           Sum       217   \n",
       "3878    SKG           Sum       218   \n",
       "3879    SKG           Sum       219   \n",
       "3880    SKG           Sum       220   \n",
       "\n",
       "                                                  title  \n",
       "0     Preview, Attend and Review Schema-Aware Curric...  \n",
       "1     Constructing Multi-Modal Dialogue Dataset by R...  \n",
       "2     Fine-grained Post-training for Improving Retri...  \n",
       "3     Adding Chit-Chat to Enhance Task-Oriented Dial...  \n",
       "4     Saying No is An Art Contextualized Fallback Re...  \n",
       "...                                                 ...  \n",
       "3876  Unsupervised Opinion Summarization as Copycat-...  \n",
       "3877  Efficient Online Summarization of Microbloggin...  \n",
       "3878         Improving ROUGE for Timeline Summarization  \n",
       "3879  Neural Summarization by Extracting Sentences a...  \n",
       "3880  Using Summarization to Discover Argument Facet...  \n",
       "\n",
       "[3881 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>domain_folder</th>\n",
       "      <th>paper_ID</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOVEL</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>0</td>\n",
       "      <td>Preview, Attend and Review Schema-Aware Curric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOVEL</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>1</td>\n",
       "      <td>Constructing Multi-Modal Dialogue Dataset by R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOVEL</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>2</td>\n",
       "      <td>Fine-grained Post-training for Improving Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOVEL</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>3</td>\n",
       "      <td>Adding Chit-Chat to Enhance Task-Oriented Dial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOVEL</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>4</td>\n",
       "      <td>Saying No is An Art Contextualized Fallback Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>216</td>\n",
       "      <td>Unsupervised Opinion Summarization as Copycat-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>217</td>\n",
       "      <td>Efficient Online Summarization of Microbloggin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>218</td>\n",
       "      <td>Improving ROUGE for Timeline Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>219</td>\n",
       "      <td>Neural Summarization by Extracting Sentences a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>220</td>\n",
       "      <td>Using Summarization to Discover Argument Facet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3881 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:26.096937Z",
     "start_time": "2026-02-18T14:27:26.088893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(paper_mapping.head())\n",
    "print(paper_mapping.groupby(\"split\").size())"
   ],
   "id": "2321246d3fa32e6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   split domain_folder  paper_ID  \\\n",
      "0  NOVEL       Dia2021         0   \n",
      "1  NOVEL       Dia2021         1   \n",
      "2  NOVEL       Dia2021         2   \n",
      "3  NOVEL       Dia2021         3   \n",
      "4  NOVEL       Dia2021         4   \n",
      "\n",
      "                                               title  \n",
      "0  Preview, Attend and Review Schema-Aware Curric...  \n",
      "1  Constructing Multi-Modal Dialogue Dataset by R...  \n",
      "2  Fine-grained Post-training for Improving Retri...  \n",
      "3  Adding Chit-Chat to Enhance Task-Oriented Dial...  \n",
      "4  Saying No is An Art Contextualized Fallback Re...  \n",
      "split\n",
      "NOVEL     601\n",
      "SKG      3280\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:26.237243Z",
     "start_time": "2026-02-18T14:27:26.231735Z"
    }
   },
   "cell_type": "code",
   "source": "len(paper_mapping)",
   "id": "9049aff92375f272",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3881"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:26.546324Z",
     "start_time": "2026-02-18T14:27:26.347885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for f in files:\n",
    "        if \"paper\" in f.lower() or \"mapping\" in f.lower():\n",
    "            print(os.path.join(root, f))"
   ],
   "id": "cfc74f547499c0a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\A survey of available corpora for building data-driven dialogue systems _ the morning paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\Building end-to-end dialogue systems using generative hierarchical neural network models _ the morning paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\Machine learning for dialog state tracking_ a review _ the morning paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\Multi-domain dialog state tracking using recurrent neural networks _ the morning paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\Notes for paper titled _How NOT To Evaluate Your Dialogue System_ An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation_ · GitHub-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\Notes for paper _End-to-end optimization of goal-driven and visually grounded dialogue systems_ · GitHub-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\Summary of _Evaluating Prerequisite Qualities for Learning End-to-end Dialog Systems_ paper · GitHub-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\Summary of _GuessWhat_! Visual object discovery through multi-modal dialogue_ paper · GitHub.pdf-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Dia\\Ten challenges in highly-interactive dialog systems _ the morning paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\MT\\Attention is all you need_ Discovering the Transformer paper _ by Eduardo Muñoz _ Towards Data Science-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\MT\\Massively Multilingual Neural Machine Translation in the Wild - Findings and Challenges · Papers I Read-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\MT\\papers_Character-based_Neural_Machine_Translation.md at master · aleju_papers-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\MT\\Refining Source Representations with Relation Networks for Neural Machine Translation · Papers I Read-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\MT\\Summary of paper _Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation_-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\MT\\Summary of _Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models_ paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\MT\\Summary of _Addressing the Rare Word Problem in Neural Machine Translation_ Paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\MT\\“Sequence to Sequence Learning with Neural Networks”_ Paper Discussion _ by Sanyam Bhutani _ Medium-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Day 127_ NLP Papers Summary - Neural Approaches to Conversational AI - Introduction - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Day 128_ NLP Papers Summary - Neural Approaches to Conversational AI - KB-QA (Symbolic Methods) - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Day 129_ NLP Papers Summary - Neural Approaches to Conversational AI - KB-QA (Neural Methods) - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Day 192_ NLP Papers Summary - Guiding Extractive Summarization with Question-Answering Rewards - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Exploring Models and Data for Image Question Answering · Papers I Read-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Learning to Count Objects in Natural Images for Visual Question Answering · Papers I Read-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering · Papers I Read-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Memory Networks _ the morning paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Notes for _Question Answering with Subgraph Embeddings_ paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Notes for _Towards AI-Complete Question Answering_ A Set of Prerequisite Toy Tasks_ Paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Reading Wikipedia to Answer Open-Domain Questions · Papers I Read-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Simple Baseline for Visual Question Answering · Papers I Read-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\Training Question Answering Models from Synthetic Data (Research Paper Summary) _ by Prakhar Mishra _ Towards Data Science-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\QA\\VQA-Visual Question Answering · Papers I Read-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\SA\\Day 103_ NLP Papers Summary - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\SA\\Day 104_ NLP Papers Summary - SentiHood_ Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\SA\\Day 105_ NLP Papers Summary - Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\SA\\Day 112_ NLP Papers Summary - A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\SA\\Day 123_ NLP Papers Summary - Context-aware Embedding for Targeted Aspect-based Sentiment Analysis - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\SA\\Day 124_ NLP Papers Summary - TLDR_ Extreme Summarization of Scientific Documents - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\SA\\Day 236_ NLP Papers Summary – A BERT based Sentiment Analysis and Key Entity Detection Approach for Online Financial Texts - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\SA\\Summary of _Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank_ paper-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 109_ NLP Papers Summary - Studying Summarization Evaluation Metrics in the Appropriate Scoring Range - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 113_ NLP Papers Summary - On Extractive and Abstractive Neural Document Summarization with Transformer Language Models - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 114_ NLP Papers Summary - A Summarization System for Scientific Documents - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 116_ NLP Papers Summary - Data-driven Summarization of Scientific Articles - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 118_ NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and Local Context - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 120_ NLP Papers Summary - A Simple Theoretical Model of Importance for Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 121_ NLP Papers Summary - Concept Pointer Network for Abstractive Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 143_ NLP Papers Summary - Unsupervised Pseudo-Labeling for Extractive Summarization on Electronic Health Records - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 144_ NLP Papers Summary - Attend to Medical Ontologies_ Content Selection for Clinical Abstractive Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 145_ NLP Papers Summary - SUPERT_ Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 146_ NLP Papers Summary - Exploring Content Selection in Summarization of Novel Chapters - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 148_ NLP Papers Summary - A Transformer-based Approach for Source Code Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 151_ NLP Papers Summary - A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 152_ NLP Papers Summary - OPINIONDIGEST_ A Simple Framework for Opinion Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 156_ NLP Papers Summary - Asking and Answering Questions to Evaluate the Factual Consistency of Summaries - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 160_ NLP Papers Summary - Extractive Summarization as Text Matching - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 167_ NLP Papers Summary - Ontology-Aware Clinical Abstractive Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 174_ NLP Papers Summary - PEGASUS_ Pre-training with Extracted Gap-sentences for Abstractive Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 185_ NLP Papers Summary - A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 186_ NLP Papers Summary - Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 188_ NLP Papers Summary - A Supervised Approach to Extractive Summarisation of Scientific Papers - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 192_ NLP Papers Summary - Guiding Extractive Summarization with Question-Answering Rewards - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Blogs\\Sum\\Day 206_ NLP Papers Summary - Transformers and Pointer-Generator Networks for Abstractive Summarization - Ryan Ong-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Novel_Papers\\MT2021\\567.Scientific Credibility of Machine Translation Research A Meta-Evaluation of 769 Papers-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\Novel_Papers\\SA2021\\366.A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 1 Research Papers)-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 2 Shared Task Papers, Day 1)-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 3 Shared Task Papers, Day 2)-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\1.Proceedings of the Third Conference on Machine Translation Research Papers-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\101.Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\15.Character Mapping and Ad-hoc Adaptation Edinburgh’s IWSLT 2020 Open Domain Translation System-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\331.Loss in Translation Learning Bilingual Word Mapping with a Retrieval Criterion-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\43.Mining Name Translations from Entity Graph Mapping-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\MT\\81.Mapping the Perfect via Translation Mining-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\Par\\118.Equation Parsing  Mapping Sentences to Grounded Equations-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\QA\\6.Talk to Papers Bringing Neural Question Answering to Academic Search-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\QA\\617.Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\grobid_files\\SKG_Papers\\SA\\112.TeamX A Sentiment Analyzer with Enhanced Lexicon Mapping and Weighting Scheme for Unbalanced Data-Grobid-out.txt\n",
      ".\\Scientific_Novelty_Detection\\JCDL_Code\\Non_Novel_Papers.py\n",
      ".\\Scientific_Novelty_Detection\\JCDL_Code\\Novel_Papers.py\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\A survey of available corpora for building data-driven dialogue systems _ the morning paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\Building end-to-end dialogue systems using generative hierarchical neural network models _ the morning paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\Machine learning for dialog state tracking_ a review _ the morning paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\Multi-domain dialog state tracking using recurrent neural networks _ the morning paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\Notes for paper titled _How NOT To Evaluate Your Dialogue System_ An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation_ · GitHub.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\Notes for paper _End-to-end optimization of goal-driven and visually grounded dialogue systems_ · GitHub.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\Summary of _Evaluating Prerequisite Qualities for Learning End-to-end Dialog Systems_ paper · GitHub.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\Summary of _GuessWhat_! Visual object discovery through multi-modal dialogue_ paper · GitHub.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Dia\\Ten challenges in highly-interactive dialog systems _ the morning paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\MT\\Attention is all you need_ Discovering the Transformer paper _ by Eduardo Muñoz _ Towards Data Science.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\MT\\Massively Multilingual Neural Machine Translation in the Wild - Findings and Challenges · Papers I Read.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\MT\\papers_Character-based_Neural_Machine_Translation.md at master · aleju_papers.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\MT\\Refining Source Representations with Relation Networks for Neural Machine Translation · Papers I Read.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\MT\\Summary of paper _Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation_.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\MT\\Summary of _Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models_ paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\MT\\Summary of _Addressing the Rare Word Problem in Neural Machine Translation_ Paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\MT\\“Sequence to Sequence Learning with Neural Networks”_ Paper Discussion _ by Sanyam Bhutani _ Medium.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Day 127_ NLP Papers Summary - Neural Approaches to Conversational AI - Introduction - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Day 128_ NLP Papers Summary - Neural Approaches to Conversational AI - KB-QA (Symbolic Methods) - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Day 129_ NLP Papers Summary - Neural Approaches to Conversational AI - KB-QA (Neural Methods) - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Day 192_ NLP Papers Summary - Guiding Extractive Summarization with Question-Answering Rewards - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Exploring Models and Data for Image Question Answering · Papers I Read.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Learning to Count Objects in Natural Images for Visual Question Answering · Papers I Read.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering · Papers I Read.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Memory Networks _ the morning paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Notes for _Question Answering with Subgraph Embeddings_ paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Notes for _Towards AI-Complete Question Answering_ A Set of Prerequisite Toy Tasks_ Paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Reading Wikipedia to Answer Open-Domain Questions · Papers I Read.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Simple Baseline for Visual Question Answering · Papers I Read.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\Training Question Answering Models from Synthetic Data (Research Paper Summary) _ by Prakhar Mishra _ Towards Data Science.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\QA\\VQA-Visual Question Answering · Papers I Read.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Day 103_ NLP Papers Summary - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Day 104_ NLP Papers Summary - SentiHood_ Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Day 105_ NLP Papers Summary - Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Day 112_ NLP Papers Summary - A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Day 123_ NLP Papers Summary - Context-aware Embedding for Targeted Aspect-based Sentiment Analysis - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Day 124_ NLP Papers Summary - TLDR_ Extreme Summarization of Scientific Documents - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Day 236_ NLP Papers Summary – A BERT based Sentiment Analysis and Key Entity Detection Approach for Online Financial Texts - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Notes for _Learning to Generate Reviews and Discovering Sentiment_ paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\SA\\Summary of _Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank_ paper.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 109_ NLP Papers Summary - Studying Summarization Evaluation Metrics in the Appropriate Scoring Range - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 113_ NLP Papers Summary - On Extractive and Abstractive Neural Document Summarization with Transformer Language Models - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 114_ NLP Papers Summary - A Summarization System for Scientific Documents - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 116_ NLP Papers Summary - Data-driven Summarization of Scientific Articles - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 118_ NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and Local Context - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 120_ NLP Papers Summary - A Simple Theoretical Model of Importance for Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 121_ NLP Papers Summary - Concept Pointer Network for Abstractive Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 143_ NLP Papers Summary - Unsupervised Pseudo-Labeling for Extractive Summarization on Electronic Health Records - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 144_ NLP Papers Summary - Attend to Medical Ontologies_ Content Selection for Clinical Abstractive Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 145_ NLP Papers Summary - SUPERT_ Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 146_ NLP Papers Summary - Exploring Content Selection in Summarization of Novel Chapters - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 148_ NLP Papers Summary - A Transformer-based Approach for Source Code Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 151_ NLP Papers Summary - A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 152_ NLP Papers Summary - OPINIONDIGEST_ A Simple Framework for Opinion Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 156_ NLP Papers Summary - Asking and Answering Questions to Evaluate the Factual Consistency of Summaries - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 160_ NLP Papers Summary - Extractive Summarization as Text Matching - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 167_ NLP Papers Summary - Ontology-Aware Clinical Abstractive Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 174_ NLP Papers Summary - PEGASUS_ Pre-training with Extracted Gap-sentences for Abstractive Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 185_ NLP Papers Summary - A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 186_ NLP Papers Summary - Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 188_ NLP Papers Summary - A Supervised Approach to Extractive Summarisation of Scientific Papers - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 192_ NLP Papers Summary - Guiding Extractive Summarization with Question-Answering Rewards - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Blogs\\Sum\\Day 206_ NLP Papers Summary - Transformers and Pointer-Generator Networks for Abstractive Summarization - Ryan Ong.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Novel_Papers_2021\\MT2021\\567.Scientific Credibility of Machine Translation Research A Meta-Evaluation of 769 Papers.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\Novel_Papers_2021\\SA2021\\366.A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 1 Research Papers).pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 2 Shared Task Papers, Day 1).pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 3 Shared Task Papers, Day 2).pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\1.Proceedings of the Third Conference on Machine Translation Research Papers.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\101.Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\15.Character Mapping and Ad-hoc Adaptation Edinburgh’s IWSLT 2020 Open Domain Translation System.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\331.Loss in Translation Learning Bilingual Word Mapping with a Retrieval Criterion.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\43.Mining Name Translations from Entity Graph Mapping.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\MT\\81.Mapping the Perfect via Translation Mining.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\Par\\118.Equation Parsing  Mapping Sentences to Grounded Equations.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\QA\\6.Talk to Papers Bringing Neural Question Answering to Academic Search.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\QA\\617.Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\SA\\112.TeamX A Sentiment Analyzer with Enhanced Lexicon Mapping and Weighting Scheme for Unbalanced Data.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\Sum\\205.TalkSumm A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks.pdf\n",
      ".\\Scientific_Novelty_Detection\\PDF_Papers\\SKG\\Sum\\52.Coherent Citation-Based Summarization of Scientific Papers.pdf\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\A survey of available corpora for building data-driven dialogue systems _ the morning paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\Building end-to-end dialogue systems using generative hierarchical neural network models _ the morning paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\Machine learning for dialog state tracking_ a review _ the morning paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\Multi-domain dialog state tracking using recurrent neural networks _ the morning paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\Notes for paper titled _How NOT To Evaluate Your Dialogue System_ An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation_ · GitHub-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\Notes for paper _End-to-end optimization of goal-driven and visually grounded dialogue systems_ · GitHub-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\Summary of _Evaluating Prerequisite Qualities for Learning End-to-end Dialog Systems_ paper · GitHub-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\Summary of _GuessWhat_! Visual object discovery through multi-modal dialogue_ paper · GitHub.pdf-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Dia\\Ten challenges in highly-interactive dialog systems _ the morning paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\MT\\Attention is all you need_ Discovering the Transformer paper _ by Eduardo Muñoz _ Towards Data Science-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\MT\\Massively Multilingual Neural Machine Translation in the Wild - Findings and Challenges · Papers I Read-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\MT\\papers_Character-based_Neural_Machine_Translation.md at master · aleju_papers-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\MT\\Refining Source Representations with Relation Networks for Neural Machine Translation · Papers I Read-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\MT\\Summary of paper _Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation_-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\MT\\Summary of _Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models_ paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\MT\\Summary of _Addressing the Rare Word Problem in Neural Machine Translation_ Paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\MT\\“Sequence to Sequence Learning with Neural Networks”_ Paper Discussion _ by Sanyam Bhutani _ Medium-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Day 127_ NLP Papers Summary - Neural Approaches to Conversational AI - Introduction - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Day 128_ NLP Papers Summary - Neural Approaches to Conversational AI - KB-QA (Symbolic Methods) - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Day 129_ NLP Papers Summary - Neural Approaches to Conversational AI - KB-QA (Neural Methods) - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Day 192_ NLP Papers Summary - Guiding Extractive Summarization with Question-Answering Rewards - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Exploring Models and Data for Image Question Answering · Papers I Read-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Learning to Count Objects in Natural Images for Visual Question Answering · Papers I Read-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering · Papers I Read-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Memory Networks _ the morning paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Notes for _Question Answering with Subgraph Embeddings_ paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Notes for _Towards AI-Complete Question Answering_ A Set of Prerequisite Toy Tasks_ Paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Reading Wikipedia to Answer Open-Domain Questions · Papers I Read-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Simple Baseline for Visual Question Answering · Papers I Read-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\Training Question Answering Models from Synthetic Data (Research Paper Summary) _ by Prakhar Mishra _ Towards Data Science-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\QA\\VQA-Visual Question Answering · Papers I Read-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\SA\\Day 103_ NLP Papers Summary - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\SA\\Day 104_ NLP Papers Summary - SentiHood_ Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\SA\\Day 105_ NLP Papers Summary - Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\SA\\Day 112_ NLP Papers Summary - A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\SA\\Day 123_ NLP Papers Summary - Context-aware Embedding for Targeted Aspect-based Sentiment Analysis - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\SA\\Day 124_ NLP Papers Summary - TLDR_ Extreme Summarization of Scientific Documents - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\SA\\Day 236_ NLP Papers Summary – A BERT based Sentiment Analysis and Key Entity Detection Approach for Online Financial Texts - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\SA\\Summary of _Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank_ paper-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 109_ NLP Papers Summary - Studying Summarization Evaluation Metrics in the Appropriate Scoring Range - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 113_ NLP Papers Summary - On Extractive and Abstractive Neural Document Summarization with Transformer Language Models - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 114_ NLP Papers Summary - A Summarization System for Scientific Documents - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 116_ NLP Papers Summary - Data-driven Summarization of Scientific Articles - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 118_ NLP Papers Summary - Extractive Summarization of Long Documents by Combining Global and Local Context - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 120_ NLP Papers Summary - A Simple Theoretical Model of Importance for Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 121_ NLP Papers Summary - Concept Pointer Network for Abstractive Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 143_ NLP Papers Summary - Unsupervised Pseudo-Labeling for Extractive Summarization on Electronic Health Records - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 144_ NLP Papers Summary - Attend to Medical Ontologies_ Content Selection for Clinical Abstractive Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 145_ NLP Papers Summary - SUPERT_ Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 146_ NLP Papers Summary - Exploring Content Selection in Summarization of Novel Chapters - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 148_ NLP Papers Summary - A Transformer-based Approach for Source Code Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 151_ NLP Papers Summary - A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 152_ NLP Papers Summary - OPINIONDIGEST_ A Simple Framework for Opinion Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 156_ NLP Papers Summary - Asking and Answering Questions to Evaluate the Factual Consistency of Summaries - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 160_ NLP Papers Summary - Extractive Summarization as Text Matching - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 167_ NLP Papers Summary - Ontology-Aware Clinical Abstractive Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 174_ NLP Papers Summary - PEGASUS_ Pre-training with Extracted Gap-sentences for Abstractive Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 185_ NLP Papers Summary - A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 186_ NLP Papers Summary - Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 188_ NLP Papers Summary - A Supervised Approach to Extractive Summarisation of Scientific Papers - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 192_ NLP Papers Summary - Guiding Extractive Summarization with Question-Answering Rewards - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Blogs\\Sum\\Day 206_ NLP Papers Summary - Transformers and Pointer-Generator Networks for Abstractive Summarization - Ryan Ong-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Novel_Papers\\MT2021\\567.Scientific Credibility of Machine Translation Research A Meta-Evaluation of 769 Papers-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\Novel_Papers\\SA2021\\366.A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 1 Research Papers)-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 2 Shared Task Papers, Day 1)-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\1.Proceedings of the Fourth Conference on Machine Translation (Volume 3 Shared Task Papers, Day 2)-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\1.Proceedings of the Third Conference on Machine Translation Research Papers-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\101.Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\15.Character Mapping and Ad-hoc Adaptation Edinburgh’s IWSLT 2020 Open Domain Translation System-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\331.Loss in Translation Learning Bilingual Word Mapping with a Retrieval Criterion-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\43.Mining Name Translations from Entity Graph Mapping-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\MT\\81.Mapping the Perfect via Translation Mining-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\Par\\118.Equation Parsing  Mapping Sentences to Grounded Equations-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\QA\\6.Talk to Papers Bringing Neural Question Answering to Academic Search-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\QA\\617.Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering-Stanza-out.txt\n",
      ".\\Scientific_Novelty_Detection\\stanza_files\\SKG_Papers\\SA\\112.TeamX A Sentiment Analyzer with Enhanced Lexicon Mapping and Weighting Scheme for Unbalanced Data-Stanza-out.txt\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:26.657443Z",
     "start_time": "2026-02-18T14:27:26.652637Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.listdir(\"Scientific_Novelty_Detection/JCDL_Code\"))",
   "id": "549a59011a158b3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Algorithm.ipynb', 'Algorithm_weights.ipynb', 'general_preprocessing.py', 'grobid_and_stanza.py', 'JCDL_Classification_Algorithm.ipynb', 'JCDL_Dataset_Statistics.ipynb', 'JCDL_Graph.ipynb', 'JCDL_Novel_Analysis.ipynb', 'JCDL_Results', 'Knowledge_Graph.ipynb', 'merge.py', 'Non_Novel_Papers.py', 'Novel_Papers.py', 'Preprocessing.py', 'Readme.md']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:26.717887Z",
     "start_time": "2026-02-18T14:27:26.712973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/stanza_files\"))"
   ],
   "id": "15bfb4c60d24744e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blogs', 'Novel_Papers', 'SKG_Papers']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:26.804360Z",
     "start_time": "2026-02-18T14:27:26.799635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for root, dirs, files in os.walk(\"stanza_files\"):\n",
    "    print(root)\n",
    "    print(files[:5])\n",
    "    break"
   ],
   "id": "3309f58a6052b5a6",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:27.051050Z",
     "start_time": "2026-02-18T14:27:27.044991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(\"Inside SKG_Papers:\")\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/stanza_files/SKG_Papers\")[:10])"
   ],
   "id": "4ed5bffbf887b866",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside SKG_Papers:\n",
      "['Dia', 'MT', 'NLI', 'Par', 'QA', 'SA', 'Sum']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:27.287077Z",
     "start_time": "2026-02-18T14:27:27.280880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Inside one domain:\")\n",
    "example_domain = os.listdir(\"Scientific_Novelty_Detection/stanza_files/SKG_Papers\")[0]\n",
    "print(\"Domain name:\", example_domain)\n",
    "print(os.listdir(f\"Scientific_Novelty_Detection/stanza_files/SKG_Papers/{example_domain}\")[:10])"
   ],
   "id": "705ba7d0e6c0828a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside one domain:\n",
      "Domain name: Dia\n",
      "['10.IIT-UHH at SemEval-2017 Task 3 Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification-Stanza-out.txt', '10.PLATO Pre-trained Dialogue Generation Model with Discrete Latent Variable-Stanza-out.txt', '10.Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks-Stanza-out.txt', '10.The Impact of Interpretation Problems on Tutorial Dialogue-Stanza-out.txt', '100.Learning the Information Status of Noun Phrases in Spoken Dialogues-Stanza-out.txt', '103.Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model-Stanza-out.txt', '104.Towards an Automatic Turing Test Learning to Evaluate Dialogue Responses-Stanza-out.txt', '107.Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System-Stanza-out.txt', '11.A Statistical Spoken Dialogue System using Complex User Goals and Value Directed Compression-Stanza-out.txt', '11.Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-Adaptive Spoken Dialogue System-Stanza-out.txt']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:27.622559Z",
     "start_time": "2026-02-18T14:27:27.444400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mapping_records = []\n",
    "\n",
    "for (split, domain), group in scind.groupby([\"split\", \"domain_folder\"]):\n",
    "\n",
    "    # Fix split name\n",
    "    if split == \"SKG\":\n",
    "        stanza_split = \"SKG_Papers\"\n",
    "    elif split == \"NOVEL\":\n",
    "        stanza_split = \"Novel_Papers\"\n",
    "    elif split == \"BLOG\":\n",
    "        stanza_split = \"Blogs\"\n",
    "        domain = domain.replace(\"_Blogs\", \"\")\n",
    "\n",
    "    pattern = f\"Scientific_Novelty_Detection/stanza_files/{stanza_split}/{domain}/*-Stanza-out.txt\"\n",
    "    directory = glob.glob(pattern)\n",
    "\n",
    "    paper_ids = sorted(group[\"paper_ID\"].unique())\n",
    "\n",
    "    for pid in paper_ids:\n",
    "        if pid < len(directory):\n",
    "            filepath = directory[pid]\n",
    "            filename = os.path.basename(filepath)\n",
    "\n",
    "            title = filename.replace(\"-Stanza-out.txt\", \"\")\n",
    "            title = re.sub(r\"^\\d+\\.\", \"\", title)\n",
    "\n",
    "            mapping_records.append({\n",
    "                \"split\": split,\n",
    "                \"domain_folder\": domain,\n",
    "                \"paper_ID\": pid,\n",
    "                \"title\": title\n",
    "            })\n",
    "        else:\n",
    "            print(\"Index out of range:\", split, domain, pid)\n",
    "\n",
    "paper_mapping = pd.DataFrame(mapping_records)\n",
    "\n",
    "print(\"Final mapped papers:\", len(paper_mapping))\n",
    "print(paper_mapping.groupby(\"split\").size())"
   ],
   "id": "4e5120ce57571f64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index out of range: SKG SA 636\n",
      "Final mapped papers: 3964\n",
      "split\n",
      "BLOG       84\n",
      "NOVEL     601\n",
      "SKG      3279\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:29.430552Z",
     "start_time": "2026-02-18T14:27:27.655580Z"
    }
   },
   "cell_type": "code",
   "source": "print(paper_mapping.groupby(\"split_folder\").size())",
   "id": "a7495ffb7343413d",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'split_folder'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(paper_mapping\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit_folder\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msize())\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:8402\u001B[0m, in \u001B[0;36mDataFrame.groupby\u001B[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001B[0m\n\u001B[0;32m   8399\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to supply one of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mby\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlevel\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   8400\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis_number(axis)\n\u001B[1;32m-> 8402\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrameGroupBy(\n\u001B[0;32m   8403\u001B[0m     obj\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   8404\u001B[0m     keys\u001B[38;5;241m=\u001B[39mby,\n\u001B[0;32m   8405\u001B[0m     axis\u001B[38;5;241m=\u001B[39maxis,\n\u001B[0;32m   8406\u001B[0m     level\u001B[38;5;241m=\u001B[39mlevel,\n\u001B[0;32m   8407\u001B[0m     as_index\u001B[38;5;241m=\u001B[39mas_index,\n\u001B[0;32m   8408\u001B[0m     sort\u001B[38;5;241m=\u001B[39msort,\n\u001B[0;32m   8409\u001B[0m     group_keys\u001B[38;5;241m=\u001B[39mgroup_keys,\n\u001B[0;32m   8410\u001B[0m     squeeze\u001B[38;5;241m=\u001B[39msqueeze,\n\u001B[0;32m   8411\u001B[0m     observed\u001B[38;5;241m=\u001B[39mobserved,\n\u001B[0;32m   8412\u001B[0m     dropna\u001B[38;5;241m=\u001B[39mdropna,\n\u001B[0;32m   8413\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001B[0m, in \u001B[0;36mGroupBy.__init__\u001B[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001B[0m\n\u001B[0;32m    962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m grouper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    963\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgroupby\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgrouper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_grouper\n\u001B[1;32m--> 965\u001B[0m     grouper, exclusions, obj \u001B[38;5;241m=\u001B[39m get_grouper(\n\u001B[0;32m    966\u001B[0m         obj,\n\u001B[0;32m    967\u001B[0m         keys,\n\u001B[0;32m    968\u001B[0m         axis\u001B[38;5;241m=\u001B[39maxis,\n\u001B[0;32m    969\u001B[0m         level\u001B[38;5;241m=\u001B[39mlevel,\n\u001B[0;32m    970\u001B[0m         sort\u001B[38;5;241m=\u001B[39msort,\n\u001B[0;32m    971\u001B[0m         observed\u001B[38;5;241m=\u001B[39mobserved,\n\u001B[0;32m    972\u001B[0m         mutated\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmutated,\n\u001B[0;32m    973\u001B[0m         dropna\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropna,\n\u001B[0;32m    974\u001B[0m     )\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj \u001B[38;5;241m=\u001B[39m obj\n\u001B[0;32m    977\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_get_axis_number(axis)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001B[0m, in \u001B[0;36mget_grouper\u001B[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001B[0m\n\u001B[0;32m    886\u001B[0m         in_axis, level, gpr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, gpr, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    887\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 888\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(gpr)\n\u001B[0;32m    889\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(gpr, Grouper) \u001B[38;5;129;01mand\u001B[39;00m gpr\u001B[38;5;241m.\u001B[39mkey \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    890\u001B[0m     \u001B[38;5;66;03m# Add key to exclusions\u001B[39;00m\n\u001B[0;32m    891\u001B[0m     exclusions\u001B[38;5;241m.\u001B[39madd(gpr\u001B[38;5;241m.\u001B[39mkey)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'split_folder'"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:33.958429Z",
     "start_time": "2026-02-18T14:27:33.232835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "triplet_files = glob.glob(\"Scientific_Novelty_Detection/Triplets/**/*.csv\", recursive=True)\n",
    "\n",
    "file_records = []\n",
    "\n",
    "for file in triplet_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    if \"SKG\" in file:\n",
    "        split = \"SKG\"\n",
    "        stanza_split = \"SKG_Papers\"\n",
    "    elif \"Novel_Papers\" in file:\n",
    "        split = \"NOVEL\"\n",
    "        stanza_split = \"Novel_Papers\"\n",
    "    elif \"Blogs\" in file:\n",
    "        split = \"BLOG\"\n",
    "        stanza_split = \"Blogs\"\n",
    "\n",
    "    filename = os.path.basename(file)\n",
    "    domain_raw = filename.replace(\"_triplets.csv\", \"\")\n",
    "\n",
    "    # Fix domain names\n",
    "    if split == \"BLOG\":\n",
    "        domain = domain_raw.replace(\"_Blogs\", \"\")\n",
    "    elif split == \"NOVEL\":\n",
    "        domain = domain_raw.replace(\"2021\", \"\")\n",
    "    else:\n",
    "        domain = domain_raw\n",
    "\n",
    "    file_records.append({\n",
    "        \"file\": file,\n",
    "        \"split\": split,\n",
    "        \"stanza_split\": stanza_split,\n",
    "        \"domain\": domain,\n",
    "        \"df\": df\n",
    "    })"
   ],
   "id": "18e36857c67d4fdd",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:34.495161Z",
     "start_time": "2026-02-18T14:27:34.397228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "mapping_records = []\n",
    "\n",
    "for record in file_records:\n",
    "\n",
    "    split = record[\"split\"]\n",
    "    stanza_split = record[\"stanza_split\"]\n",
    "    domain = record[\"domain\"]\n",
    "    df = record[\"df\"]\n",
    "\n",
    "    paper_ids = sorted(df[\"paper_ID\"].unique())\n",
    "\n",
    "    pattern = f\"Scientific_Novelty_Detection/stanza_files/{stanza_split}/{domain}/*-Stanza-out.txt\"\n",
    "    directory = glob.glob(pattern)\n",
    "\n",
    "    print(split, domain, \"Triplet papers:\", len(paper_ids),\n",
    "          \"Stanza files:\", len(directory))\n",
    "\n",
    "    for pid in paper_ids:\n",
    "        if pid < len(directory):\n",
    "            filepath = directory[pid]\n",
    "            filename = os.path.basename(filepath)\n",
    "\n",
    "            title = filename.replace(\"-Stanza-out.txt\", \"\")\n",
    "            title = re.sub(r\"^\\d+\\.\", \"\", title)\n",
    "\n",
    "            mapping_records.append({\n",
    "                \"split\": split,\n",
    "                \"domain\": domain,\n",
    "                \"paper_ID\": pid,\n",
    "                \"title\": title\n",
    "            })\n",
    "        else:\n",
    "            print(\"Index mismatch:\", split, domain, pid)\n",
    "\n",
    "paper_mapping = pd.DataFrame(mapping_records)\n",
    "\n",
    "print(\"Final mapped papers:\", len(paper_mapping))\n",
    "print(paper_mapping.groupby(\"split\").size())"
   ],
   "id": "8a968c9df23a6cf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOG Dia Triplet papers: 11 Stanza files: 11\n",
      "BLOG MT Triplet papers: 12 Stanza files: 12\n",
      "BLOG QA Triplet papers: 20 Stanza files: 20\n",
      "BLOG SA Triplet papers: 16 Stanza files: 16\n",
      "BLOG Sum Triplet papers: 25 Stanza files: 25\n",
      "NOVEL Dia Triplet papers: 71 Stanza files: 0\n",
      "Index mismatch: NOVEL Dia 0\n",
      "Index mismatch: NOVEL Dia 1\n",
      "Index mismatch: NOVEL Dia 2\n",
      "Index mismatch: NOVEL Dia 3\n",
      "Index mismatch: NOVEL Dia 4\n",
      "Index mismatch: NOVEL Dia 5\n",
      "Index mismatch: NOVEL Dia 6\n",
      "Index mismatch: NOVEL Dia 7\n",
      "Index mismatch: NOVEL Dia 8\n",
      "Index mismatch: NOVEL Dia 9\n",
      "Index mismatch: NOVEL Dia 10\n",
      "Index mismatch: NOVEL Dia 11\n",
      "Index mismatch: NOVEL Dia 12\n",
      "Index mismatch: NOVEL Dia 13\n",
      "Index mismatch: NOVEL Dia 14\n",
      "Index mismatch: NOVEL Dia 15\n",
      "Index mismatch: NOVEL Dia 16\n",
      "Index mismatch: NOVEL Dia 17\n",
      "Index mismatch: NOVEL Dia 18\n",
      "Index mismatch: NOVEL Dia 19\n",
      "Index mismatch: NOVEL Dia 20\n",
      "Index mismatch: NOVEL Dia 21\n",
      "Index mismatch: NOVEL Dia 22\n",
      "Index mismatch: NOVEL Dia 23\n",
      "Index mismatch: NOVEL Dia 24\n",
      "Index mismatch: NOVEL Dia 25\n",
      "Index mismatch: NOVEL Dia 26\n",
      "Index mismatch: NOVEL Dia 27\n",
      "Index mismatch: NOVEL Dia 28\n",
      "Index mismatch: NOVEL Dia 29\n",
      "Index mismatch: NOVEL Dia 30\n",
      "Index mismatch: NOVEL Dia 31\n",
      "Index mismatch: NOVEL Dia 32\n",
      "Index mismatch: NOVEL Dia 33\n",
      "Index mismatch: NOVEL Dia 34\n",
      "Index mismatch: NOVEL Dia 35\n",
      "Index mismatch: NOVEL Dia 37\n",
      "Index mismatch: NOVEL Dia 38\n",
      "Index mismatch: NOVEL Dia 39\n",
      "Index mismatch: NOVEL Dia 40\n",
      "Index mismatch: NOVEL Dia 41\n",
      "Index mismatch: NOVEL Dia 42\n",
      "Index mismatch: NOVEL Dia 43\n",
      "Index mismatch: NOVEL Dia 44\n",
      "Index mismatch: NOVEL Dia 45\n",
      "Index mismatch: NOVEL Dia 46\n",
      "Index mismatch: NOVEL Dia 47\n",
      "Index mismatch: NOVEL Dia 48\n",
      "Index mismatch: NOVEL Dia 49\n",
      "Index mismatch: NOVEL Dia 50\n",
      "Index mismatch: NOVEL Dia 51\n",
      "Index mismatch: NOVEL Dia 52\n",
      "Index mismatch: NOVEL Dia 53\n",
      "Index mismatch: NOVEL Dia 54\n",
      "Index mismatch: NOVEL Dia 55\n",
      "Index mismatch: NOVEL Dia 56\n",
      "Index mismatch: NOVEL Dia 57\n",
      "Index mismatch: NOVEL Dia 58\n",
      "Index mismatch: NOVEL Dia 59\n",
      "Index mismatch: NOVEL Dia 60\n",
      "Index mismatch: NOVEL Dia 61\n",
      "Index mismatch: NOVEL Dia 62\n",
      "Index mismatch: NOVEL Dia 63\n",
      "Index mismatch: NOVEL Dia 64\n",
      "Index mismatch: NOVEL Dia 65\n",
      "Index mismatch: NOVEL Dia 66\n",
      "Index mismatch: NOVEL Dia 67\n",
      "Index mismatch: NOVEL Dia 68\n",
      "Index mismatch: NOVEL Dia 69\n",
      "Index mismatch: NOVEL Dia 70\n",
      "Index mismatch: NOVEL Dia 71\n",
      "NOVEL MT Triplet papers: 299 Stanza files: 0\n",
      "Index mismatch: NOVEL MT 0\n",
      "Index mismatch: NOVEL MT 2\n",
      "Index mismatch: NOVEL MT 3\n",
      "Index mismatch: NOVEL MT 4\n",
      "Index mismatch: NOVEL MT 5\n",
      "Index mismatch: NOVEL MT 6\n",
      "Index mismatch: NOVEL MT 7\n",
      "Index mismatch: NOVEL MT 8\n",
      "Index mismatch: NOVEL MT 9\n",
      "Index mismatch: NOVEL MT 10\n",
      "Index mismatch: NOVEL MT 11\n",
      "Index mismatch: NOVEL MT 12\n",
      "Index mismatch: NOVEL MT 14\n",
      "Index mismatch: NOVEL MT 15\n",
      "Index mismatch: NOVEL MT 16\n",
      "Index mismatch: NOVEL MT 17\n",
      "Index mismatch: NOVEL MT 18\n",
      "Index mismatch: NOVEL MT 19\n",
      "Index mismatch: NOVEL MT 20\n",
      "Index mismatch: NOVEL MT 21\n",
      "Index mismatch: NOVEL MT 22\n",
      "Index mismatch: NOVEL MT 23\n",
      "Index mismatch: NOVEL MT 24\n",
      "Index mismatch: NOVEL MT 25\n",
      "Index mismatch: NOVEL MT 26\n",
      "Index mismatch: NOVEL MT 27\n",
      "Index mismatch: NOVEL MT 28\n",
      "Index mismatch: NOVEL MT 29\n",
      "Index mismatch: NOVEL MT 30\n",
      "Index mismatch: NOVEL MT 31\n",
      "Index mismatch: NOVEL MT 33\n",
      "Index mismatch: NOVEL MT 34\n",
      "Index mismatch: NOVEL MT 35\n",
      "Index mismatch: NOVEL MT 36\n",
      "Index mismatch: NOVEL MT 37\n",
      "Index mismatch: NOVEL MT 38\n",
      "Index mismatch: NOVEL MT 39\n",
      "Index mismatch: NOVEL MT 40\n",
      "Index mismatch: NOVEL MT 41\n",
      "Index mismatch: NOVEL MT 42\n",
      "Index mismatch: NOVEL MT 43\n",
      "Index mismatch: NOVEL MT 44\n",
      "Index mismatch: NOVEL MT 45\n",
      "Index mismatch: NOVEL MT 46\n",
      "Index mismatch: NOVEL MT 47\n",
      "Index mismatch: NOVEL MT 48\n",
      "Index mismatch: NOVEL MT 49\n",
      "Index mismatch: NOVEL MT 50\n",
      "Index mismatch: NOVEL MT 51\n",
      "Index mismatch: NOVEL MT 52\n",
      "Index mismatch: NOVEL MT 53\n",
      "Index mismatch: NOVEL MT 54\n",
      "Index mismatch: NOVEL MT 55\n",
      "Index mismatch: NOVEL MT 56\n",
      "Index mismatch: NOVEL MT 57\n",
      "Index mismatch: NOVEL MT 58\n",
      "Index mismatch: NOVEL MT 59\n",
      "Index mismatch: NOVEL MT 60\n",
      "Index mismatch: NOVEL MT 61\n",
      "Index mismatch: NOVEL MT 62\n",
      "Index mismatch: NOVEL MT 63\n",
      "Index mismatch: NOVEL MT 64\n",
      "Index mismatch: NOVEL MT 65\n",
      "Index mismatch: NOVEL MT 66\n",
      "Index mismatch: NOVEL MT 67\n",
      "Index mismatch: NOVEL MT 68\n",
      "Index mismatch: NOVEL MT 69\n",
      "Index mismatch: NOVEL MT 70\n",
      "Index mismatch: NOVEL MT 71\n",
      "Index mismatch: NOVEL MT 72\n",
      "Index mismatch: NOVEL MT 73\n",
      "Index mismatch: NOVEL MT 74\n",
      "Index mismatch: NOVEL MT 75\n",
      "Index mismatch: NOVEL MT 76\n",
      "Index mismatch: NOVEL MT 77\n",
      "Index mismatch: NOVEL MT 78\n",
      "Index mismatch: NOVEL MT 79\n",
      "Index mismatch: NOVEL MT 80\n",
      "Index mismatch: NOVEL MT 81\n",
      "Index mismatch: NOVEL MT 82\n",
      "Index mismatch: NOVEL MT 83\n",
      "Index mismatch: NOVEL MT 84\n",
      "Index mismatch: NOVEL MT 85\n",
      "Index mismatch: NOVEL MT 86\n",
      "Index mismatch: NOVEL MT 87\n",
      "Index mismatch: NOVEL MT 88\n",
      "Index mismatch: NOVEL MT 89\n",
      "Index mismatch: NOVEL MT 90\n",
      "Index mismatch: NOVEL MT 91\n",
      "Index mismatch: NOVEL MT 92\n",
      "Index mismatch: NOVEL MT 93\n",
      "Index mismatch: NOVEL MT 94\n",
      "Index mismatch: NOVEL MT 95\n",
      "Index mismatch: NOVEL MT 96\n",
      "Index mismatch: NOVEL MT 97\n",
      "Index mismatch: NOVEL MT 98\n",
      "Index mismatch: NOVEL MT 99\n",
      "Index mismatch: NOVEL MT 100\n",
      "Index mismatch: NOVEL MT 101\n",
      "Index mismatch: NOVEL MT 102\n",
      "Index mismatch: NOVEL MT 103\n",
      "Index mismatch: NOVEL MT 104\n",
      "Index mismatch: NOVEL MT 105\n",
      "Index mismatch: NOVEL MT 106\n",
      "Index mismatch: NOVEL MT 107\n",
      "Index mismatch: NOVEL MT 108\n",
      "Index mismatch: NOVEL MT 109\n",
      "Index mismatch: NOVEL MT 110\n",
      "Index mismatch: NOVEL MT 111\n",
      "Index mismatch: NOVEL MT 112\n",
      "Index mismatch: NOVEL MT 113\n",
      "Index mismatch: NOVEL MT 114\n",
      "Index mismatch: NOVEL MT 115\n",
      "Index mismatch: NOVEL MT 116\n",
      "Index mismatch: NOVEL MT 117\n",
      "Index mismatch: NOVEL MT 118\n",
      "Index mismatch: NOVEL MT 119\n",
      "Index mismatch: NOVEL MT 120\n",
      "Index mismatch: NOVEL MT 121\n",
      "Index mismatch: NOVEL MT 122\n",
      "Index mismatch: NOVEL MT 123\n",
      "Index mismatch: NOVEL MT 124\n",
      "Index mismatch: NOVEL MT 125\n",
      "Index mismatch: NOVEL MT 126\n",
      "Index mismatch: NOVEL MT 127\n",
      "Index mismatch: NOVEL MT 128\n",
      "Index mismatch: NOVEL MT 129\n",
      "Index mismatch: NOVEL MT 130\n",
      "Index mismatch: NOVEL MT 131\n",
      "Index mismatch: NOVEL MT 132\n",
      "Index mismatch: NOVEL MT 133\n",
      "Index mismatch: NOVEL MT 134\n",
      "Index mismatch: NOVEL MT 135\n",
      "Index mismatch: NOVEL MT 136\n",
      "Index mismatch: NOVEL MT 137\n",
      "Index mismatch: NOVEL MT 138\n",
      "Index mismatch: NOVEL MT 139\n",
      "Index mismatch: NOVEL MT 140\n",
      "Index mismatch: NOVEL MT 141\n",
      "Index mismatch: NOVEL MT 142\n",
      "Index mismatch: NOVEL MT 143\n",
      "Index mismatch: NOVEL MT 144\n",
      "Index mismatch: NOVEL MT 146\n",
      "Index mismatch: NOVEL MT 147\n",
      "Index mismatch: NOVEL MT 148\n",
      "Index mismatch: NOVEL MT 150\n",
      "Index mismatch: NOVEL MT 151\n",
      "Index mismatch: NOVEL MT 152\n",
      "Index mismatch: NOVEL MT 153\n",
      "Index mismatch: NOVEL MT 154\n",
      "Index mismatch: NOVEL MT 155\n",
      "Index mismatch: NOVEL MT 156\n",
      "Index mismatch: NOVEL MT 157\n",
      "Index mismatch: NOVEL MT 158\n",
      "Index mismatch: NOVEL MT 159\n",
      "Index mismatch: NOVEL MT 160\n",
      "Index mismatch: NOVEL MT 161\n",
      "Index mismatch: NOVEL MT 162\n",
      "Index mismatch: NOVEL MT 163\n",
      "Index mismatch: NOVEL MT 164\n",
      "Index mismatch: NOVEL MT 165\n",
      "Index mismatch: NOVEL MT 166\n",
      "Index mismatch: NOVEL MT 167\n",
      "Index mismatch: NOVEL MT 168\n",
      "Index mismatch: NOVEL MT 169\n",
      "Index mismatch: NOVEL MT 170\n",
      "Index mismatch: NOVEL MT 171\n",
      "Index mismatch: NOVEL MT 172\n",
      "Index mismatch: NOVEL MT 173\n",
      "Index mismatch: NOVEL MT 174\n",
      "Index mismatch: NOVEL MT 175\n",
      "Index mismatch: NOVEL MT 176\n",
      "Index mismatch: NOVEL MT 177\n",
      "Index mismatch: NOVEL MT 178\n",
      "Index mismatch: NOVEL MT 179\n",
      "Index mismatch: NOVEL MT 180\n",
      "Index mismatch: NOVEL MT 181\n",
      "Index mismatch: NOVEL MT 182\n",
      "Index mismatch: NOVEL MT 183\n",
      "Index mismatch: NOVEL MT 184\n",
      "Index mismatch: NOVEL MT 185\n",
      "Index mismatch: NOVEL MT 186\n",
      "Index mismatch: NOVEL MT 187\n",
      "Index mismatch: NOVEL MT 188\n",
      "Index mismatch: NOVEL MT 189\n",
      "Index mismatch: NOVEL MT 190\n",
      "Index mismatch: NOVEL MT 191\n",
      "Index mismatch: NOVEL MT 192\n",
      "Index mismatch: NOVEL MT 193\n",
      "Index mismatch: NOVEL MT 194\n",
      "Index mismatch: NOVEL MT 195\n",
      "Index mismatch: NOVEL MT 196\n",
      "Index mismatch: NOVEL MT 197\n",
      "Index mismatch: NOVEL MT 198\n",
      "Index mismatch: NOVEL MT 199\n",
      "Index mismatch: NOVEL MT 200\n",
      "Index mismatch: NOVEL MT 201\n",
      "Index mismatch: NOVEL MT 202\n",
      "Index mismatch: NOVEL MT 203\n",
      "Index mismatch: NOVEL MT 204\n",
      "Index mismatch: NOVEL MT 205\n",
      "Index mismatch: NOVEL MT 206\n",
      "Index mismatch: NOVEL MT 207\n",
      "Index mismatch: NOVEL MT 208\n",
      "Index mismatch: NOVEL MT 209\n",
      "Index mismatch: NOVEL MT 210\n",
      "Index mismatch: NOVEL MT 211\n",
      "Index mismatch: NOVEL MT 212\n",
      "Index mismatch: NOVEL MT 213\n",
      "Index mismatch: NOVEL MT 214\n",
      "Index mismatch: NOVEL MT 215\n",
      "Index mismatch: NOVEL MT 216\n",
      "Index mismatch: NOVEL MT 217\n",
      "Index mismatch: NOVEL MT 218\n",
      "Index mismatch: NOVEL MT 219\n",
      "Index mismatch: NOVEL MT 220\n",
      "Index mismatch: NOVEL MT 221\n",
      "Index mismatch: NOVEL MT 222\n",
      "Index mismatch: NOVEL MT 223\n",
      "Index mismatch: NOVEL MT 224\n",
      "Index mismatch: NOVEL MT 225\n",
      "Index mismatch: NOVEL MT 226\n",
      "Index mismatch: NOVEL MT 227\n",
      "Index mismatch: NOVEL MT 228\n",
      "Index mismatch: NOVEL MT 229\n",
      "Index mismatch: NOVEL MT 230\n",
      "Index mismatch: NOVEL MT 231\n",
      "Index mismatch: NOVEL MT 232\n",
      "Index mismatch: NOVEL MT 233\n",
      "Index mismatch: NOVEL MT 234\n",
      "Index mismatch: NOVEL MT 235\n",
      "Index mismatch: NOVEL MT 236\n",
      "Index mismatch: NOVEL MT 237\n",
      "Index mismatch: NOVEL MT 238\n",
      "Index mismatch: NOVEL MT 239\n",
      "Index mismatch: NOVEL MT 241\n",
      "Index mismatch: NOVEL MT 242\n",
      "Index mismatch: NOVEL MT 243\n",
      "Index mismatch: NOVEL MT 244\n",
      "Index mismatch: NOVEL MT 245\n",
      "Index mismatch: NOVEL MT 246\n",
      "Index mismatch: NOVEL MT 247\n",
      "Index mismatch: NOVEL MT 248\n",
      "Index mismatch: NOVEL MT 249\n",
      "Index mismatch: NOVEL MT 250\n",
      "Index mismatch: NOVEL MT 251\n",
      "Index mismatch: NOVEL MT 252\n",
      "Index mismatch: NOVEL MT 253\n",
      "Index mismatch: NOVEL MT 254\n",
      "Index mismatch: NOVEL MT 255\n",
      "Index mismatch: NOVEL MT 256\n",
      "Index mismatch: NOVEL MT 257\n",
      "Index mismatch: NOVEL MT 258\n",
      "Index mismatch: NOVEL MT 259\n",
      "Index mismatch: NOVEL MT 260\n",
      "Index mismatch: NOVEL MT 261\n",
      "Index mismatch: NOVEL MT 262\n",
      "Index mismatch: NOVEL MT 263\n",
      "Index mismatch: NOVEL MT 264\n",
      "Index mismatch: NOVEL MT 265\n",
      "Index mismatch: NOVEL MT 266\n",
      "Index mismatch: NOVEL MT 267\n",
      "Index mismatch: NOVEL MT 268\n",
      "Index mismatch: NOVEL MT 269\n",
      "Index mismatch: NOVEL MT 270\n",
      "Index mismatch: NOVEL MT 271\n",
      "Index mismatch: NOVEL MT 272\n",
      "Index mismatch: NOVEL MT 273\n",
      "Index mismatch: NOVEL MT 274\n",
      "Index mismatch: NOVEL MT 275\n",
      "Index mismatch: NOVEL MT 276\n",
      "Index mismatch: NOVEL MT 277\n",
      "Index mismatch: NOVEL MT 278\n",
      "Index mismatch: NOVEL MT 279\n",
      "Index mismatch: NOVEL MT 280\n",
      "Index mismatch: NOVEL MT 281\n",
      "Index mismatch: NOVEL MT 282\n",
      "Index mismatch: NOVEL MT 283\n",
      "Index mismatch: NOVEL MT 284\n",
      "Index mismatch: NOVEL MT 286\n",
      "Index mismatch: NOVEL MT 287\n",
      "Index mismatch: NOVEL MT 288\n",
      "Index mismatch: NOVEL MT 289\n",
      "Index mismatch: NOVEL MT 290\n",
      "Index mismatch: NOVEL MT 291\n",
      "Index mismatch: NOVEL MT 292\n",
      "Index mismatch: NOVEL MT 294\n",
      "Index mismatch: NOVEL MT 295\n",
      "Index mismatch: NOVEL MT 296\n",
      "Index mismatch: NOVEL MT 297\n",
      "Index mismatch: NOVEL MT 298\n",
      "Index mismatch: NOVEL MT 299\n",
      "Index mismatch: NOVEL MT 300\n",
      "Index mismatch: NOVEL MT 301\n",
      "Index mismatch: NOVEL MT 302\n",
      "Index mismatch: NOVEL MT 303\n",
      "Index mismatch: NOVEL MT 304\n",
      "Index mismatch: NOVEL MT 305\n",
      "Index mismatch: NOVEL MT 306\n",
      "NOVEL QA Triplet papers: 86 Stanza files: 0\n",
      "Index mismatch: NOVEL QA 0\n",
      "Index mismatch: NOVEL QA 1\n",
      "Index mismatch: NOVEL QA 2\n",
      "Index mismatch: NOVEL QA 3\n",
      "Index mismatch: NOVEL QA 4\n",
      "Index mismatch: NOVEL QA 5\n",
      "Index mismatch: NOVEL QA 6\n",
      "Index mismatch: NOVEL QA 7\n",
      "Index mismatch: NOVEL QA 8\n",
      "Index mismatch: NOVEL QA 9\n",
      "Index mismatch: NOVEL QA 10\n",
      "Index mismatch: NOVEL QA 11\n",
      "Index mismatch: NOVEL QA 12\n",
      "Index mismatch: NOVEL QA 13\n",
      "Index mismatch: NOVEL QA 14\n",
      "Index mismatch: NOVEL QA 15\n",
      "Index mismatch: NOVEL QA 16\n",
      "Index mismatch: NOVEL QA 17\n",
      "Index mismatch: NOVEL QA 18\n",
      "Index mismatch: NOVEL QA 19\n",
      "Index mismatch: NOVEL QA 20\n",
      "Index mismatch: NOVEL QA 21\n",
      "Index mismatch: NOVEL QA 22\n",
      "Index mismatch: NOVEL QA 23\n",
      "Index mismatch: NOVEL QA 24\n",
      "Index mismatch: NOVEL QA 25\n",
      "Index mismatch: NOVEL QA 26\n",
      "Index mismatch: NOVEL QA 27\n",
      "Index mismatch: NOVEL QA 28\n",
      "Index mismatch: NOVEL QA 29\n",
      "Index mismatch: NOVEL QA 30\n",
      "Index mismatch: NOVEL QA 31\n",
      "Index mismatch: NOVEL QA 32\n",
      "Index mismatch: NOVEL QA 33\n",
      "Index mismatch: NOVEL QA 34\n",
      "Index mismatch: NOVEL QA 35\n",
      "Index mismatch: NOVEL QA 36\n",
      "Index mismatch: NOVEL QA 37\n",
      "Index mismatch: NOVEL QA 38\n",
      "Index mismatch: NOVEL QA 39\n",
      "Index mismatch: NOVEL QA 40\n",
      "Index mismatch: NOVEL QA 43\n",
      "Index mismatch: NOVEL QA 44\n",
      "Index mismatch: NOVEL QA 45\n",
      "Index mismatch: NOVEL QA 46\n",
      "Index mismatch: NOVEL QA 47\n",
      "Index mismatch: NOVEL QA 48\n",
      "Index mismatch: NOVEL QA 49\n",
      "Index mismatch: NOVEL QA 50\n",
      "Index mismatch: NOVEL QA 51\n",
      "Index mismatch: NOVEL QA 52\n",
      "Index mismatch: NOVEL QA 53\n",
      "Index mismatch: NOVEL QA 54\n",
      "Index mismatch: NOVEL QA 55\n",
      "Index mismatch: NOVEL QA 56\n",
      "Index mismatch: NOVEL QA 57\n",
      "Index mismatch: NOVEL QA 58\n",
      "Index mismatch: NOVEL QA 59\n",
      "Index mismatch: NOVEL QA 60\n",
      "Index mismatch: NOVEL QA 61\n",
      "Index mismatch: NOVEL QA 62\n",
      "Index mismatch: NOVEL QA 63\n",
      "Index mismatch: NOVEL QA 64\n",
      "Index mismatch: NOVEL QA 65\n",
      "Index mismatch: NOVEL QA 66\n",
      "Index mismatch: NOVEL QA 67\n",
      "Index mismatch: NOVEL QA 68\n",
      "Index mismatch: NOVEL QA 69\n",
      "Index mismatch: NOVEL QA 70\n",
      "Index mismatch: NOVEL QA 71\n",
      "Index mismatch: NOVEL QA 72\n",
      "Index mismatch: NOVEL QA 73\n",
      "Index mismatch: NOVEL QA 74\n",
      "Index mismatch: NOVEL QA 75\n",
      "Index mismatch: NOVEL QA 76\n",
      "Index mismatch: NOVEL QA 77\n",
      "Index mismatch: NOVEL QA 78\n",
      "Index mismatch: NOVEL QA 79\n",
      "Index mismatch: NOVEL QA 80\n",
      "Index mismatch: NOVEL QA 81\n",
      "Index mismatch: NOVEL QA 82\n",
      "Index mismatch: NOVEL QA 83\n",
      "Index mismatch: NOVEL QA 84\n",
      "Index mismatch: NOVEL QA 85\n",
      "Index mismatch: NOVEL QA 86\n",
      "Index mismatch: NOVEL QA 87\n",
      "NOVEL SA Triplet papers: 70 Stanza files: 0\n",
      "Index mismatch: NOVEL SA 0\n",
      "Index mismatch: NOVEL SA 1\n",
      "Index mismatch: NOVEL SA 2\n",
      "Index mismatch: NOVEL SA 3\n",
      "Index mismatch: NOVEL SA 4\n",
      "Index mismatch: NOVEL SA 5\n",
      "Index mismatch: NOVEL SA 6\n",
      "Index mismatch: NOVEL SA 7\n",
      "Index mismatch: NOVEL SA 8\n",
      "Index mismatch: NOVEL SA 9\n",
      "Index mismatch: NOVEL SA 10\n",
      "Index mismatch: NOVEL SA 11\n",
      "Index mismatch: NOVEL SA 12\n",
      "Index mismatch: NOVEL SA 13\n",
      "Index mismatch: NOVEL SA 14\n",
      "Index mismatch: NOVEL SA 15\n",
      "Index mismatch: NOVEL SA 16\n",
      "Index mismatch: NOVEL SA 17\n",
      "Index mismatch: NOVEL SA 18\n",
      "Index mismatch: NOVEL SA 19\n",
      "Index mismatch: NOVEL SA 20\n",
      "Index mismatch: NOVEL SA 21\n",
      "Index mismatch: NOVEL SA 22\n",
      "Index mismatch: NOVEL SA 23\n",
      "Index mismatch: NOVEL SA 24\n",
      "Index mismatch: NOVEL SA 25\n",
      "Index mismatch: NOVEL SA 26\n",
      "Index mismatch: NOVEL SA 27\n",
      "Index mismatch: NOVEL SA 28\n",
      "Index mismatch: NOVEL SA 29\n",
      "Index mismatch: NOVEL SA 30\n",
      "Index mismatch: NOVEL SA 31\n",
      "Index mismatch: NOVEL SA 32\n",
      "Index mismatch: NOVEL SA 33\n",
      "Index mismatch: NOVEL SA 34\n",
      "Index mismatch: NOVEL SA 35\n",
      "Index mismatch: NOVEL SA 36\n",
      "Index mismatch: NOVEL SA 37\n",
      "Index mismatch: NOVEL SA 38\n",
      "Index mismatch: NOVEL SA 39\n",
      "Index mismatch: NOVEL SA 40\n",
      "Index mismatch: NOVEL SA 41\n",
      "Index mismatch: NOVEL SA 42\n",
      "Index mismatch: NOVEL SA 43\n",
      "Index mismatch: NOVEL SA 44\n",
      "Index mismatch: NOVEL SA 45\n",
      "Index mismatch: NOVEL SA 46\n",
      "Index mismatch: NOVEL SA 47\n",
      "Index mismatch: NOVEL SA 48\n",
      "Index mismatch: NOVEL SA 49\n",
      "Index mismatch: NOVEL SA 50\n",
      "Index mismatch: NOVEL SA 51\n",
      "Index mismatch: NOVEL SA 52\n",
      "Index mismatch: NOVEL SA 53\n",
      "Index mismatch: NOVEL SA 54\n",
      "Index mismatch: NOVEL SA 55\n",
      "Index mismatch: NOVEL SA 56\n",
      "Index mismatch: NOVEL SA 57\n",
      "Index mismatch: NOVEL SA 58\n",
      "Index mismatch: NOVEL SA 59\n",
      "Index mismatch: NOVEL SA 60\n",
      "Index mismatch: NOVEL SA 61\n",
      "Index mismatch: NOVEL SA 62\n",
      "Index mismatch: NOVEL SA 63\n",
      "Index mismatch: NOVEL SA 64\n",
      "Index mismatch: NOVEL SA 65\n",
      "Index mismatch: NOVEL SA 66\n",
      "Index mismatch: NOVEL SA 67\n",
      "Index mismatch: NOVEL SA 68\n",
      "Index mismatch: NOVEL SA 69\n",
      "NOVEL Sum Triplet papers: 75 Stanza files: 0\n",
      "Index mismatch: NOVEL Sum 0\n",
      "Index mismatch: NOVEL Sum 1\n",
      "Index mismatch: NOVEL Sum 2\n",
      "Index mismatch: NOVEL Sum 3\n",
      "Index mismatch: NOVEL Sum 4\n",
      "Index mismatch: NOVEL Sum 5\n",
      "Index mismatch: NOVEL Sum 6\n",
      "Index mismatch: NOVEL Sum 7\n",
      "Index mismatch: NOVEL Sum 8\n",
      "Index mismatch: NOVEL Sum 9\n",
      "Index mismatch: NOVEL Sum 10\n",
      "Index mismatch: NOVEL Sum 11\n",
      "Index mismatch: NOVEL Sum 12\n",
      "Index mismatch: NOVEL Sum 13\n",
      "Index mismatch: NOVEL Sum 14\n",
      "Index mismatch: NOVEL Sum 15\n",
      "Index mismatch: NOVEL Sum 16\n",
      "Index mismatch: NOVEL Sum 17\n",
      "Index mismatch: NOVEL Sum 18\n",
      "Index mismatch: NOVEL Sum 19\n",
      "Index mismatch: NOVEL Sum 20\n",
      "Index mismatch: NOVEL Sum 21\n",
      "Index mismatch: NOVEL Sum 22\n",
      "Index mismatch: NOVEL Sum 23\n",
      "Index mismatch: NOVEL Sum 24\n",
      "Index mismatch: NOVEL Sum 26\n",
      "Index mismatch: NOVEL Sum 27\n",
      "Index mismatch: NOVEL Sum 28\n",
      "Index mismatch: NOVEL Sum 29\n",
      "Index mismatch: NOVEL Sum 30\n",
      "Index mismatch: NOVEL Sum 31\n",
      "Index mismatch: NOVEL Sum 32\n",
      "Index mismatch: NOVEL Sum 33\n",
      "Index mismatch: NOVEL Sum 34\n",
      "Index mismatch: NOVEL Sum 35\n",
      "Index mismatch: NOVEL Sum 36\n",
      "Index mismatch: NOVEL Sum 37\n",
      "Index mismatch: NOVEL Sum 38\n",
      "Index mismatch: NOVEL Sum 39\n",
      "Index mismatch: NOVEL Sum 41\n",
      "Index mismatch: NOVEL Sum 42\n",
      "Index mismatch: NOVEL Sum 43\n",
      "Index mismatch: NOVEL Sum 44\n",
      "Index mismatch: NOVEL Sum 45\n",
      "Index mismatch: NOVEL Sum 46\n",
      "Index mismatch: NOVEL Sum 47\n",
      "Index mismatch: NOVEL Sum 48\n",
      "Index mismatch: NOVEL Sum 49\n",
      "Index mismatch: NOVEL Sum 50\n",
      "Index mismatch: NOVEL Sum 52\n",
      "Index mismatch: NOVEL Sum 53\n",
      "Index mismatch: NOVEL Sum 54\n",
      "Index mismatch: NOVEL Sum 55\n",
      "Index mismatch: NOVEL Sum 56\n",
      "Index mismatch: NOVEL Sum 57\n",
      "Index mismatch: NOVEL Sum 58\n",
      "Index mismatch: NOVEL Sum 59\n",
      "Index mismatch: NOVEL Sum 60\n",
      "Index mismatch: NOVEL Sum 61\n",
      "Index mismatch: NOVEL Sum 62\n",
      "Index mismatch: NOVEL Sum 63\n",
      "Index mismatch: NOVEL Sum 64\n",
      "Index mismatch: NOVEL Sum 65\n",
      "Index mismatch: NOVEL Sum 66\n",
      "Index mismatch: NOVEL Sum 67\n",
      "Index mismatch: NOVEL Sum 68\n",
      "Index mismatch: NOVEL Sum 69\n",
      "Index mismatch: NOVEL Sum 70\n",
      "Index mismatch: NOVEL Sum 71\n",
      "Index mismatch: NOVEL Sum 72\n",
      "Index mismatch: NOVEL Sum 73\n",
      "Index mismatch: NOVEL Sum 74\n",
      "Index mismatch: NOVEL Sum 75\n",
      "Index mismatch: NOVEL Sum 76\n",
      "Index mismatch: NOVEL Sum 77\n",
      "SKG Dia Triplet papers: 304 Stanza files: 319\n",
      "SKG MT Triplet papers: 1424 Stanza files: 1476\n",
      "SKG NLI Triplet papers: 20 Stanza files: 21\n",
      "SKG Par Triplet papers: 222 Stanza files: 226\n",
      "SKG QA Triplet papers: 481 Stanza files: 493\n",
      "SKG SA Triplet papers: 613 Stanza files: 636\n",
      "Index mismatch: SKG SA 636\n",
      "SKG Sum Triplet papers: 216 Stanza files: 221\n",
      "Final mapped papers: 3363\n",
      "split\n",
      "BLOG      84\n",
      "SKG     3279\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:35.217031Z",
     "start_time": "2026-02-18T14:27:35.042033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_openalex_metadata_exact(title):\n",
    "    url = \"https://api.openalex.org/works\"\n",
    "\n",
    "    params = {\n",
    "        \"filter\": f'title.search:\"{title}\"',\n",
    "        \"mailto\": \"your_email@gmail.com\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        results = r.json()[\"results\"]\n",
    "\n",
    "        if len(results) > 0:\n",
    "            best = results[0]\n",
    "\n",
    "            return {\n",
    "                \"openalex_id\": best.get(\"id\"),\n",
    "                \"publication_year\": best.get(\"publication_year\"),\n",
    "                \"citation_count\": best.get(\"cited_by_count\"),\n",
    "                \"referenced_works\": best.get(\"referenced_works\"),\n",
    "                \"display_name\": best.get(\"display_name\")\n",
    "            }\n",
    "\n",
    "    return None"
   ],
   "id": "981348e4f5a82d7",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:57.068456Z",
     "start_time": "2026-02-18T14:27:35.475401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = paper_mapping.head(10)\n",
    "\n",
    "for _, row in sample.iterrows():\n",
    "    print(row[\"title\"])\n",
    "    print(get_openalex_metadata_exact(row[\"title\"]))\n",
    "    time.sleep(1)"
   ],
   "id": "eb3062a29d11c473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A survey of available corpora for building data-driven dialogue systems _ the morning paper\n",
      "None\n",
      "Building end-to-end dialogue systems using generative hierarchical neural network models _ the morning paper\n",
      "None\n",
      "Google AI Blog_ Towards a Conversational Agent that Can Chat About…Anything.pdf\n",
      "None\n",
      "Machine learning for dialog state tracking_ a review _ the morning paper\n",
      "None\n",
      "Multi-domain dialog state tracking using recurrent neural networks _ the morning paper\n",
      "None\n",
      "Notes for paper titled _How NOT To Evaluate Your Dialogue System_ An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation_ · GitHub\n",
      "None\n",
      "Notes for paper _End-to-end optimization of goal-driven and visually grounded dialogue systems_ · GitHub\n",
      "None\n",
      "Summary of _Evaluating Prerequisite Qualities for Learning End-to-end Dialog Systems_ paper · GitHub\n",
      "None\n",
      "Summary of _GuessWhat_! Visual object discovery through multi-modal dialogue_ paper · GitHub.pdf\n",
      "None\n",
      "Ten challenges in highly-interactive dialog systems _ the morning paper\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:57.089946Z",
     "start_time": "2026-02-18T14:27:57.081947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"Scientific_Novelty_Detection/stanza_files/SKG_Papers/Dia/*-Stanza-out.txt\")\n",
    "print(files[0])"
   ],
   "id": "499250dd44372e4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientific_Novelty_Detection/stanza_files/SKG_Papers/Dia\\10.IIT-UHH at SemEval-2017 Task 3 Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification-Stanza-out.txt\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:57.156811Z",
     "start_time": "2026-02-18T14:27:57.138165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(files[0], encoding=\"ISO-8859-1\") as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "for line in text[:50]:\n",
    "    print(line.strip())"
   ],
   "id": "b008279f2285aa80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "IIT - UHH at SemEval - 2017 Task 3 : Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification\n",
      "abstract\n",
      "In this paper we present the system for Answer Selection and Ranking in Community Question Answering , which we build as part of our participation in SemEval - 2017 Task 3 .\n",
      "We develop a Support Vector Machine ( SVM ) based system that makes use of textual , domain-specific , wordembedding and topic-modeling features .\n",
      "In addition , we propose a novel method for dialogue chain identification in comment threads .\n",
      "Our primary submission won subtask C , outperforming other systems in all the primary evaluation metrics .\n",
      "We performed well in other English subtasks , ranking third in subtask A and eighth in subtask B .\n",
      "We also developed open source toolkits for all the three English subtasks by the name cQARank 1 .\n",
      "Introduction\n",
      "This paper presents the system built for participation in the SemEval - 2017 Shared Task 3 on Community Question Answering ( CQA ) .\n",
      "The task aims to classify and rank a candidate text c in relevance to a target text t. Based on the nature of the candidate and target texts , the main task is subdivided into three subtasks in which the teams are expected to solve the problem of Question - Comment similarity , Question - Question similarity and Question - External Comment similarity ( Nakov et al. , 2017 ) .\n",
      "In this work , we propose a rich feature - based system for solving these problems .\n",
      "We create an architecture which integrates textual , semantic and domain-specific features to achieve good results in the proposed task .\n",
      "Due to the extremely noisy nature of the social forum data , we also develop a customized preprocessing pipeline , rather than using the standard tools .\n",
      "We use Support Vector Machine ( SVM ) ( Cortes and Vapnik , 1995 ) for classification , and its confidence score for ranking .\n",
      "We initially define a generic set of features to develop a robust system for all three subtasks , then include additional features based on the nature of the subtasks .\n",
      "To adapt the system to subtasks B and C , we include features extracted from the scores of the other subtasks , propagating meaningful information essential in an incremental setting .\n",
      "We propose a novel method for identification of dialogue groups in the comment thread by constructing a user interaction graph and also incorporate features from this graph in our system .\n",
      "Our algorithm outputs mutually disjoint groups of users who are involved in conversation with each other in the comment thread .\n",
      "The rest of the paper is organized as follows : Section 2 describes the related work .\n",
      "Sections 3 , 4 , and 5 elucidate the system architecture , features used and algorithms developed .\n",
      "Section 6 provides experimentation details and reports the official results .\n",
      "Related Work In Question\n",
      "Answering , answer selection and ranking has been a major research concern in Natural Language Processing ( NLP ) during the past few years .\n",
      "The problem becomes more interesting for Community Question Answering due to the highly unstructured and noisy nature of the data .\n",
      "Also , domain knowledge plays a major role in such an environment , where meta data of users and context based learning can capture trends well .\n",
      "The task on Community Question Answering in SemEval began in 2015 , where the objective was to classify comments in a thread as Good , Bad or PotentiallyUseful .\n",
      "In subsequent years , the task was extended and modified to focus on ranking and duplicate question detection in a cross domain setting .\n",
      "In their 2015 system , Belinkov ( 2015 ) used word vectors of the question and of the comment , various text - based similarities and meta data features .\n",
      "Nicosia ( 2015 ) derived features from a comment in the context of the entire thread .\n",
      "They also modelled potential dialogues by identifying interlacing comments between users .\n",
      "Establishing similarity between Questions and External comments ( subtask C ) is quite challenging , which can be tackled by propagating useful context and information from other subtasks .\n",
      "Filice ( 2016 ) introduced an interesting approach of stacking classifiers across subtasks and Wu & Lan ( 2016 ) proposed a method of reducing the errors that propagated as a result of this stacking .\n",
      "System Description\n",
      "System Pipeline\n",
      "The system architecture of our submission to subtask A is depicted in Figure 1 .\n",
      "We explain the pre-processing pipeline in the next subsection .\n",
      "The cleaned data is fed into our supervised machine learning framework .\n",
      "We train our wordembedding model on the unannotated and training data 2 provided by the organizers , and train a probabilistic topic model on the training data .\n",
      "The detailed description of features is provided in the following section .\n",
      "After obtaining the feature vectors , we perform feature selection using wrapper methods to maximize the accuracy on the development set .\n",
      "We Z-score normalize the feature vectors and feed them to a SVM .\n",
      "We tune the hyperparameters of SVM and and generate classification labels and probabilities , the latter being used for computing the MAP score .\n",
      "Preprocessing Pipeline\n",
      "Due to the highly unstructured , spelling and grammatical error-prone nature of the data , adaptation of any standard tokenization pipeline was not well motivated .\n",
      "We customized the preprocessing according to the nature of the data .\n",
      "We unescaped HTML special characters and removed URLs , emails , HTML tags , image description tags , punctuations and slang words ( from a defined dictionary ) .\n",
      "Finally , we expanded apostrophe words and 2 http://alt.qcri.org/semeval2017/ task3/index.php?id=data-and-tools removed stopwords .\n",
      "The cleaned data is then used in all further experiments .\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:57.216489Z",
     "start_time": "2026-02-18T14:27:57.212311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def clean_title(filename):\n",
    "    title = filename.replace(\"-Stanza-out.txt\", \"\")\n",
    "    title = re.sub(r\"^\\d+\\.\", \"\", title)\n",
    "    title = title.strip()\n",
    "    return title"
   ],
   "id": "66a8cb442b23bb02",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:27:58.352200Z",
     "start_time": "2026-02-18T14:27:57.294308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "title = \"IIT-UHH at SemEval-2017 Task 3 Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification\"\n",
    "\n",
    "get_openalex_metadata_exact(title)"
   ],
   "id": "e89a75ce88458de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openalex_id': 'https://openalex.org/W2751927167',\n",
       " 'publication_year': 2017,\n",
       " 'citation_count': 14,\n",
       " 'referenced_works': ['https://openalex.org/W2096765155',\n",
       "  'https://openalex.org/W2109943925',\n",
       "  'https://openalex.org/W2131744502',\n",
       "  'https://openalex.org/W2133286915',\n",
       "  'https://openalex.org/W2148143831',\n",
       "  'https://openalex.org/W2153579005',\n",
       "  'https://openalex.org/W2153635508',\n",
       "  'https://openalex.org/W2251181482',\n",
       "  'https://openalex.org/W2251506003',\n",
       "  'https://openalex.org/W2251598732',\n",
       "  'https://openalex.org/W2468484304',\n",
       "  'https://openalex.org/W2468672598',\n",
       "  'https://openalex.org/W2469197239',\n",
       "  'https://openalex.org/W2471091747',\n",
       "  'https://openalex.org/W2915240437',\n",
       "  'https://openalex.org/W4239510810',\n",
       "  'https://openalex.org/W4294170691'],\n",
       " 'display_name': 'IIT-UHH at SemEval-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:28:01.256097Z",
     "start_time": "2026-02-18T14:27:58.393203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = paper_mapping[paper_mapping[\"split\"]==\"SKG\"].head(3)\n",
    "\n",
    "for t in sample[\"title\"]:\n",
    "    print(t)\n",
    "    print(get_openalex_metadata_exact(t))"
   ],
   "id": "d63880f8fa5075f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IIT-UHH at SemEval-2017 Task 3 Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification\n",
      "{'openalex_id': 'https://openalex.org/W2751927167', 'publication_year': 2017, 'citation_count': 14, 'referenced_works': ['https://openalex.org/W2096765155', 'https://openalex.org/W2109943925', 'https://openalex.org/W2131744502', 'https://openalex.org/W2133286915', 'https://openalex.org/W2148143831', 'https://openalex.org/W2153579005', 'https://openalex.org/W2153635508', 'https://openalex.org/W2251181482', 'https://openalex.org/W2251506003', 'https://openalex.org/W2251598732', 'https://openalex.org/W2468484304', 'https://openalex.org/W2468672598', 'https://openalex.org/W2469197239', 'https://openalex.org/W2471091747', 'https://openalex.org/W2915240437', 'https://openalex.org/W4239510810', 'https://openalex.org/W4294170691'], 'display_name': 'IIT-UHH at SemEval-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification'}\n",
      "PLATO Pre-trained Dialogue Generation Model with Discrete Latent Variable\n",
      "{'openalex_id': 'https://openalex.org/W3035451444', 'publication_year': 2020, 'citation_count': 227, 'referenced_works': ['https://openalex.org/W1522301498', 'https://openalex.org/W1566289585', 'https://openalex.org/W1591706642', 'https://openalex.org/W1889081078', 'https://openalex.org/W1958706068', 'https://openalex.org/W2133012565', 'https://openalex.org/W2143177362', 'https://openalex.org/W2157331557', 'https://openalex.org/W2328886022', 'https://openalex.org/W2525778437', 'https://openalex.org/W2761590056', 'https://openalex.org/W2805005636', 'https://openalex.org/W2807873315', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898875342', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914204778', 'https://openalex.org/W2916898195', 'https://openalex.org/W2945260553', 'https://openalex.org/W2948336019', 'https://openalex.org/W2951583236', 'https://openalex.org/W2951697502', 'https://openalex.org/W2953039584', 'https://openalex.org/W2962717182', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963330684', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964213933', 'https://openalex.org/W2964587107', 'https://openalex.org/W2970597249', 'https://openalex.org/W2970682219', 'https://openalex.org/W2973049837', 'https://openalex.org/W2988937804', 'https://openalex.org/W4247864677', 'https://openalex.org/W4288246040', 'https://openalex.org/W4288624561'], 'display_name': 'PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable'}\n",
      "Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks\n",
      "{'openalex_id': 'https://openalex.org/W2739836841', 'publication_year': 2017, 'citation_count': 11, 'referenced_works': ['https://openalex.org/W142644632', 'https://openalex.org/W630532510', 'https://openalex.org/W1504478957', 'https://openalex.org/W1508665521', 'https://openalex.org/W1832693441', 'https://openalex.org/W1846690939', 'https://openalex.org/W1993482042', 'https://openalex.org/W2135256083', 'https://openalex.org/W2165880886', 'https://openalex.org/W2167084966', 'https://openalex.org/W2186845332', 'https://openalex.org/W2251143283', 'https://openalex.org/W2402326365', 'https://openalex.org/W2423024114', 'https://openalex.org/W2493916176', 'https://openalex.org/W2566645459', 'https://openalex.org/W2962854379', 'https://openalex.org/W2964228006', 'https://openalex.org/W2964331270'], 'display_name': 'Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks'}\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:28:01.295513Z",
     "start_time": "2026-02-18T14:28:01.290152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import time\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def get_openalex_metadata_exact(title):\n",
    "    url = \"https://api.openalex.org/works\"\n",
    "\n",
    "    params = {\n",
    "        \"filter\": f'title.search:\"{title}\"',\n",
    "        \"mailto\": \"kulsparsh2005@gmail.com\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        results = r.json()[\"results\"]\n",
    "\n",
    "        if len(results) > 0:\n",
    "            best = results[0]\n",
    "\n",
    "            # Validate similarity\n",
    "            if similar(title, best[\"display_name\"]) > 0.85:\n",
    "                return {\n",
    "                    \"openalex_id\": best.get(\"id\"),\n",
    "                    \"publication_year\": best.get(\"publication_year\"),\n",
    "                    \"citation_count\": best.get(\"cited_by_count\"),\n",
    "                    \"referenced_works\": best.get(\"referenced_works\")\n",
    "                }\n",
    "\n",
    "    return None"
   ],
   "id": "667d61cd20167f51",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:28:26.719755Z",
     "start_time": "2026-02-18T14:28:26.703756Z"
    }
   },
   "cell_type": "code",
   "source": "paper_mapping",
   "id": "5fca3e61bc51d3a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     split domain  paper_ID                                              title\n",
       "0     BLOG    Dia         0  A survey of available corpora for building dat...\n",
       "1     BLOG    Dia         1  Building end-to-end dialogue systems using gen...\n",
       "2     BLOG    Dia         2  Google AI Blog_ Towards a Conversational Agent...\n",
       "3     BLOG    Dia         3  Machine learning for dialog state tracking_ a ...\n",
       "4     BLOG    Dia         4  Multi-domain dialog state tracking using recur...\n",
       "...    ...    ...       ...                                                ...\n",
       "3358   SKG    Sum       216  On the Abstractiveness of Neural Document Summ...\n",
       "3359   SKG    Sum       217  BrailleSUM A News Summarization System for the...\n",
       "3360   SKG    Sum       218  A Discourse-Aware Attention Model for Abstract...\n",
       "3361   SKG    Sum       219  A Mixed Hierarchical Attention Based Encoder-D...\n",
       "3362   SKG    Sum       220  Improving the Similarity Measure of Determinan...\n",
       "\n",
       "[3363 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>domain</th>\n",
       "      <th>paper_ID</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>Dia</td>\n",
       "      <td>0</td>\n",
       "      <td>A survey of available corpora for building dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>Dia</td>\n",
       "      <td>1</td>\n",
       "      <td>Building end-to-end dialogue systems using gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>Dia</td>\n",
       "      <td>2</td>\n",
       "      <td>Google AI Blog_ Towards a Conversational Agent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>Dia</td>\n",
       "      <td>3</td>\n",
       "      <td>Machine learning for dialog state tracking_ a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>Dia</td>\n",
       "      <td>4</td>\n",
       "      <td>Multi-domain dialog state tracking using recur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>216</td>\n",
       "      <td>On the Abstractiveness of Neural Document Summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>217</td>\n",
       "      <td>BrailleSUM A News Summarization System for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>218</td>\n",
       "      <td>A Discourse-Aware Attention Model for Abstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>219</td>\n",
       "      <td>A Mixed Hierarchical Attention Based Encoder-D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>SKG</td>\n",
       "      <td>Sum</td>\n",
       "      <td>220</td>\n",
       "      <td>Improving the Similarity Measure of Determinan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3363 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:03:43.313684Z",
     "start_time": "2026-02-18T14:28:50.708874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metadata_records = []\n",
    "\n",
    "for _, row in paper_mapping.iterrows():\n",
    "\n",
    "    if row[\"split\"] in [\"SKG\", \"NOVEL\"]:\n",
    "\n",
    "        meta = get_openalex_metadata_exact(row[\"title\"])\n",
    "\n",
    "        if meta:\n",
    "            metadata_records.append({\n",
    "                **row,\n",
    "                **meta\n",
    "            })\n",
    "            print(\"Match: \", row[\"title\"])\n",
    "        else:\n",
    "            print(\"No match:\", row[\"title\"])\n",
    "\n",
    "        time.sleep(1)  # respect rate limits\n",
    "\n",
    "    else:\n",
    "        # BLOG papers\n",
    "        metadata_records.append({\n",
    "            **row,\n",
    "            \"openalex_id\": None,\n",
    "            \"publication_year\": 2021,\n",
    "            \"citation_count\": 0,\n",
    "            \"referenced_works\": []\n",
    "        })\n",
    "\n",
    "paper_metadata = pd.DataFrame(metadata_records)"
   ],
   "id": "917219a1cfeba300",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match:  IIT-UHH at SemEval-2017 Task 3 Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification\n",
      "Match:  PLATO Pre-trained Dialogue Generation Model with Discrete Latent Variable\n",
      "Match:  Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks\n",
      "Match:  The Impact of Interpretation Problems on Tutorial Dialogue\n",
      "Match:  Learning the Information Status of Noun Phrases in Spoken Dialogues\n",
      "Match:  Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model\n",
      "Match:  Towards an Automatic Turing Test Learning to Evaluate Dialogue Responses\n",
      "Match:  Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System\n",
      "Match:  A Statistical Spoken Dialogue System using Complex User Goals and Value Directed Compression\n",
      "Match:  Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-Adaptive Spoken Dialogue System\n",
      "Match:  Representing Movie Characters in Dialogues\n",
      "Match:  Safe In-vehicle Dialogue Using Learned Predictions of User Utterances\n",
      "Match:  Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network\n",
      "Match:  Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems\n",
      "Match:  Feudal Reinforcement Learning for Dialogue Management in Large Domains\n",
      "Match:  An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue\n",
      "Match:  tucSage Grammar Rule Induction for Spoken Dialogue Systems via Probabilistic Candidate Selection\n",
      "Match:  Chat Detection in an Intelligent Assistant Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems\n",
      "Match:  A Compare Aggregate Transformer for Understanding Document-grounded Dialogue\n",
      "Match:  Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue\n",
      "Match:  DialSQL Dialogue Based Structured Query Generation\n",
      "Match:  Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation\n",
      "Match:  Skeleton-to-Response Dialogue Generation Guided by Retrieval Memory\n",
      "Match:  Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling\n",
      "Match:  Improving Knowledge-Aware Dialogue Response Generation by Using Human-Written Prototype Dialogues\n",
      "Match:  Learning about Voice Search for Spoken Dialogue Systems\n",
      "Match:  Deep Reinforcement Learning for Dialogue Generation\n",
      "Match:  Open Dialogue Management for Relational Databases\n",
      "Match:  Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems\n",
      "Match:  Game-Based Video-Context Dialogue\n",
      "Match:  OpenDial A Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules\n",
      "Match:  Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking\n",
      "Match:  Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables\n",
      "Match:  Modeling Multi-Action Policy for Task-Oriented Dialogues\n",
      "Match:  MuTual A Dataset for Multi-Turn Dialogue Reasoning\n",
      "Match:  Dialogue Management based on Sentence Clustering\n",
      "Match:  You Impress Me Dialogue Generation via Mutual Persona Perception\n",
      "Match:  Sequicity Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures\n",
      "Match:  An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking\n",
      "Match:  Global-Locally Self-Attentive Encoder for Dialogue State Tracking\n",
      "Match:  Knowledge Diffusion for Neural Dialogue Generation\n",
      "Match:  Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever\n",
      "Match:  FASTDial Abstracting Dialogue Policies for Fast Development of Task Oriented Agents\n",
      "Match:  Framework for the Development of Spoken Dialogue System based on Collaboratively Constructed Semantic Resources\n",
      "Match:  PyDial A Multi-domain Statistical Dialogue System Toolkit\n",
      "Match:  Scaling Multi-Domain Dialogue State Tracking via Query Reformulation\n",
      "Match:  Neural Dialogue State Tracking with Temporally Expressive Networks\n",
      "Match:  BiST Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues\n",
      "Match:  UniConv A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues\n",
      "Match:  GraphDialog Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems\n",
      "Match:  Conversation Model Fine-Tuning for Classifying Client Utterances in Counseling Dialogues\n",
      "Match:  Structured Attention for Unsupervised Dialogue Structure Induction\n",
      "Match:  Cross Copy Network for Dialogue Generation\n",
      "Match:  The Teams Corpus and Entrainment in Multi-Party Spoken Dialogues\n",
      "Match:  Multi-turn Response Selection using Dialogue Dependency Relations\n",
      "Match:  Parallel Interactive Networks for Multi-Domain Dialogue State Generation\n",
      "Match:  RMM A Recursive Mental Model for Dialogue Navigation\n",
      "Match:  DialogueGCN A Graph Convolutional Neural Network for Emotion Recognition in Conversation\n",
      "Match:  Harnessing Sequence Labeling for Sarcasm Detection in Dialogue from TV Series ‘Friends’\n",
      "Match:  Multi-domain Neural Network Language Generation for Spoken Dialogue Systems\n",
      "Match:  Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings\n",
      "Match:  Evaluating a City Exploration Dialogue System with Integrated Question-Answering and Pedestrian Navigation\n",
      "Match:  Neural Belief Tracker Data-Driven Dialogue State Tracking\n",
      "Match:  Public Dialogue Analysis of Tolerance in Online Discussions\n",
      "Match:  A Long Short-Term Memory Framework for Predicting Humor in Dialogues\n",
      "Match:  Learning Goal-oriented Dialogue Policy with opposite Agent Awareness\n",
      "Match:  Multimodal Menu-based Dialogue with Speech Cursor in DICO II+\n",
      "Match:  ScoutBot A Dialogue System for Collaborative Navigation\n",
      "Match:  Box of Lies Multimodal Deception Detection in Dialogues\n",
      "Match:  Linguistic Cues to Deception and Perceived Deception in Interview Dialogues\n",
      "Match:  Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems\n",
      "Match:  Modeling Student Response Times Towards Efficient One-on-one Tutoring Dialogues\n",
      "Match:  Non-Topical Coherence in Social Talk A Call for Dialogue Model Enrichment\n",
      "Match:  Multi-Turn Dialogue Generation in E-Commerce Platform with the Context of Historical Dialogue\n",
      "Match:  Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks\n",
      "Match:  The PhotoBook Dataset Building Common Ground through Visually-Grounded Dialogue\n",
      "Match:  Are You for Real Detecting Identity Fraud via Dialogue Interactions\n",
      "Match:  Negative Training for Neural Dialogue Response Generation\n",
      "Match:  Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems\n",
      "Match:  An Impossible Dialogue! Nominal Utterances and Populist Rhetoric in an Italian Twitter Corpus of Hate Speech against Immigrants\n",
      "Match:  Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration\n",
      "Match:  DyKgChat Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs\n",
      "Match:  Hidden Softmax Sequence Model for Dialogue Structure Analysis\n",
      "Match:  How Time Matters Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues\n",
      "Match:  Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework\n",
      "Match:  Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation\n",
      "No match: Non-Cooperation in Dialogue\n",
      "Match:  One Time of Interaction May Not Be Enough Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues\n",
      "Match:  ConvLab-2 An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems\n",
      "Match:  Importance-Driven Turn-Bidding for Spoken Dialogue Systems\n",
      "Match:  Towards Quality-Adaptive Spoken Dialogue Management\n",
      "Match:  Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning\n",
      "Match:  Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\n",
      "Match:  A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency in Dialogue Generation\n",
      "Match:  Deep Dyna-Q Integrating Planning for Task-Completion Dialogue Policy Learning\n",
      "Match:  Recommendation as a Communication Game Self-Supervised Bot-Play for Goal-oriented Dialogue\n",
      "Match:  A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response Selection\n",
      "Match:  Personalizing Dialogue Agents I have a dog, do you have pets too\n",
      "Match:  Guided Dialogue Policy Learning without Adversarial Learning in the Loop\n",
      "Match:  MultiDM-GCN Aspect-guided Response Generation in Multi-domain Multi-modal Dialogue System using Graph Convolutional Network\n",
      "Match:  Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems\n",
      "Match:  Grounding Conversations with Improvised Dialogues\n",
      "Match:  DeepPavlov Open-Source Library for Dialogue Systems\n",
      "Match:  LIDA Lightweight Interactive Dialogue Annotator\n",
      "Match:  Learning an Unreferenced Metric for Online Dialogue Evaluation\n",
      "Match:  Neural Generation of Dialogue Response Timings\n",
      "Match:  The Dialogue Dodecathlon Open-Domain Knowledge and Image Grounded Conversational Agents\n",
      "Match:  Augmenting Abstract Meaning Representation for Human-Robot Dialogue\n",
      "Match:  Chatbot with a Discourse Structure-Driven Dialogue Management\n",
      "Match:  Adversarial Learning for Neural Dialogue Generation\n",
      "Match:  How NOT To Evaluate Your Dialogue System An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation\n",
      "Match:  How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue\n",
      "Match:  On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems\n",
      "Match:  Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences\n",
      "Match:  Conditional Generation and Snapshot Learning in Neural Dialogue Systems\n",
      "Match:  Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning\n",
      "Match:  Toward Stance-based Personas for Opinionated Dialogues\n",
      "Match:  A Research Platform for Multi-Robot Dialogue with Humans\n",
      "Match:  Robust Coreference Resolution and Entity Linking on Dialogues Character Identification on TV Show Transcripts\n",
      "Match:  Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking\n",
      "Match:  Learning to Plan and Realize Separately for Open-Ended Dialogue Systems\n",
      "Match:  Subgoal Discovery for Hierarchical Dialogue Policy Learning\n",
      "Match:  Decoupling Strategy and Generation in Negotiation Dialogues\n",
      "No match: Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning\n",
      "No match: Wikification of Concept Mentions within Spoken Dialogues Using Domain Constraints from Wikipedia\n",
      "No match: A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses\n",
      "No match: Samvaadhana A Telugu Dialogue System in Hospital Domain\n",
      "No match: uBLEU Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems\n",
      "No match: Knowledge-Grounded Dialogue Generation with Pre-trained Language Models\n",
      "No match: MinTL Minimalist Transfer Learning for Task-Oriented Dialogue Systems\n",
      "No match: Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation\n",
      "No match: Counterfactual Off-Policy Training for Neural Dialogue Generation\n",
      "No match: Dialogue Distillation Open-Domain Dialogue Augmentation Using Unpaired Data\n",
      "No match: Knowing What You Know Calibrating Dialogue Belief State Distributions via Ensembles\n",
      "No match: Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network\n",
      "No match: Dialogue-Act Prediction of Future Responses Based on Conversation History\n",
      "No match: Toward Dialogue Modeling A Semantic Annotation Scheme for Questions and Answers\n",
      "No match: AttnIO Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue\n",
      "No match: Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training\n",
      "No match: Learning Personas from Dialogue with Attentive Memory Networks\n",
      "No match: Dialogue Response Ranking Training with Large-Scale Human Feedback Data\n",
      "No match: Training Millions of Personalized Dialogue Agents\n",
      "No match: Dialogue over Context and Structured Knowledge using a Neural Network Model with External Memories\n",
      "No match: Hello, It’s GPT-2 - How Can I Help You Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems\n",
      "No match: SemEval-2014 Task 2 Grammar Induction for Spoken Dialogue Systems\n",
      "No match: Towards Relational POMDPs for Adaptive Dialogue Management\n",
      "No match: Using Paraphrasing and Memory-Augmented Models to Combat Data Sparsity in Question Interpretation with a Virtual Patient Dialogue System\n",
      "No match: Latent Variable Dialogue Models and their Diversity\n",
      "No match: Towards Universal Dialogue State Tracking\n",
      "No match: Rethinking Supervised Learning and Reinforcement Learning in Task-Oriented Dialogue Systems\n",
      "No match: Incrementally Tracking Reference in HumanHuman Dialogue Using Linguistic and Extra-Linguistic Information\n",
      "No match: PolyResponse A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking\n",
      "No match: Approximation of Response Knowledge Retrieval in Knowledge-grounded Dialogue Generation\n",
      "No match: Spot The Bot A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems\n",
      "No match: On-line Dialogue Policy Learning with Companion Teaching\n",
      "No match: PyOpenDial A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules\n",
      "No match: Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation\n",
      "No match: Dr. Summarize Global Summarization of Medical Dialogue by Exploiting Local Structures.\n",
      "No match: Evaluating and Enhancing the Robustness of Dialogue Systems A Case Study on a Negotiation Agent\n",
      "No match: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\n",
      "No match: Task-oriented Dialogue System for Automatic Diagnosis\n",
      "No match: Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations\n",
      "No match: Linguistically-Informed Specificity and Semantic Plausibility for Dialogue Generation\n",
      "No match: Generating Dialogue Responses from a Semantic Latent Space\n",
      "No match: Generalizable and Explainable Dialogue Generation via Explicit Action Learning\n",
      "No match: Learning from Dialogue after Deployment Feed Yourself, Chatbot!\n",
      "No match: Grounding Strategic Conversation Using Negotiation Dialogues to Predict Trades in a Win-Lose Game\n",
      "No match: Incremental Learning from Scratch for Task-Oriented Dialogue Systems\n",
      "No match: ReCoSa Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation\n",
      "No match: Dialogue Natural Language Inference\n",
      "No match: Budgeted Policy Learning for Task-Oriented Dialogue Systems\n",
      "No match: Towards Low-Resource Semi-Supervised Dialogue Generation with Meta-Learning\n",
      "No match: A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification\n",
      "No match: Are Training Samples Correlated Learning to Generate Dialogue Responses with Multiple References\n",
      "No match: Dialogue Act Classification with Context-Aware Self-Attention\n",
      "No match: Self-Supervised Dialogue Learning\n",
      "No match: What do Entity-Centric Models Learn Insights from Entity Linking in Multi-Party Dialogue\n",
      "No match: Mimic and Rephrase Reflective Listening in Open-Ended Dialogue\n",
      "No match: Evaluating Coherence in Dialogue Systems using Entailment\n",
      "No match: Towards Situated Dialogue Revisiting Referring Expression Generation\n",
      "No match: Achieving Common Ground in Multi-modal Dialogue\n",
      "No match: Dual Latent Variable Model for Low-Resource Natural Language Generation in Dialogue Systems\n",
      "No match: Improving Multi-turn Dialogue Modelling with Utterance ReWriter\n",
      "No match: Utterance-Unit Annotation for the JSL Dialogue Corpus Toward a Multimodal Approach to Corpus Linguistics\n",
      "No match: Improving Limited Labeled Dialogue State Tracking with Self-Supervision\n",
      "No match: Towards Emotion-aided Multi-modal Dialogue Act Classification\n",
      "No match: Probing Task-Oriented Dialogue Representation from Language Models\n",
      "No match: Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning\n",
      "No match: Movie-DiC a Movie Dialogue Corpus for Research and Development\n",
      "No match: Discriminative Deep Dyna-Q Robust Planning for Dialogue Policy Learning\n",
      "No match: A Hierarchical Neural Model for Learning Sequences of Dialogue Acts\n",
      "No match: AirDialogue An Environment for Goal-Oriented Dialogue Research\n",
      "No match: Don’t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training\n",
      "No match: A Network-based End-to-End Trainable Task-oriented Dialogue System\n",
      "No match: Data-oriented Monologue-to-Dialogue Generation\n",
      "No match: Dialogue-Based Relation Extraction\n",
      "No match: More Diverse Dialogue Datasets via Diversity-Informed Data Collection\n",
      "No match: Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks\n",
      "No match: Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access\n",
      "No match: Storytelling with Dialogue A Critical Role Dungeons and Dragons Dataset\n",
      "No match: Multi-Domain Goal-Oriented Dialogues (MultiDoGO) Strategies toward Curating and Annotating Large Scale Dialogue Data\n",
      "No match: Build it Break it Fix it for Dialogue Safety Robustness from Adversarial Human Attack\n",
      "No match: GECOR An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue\n",
      "No match: NEXUS Network Connecting the Preceding and the Following in Dialogue Generation\n",
      "No match: Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Exophoric References in Situated Dialogue\n",
      "No match: Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues\n",
      "No match: Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models\n",
      "No match: Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies\n",
      "No match: Generating Expository Dialogue from Monologue Motivation, Corpus and Preliminary Rules\n",
      "No match: Improving Response Selection in Multi-Turn Dialogue Systems by Incorporating Domain Knowledge\n",
      "No match: Designing Precise and Robust Dialogue Response Evaluators\n",
      "No match: Dialogue Systems Using Online Learning Beyond Empirical Methods\n",
      "No match: Discovering Latent Structure in Task-Oriented Dialogues\n",
      "No match: Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring\n",
      "No match: Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering\n",
      "No match: Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph\n",
      "No match: Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness\n",
      "No match: What You See is What You Get Visual Pronoun Coreference Resolution in Dialogues\n",
      "No match: Generate, Delete and Rewrite A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation\n",
      "No match: Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks\n",
      "No match: Video-Grounded Dialogues with Pretrained Generation Language Models\n",
      "No match: Entropy Converges Between Dialogue Participants Explanations from an Information-Theoretic Perspective\n",
      "No match: Template Guided Text Generation for Task-Oriented Dialogue\n",
      "No match: Dialogue focus tracking for zero pronoun resolution\n",
      "No match: Regularizing Dialogue Generation by Imitating Implicit Scenarios\n",
      "No match: Know More about Each Other Evolving Dialogue Strategy via Compound Assessment\n",
      "No match: Semantic Role Labeling Guided Multi-turn Dialogue ReWriter\n",
      "No match: Training Neural Response Selection for Task-Oriented Dialogue Systems\n",
      "No match: Collaborative Dialogue in Minecraft\n",
      "No match: Profile Consistency Identification for Open-domain Dialogue Agents\n",
      "No match: Efficient Dialogue State Tracking by Selectively Overwriting Memory\n",
      "No match: Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System\n",
      "No match: Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference\n",
      "No match: Personalizing Dialogue Agents via Meta-Learning\n",
      "No match: Reading Turn by Turn Hierarchical Attention Architecture for Spoken Dialogue Comprehension\n",
      "No match: MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling\n",
      "No match: Rationally Reappraising ATIS-based Dialogue Systems\n",
      "No match: Evaluating Dialogue Generation Systems via Response Selection\n",
      "No match: A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking\n",
      "No match: Data Manipulation Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight\n",
      "No match: Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems\n",
      "No match: Learning Efficient Dialogue Policy from Demonstrations through Shaping\n",
      "No match: Persuasion for Good Towards a Personalized Persuasive Dialogue System for Social Good\n",
      "No match: SAS Dialogue State Tracking via Slot Attention and Slot Information Sharing\n",
      "No match: MIE A Medical Information Extractor towards Medical Dialogues\n",
      "No match: Improving Dialogue State Tracking by Discerning the Relevant Context\n",
      "No match: Co-reference via Pointing and Haptics in Multi-Modal Dialogues\n",
      "No match: Spectral Analysis of Information Density in Dialogue Predicts Collaborative Task Performance\n",
      "No match: CIMA A Large Open Access Dialogue Dataset for Tutoring\n",
      "No match: Data Collection for Dialogue System A Startup Perspective\n",
      "No match: Dialogue State Tracking with Explicit Slot Connection Modeling\n",
      "No match: FERNet Fine-grained Extraction and Reasoning Network for Emotion Recognition in Dialogues\n",
      "No match: Statistical User Simulation for Spoken Dialogue Systems What for, Which Data, Which Future\n",
      "No match: Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation\n",
      "No match: Diversifying Dialogue Generation with Non-Conversational Text\n",
      "No match: KdConv A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation\n",
      "No match: Meta-Reinforced Multi-Domain State Generator for Dialogue Systems\n",
      "No match: Modeling Long Context for Task-Oriented Dialogue State Generation\n",
      "No match: Multi-Domain Dialogue Acts and Response Co-Generation\n",
      "No match: Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning\n",
      "No match: On Quality Ratings for Spoken Dialogue Systems – Experts vs. Users\n",
      "No match: doc2dial A Goal-Oriented Document-Grounded Dialogue Dataset\n",
      "No match: Implicit Discourse Relation Identification for Open-domain Dialogues\n",
      "No match: Will I Sound Like Me Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness\n",
      "No match: TOD-BERT Pre-trained Natural Language Understanding for Task-Oriented Dialogue\n",
      "No match: A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions\n",
      "No match: RiSAWOZ A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling\n",
      "No match: Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking\n",
      "No match: Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness\n",
      "No match: Bootstrapping a Neural Conversational Agent with Dialogue Self-Play, Crowdsourcing and On-Line Reinforcement Learning\n",
      "No match: The Future of Spoken Dialogue Systems is in their Past Long-Term Adaptive, Conversational Assistants\n",
      "No match: “I Object!” Modeling Latent Pragmatic Effects in Courtroom Dialogues\n",
      "No match: Group-wise Contrastive Learning for Neural Dialogue Generation\n",
      "No match: Auto-Dialabel Labeling Dialogue Data with Unsupervised Learning\n",
      "No match: The World is Not Binary Learning to Rank with Grayscale Data for Dialogue Response Selection\n",
      "No match: GRADE Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems\n",
      "No match: MedDialog Large-scale Medical Dialogue Datasets\n",
      "No match: A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue\n",
      "No match: Actor-Double-Critic Incorporating Model-Based Critic for Task-Oriented Dialogue Systems\n",
      "No match: An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation\n",
      "No match: Goal-Embedded Dual Hierarchical Model for Task-Oriented Dialogue Generation\n",
      "No match: Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents\n",
      "No match: Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems\n",
      "No match: Unsupervised Dialogue Act Induction using Gaussian Mixtures\n",
      "No match: IRIS a Chat-oriented Dialogue System based on the Vector Space Model\n",
      "No match: SemEval 2018 Task 4 Character Identification on Multiparty Dialogues\n",
      "No match: AirConcierge Generating Task-Oriented Dialogue via Efficient Large-Scale Knowledge Retrieval\n",
      "No match: Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue\n",
      "No match: Pretrained Language Models for Dialogue Generation with Multiple Input Sources\n",
      "No match: A Generative Attentional Neural Network Model for Dialogue Act Classification\n",
      "No match: Classifying Dialogue Acts in One-on-One Live Chats\n",
      "No match: Semantic Grounding in Dialogue for Complex Problem Solving\n",
      "No match: SAIL-GRS Grammar Induction for Spoken Dialogue Systems using CF-IRF Rule Similarity\n",
      "No match: Automatic Dialogue Generation with Expressed Emotions\n",
      "No match: Deep Active Learning for Dialogue Generation\n",
      "No match: Dialogue-Oriented Review Summary Generation for Spoken Dialogue Recommendation Systems\n",
      "No match: Incremental Spoken Dialogue Systems Tools and Data\n",
      "No match: Learning to Adapt to Unknown Users Referring Expression Generation in Spoken Dialogue Systems\n",
      "No match: Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings\n",
      "No match: Autonomous Self-Assessment of Autocorrections Exploring Text Message Dialogues\n",
      "No match: Joint Modeling of Content and Discourse Relations in Dialogues\n",
      "No match: Exploring Convolutional and Recurrent Neural Networks in Sequential Labelling for Dialogue Topic Tracking\n",
      "No match: GCDST A Graph-based and Copy-augmented Multi-domain Dialogue State Tracking\n",
      "No match: Predicting and Eliciting Addressee’s Emotion in Online Dialogue\n",
      "No match: On the Linguistic Representational Power of Neural Machine Translation Models\n",
      "No match: Proceedings of the 15th International Conference on Spoken Language Translation\n",
      "No match: Proceedings of the 17th International Conference on Spoken Language Translation\n",
      "No match: Proceedings of the 3rd Workshop on Neural Generation and Translation\n",
      "No match: Proceedings of the 6th Workshop on Asian Translation\n",
      "No match: Proceedings of the 7th Workshop on Asian Translation\n",
      "No match: Proceedings of the Eighth Workshop on Statistical Machine Translation\n",
      "No match: Proceedings of the Fifth Conference on Machine Translation\n",
      "No match: Proceedings of the Fourth Conference on Machine Translation (Volume 1 Research Papers)\n",
      "No match: Proceedings of the Fourth Conference on Machine Translation (Volume 2 Shared Task Papers, Day 1)\n",
      "No match: Proceedings of the Fourth Conference on Machine Translation (Volume 3 Shared Task Papers, Day 2)\n",
      "No match: Proceedings of the Fourth Workshop on Neural Generation and Translation\n",
      "No match: Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR\n",
      "No match: Proceedings of the Ninth Workshop on Statistical Machine Translation\n",
      "No match: Proceedings of the Second Conference on Machine Translation\n",
      "No match: Proceedings of the Seventh Workshop on Statistical Machine Translation\n",
      "No match: Proceedings of the Sixth Workshop on Statistical Machine Translation\n",
      "No match: Proceedings of the Tenth Workshop on Statistical Machine Translation\n",
      "No match: Proceedings of the Third Conference on Machine Translation Research Papers\n",
      "No match: CMU’s Machine Translation System for IWSLT 2019\n",
      "No match: Customizing Neural Machine Translation for Subtitling\n",
      "No match: DBMS-KU Interpolation for WMT19 News Translation Task\n",
      "No match: Edinburgh’s Phrase-based Machine Translation Systems for WMT-14\n",
      "No match: English-Myanmar Supervised and Unsupervised NMT NICT’s Machine Translation Systems at WAT-2019\n",
      "No match: Improving Neural Machine Translation Models with Monolingual Data\n",
      "No match: Linguistically Motivated Subwords for English-Tamil Translation University of Groningen’s Submission to WMT-2020\n",
      "No match: Machine Translation Human Evaluation an investigation of evaluation based on Post-Editing and its relation with Direct Assessment\n",
      "No match: NICT‘s Submission To WAT 2020 How Effective Are Simple Many-To-Many Neural Machine Translation Models\n",
      "No match: Semantic Parsing as Machine Translation\n",
      "No match: SRPOL’s System for the IWSLT 2020 End-to-End Speech Translation Task\n",
      "No match: TMop a Tool for Unsupervised Translation Memory Cleaning\n",
      "No match: Tree as a Pivot Syntactic Matching Methods in Pivot Translation\n",
      "No match: Yandex School of Data Analysis Machine Translation Systems for WMT13\n",
      "No match: Extended Study on Using Pretrained Language Models and YiSi-1 for Machine Translation Evaluation\n",
      "No match: Learning to Translate in Real-time with Neural Machine Translation\n",
      "No match: Mixing Multiple Translation Models in Statistical Machine Translation\n",
      "No match: A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions\n",
      "No match: Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\n",
      "No match: Entity Projection via Machine Translation for Cross-Lingual NER\n",
      "No match: Hierarchical Chunk-to-String Translation\n",
      "No match: Machine Translation of Spanish Personal and Possessive Pronouns Using Anaphora Probabilities\n",
      "No match: Machine Translation Reference-less Evaluation using YiSi-2 with Bilingual Mappings of Massive Multilingual Language Model\n",
      "No match: Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization\n",
      "No match: SwitchOut an Efficient Data Augmentation Algorithm for Neural Machine Translation\n",
      "No match: Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder\n",
      "No match: Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism\n",
      "No match: Personalized Machine Translation Preserving Original Author Traits\n",
      "No match: Incorporating Structural Alignment Biases into an Attentional Neural Translation Model\n",
      "No match: Learning to Evaluate Translation Beyond English BLEURT Submissions to the WMT Metrics 2020 Shared Task\n",
      "No match: Morphological Modeling for Machine Translation of English-Iraqi Arabic Spoken Dialogs\n",
      "No match: SSMTA Machine Translation Evaluation View To Paragraph-to-Sentence Semantic Similarity\n",
      "No match: Continuous Adaptation to User Feedback for Statistical Machine Translation\n",
      "No match: Cross Language Text Classification by Model Translation and Semi-Supervised Learning\n",
      "No match: Rapid Adaptation of Neural Machine Translation to New Languages\n",
      "No match: Synthesizing Compound Words for Machine Translation\n",
      "No match: Towards a Better Evaluation of Metrics for Machine Translation\n",
      "No match: Unsupervised Training for Large Vocabulary Translation Using Sparse Lexicon and Word Classes\n",
      "No match: Adaptive Knowledge Sharing in Multi-Task Learning Improving Low-Resource Neural Machine Translation\n",
      "No match: Co-reference Resolution of Elided Subjects and Possessive Pronouns in Spanish-English Statistical Machine Translation\n",
      "No match: Compact Personalized Models for Neural Machine Translation\n",
      "No match: Incorporate Semantic Structures into Machine Translation Evaluation via UCCA\n",
      "No match: Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation\n",
      "No match: A Joint Sequence Translation Model with Integrated Reordering\n",
      "No match: FBK Cross-Lingual Textual Entailment Without Translation\n",
      "No match: Incorporating Global Visual Features into Attention-based Neural Machine Translation.\n",
      "No match: A Unified Model for Soft Linguistic Reordering Constraints in Statistical Machine Translation\n",
      "No match: Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation\n",
      "No match: Joint Language and Translation Modeling with Recurrent Neural Networks\n",
      "No match: Visualizing and Understanding Neural Machine Translation\n",
      "No match: Are Two Heads Better than One Crowdsourced Translation via a Two-Step Collaboration of Non-Professional Translators and Editors\n",
      "No match: Sentence Simplification by Monolingual Machine Translation\n",
      "No match: Document-Wide Decoding for Phrase-Based Statistical Machine Translation\n",
      "No match: ICT A Translation based Method for Cross-lingual Textual Entailment\n",
      "No match: Translation with Source Constituency and Dependency Trees\n",
      "No match: Assessing Back-Translation as a Corpus Generation Strategy for non-English Tasks A Study in Reading Comprehension and Word Sense Disambiguation\n",
      "No match: Blast A Tool for Error Analysis of Machine Translation Output\n",
      "No match: Deep architectures for Neural Machine Translation\n",
      "No match: DOMCAT A Bilingual Concordancer for Domain-Specific Computer Assisted Translation\n",
      "No match: EU-BRIDGE MT Combined Machine Translation\n",
      "No match: Graph-Based Translation Via Graph Segmentation\n",
      "No match: Integration of Dubbing Constraints into Machine Translation\n",
      "No match: Lattice Desegmentation for Statistical Machine Translation\n",
      "No match: Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings\n",
      "No match: Referential Cohesion A Challenge for Machine Translation Evaluation\n",
      "No match: RTM-DCU Predicting Semantic Similarity with Referential Translation Machines\n",
      "No match: SuperNMT Neural Machine Translation with Semantic Supersenses and Syntactic Supertags\n",
      "No match: The Karlsruhe Institute of Technology Translation Systems for the WMT 2013\n",
      "No match: The LIG system for the English-Czech Text Translation Task of IWSLT 2019\n",
      "No match: The TALP-UPC System Description for WMT20 News Translation Task Multilingual Adaptation for Low Resource MT\n",
      "No match: The University of Helsinki Submission to the IWSLT2020 Offline SpeechTranslation Task\n",
      "No match: The Unreasonable Volatility of Neural Machine Translation Models\n",
      "No match: The USTC-NEL Speech Translation system at IWSLT 2018\n",
      "No match: Tuning Phrase-Based Segmented Translation for a Morphologically Complex Target Language\n",
      "No match: UCSMNLP Statistical Machine Translation for WAT 2019\n",
      "No match: Unsupervised Neural Machine Translation for English and Manipuri\n",
      "No match: Zero-Resource Neural Machine Translation with Monolingual Pivot Data\n",
      "No match: An Attentional Model for Speech Translation Without Transcription\n",
      "No match: Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation\n",
      "No match: Monolingual Marginal Matching for Translation Model Adaptation\n",
      "No match: SAGAN A Machine Translation Approach for Cross-Lingual Textual Entailment\n",
      "No match: Efficient Left-to-Right Hierarchical Phrase-Based Translation with Improved Reordering\n",
      "No match: Information Density and Quality Estimation Features as Translationese Indicators for Human Translation Classification\n",
      "No match: Online Relative Margin Maximization for Statistical Machine Translation\n",
      "No match: Polylingual Tree-Based Topic Models for Translation Domain Adaptation\n",
      "No match: A Systematic Exploration of Diversity in Machine Translation\n",
      "No match: Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation\n",
      "No match: Interpretese vs. Translationese The Uniqueness of Human Strategies in Simultaneous Interpretation\n",
      "No match: LSTM Neural Reordering Feature for Statistical Machine Translation\n",
      "No match: A Novel Approach to Dropped Pronoun Translation\n",
      "No match: Multi-Hypothesis Machine Translation Evaluation\n",
      "No match: PATQUEST Papago Translation Quality Estimation\n",
      "No match: Adaptation of Reordering Models for Statistical Machine Translation\n",
      "No match: Multimodal Quality Estimation for Machine Translation\n",
      "No match: A Stochastic Decoder for Neural Machine Translation\n",
      "No match: Combining Character and Word Information in Neural Machine Translation Using a Multi-Level Attention\n",
      "No match: Forest-Based Neural Machine Translation\n",
      "No match: When a Good Translation is Wrong in Context Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion\n",
      "No match: A Compact and Language-Sensitive Multilingual Translation Method\n",
      "No match: Context-Aware Neural Machine Translation Learns Anaphora Resolution\n",
      "No match: Document Context Neural Machine Translation with Memory Networks\n",
      "No match: Evaluating Discourse Phenomena in Neural Machine Translation\n",
      "No match: It Depends on the Translation Unsupervised Dependency Parsing via Word Alignment\n",
      "No match: Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations\n",
      "No match: Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation\n",
      "No match: An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task\n",
      "No match: Bilingually-constrained Phrase Embeddings for Machine Translation\n",
      "No match: E-rating Machine Translation\n",
      "No match: Lightly-Supervised Word Sense Translation Error Detection for an Interactive Conversational Spoken Language Translation System\n",
      "No match: Multimodal Neural Machine Translation for English to Hindi\n",
      "No match: Neural Poetry Translation\n",
      "No match: NTT Neural Machine Translation Systems at WAT 2019\n",
      "No match: On the use of BERT for Neural Machine Translation\n",
      "No match: Phrasal A Toolkit for New Directions in Statistical Machine Translation\n",
      "No match: Samsung’s System for the IWSLT 2019 End-to-End Speech Translation Task\n",
      "No match: SOURCE SOURce-Conditional Elmo-style Model for Machine Translation Quality Estimation\n",
      "No match: Speech-Enabled Hybrid Multilingual Translation for Mobile Devices\n",
      "No match: The ADAPT System Description for the IWSLT 2018 Basque to English Translation Task\n",
      "No match: The RWTH Aachen Machine Translation System for WMT 2010\n",
      "No match: The TALP-UPC Machine Translation Systems for WMT19 News Translation Task Pivoting Techniques for Low Resource MT\n",
      "No match: Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation\n",
      "No match: TÜBİTAK-BİLGEM German-English Machine Translation Systems for W13\n",
      "No match: Widening the Representation Bottleneck in Neural Machine Translation with Lexical Shortcuts\n",
      "No match: Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition\n",
      "No match: Adaptation Data Selection using Neural Language Models Experiments in Machine Translation\n",
      "No match: Competence-based Curriculum Learning for Neural Machine Translation\n",
      "No match: Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation\n",
      "No match: Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRemoteDisconnected\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[1;32m--> 714\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    715\u001B[0m     conn,\n\u001B[0;32m    716\u001B[0m     method,\n\u001B[0;32m    717\u001B[0m     url,\n\u001B[0;32m    718\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[0;32m    719\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[0;32m    720\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m    721\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    722\u001B[0m )\n\u001B[0;32m    724\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[0;32m    726\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[0;32m    727\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    462\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m             \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    464\u001B[0m             \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m             \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m--> 466\u001B[0m             six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 461\u001B[0m     httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m     \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m     \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1377\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1378\u001B[0m     response\u001B[38;5;241m.\u001B[39mbegin()\n\u001B[0;32m   1379\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_status()\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:287\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m line:\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Presumably, the server closed the connection before\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# sending a valid response.\u001B[39;00m\n\u001B[1;32m--> 287\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RemoteDisconnected(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRemote end closed connection without\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    288\u001B[0m                              \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m response\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mRemoteDisconnected\u001B[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mProtocolError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[0;32m    487\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    488\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m    489\u001B[0m         body\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mbody,\n\u001B[0;32m    490\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    491\u001B[0m         redirect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    492\u001B[0m         assert_same_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    493\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    494\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    495\u001B[0m         retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries,\n\u001B[0;32m    496\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    497\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    498\u001B[0m     )\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:798\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    796\u001B[0m     e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, e)\n\u001B[1;32m--> 798\u001B[0m retries \u001B[38;5;241m=\u001B[39m retries\u001B[38;5;241m.\u001B[39mincrement(\n\u001B[0;32m    799\u001B[0m     method, url, error\u001B[38;5;241m=\u001B[39me, _pool\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, _stacktrace\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mexc_info()[\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m    800\u001B[0m )\n\u001B[0;32m    801\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:550\u001B[0m, in \u001B[0;36mRetry.increment\u001B[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[0;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m read \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_method_retryable(method):\n\u001B[1;32m--> 550\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m six\u001B[38;5;241m.\u001B[39mreraise(\u001B[38;5;28mtype\u001B[39m(error), error, _stacktrace)\n\u001B[0;32m    551\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m read \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\packages\\six.py:769\u001B[0m, in \u001B[0;36mreraise\u001B[1;34m(tp, value, tb)\u001B[0m\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m value\u001B[38;5;241m.\u001B[39m__traceback__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tb:\n\u001B[1;32m--> 769\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m value\u001B[38;5;241m.\u001B[39mwith_traceback(tb)\n\u001B[0;32m    770\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m value\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[1;32m--> 714\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    715\u001B[0m     conn,\n\u001B[0;32m    716\u001B[0m     method,\n\u001B[0;32m    717\u001B[0m     url,\n\u001B[0;32m    718\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[0;32m    719\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[0;32m    720\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m    721\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    722\u001B[0m )\n\u001B[0;32m    724\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[0;32m    726\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[0;32m    727\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    462\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m             \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    464\u001B[0m             \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m             \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m--> 466\u001B[0m             six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 461\u001B[0m     httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m     \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m     \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1377\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1378\u001B[0m     response\u001B[38;5;241m.\u001B[39mbegin()\n\u001B[0;32m   1379\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_status()\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:287\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m line:\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Presumably, the server closed the connection before\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# sending a valid response.\u001B[39;00m\n\u001B[1;32m--> 287\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RemoteDisconnected(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRemote end closed connection without\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    288\u001B[0m                              \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m response\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mProtocolError\u001B[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 7\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, row \u001B[38;5;129;01min\u001B[39;00m paper_mapping\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSKG\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOVEL\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m----> 7\u001B[0m         meta \u001B[38;5;241m=\u001B[39m get_openalex_metadata_exact(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m meta:\n\u001B[0;32m     10\u001B[0m             metadata_records\u001B[38;5;241m.\u001B[39mappend({\n\u001B[0;32m     11\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrow,\n\u001B[0;32m     12\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmeta\n\u001B[0;32m     13\u001B[0m             })\n",
      "Cell \u001B[1;32mIn[38], line 16\u001B[0m, in \u001B[0;36mget_openalex_metadata_exact\u001B[1;34m(title)\u001B[0m\n\u001B[0;32m      9\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://api.openalex.org/works\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     11\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle.search:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtitle\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmailto\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkulsparsh2005@gmail.com\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     14\u001B[0m }\n\u001B[1;32m---> 16\u001B[0m r \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(url, params\u001B[38;5;241m=\u001B[39mparams)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m     19\u001B[0m     results \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mjson()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001B[0m, in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m request(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, params\u001B[38;5;241m=\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[0;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:501\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[0;32m    487\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    488\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    497\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    498\u001B[0m     )\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m--> 501\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[0;32m    503\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m MaxRetryError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    504\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, ConnectTimeoutError):\n\u001B[0;32m    505\u001B[0m         \u001B[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001B[39;00m\n",
      "\u001B[1;31mConnectionError\u001B[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T17:27:47.375104Z",
     "start_time": "2026-02-18T17:27:47.335901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"ScientificNoveltyProject/1.0 (your_email@gmail.com)\"\n",
    "})"
   ],
   "id": "cab3656175623214",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T17:58:47.989839Z",
     "start_time": "2026-02-18T17:58:47.975830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def get_openalex_metadata_exact(title, retries=5):\n",
    "\n",
    "    url = \"https://api.openalex.org/works\"\n",
    "\n",
    "    params = {\n",
    "        \"filter\": f'title.search:\"{title}\"',\n",
    "        \"mailto\": \"your_email@gmail.com\"\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = session.get(url, params=params, timeout=15)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                results = r.json()[\"results\"]\n",
    "\n",
    "                if len(results) > 0:\n",
    "                    best = results[0]\n",
    "\n",
    "                    if similar(title, best[\"display_name\"]) > 0.85:\n",
    "                        return {\n",
    "                            \"openalex_id\": best.get(\"id\"),\n",
    "                            \"publication_year\": 2021,\n",
    "                            \"citation_count\": best.get(\"cited_by_count\"),\n",
    "                            \"referenced_works\": best.get(\"referenced_works\")\n",
    "                        }\n",
    "                return None\n",
    "\n",
    "            elif r.status_code == 429:\n",
    "                # Rate limited\n",
    "                wait = 2 ** attempt\n",
    "                print(\"Rate limited. Sleeping:\", wait)\n",
    "                time.sleep(wait)\n",
    "\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"Error: {e}. Retrying in {wait}s\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    return None"
   ],
   "id": "91bd9fb4cc04191e",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:32:17.634315Z",
     "start_time": "2026-02-18T18:30:31.243331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metadata_records = []\n",
    "\n",
    "for i, row in enumerate(paper_mapping.iterrows()):\n",
    "\n",
    "    if row[1][\"split\"] in [\"SKG\", \"NOVEL\"]:\n",
    "        meta = get_openalex_metadata_exact(row[1][\"title\"])\n",
    "    else:\n",
    "        meta = {\n",
    "            \"openalex_id\": None,\n",
    "            \"publication_year\": 2021,\n",
    "            \"citation_count\": 0,\n",
    "            \"referenced_works\": []\n",
    "        }\n",
    "\n",
    "    metadata_records.append({**row[1], **(meta or {})})\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"Processed:\", i)\n",
    "        pd.DataFrame(metadata_records).to_csv(\"partial_metadata.csv\", index=False)\n",
    "\n",
    "    # time.sleep(0.3)"
   ],
   "id": "d4c027dc44d2259",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0\n",
      "Processed: 50\n",
      "Rate limited. Sleeping: 1\n",
      "Rate limited. Sleeping: 2\n",
      "Rate limited. Sleeping: 4\n",
      "Rate limited. Sleeping: 8\n",
      "Rate limited. Sleeping: 16\n",
      "Rate limited. Sleeping: 1\n",
      "Rate limited. Sleeping: 2\n",
      "Rate limited. Sleeping: 4\n",
      "Rate limited. Sleeping: 8\n",
      "Rate limited. Sleeping: 16\n",
      "Rate limited. Sleeping: 1\n",
      "Rate limited. Sleeping: 2\n",
      "Rate limited. Sleeping: 4\n",
      "Rate limited. Sleeping: 8\n",
      "Rate limited. Sleeping: 16\n",
      "Rate limited. Sleeping: 1\n",
      "Rate limited. Sleeping: 2\n",
      "Rate limited. Sleeping: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, row \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(paper_mapping\u001B[38;5;241m.\u001B[39miterrows()):\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m row[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSKG\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOVEL\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m----> 6\u001B[0m         meta \u001B[38;5;241m=\u001B[39m get_openalex_metadata_exact(row[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m      8\u001B[0m         meta \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      9\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenalex_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     10\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpublication_year\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m2021\u001B[39m,\n\u001B[0;32m     11\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcitation_count\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m     12\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreferenced_works\u001B[39m\u001B[38;5;124m\"\u001B[39m: []\n\u001B[0;32m     13\u001B[0m         }\n",
      "Cell \u001B[1;32mIn[45], line 39\u001B[0m, in \u001B[0;36mget_openalex_metadata_exact\u001B[1;34m(title, retries)\u001B[0m\n\u001B[0;32m     37\u001B[0m     wait \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m attempt\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRate limited. Sleeping:\u001B[39m\u001B[38;5;124m\"\u001B[39m, wait)\n\u001B[1;32m---> 39\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(wait)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:35:44.790526Z",
     "start_time": "2026-02-18T18:35:44.781448Z"
    }
   },
   "cell_type": "code",
   "source": "print(paper_mapping[\"split\"].unique())",
   "id": "a9384e8c714a3f14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BLOG' 'SKG']\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:36:47.176846Z",
     "start_time": "2026-02-18T18:36:47.167862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "novel_files = glob.glob(\"*/Triplets/Novel_Papers/**/*.csv\", recursive=True)\n",
    "print(len(novel_files))\n",
    "print(novel_files[:5])"
   ],
   "id": "84d37bbaae4b5106",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['Scientific_Novelty_Detection\\\\Triplets\\\\Novel_Papers\\\\Dia2021_triplets.csv', 'Scientific_Novelty_Detection\\\\Triplets\\\\Novel_Papers\\\\MT2021_triplets.csv', 'Scientific_Novelty_Detection\\\\Triplets\\\\Novel_Papers\\\\QA2021_triplets.csv', 'Scientific_Novelty_Detection\\\\Triplets\\\\Novel_Papers\\\\SA2021_triplets.csv', 'Scientific_Novelty_Detection\\\\Triplets\\\\Novel_Papers\\\\Sum2021_triplets.csv']\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:37:25.394102Z",
     "start_time": "2026-02-18T18:37:25.389104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.listdir(\"Scientific_Novelty_Detection/stanza_files/Novel_Papers\"))"
   ],
   "id": "df4e47b07cd5038f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dia2021', 'MT2021', 'QA2021', 'SA2021', 'Sum2021']\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:34:22.218119Z",
     "start_time": "2026-02-18T18:34:22.206444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "novel_mapping = paper_mapping[\n",
    "    paper_mapping[\"split\"] == \"NOVEL\"\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "print(\"Total NOVEL papers:\", len(novel_mapping))"
   ],
   "id": "8f8e2f31a77300b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NOVEL papers: 0\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:34:42.537029Z",
     "start_time": "2026-02-18T18:34:42.526731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"ScientificNoveltyProject/1.0 (your_email@gmail.com)\"\n",
    "})"
   ],
   "id": "85858f7f695badc6",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:34:49.454401Z",
     "start_time": "2026-02-18T18:34:49.447401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def get_openalex_2021(title):\n",
    "    url = \"https://api.openalex.org/works\"\n",
    "    params = {\n",
    "        \"filter\": f'title.search:\"{title}\"',\n",
    "        \"mailto\": \"your_email@gmail.com\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = session.get(url, params=params, timeout=15)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            results = r.json()[\"results\"]\n",
    "\n",
    "            if len(results) > 0:\n",
    "                best = results[0]\n",
    "\n",
    "                if (\n",
    "                        best.get(\"publication_year\") == 2021\n",
    "                        and similar(title, best[\"display_name\"]) > 0.85\n",
    "                ):\n",
    "                    return {\n",
    "                        \"openalex_id\": best.get(\"id\"),\n",
    "                        \"publication_year\": 2021,\n",
    "                        \"citation_count\": best.get(\"cited_by_count\"),\n",
    "                        \"referenced_works\": best.get(\"referenced_works\")\n",
    "                    }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    return None"
   ],
   "id": "9b8bdc38fb9e52f7",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:34:56.004651Z",
     "start_time": "2026-02-18T18:34:55.996781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metadata_records = []\n",
    "\n",
    "for i, row in novel_mapping.iterrows():\n",
    "\n",
    "    meta = get_openalex_2021(row[\"title\"])\n",
    "\n",
    "    if meta:\n",
    "        metadata_records.append({\n",
    "            **row,\n",
    "            **meta\n",
    "        })\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(\"Processed:\", i)\n",
    "\n",
    "    time.sleep(0.5)  # slower = safer"
   ],
   "id": "9d4adbd82181fe57",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T18:35:01.521645Z",
     "start_time": "2026-02-18T18:35:01.516736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paper_metadata = pd.DataFrame(metadata_records)\n",
    "\n",
    "print(\"Matched 2021 papers:\", len(paper_metadata))"
   ],
   "id": "eaa3cc5caf4ed313",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 2021 papers: 0\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T03:12:15.408583Z",
     "start_time": "2026-02-19T03:12:15.361159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"ScientificNoveltyProject/1.0 (your_email@gmail.com)\"\n",
    "})\n",
    "\n",
    "def download_openalex_year(year):\n",
    "\n",
    "    works = []\n",
    "    cursor = \"*\"\n",
    "\n",
    "    while True:\n",
    "\n",
    "        url = \"https://api.openalex.org/works\"\n",
    "\n",
    "        params = {\n",
    "            \"filter\": f\"publication_year:{year}\",\n",
    "            \"per-page\": 200,\n",
    "            \"cursor\": cursor,\n",
    "            \"mailto\": \"your_email@gmail.com\"\n",
    "        }\n",
    "\n",
    "        r = session.get(url, params=params)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print(\"Error:\", r.status_code)\n",
    "            break\n",
    "\n",
    "        data = r.json()\n",
    "        results = data[\"results\"]\n",
    "\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        for w in results:\n",
    "            works.append({\n",
    "                \"openalex_id\": w.get(\"id\"),\n",
    "                \"title\": w.get(\"display_name\"),\n",
    "                \"publication_year\": w.get(\"publication_year\"),\n",
    "                \"citation_count\": w.get(\"cited_by_count\"),\n",
    "                \"referenced_works\": w.get(\"referenced_works\")\n",
    "            })\n",
    "\n",
    "        cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "\n",
    "        if not cursor:\n",
    "            break\n",
    "\n",
    "        print(\"Downloaded:\", len(works))\n",
    "        time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame(works)"
   ],
   "id": "eae44a5392b29993",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T03:32:39.571649Z",
     "start_time": "2026-02-19T03:12:38.481714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "years = list(range(2015, 2022))\n",
    "\n",
    "all_openalex = []\n",
    "\n",
    "for y in years:\n",
    "    print(\"Downloading year:\", y)\n",
    "    df_year = download_openalex_year(y)\n",
    "    all_openalex.append(df_year)\n",
    "\n",
    "openalex_df = pd.concat(all_openalex, ignore_index=True)\n",
    "\n",
    "openalex_df.to_csv(\"openalex_bulk_2015_2021.csv\", index=False)\n",
    "\n",
    "print(\"Total works downloaded:\", len(openalex_df))"
   ],
   "id": "893c250aeb35975e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading year: 2015\n",
      "Downloaded: 200\n",
      "Downloaded: 400\n",
      "Downloaded: 600\n",
      "Downloaded: 800\n",
      "Downloaded: 1000\n",
      "Downloaded: 1200\n",
      "Downloaded: 1400\n",
      "Downloaded: 1600\n",
      "Downloaded: 1800\n",
      "Downloaded: 2000\n",
      "Downloaded: 2200\n",
      "Downloaded: 2400\n",
      "Downloaded: 2600\n",
      "Downloaded: 2800\n",
      "Downloaded: 3000\n",
      "Downloaded: 3200\n",
      "Downloaded: 3400\n",
      "Downloaded: 3600\n",
      "Downloaded: 3800\n",
      "Downloaded: 4000\n",
      "Downloaded: 4200\n",
      "Downloaded: 4400\n",
      "Downloaded: 4600\n",
      "Downloaded: 4800\n",
      "Downloaded: 5000\n",
      "Downloaded: 5200\n",
      "Downloaded: 5400\n",
      "Downloaded: 5600\n",
      "Downloaded: 5800\n",
      "Downloaded: 6000\n",
      "Downloaded: 6200\n",
      "Downloaded: 6400\n",
      "Downloaded: 6600\n",
      "Downloaded: 6800\n",
      "Downloaded: 7000\n",
      "Downloaded: 7200\n",
      "Downloaded: 7400\n",
      "Downloaded: 7600\n",
      "Downloaded: 7800\n",
      "Downloaded: 8000\n",
      "Downloaded: 8200\n",
      "Downloaded: 8400\n",
      "Downloaded: 8600\n",
      "Downloaded: 8800\n",
      "Downloaded: 9000\n",
      "Downloaded: 9200\n",
      "Downloaded: 9400\n",
      "Downloaded: 9600\n",
      "Downloaded: 9800\n",
      "Downloaded: 10000\n",
      "Downloaded: 10200\n",
      "Downloaded: 10400\n",
      "Downloaded: 10600\n",
      "Downloaded: 10800\n",
      "Downloaded: 11000\n",
      "Downloaded: 11200\n",
      "Downloaded: 11400\n",
      "Downloaded: 11600\n",
      "Downloaded: 11800\n",
      "Downloaded: 12000\n",
      "Downloaded: 12200\n",
      "Downloaded: 12400\n",
      "Downloaded: 12600\n",
      "Downloaded: 12800\n",
      "Downloaded: 13000\n",
      "Downloaded: 13200\n",
      "Downloaded: 13400\n",
      "Downloaded: 13600\n",
      "Downloaded: 13800\n",
      "Downloaded: 14000\n",
      "Downloaded: 14200\n",
      "Downloaded: 14400\n",
      "Downloaded: 14600\n",
      "Downloaded: 14800\n",
      "Downloaded: 15000\n",
      "Downloaded: 15200\n",
      "Downloaded: 15400\n",
      "Downloaded: 15600\n",
      "Downloaded: 15800\n",
      "Downloaded: 16000\n",
      "Downloaded: 16200\n",
      "Downloaded: 16400\n",
      "Downloaded: 16600\n",
      "Downloaded: 16800\n",
      "Downloaded: 17000\n",
      "Downloaded: 17200\n",
      "Downloaded: 17400\n",
      "Downloaded: 17600\n",
      "Downloaded: 17800\n",
      "Downloaded: 18000\n",
      "Downloaded: 18200\n",
      "Downloaded: 18400\n",
      "Downloaded: 18600\n",
      "Downloaded: 18800\n",
      "Downloaded: 19000\n",
      "Downloaded: 19200\n",
      "Downloaded: 19400\n",
      "Downloaded: 19600\n",
      "Downloaded: 19800\n",
      "Downloaded: 20000\n",
      "Downloaded: 20200\n",
      "Downloaded: 20400\n",
      "Downloaded: 20600\n",
      "Downloaded: 20800\n",
      "Downloaded: 21000\n",
      "Downloaded: 21200\n",
      "Downloaded: 21400\n",
      "Downloaded: 21600\n",
      "Downloaded: 21800\n",
      "Downloaded: 22000\n",
      "Downloaded: 22200\n",
      "Downloaded: 22400\n",
      "Downloaded: 22600\n",
      "Downloaded: 22800\n",
      "Downloaded: 23000\n",
      "Downloaded: 23200\n",
      "Downloaded: 23400\n",
      "Downloaded: 23600\n",
      "Downloaded: 23800\n",
      "Downloaded: 24000\n",
      "Downloaded: 24200\n",
      "Downloaded: 24400\n",
      "Downloaded: 24600\n",
      "Downloaded: 24800\n",
      "Downloaded: 25000\n",
      "Downloaded: 25200\n",
      "Downloaded: 25400\n",
      "Downloaded: 25600\n",
      "Downloaded: 25800\n",
      "Downloaded: 26000\n",
      "Downloaded: 26200\n",
      "Downloaded: 26400\n",
      "Downloaded: 26600\n",
      "Downloaded: 26800\n",
      "Downloaded: 27000\n",
      "Downloaded: 27200\n",
      "Downloaded: 27400\n",
      "Downloaded: 27600\n",
      "Downloaded: 27800\n",
      "Downloaded: 28000\n",
      "Downloaded: 28200\n",
      "Downloaded: 28400\n",
      "Downloaded: 28600\n",
      "Downloaded: 28800\n",
      "Downloaded: 29000\n",
      "Downloaded: 29200\n",
      "Downloaded: 29400\n",
      "Downloaded: 29600\n",
      "Downloaded: 29800\n",
      "Downloaded: 30000\n",
      "Downloaded: 30200\n",
      "Downloaded: 30400\n",
      "Downloaded: 30600\n",
      "Downloaded: 30800\n",
      "Downloaded: 31000\n",
      "Downloaded: 31200\n",
      "Downloaded: 31400\n",
      "Downloaded: 31600\n",
      "Downloaded: 31800\n",
      "Downloaded: 32000\n",
      "Downloaded: 32200\n",
      "Downloaded: 32400\n",
      "Downloaded: 32600\n",
      "Downloaded: 32800\n",
      "Downloaded: 33000\n",
      "Downloaded: 33200\n",
      "Downloaded: 33400\n",
      "Downloaded: 33600\n",
      "Downloaded: 33800\n",
      "Downloaded: 34000\n",
      "Downloaded: 34200\n",
      "Downloaded: 34400\n",
      "Downloaded: 34600\n",
      "Downloaded: 34800\n",
      "Downloaded: 35000\n",
      "Downloaded: 35200\n",
      "Downloaded: 35400\n",
      "Downloaded: 35600\n",
      "Downloaded: 35800\n",
      "Downloaded: 36000\n",
      "Downloaded: 36200\n",
      "Downloaded: 36400\n",
      "Downloaded: 36600\n",
      "Downloaded: 36800\n",
      "Downloaded: 37000\n",
      "Downloaded: 37200\n",
      "Downloaded: 37400\n",
      "Downloaded: 37600\n",
      "Downloaded: 37800\n",
      "Downloaded: 38000\n",
      "Downloaded: 38200\n",
      "Downloaded: 38400\n",
      "Downloaded: 38600\n",
      "Downloaded: 38800\n",
      "Downloaded: 39000\n",
      "Downloaded: 39200\n",
      "Downloaded: 39400\n",
      "Downloaded: 39600\n",
      "Downloaded: 39800\n",
      "Downloaded: 40000\n",
      "Downloaded: 40200\n",
      "Downloaded: 40400\n",
      "Downloaded: 40600\n",
      "Downloaded: 40800\n",
      "Downloaded: 41000\n",
      "Downloaded: 41200\n",
      "Downloaded: 41400\n",
      "Downloaded: 41600\n",
      "Downloaded: 41800\n",
      "Downloaded: 42000\n",
      "Downloaded: 42200\n",
      "Downloaded: 42400\n",
      "Downloaded: 42600\n",
      "Downloaded: 42800\n",
      "Downloaded: 43000\n",
      "Downloaded: 43200\n",
      "Downloaded: 43400\n",
      "Downloaded: 43600\n",
      "Downloaded: 43800\n",
      "Downloaded: 44000\n",
      "Downloaded: 44200\n",
      "Downloaded: 44400\n",
      "Downloaded: 44600\n",
      "Downloaded: 44800\n",
      "Downloaded: 45000\n",
      "Downloaded: 45200\n",
      "Downloaded: 45400\n",
      "Downloaded: 45600\n",
      "Downloaded: 45800\n",
      "Downloaded: 46000\n",
      "Downloaded: 46200\n",
      "Downloaded: 46400\n",
      "Downloaded: 46600\n",
      "Downloaded: 46800\n",
      "Downloaded: 47000\n",
      "Downloaded: 47200\n",
      "Downloaded: 47400\n",
      "Downloaded: 47600\n",
      "Downloaded: 47800\n",
      "Downloaded: 48000\n",
      "Downloaded: 48200\n",
      "Downloaded: 48400\n",
      "Downloaded: 48600\n",
      "Downloaded: 48800\n",
      "Downloaded: 49000\n",
      "Downloaded: 49200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[67], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m years:\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading year:\u001B[39m\u001B[38;5;124m\"\u001B[39m, y)\n\u001B[1;32m----> 7\u001B[0m     df_year \u001B[38;5;241m=\u001B[39m download_openalex_year(y)\n\u001B[0;32m      8\u001B[0m     all_openalex\u001B[38;5;241m.\u001B[39mappend(df_year)\n\u001B[0;32m     10\u001B[0m openalex_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(all_openalex, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[66], line 26\u001B[0m, in \u001B[0;36mdownload_openalex_year\u001B[1;34m(year)\u001B[0m\n\u001B[0;32m     17\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://api.openalex.org/works\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     19\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpublication_year:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00myear\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mper-page\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m200\u001B[39m,\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcursor\u001B[39m\u001B[38;5;124m\"\u001B[39m: cursor,\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmailto\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_email@gmail.com\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     24\u001B[0m }\n\u001B[1;32m---> 26\u001B[0m r \u001B[38;5;241m=\u001B[39m session\u001B[38;5;241m.\u001B[39mget(url, params\u001B[38;5;241m=\u001B[39mparams)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError:\u001B[39m\u001B[38;5;124m\"\u001B[39m, r\u001B[38;5;241m.\u001B[39mstatus_code)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001B[0m, in \u001B[0;36mSession.get\u001B[1;34m(self, url, **kwargs)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[0;32m    595\u001B[0m \n\u001B[0;32m    596\u001B[0m \u001B[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m    597\u001B[0m \u001B[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001B[39;00m\n\u001B[0;32m    598\u001B[0m \u001B[38;5;124;03m:rtype: requests.Response\u001B[39;00m\n\u001B[0;32m    599\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    601\u001B[0m kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 602\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:747\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    744\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[1;32m--> 747\u001B[0m     r\u001B[38;5;241m.\u001B[39mcontent\n\u001B[0;32m    749\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:899\u001B[0m, in \u001B[0;36mResponse.content\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    897\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 899\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_content(CONTENT_CHUNK_SIZE)) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    902\u001B[0m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[0;32m    903\u001B[0m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[1;34m()\u001B[0m\n\u001B[0;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    815\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 816\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    817\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    818\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:624\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m    608\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    609\u001B[0m \u001B[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001B[39;00m\n\u001B[0;32m    610\u001B[0m \u001B[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;124;03m    'content-encoding' header.\u001B[39;00m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunked \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupports_chunked_reads():\n\u001B[1;32m--> 624\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_chunked(amt, decode_content\u001B[38;5;241m=\u001B[39mdecode_content):\n\u001B[0;32m    625\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m line\n\u001B[0;32m    626\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:828\u001B[0m, in \u001B[0;36mHTTPResponse.read_chunked\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m    825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 828\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_chunk_length()\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_left \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    830\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:758\u001B[0m, in \u001B[0;36mHTTPResponse._update_chunk_length\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    756\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_left \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    757\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 758\u001B[0m line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline()\n\u001B[0;32m    759\u001B[0m line \u001B[38;5;241m=\u001B[39m line\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    760\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\ssl.py:1278\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1274\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1275\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1276\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1277\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread(nbytes, buffer)\n\u001B[0;32m   1279\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1280\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\ssl.py:1134\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1133\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1134\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[0;32m   1135\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T05:53:58.221996Z",
     "start_time": "2026-02-19T05:53:35.086440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# ============================================\n",
    "# PART 1: Load SciND Papers and Extract Titles\n",
    "# ============================================\n",
    "\n",
    "print(\"📚 Loading SciND papers...\")\n",
    "triplet_files = glob.glob(\"*/Triplets/**/*.csv\", recursive=True)\n",
    "\n",
    "mapping_records = []\n",
    "\n",
    "for file in triplet_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Identify split\n",
    "    if \"SKG\" in file:\n",
    "        split = \"SKG\"\n",
    "        stanza_split = \"SKG_Papers\"\n",
    "    elif \"Novel_Papers\" in file:\n",
    "        split = \"NOVEL\"\n",
    "        stanza_split = \"Novel_Papers\"\n",
    "    elif \"Blogs\" in file:\n",
    "        split = \"BLOG\"\n",
    "        stanza_split = \"Blogs\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    filename = os.path.basename(file)\n",
    "    domain_raw = filename.replace(\"_triplets.csv\", \"\")\n",
    "\n",
    "    # BLOG domain cleanup\n",
    "    if split == \"BLOG\":\n",
    "        domain = domain_raw.replace(\"_Blogs\", \"\")\n",
    "    else:\n",
    "        domain = domain_raw\n",
    "\n",
    "    paper_ids = sorted(df[\"paper_ID\"].unique())\n",
    "\n",
    "    pattern = f\"*/stanza_files/{stanza_split}/{domain}/*-Stanza-out.txt\"\n",
    "    directory = glob.glob(pattern)\n",
    "\n",
    "    for pid in paper_ids:\n",
    "        if pid < len(directory):\n",
    "            filepath = directory[pid]\n",
    "            fname = os.path.basename(filepath)\n",
    "\n",
    "            title = fname.replace(\"-Stanza-out.txt\", \"\")\n",
    "            title = re.sub(r\"^\\d+\\.\", \"\", title).strip()\n",
    "\n",
    "            mapping_records.append({\n",
    "                \"split\": split,\n",
    "                \"domain\": domain,\n",
    "                \"paper_ID\": pid,\n",
    "                \"title\": title\n",
    "            })\n",
    "\n",
    "paper_mapping = pd.DataFrame(mapping_records)\n",
    "print(f\"✅ Loaded {len(paper_mapping)} papers\")\n",
    "print(f\"   Split distribution:\\n{paper_mapping['split'].value_counts()}\")\n",
    "\n",
    "# Keep only scholarly papers (SKG and NOVEL)\n",
    "paper_mapping = paper_mapping[\n",
    "    paper_mapping[\"split\"].isin([\"SKG\", \"NOVEL\"])\n",
    "].reset_index(drop=True)\n",
    "print(f\"✅ Keeping {len(paper_mapping)} scholarly papers\")\n",
    "\n",
    "# ============================================\n",
    "# PART 2: Smart OpenAlex API Client\n",
    "# ============================================\n",
    "\n",
    "class OpenAlexClient:\n",
    "    def __init__(self, email=\"=kulsparsh2005@gmail.com\", delay=0.1):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": f\"ScientificNoveltyProject/1.0 ({email})\"\n",
    "        })\n",
    "        self.email = email\n",
    "        self.delay = delay\n",
    "        self.last_request = 0\n",
    "        self.stats = {\n",
    "            \"total\": 0,\n",
    "            \"success\": 0,\n",
    "            \"rate_limited\": 0,\n",
    "            \"not_found\": 0\n",
    "        }\n",
    "\n",
    "    def _wait_for_rate_limit(self):\n",
    "        \"\"\"Ensure we don't exceed 10 requests per second\"\"\"\n",
    "        elapsed = time.time() - self.last_request\n",
    "        if elapsed < self.delay:\n",
    "            time.sleep(self.delay - elapsed)\n",
    "\n",
    "    def normalize_title(self, title):\n",
    "        \"\"\"Normalize title for better matching\"\"\"\n",
    "        # Remove special characters, extra spaces\n",
    "        title = re.sub(r'[^\\w\\s]', ' ', title.lower())\n",
    "        title = re.sub(r'\\s+', ' ', title).strip()\n",
    "        return title\n",
    "\n",
    "    def similarity(self, a, b):\n",
    "        \"\"\"Compute title similarity\"\"\"\n",
    "        return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "    def search_paper(self, title, year=None):\n",
    "        \"\"\"\n",
    "        Search for a paper in OpenAlex with multiple strategies\n",
    "        \"\"\"\n",
    "        self.stats[\"total\"] += 1\n",
    "        self._wait_for_rate_limit()\n",
    "\n",
    "        # Strategy 1: Exact title search with filter\n",
    "        params = {\n",
    "            \"filter\": f'title.search:\"{title}\"',\n",
    "            \"sort\": \"relevance_score:desc\",\n",
    "            \"per-page\": 2000,\n",
    "            \"mailto\": self.email\n",
    "        }\n",
    "        if year:\n",
    "            params[\"filter\"] += f\",publication_year:{year}\"\n",
    "\n",
    "        try:\n",
    "            r = self.session.get(\n",
    "                \"https://api.openalex.org/works\",\n",
    "                params=params,\n",
    "                timeout=15\n",
    "            )\n",
    "            self.last_request = time.time()\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                results = r.json()[\"results\"]\n",
    "\n",
    "                if results:\n",
    "                    # Check each result for similarity\n",
    "                    best_match = None\n",
    "                    best_score = 0\n",
    "\n",
    "                    for paper in results:\n",
    "                        score = self.similarity(title, paper[\"display_name\"])\n",
    "                        if score > best_score and score > 0.6:  # Lower threshold\n",
    "                            best_score = score\n",
    "                            best_match = paper\n",
    "\n",
    "                    if best_match:\n",
    "                        self.stats[\"success\"] += 1\n",
    "                        return {\n",
    "                            \"openalex_id\": best_match.get(\"id\"),\n",
    "                            \"publication_year\": best_match.get(\"publication_year\"),\n",
    "                            \"citation_count\": best_match.get(\"cited_by_count\"),\n",
    "                            \"referenced_works\": best_match.get(\"referenced_works\", []),\n",
    "                            \"title_matched\": best_match.get(\"display_name\"),\n",
    "                            \"similarity_score\": best_score\n",
    "                        }\n",
    "\n",
    "                self.stats[\"not_found\"] += 1\n",
    "                return None\n",
    "\n",
    "            elif r.status_code == 429:\n",
    "                self.stats[\"rate_limited\"] += 1\n",
    "                print(f\"⚠️ Rate limited. Waiting longer...\")\n",
    "                time.sleep(2)  # Simple backoff\n",
    "                return self.search_paper(title, year)  # Retry\n",
    "\n",
    "            else:\n",
    "                self.stats[\"not_found\"] += 1\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error searching '{title[:50]}...': {e}\")\n",
    "            self.stats[\"not_found\"] += 1\n",
    "            return None\n",
    "\n",
    "    def batch_search(self, papers_df, title_col=\"title\", year_col=None,\n",
    "                     save_every=100, checkpoint_file=\"openalex_matches.csv\"):\n",
    "        \"\"\"\n",
    "        Search multiple papers with checkpoints\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Load existing checkpoint if available\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            existing = pd.read_csv(checkpoint_file)\n",
    "            results = existing.to_dict('records')\n",
    "            print(f\"📥 Loaded {len(results)} existing matches from checkpoint\")\n",
    "            processed_ids = set(existing.get('paper_ID', []))\n",
    "        else:\n",
    "            processed_ids = set()\n",
    "\n",
    "        for idx, row in papers_df.iterrows():\n",
    "            paper_id = row.get('paper_ID', idx)\n",
    "\n",
    "            if paper_id in processed_ids:\n",
    "                continue\n",
    "\n",
    "            title = row[title_col]\n",
    "            year = row.get(year_col) if year_col else None\n",
    "\n",
    "            print(f\"🔍 Searching {idx+1}/{len(papers_df)}: {title[:50]}...\")\n",
    "\n",
    "            match = self.search_paper(title, year)\n",
    "\n",
    "            if match:\n",
    "                results.append({\n",
    "                    'paper_ID': paper_id,\n",
    "                    'split': row.get('split'),\n",
    "                    'domain': row.get('domain'),\n",
    "                    'title': title,\n",
    "                    **match\n",
    "                })\n",
    "            else:\n",
    "                # Store even non-matches to avoid re-searching\n",
    "                results.append({\n",
    "                    'paper_ID': paper_id,\n",
    "                    'split': row.get('split'),\n",
    "                    'domain': row.get('domain'),\n",
    "                    'title': title,\n",
    "                    'openalex_id': None,\n",
    "                    'publication_year': None,\n",
    "                    'citation_count': None,\n",
    "                    'referenced_works': None\n",
    "                })\n",
    "\n",
    "            # Save checkpoint periodically\n",
    "            if (idx + 1) % save_every == 0:\n",
    "                pd.DataFrame(results).to_csv(checkpoint_file, index=False)\n",
    "                print(f\"💾 Checkpoint saved: {len(results)} papers processed\")\n",
    "                print(f\"📊 Stats: {self.stats}\")\n",
    "\n",
    "        # Final save\n",
    "        pd.DataFrame(results).to_csv(checkpoint_file, index=False)\n",
    "        print(f\"✅ Batch complete! Final stats: {self.stats}\")\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# ============================================\n",
    "# PART 3: Run the Search\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n🚀 Starting OpenAlex search...\")\n",
    "client = OpenAlexClient(email=\"your_email@gmail.com\", delay=0.12)  # ~8 req/sec\n",
    "\n",
    "# Search for all papers\n",
    "paper_metadata = client.batch_search(\n",
    "    paper_mapping,\n",
    "    title_col=\"title\",\n",
    "    save_every=50,\n",
    "    checkpoint_file=\"openalex_matches_checkpoint1.csv\"\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Final Results:\")\n",
    "print(f\"   Total papers: {len(paper_metadata)}\")\n",
    "print(f\"   Matched: {paper_metadata['openalex_id'].notna().sum()}\")\n",
    "print(f\"   Match rate: {paper_metadata['openalex_id'].notna().mean():.1%}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 4: Build the Knowledge Graph\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n🏗️ Building Knowledge Graph...\")\n",
    "\n",
    "# Filter to matched papers only\n",
    "matched_papers = paper_metadata[paper_metadata['openalex_id'].notna()].copy()\n",
    "\n",
    "# 4.1 Create nodes.csv\n",
    "nodes = []\n",
    "\n",
    "# Paper nodes\n",
    "for _, row in matched_papers.iterrows():\n",
    "    node_id = f\"P_{row['openalex_id'].split('/')[-1]}\"  # Extract last part of OpenAlex ID\n",
    "    nodes.append({\n",
    "        'node_id': node_id,\n",
    "        'node_type': 'Paper',\n",
    "        'year': row['publication_year'],\n",
    "        'domain': row['domain'],\n",
    "        'split': row['split'],\n",
    "        'name': None\n",
    "    })\n",
    "\n",
    "# Load triples to get entities\n",
    "all_triples = []\n",
    "for file in triplet_files:\n",
    "    df = pd.read_csv(file)\n",
    "    all_triples.append(df)\n",
    "triples_df = pd.concat(all_triples, ignore_index=True)\n",
    "\n",
    "# Filter triples for matched papers\n",
    "matched_paper_ids = set(matched_papers['paper_ID'])\n",
    "triples_df = triples_df[triples_df['paper_ID'].isin(matched_paper_ids)]\n",
    "\n",
    "# Entity nodes\n",
    "unique_entities = set()\n",
    "for _, row in triples_df.iterrows():\n",
    "    # Assuming the triple format is [paper_id, predicate, object]\n",
    "    # Adjust based on your actual columns\n",
    "    if len(row) >= 3:\n",
    "        unique_entities.add(str(row[2]))  # object entity\n",
    "\n",
    "for entity in unique_entities:\n",
    "    entity_hash = hashlib.md5(entity.encode()).hexdigest()[:8]\n",
    "    nodes.append({\n",
    "        'node_id': f\"E_{entity_hash}\",\n",
    "        'node_type': 'Entity',\n",
    "        'year': None,\n",
    "        'domain': None,\n",
    "        'split': None,\n",
    "        'name': entity\n",
    "    })\n",
    "\n",
    "nodes_df = pd.DataFrame(nodes)\n",
    "nodes_df.to_csv('nodes.csv', index=False)\n",
    "print(f\"✅ Created nodes.csv with {len(nodes_df)} nodes\")\n",
    "\n",
    "# 4.2 Create knowledge_edges.csv\n",
    "# Create entity lookup\n",
    "entity_to_id = {}\n",
    "for node in nodes:\n",
    "    if node['node_type'] == 'Entity':\n",
    "        entity_to_id[node['name']] = node['node_id']\n",
    "\n",
    "knowledge_edges = []\n",
    "for _, row in triples_df.iterrows():\n",
    "    paper_id = row['paper_ID']\n",
    "    # Find corresponding OpenAlex ID\n",
    "    paper_row = matched_papers[matched_papers['paper_ID'] == paper_id].iloc[0]\n",
    "    source_id = f\"P_{paper_row['openalex_id'].split('/')[-1]}\"\n",
    "\n",
    "    # Get entity (assuming object is in column 2)\n",
    "    entity = str(row[2])\n",
    "    if entity in entity_to_id:\n",
    "        knowledge_edges.append({\n",
    "            'source': source_id,\n",
    "            'target': entity_to_id[entity],\n",
    "            'predicate': str(row[1]),  # predicate is column 1\n",
    "            'year': paper_row['publication_year']\n",
    "        })\n",
    "\n",
    "knowledge_df = pd.DataFrame(knowledge_edges)\n",
    "knowledge_df.to_csv('knowledge_edges.csv', index=False)\n",
    "print(f\"✅ Created knowledge_edges.csv with {len(knowledge_df)} edges\")\n",
    "\n",
    "# 4.3 Create citation_edges.csv\n",
    "citation_edges = []\n",
    "paper_id_to_node = {}\n",
    "\n",
    "# Create paper ID mapping\n",
    "for node in nodes:\n",
    "    if node['node_type'] == 'Paper':\n",
    "        paper_id_to_node[node['node_id']] = node\n",
    "\n",
    "# Collect all citations\n",
    "for _, row in matched_papers.iterrows():\n",
    "    source_id = f\"P_{row['openalex_id'].split('/')[-1]}\"\n",
    "    cited_works = row.get('referenced_works', [])\n",
    "\n",
    "    if cited_works and isinstance(cited_works, list):\n",
    "        for cited in cited_works:\n",
    "            # Extract OpenAlex ID from URL\n",
    "            cited_id = f\"P_{cited.split('/')[-1]}\"\n",
    "\n",
    "            # Check if cited paper exists in our nodes\n",
    "            if cited_id in paper_id_to_node:\n",
    "                citation_edges.append({\n",
    "                    'source': source_id,\n",
    "                    'target': cited_id,\n",
    "                    'year': row['publication_year']\n",
    "                })\n",
    "            else:\n",
    "                # Create minimal node for cited paper\n",
    "                nodes.append({\n",
    "                    'node_id': cited_id,\n",
    "                    'node_type': 'Paper',\n",
    "                    'year': None,  # We don't know the year\n",
    "                    'domain': None,\n",
    "                    'split': None,\n",
    "                    'name': None\n",
    "                })\n",
    "                paper_id_to_node[cited_id] = nodes[-1]\n",
    "\n",
    "                citation_edges.append({\n",
    "                    'source': source_id,\n",
    "                    'target': cited_id,\n",
    "                    'year': row['publication_year']\n",
    "                })\n",
    "\n",
    "citation_df = pd.DataFrame(citation_edges)\n",
    "citation_df.to_csv('citation_edges.csv', index=False)\n",
    "print(f\"✅ Created citation_edges.csv with {len(citation_df)} edges\")\n",
    "\n",
    "# Update nodes.csv with new cited paper nodes\n",
    "nodes_df = pd.DataFrame(nodes)\n",
    "nodes_df.to_csv('nodes.csv', index=False)\n",
    "print(f\"✅ Updated nodes.csv with {len(nodes_df)} nodes\")\n",
    "\n",
    "# ============================================\n",
    "# PART 5: Summary Statistics\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n📊 FINAL KG STATISTICS:\")\n",
    "print(f\"   Total nodes: {len(nodes_df)}\")\n",
    "print(f\"     - Paper nodes: {len(nodes_df[nodes_df['node_type'] == 'Paper'])}\")\n",
    "print(f\"     - Entity nodes: {len(nodes_df[nodes_df['node_type'] == 'Entity'])}\")\n",
    "print(f\"   Total knowledge edges: {len(knowledge_df)}\")\n",
    "print(f\"   Total citation edges: {len(citation_df)}\")\n",
    "print(f\"   Match rate with OpenAlex: {len(matched_papers)}/{len(paper_mapping)} = {len(matched_papers)/len(paper_mapping):.1%}\")"
   ],
   "id": "d5a08ef93bb9701b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Loading SciND papers...\n",
      "✅ Loaded 3964 papers\n",
      "   Split distribution:\n",
      "SKG      3279\n",
      "NOVEL     601\n",
      "BLOG       84\n",
      "Name: split, dtype: int64\n",
      "✅ Keeping 3880 scholarly papers\n",
      "\n",
      "🚀 Starting OpenAlex search...\n",
      "🔍 Searching 1/3880: Preview, Attend and Review Schema-Aware Curriculum...\n",
      "⚠️ Rate limited. Waiting longer...\n",
      "⚠️ Rate limited. Waiting longer...\n",
      "⚠️ Rate limited. Waiting longer...\n",
      "⚠️ Rate limited. Waiting longer...\n",
      "⚠️ Rate limited. Waiting longer...\n",
      "⚠️ Rate limited. Waiting longer...\n",
      "⚠️ Rate limited. Waiting longer...\n",
      "⚠️ Rate limited. Waiting longer...\n",
      "⚠️ Rate limited. Waiting longer...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[69], line 251\u001B[0m\n\u001B[0;32m    248\u001B[0m client \u001B[38;5;241m=\u001B[39m OpenAlexClient(email\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour_email@gmail.com\u001B[39m\u001B[38;5;124m\"\u001B[39m, delay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.12\u001B[39m)  \u001B[38;5;66;03m# ~8 req/sec\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# Search for all papers\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m paper_metadata \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mbatch_search(\n\u001B[0;32m    252\u001B[0m     paper_mapping,\n\u001B[0;32m    253\u001B[0m     title_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    254\u001B[0m     save_every\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[0;32m    255\u001B[0m     checkpoint_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenalex_matches_checkpoint1.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    256\u001B[0m )\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m📊 Final Results:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m   Total papers: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(paper_metadata)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[69], line 208\u001B[0m, in \u001B[0;36mOpenAlexClient.batch_search\u001B[1;34m(self, papers_df, title_col, year_col, save_every, checkpoint_file)\u001B[0m\n\u001B[0;32m    204\u001B[0m year \u001B[38;5;241m=\u001B[39m row\u001B[38;5;241m.\u001B[39mget(year_col) \u001B[38;5;28;01mif\u001B[39;00m year_col \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m🔍 Searching \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(papers_df)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtitle[:\u001B[38;5;241m50\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 208\u001B[0m match \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_paper(title, year)\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m match:\n\u001B[0;32m    211\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend({\n\u001B[0;32m    212\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpaper_ID\u001B[39m\u001B[38;5;124m'\u001B[39m: paper_id,\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m'\u001B[39m: row\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    216\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmatch\n\u001B[0;32m    217\u001B[0m     })\n",
      "Cell \u001B[1;32mIn[69], line 170\u001B[0m, in \u001B[0;36mOpenAlexClient.search_paper\u001B[1;34m(self, title, year)\u001B[0m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ Rate limited. Waiting longer...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    169\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Simple backoff\u001B[39;00m\n\u001B[1;32m--> 170\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_paper(title, year)  \u001B[38;5;66;03m# Retry\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    173\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstats[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot_found\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[1;32mIn[69], line 170\u001B[0m, in \u001B[0;36mOpenAlexClient.search_paper\u001B[1;34m(self, title, year)\u001B[0m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ Rate limited. Waiting longer...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    169\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Simple backoff\u001B[39;00m\n\u001B[1;32m--> 170\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_paper(title, year)  \u001B[38;5;66;03m# Retry\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    173\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstats[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot_found\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "    \u001B[1;31m[... skipping similar frames: OpenAlexClient.search_paper at line 170 (5 times)]\u001B[0m\n",
      "Cell \u001B[1;32mIn[69], line 170\u001B[0m, in \u001B[0;36mOpenAlexClient.search_paper\u001B[1;34m(self, title, year)\u001B[0m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ Rate limited. Waiting longer...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    169\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Simple backoff\u001B[39;00m\n\u001B[1;32m--> 170\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_paper(title, year)  \u001B[38;5;66;03m# Retry\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    173\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstats[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot_found\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[1;32mIn[69], line 169\u001B[0m, in \u001B[0;36mOpenAlexClient.search_paper\u001B[1;34m(self, title, year)\u001B[0m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstats[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrate_limited\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m⚠️ Rate limited. Waiting longer...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 169\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# Simple backoff\u001B[39;00m\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_paper(title, year)  \u001B[38;5;66;03m# Retry\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:24:06.129202Z",
     "start_time": "2026-02-19T06:24:06.079174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OpenAlexClient:\n",
    "    def __init__(self, api_key=None, delay=0.02):\n",
    "        \"\"\"\n",
    "        delay=0.02  → ~50 requests/sec (safe for Premium users)\n",
    "        Adjust if your plan allows more.\n",
    "        \"\"\"\n",
    "\n",
    "        self.api_key = api_key or os.getenv(\"OPENALEX_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OpenAlex API key not found. Set OPENALEX_API_KEY env variable.\")\n",
    "\n",
    "        self.session = requests.Session()\n",
    "\n",
    "        # 🔴 AUTHENTICATED POOL (Premium)\n",
    "        self.session.headers.update({\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"User-Agent\": \"ScientificNoveltyProject/1.0\"\n",
    "        })\n",
    "\n",
    "        self.delay = delay\n",
    "        self.last_request = 0\n",
    "\n",
    "        self.stats = {\n",
    "            \"total\": 0,\n",
    "            \"success\": 0,\n",
    "            \"rate_limited\": 0,\n",
    "            \"not_found\": 0\n",
    "        }"
   ],
   "id": "24c3658a5767d08c",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:24:11.192206Z",
     "start_time": "2026-02-19T06:24:11.183611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _wait_for_rate_limit(self):\n",
    "    \"\"\"\n",
    "    Premium pool allows much higher throughput.\n",
    "    We only add a tiny pacing delay to avoid bursts.\n",
    "    \"\"\"\n",
    "    elapsed = time.time() - self.last_request\n",
    "    if elapsed < self.delay:\n",
    "        time.sleep(self.delay - elapsed)"
   ],
   "id": "41cba824cd65dc23",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:24:18.769660Z",
     "start_time": "2026-02-19T06:24:18.764830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params = {\n",
    "    \"filter\": f'title.search:\"{title}\"',\n",
    "    \"sort\": \"relevance_score:desc\",\n",
    "    \"per-page\": 5\n",
    "}"
   ],
   "id": "bab39d970653604c",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:24:23.660348Z",
     "start_time": "2026-02-19T06:24:23.644114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "elif r.status_code == 429:\n",
    "self.stats[\"rate_limited\"] += 1\n",
    "\n",
    "wait_time = 1.5 ** min(self.stats[\"rate_limited\"], 6)\n",
    "print(f\"⚠️ Rate limited. Backing off {wait_time:.2f}s\")\n",
    "\n",
    "time.sleep(wait_time)\n",
    "return self.search_paper(title, year)"
   ],
   "id": "7513260818e67bce",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1604640681.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[73], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    elif r.status_code == 429:\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:33:05.160601Z",
     "start_time": "2026-02-19T06:29:32.868710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# SciND → OpenAlex Matching + Knowledge Graph Construction\n",
    "# PREMIUM (API KEY) VERSION — SINGLE SCRIPT\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from difflib import SequenceMatcher\n",
    "import hashlib\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Make sure you set this BEFORE running:\n",
    "# Windows (PowerShell): setx OPENALEX_API_KEY \"YOUR_KEY\"\n",
    "# Linux/Mac: export OPENALEX_API_KEY=\"YOUR_KEY\"\n",
    "# ============================================================\n",
    "# MANUALLY SPECIFY YOUR OPENALEX PREMIUM API KEY HERE\n",
    "# ============================================================\n",
    "\n",
    "API_KEY = \"b2opHjuXMNQJy9aTaB4bZF\"\n",
    "\n",
    "if not API_KEY or API_KEY != \"b2opHjuXMNQJy9aTaB4bZF\":\n",
    "    raise ValueError(\"❌ Please paste your OpenAlex API key into API_KEY.\")\n",
    "\n",
    "REQUEST_DELAY = 0.02   # ~50 req/sec (safe for Premium)\n",
    "CHECKPOINT_FILE = \"openalex_matches_checkpoint.csv\"\n",
    "\n",
    "# ============================================================\n",
    "# PART 1: Load SciND Papers and Extract Titles\n",
    "# ============================================================\n",
    "\n",
    "print(\"📚 Loading SciND papers...\")\n",
    "triplet_files = glob.glob(\"*/Triplets/**/*.csv\", recursive=True)\n",
    "\n",
    "mapping_records = []\n",
    "\n",
    "for file in triplet_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    if \"SKG\" in file:\n",
    "        split = \"SKG\"\n",
    "        stanza_split = \"SKG_Papers\"\n",
    "    elif \"Novel_Papers\" in file:\n",
    "        split = \"NOVEL\"\n",
    "        stanza_split = \"Novel_Papers\"\n",
    "    elif \"Blogs\" in file:\n",
    "        split = \"BLOG\"\n",
    "        stanza_split = \"Blogs\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    filename = os.path.basename(file)\n",
    "    domain_raw = filename.replace(\"_triplets.csv\", \"\")\n",
    "    domain = domain_raw.replace(\"_Blogs\", \"\") if split == \"BLOG\" else domain_raw\n",
    "\n",
    "    paper_ids = sorted(df[\"paper_ID\"].unique())\n",
    "\n",
    "    pattern = f\"*/stanza_files/{stanza_split}/{domain}/*-Stanza-out.txt\"\n",
    "    directory = glob.glob(pattern)\n",
    "\n",
    "    for pid in paper_ids:\n",
    "        if pid < len(directory):\n",
    "            filepath = directory[pid]\n",
    "            fname = os.path.basename(filepath)\n",
    "\n",
    "            title = fname.replace(\"-Stanza-out.txt\", \"\")\n",
    "            title = re.sub(r\"^\\d+\\.\", \"\", title).strip()\n",
    "\n",
    "            mapping_records.append({\n",
    "                \"split\": split,\n",
    "                \"domain\": domain,\n",
    "                \"paper_ID\": pid,\n",
    "                \"title\": title\n",
    "            })\n",
    "\n",
    "paper_mapping = pd.DataFrame(mapping_records)\n",
    "\n",
    "paper_mapping = paper_mapping[\n",
    "    paper_mapping[\"split\"].isin([\"SKG\", \"NOVEL\"])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Loaded {len(paper_mapping)} scholarly papers\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 2: Premium OpenAlex Client (Authenticated Pool)\n",
    "# ============================================================\n",
    "\n",
    "class OpenAlexClient:\n",
    "    def __init__(self, api_key, delay=0.02):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"User-Agent\": \"ScientificNoveltyProject/1.0\"\n",
    "        })\n",
    "\n",
    "        self.delay = delay\n",
    "        self.last_request = 0\n",
    "\n",
    "        self.stats = {\n",
    "            \"total\": 0,\n",
    "            \"success\": 0,\n",
    "            \"rate_limited\": 0,\n",
    "            \"not_found\": 0\n",
    "        }\n",
    "\n",
    "    def _wait(self):\n",
    "        elapsed = time.time() - self.last_request\n",
    "        if elapsed < self.delay:\n",
    "            time.sleep(self.delay - elapsed)\n",
    "\n",
    "    def similarity(self, a, b):\n",
    "        return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "    def search_paper(self, title):\n",
    "        self.stats[\"total\"] += 1\n",
    "        self._wait()\n",
    "\n",
    "        params = {\n",
    "            \"filter\": f'title.search:\"{title}\"',\n",
    "            \"sort\": \"relevance_score:desc\",\n",
    "            \"per-page\": 5,\n",
    "            \"select\": \"id,display_name,publication_year,cited_by_count,referenced_works\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            r = self.session.get(\"https://api.openalex.org/works\", params=params, timeout=15)\n",
    "            self.last_request = time.time()\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                results = r.json()[\"results\"]\n",
    "\n",
    "                best_match = None\n",
    "                best_score = 0\n",
    "\n",
    "                for paper in results:\n",
    "                    score = self.similarity(title, paper[\"display_name\"])\n",
    "                    if score > best_score and score > 0.6:\n",
    "                        best_match = paper\n",
    "                        best_score = score\n",
    "\n",
    "                if best_match:\n",
    "                    self.stats[\"success\"] += 1\n",
    "                    return {\n",
    "                        \"openalex_id\": best_match[\"id\"],\n",
    "                        \"publication_year\": best_match[\"publication_year\"],\n",
    "                        \"citation_count\": best_match[\"cited_by_count\"],\n",
    "                        \"referenced_works\": best_match.get(\"referenced_works\", [])\n",
    "                    }\n",
    "\n",
    "                self.stats[\"not_found\"] += 1\n",
    "                return None\n",
    "\n",
    "            elif r.status_code == 429:\n",
    "                self.stats[\"rate_limited\"] += 1\n",
    "                wait = 1.5 ** min(self.stats[\"rate_limited\"], 6)\n",
    "                print(f\"⚠️ Rate limited. Backoff {wait:.2f}s\")\n",
    "                time.sleep(wait)\n",
    "                return self.search_paper(title)\n",
    "\n",
    "            else:\n",
    "                self.stats[\"not_found\"] += 1\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            self.stats[\"not_found\"] += 1\n",
    "            return None\n",
    "\n",
    "    def batch_search(self, papers_df):\n",
    "        results = []\n",
    "\n",
    "        if os.path.exists(CHECKPOINT_FILE):\n",
    "            existing = pd.read_csv(CHECKPOINT_FILE)\n",
    "            results = existing.to_dict(\"records\")\n",
    "            processed = set(existing[\"paper_ID\"])\n",
    "            print(f\"📥 Loaded checkpoint with {len(processed)} papers\")\n",
    "        else:\n",
    "            processed = set()\n",
    "\n",
    "        for idx, row in papers_df.iterrows():\n",
    "            if row[\"paper_ID\"] in processed:\n",
    "                continue\n",
    "\n",
    "            print(f\"🔍 {idx+1}/{len(papers_df)} {row['title'][:60]}\")\n",
    "\n",
    "            match = self.search_paper(row[\"title\"])\n",
    "\n",
    "            result = {**row.to_dict(), **(match or {\n",
    "                \"openalex_id\": None,\n",
    "                \"publication_year\": None,\n",
    "                \"citation_count\": None,\n",
    "                \"referenced_works\": None\n",
    "            })}\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                pd.DataFrame(results).to_csv(CHECKPOINT_FILE, index=False)\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(CHECKPOINT_FILE, index=False)\n",
    "        return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PART 3: Run Matching\n",
    "# ============================================================\n",
    "\n",
    "client = OpenAlexClient(API_KEY, REQUEST_DELAY)\n",
    "paper_metadata = client.batch_search(paper_mapping)\n",
    "\n",
    "matched_papers = paper_metadata[paper_metadata[\"openalex_id\"].notna()].copy()\n",
    "\n",
    "print(f\"✅ Match rate: {len(matched_papers)}/{len(paper_mapping)}\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 4: Build Knowledge Graph\n",
    "# ============================================================\n",
    "\n",
    "print(\"🏗️ Building Knowledge Graph...\")\n",
    "\n",
    "nodes = []\n",
    "\n",
    "# Paper Nodes\n",
    "for _, row in matched_papers.iterrows():\n",
    "    nodes.append({\n",
    "        \"node_id\": f\"P_{row['openalex_id'].split('/')[-1]}\",\n",
    "        \"node_type\": \"Paper\",\n",
    "        \"year\": row[\"publication_year\"],\n",
    "        \"domain\": row[\"domain\"],\n",
    "        \"split\": row[\"split\"],\n",
    "        \"name\": None\n",
    "    })\n",
    "\n",
    "# Load all triples\n",
    "triples_df = pd.concat([pd.read_csv(f) for f in triplet_files], ignore_index=True)\n",
    "triples_df = triples_df[triples_df[\"paper_ID\"].isin(set(matched_papers[\"paper_ID\"]))]\n",
    "\n",
    "# Entity Nodes\n",
    "entities = set(triples_df.iloc[:, 2].astype(str))\n",
    "\n",
    "entity_lookup = {}\n",
    "for ent in entities:\n",
    "    eid = \"E_\" + hashlib.md5(ent.encode()).hexdigest()[:8]\n",
    "    entity_lookup[ent] = eid\n",
    "    nodes.append({\n",
    "        \"node_id\": eid,\n",
    "        \"node_type\": \"Entity\",\n",
    "        \"year\": None,\n",
    "        \"domain\": None,\n",
    "        \"split\": None,\n",
    "        \"name\": ent\n",
    "    })\n",
    "\n",
    "nodes_df = pd.DataFrame(nodes)\n",
    "nodes_df.to_csv(\"nodes.csv\", index=False)\n",
    "\n",
    "# Knowledge Edges\n",
    "knowledge_edges = []\n",
    "paper_lookup = {\n",
    "    r.paper_ID: f\"P_{r.openalex_id.split('/')[-1]}\"\n",
    "    for r in matched_papers.itertuples()\n",
    "}\n",
    "\n",
    "for row in triples_df.itertuples():\n",
    "    entity = str(row[3])\n",
    "    if entity in entity_lookup:\n",
    "        knowledge_edges.append({\n",
    "            \"source\": paper_lookup[row.paper_ID],\n",
    "            \"target\": entity_lookup[entity],\n",
    "            \"predicate\": str(row[2])\n",
    "        })\n",
    "\n",
    "pd.DataFrame(knowledge_edges).to_csv(\"knowledge_edges.csv\", index=False)\n",
    "\n",
    "# Citation Edges\n",
    "citation_edges = []\n",
    "\n",
    "for _, row in matched_papers.iterrows():\n",
    "    src = f\"P_{row['openalex_id'].split('/')[-1]}\"\n",
    "    refs = row[\"referenced_works\"]\n",
    "\n",
    "    if isinstance(refs, list):\n",
    "        for ref in refs:\n",
    "            citation_edges.append({\n",
    "                \"source\": src,\n",
    "                \"target\": f\"P_{ref.split('/')[-1]}\",\n",
    "                \"year\": row[\"publication_year\"]\n",
    "            })\n",
    "\n",
    "pd.DataFrame(citation_edges).to_csv(\"citation_edges.csv\", index=False)\n",
    "\n",
    "print(\"✅ Knowledge Graph Built Successfully\")\n",
    "print(f\"Nodes: {len(nodes_df)}\")\n",
    "print(f\"Knowledge Edges: {len(knowledge_edges)}\")\n",
    "print(f\"Citation Edges: {len(citation_edges)}\")"
   ],
   "id": "8ee99766b4e5a031",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Loading SciND papers...\n",
      "✅ Loaded 3880 scholarly papers\n",
      "📥 Loaded checkpoint with 1058 papers\n",
      "🔍 1951/3880 The MuCoW Test Suite at WMT 2019 Automatically Harvested Mul\n",
      "🔍 1952/3880 UBA Using Automatic Translation and Wikipedia for Cross-Ling\n",
      "🔍 1953/3880 Phrase-Based & Neural Unsupervised Machine Translation\n",
      "🔍 1954/3880 Soft Contextual Data Augmentation for Neural Machine Transla\n",
      "🔍 1955/3880 Are we Estimating or Guesstimating Translation Quality\n",
      "🔍 1956/3880 Depth Growing for Neural Machine Translation\n",
      "🔍 1957/3880 A3-108 Machine Translation System for Similar Language Trans\n",
      "🔍 1958/3880 Approaching Neural Grammatical Error Correction as a Low-Res\n",
      "🔍 1959/3880 Evaluating the Learning Curve of Domain Adaptive Statistical\n",
      "🔍 1960/3880 Factored Translation with Unsupervised Word Clusters\n",
      "🔍 1961/3880 Learning Better Rule Extraction with Translation Span Alignm\n",
      "🔍 1962/3880 Perplexity Minimization for Translation Model Domain Adaptat\n",
      "🔍 1963/3880 SAO WMT19 Test Suite Machine Translation of Audit Reports\n",
      "🔍 1964/3880 A Shared Task on Bandit Learning for Machine Translation\n",
      "🔍 1965/3880 Enhancing Statistical Machine Translation with Character Ali\n",
      "🔍 1966/3880 Sequence to Sequence Mixture Model for Diverse Machine Trans\n",
      "🔍 1967/3880 The University of Maryland’s Submissions to the WMT20 Chat T\n",
      "🔍 1968/3880 WMDO Fluency-based Word Mover’s Distance for Machine Transla\n",
      "🔍 1969/3880 Understanding Data Augmentation in Neural Machine Translatio\n",
      "🔍 1970/3880 Simple and Effective Noisy Channel Modeling for Neural Machi\n",
      "🔍 1971/3880 Hint-Based Training for Non-Autoregressive Machine Translati\n",
      "🔍 1972/3880 An Investigation of Machine Translation Evaluation Metrics i\n",
      "🔍 1973/3880 Efficient Solutions for Word Reordering in German-English Ph\n",
      "🔍 1974/3880 Improving Evaluation of Document-level Machine Translation Q\n",
      "🔍 1975/3880 Meteor++ 2.0 Adopt Syntactic Level Paraphrase Knowledge into\n",
      "🔍 1976/3880 Phrase Model Training for Statistical Machine Translation wi\n",
      "🔍 1977/3880 Predicting Human-Targeted Translation Edit Rate via Untraine\n",
      "🔍 1978/3880 STIL - Simultaneous Slot Filling, Translation, Intent Classi\n",
      "🔍 1979/3880 Translation Model Size Reduction for Hierarchical Phrase-bas\n",
      "🔍 1980/3880 Using Context Vectors in Improving a Machine Translation Sys\n",
      "🔍 1981/3880 Generalized Data Augmentation for Low-Resource Translation\n",
      "🔍 1982/3880 Better OOV Translation with Bilingual Terminology Mining\n",
      "🔍 1983/3880 Simultaneous Translation with Flexible Policy via Restricted\n",
      "🔍 1984/3880 Target Conditioned Sampling Optimizing Data Selection for Mu\n",
      "🔍 1985/3880 A Phrase Orientation Model for Hierarchical Machine Translat\n",
      "🔍 1986/3880 Character-based Neural Machine Translation\n",
      "🔍 1987/3880 Dependency Analysis of Scrambled References for Better Evalu\n",
      "🔍 1988/3880 LIUM’s SMT Machine Translation Systems for WMT 2011\n",
      "🔍 1989/3880 Neural Machine Translation by Minimising the Bayes-risk with\n",
      "🔍 1990/3880 SimulMT to SimulST Adapting Simultaneous Text Translation to\n",
      "🔍 1991/3880 Task Alternation in Parallel Sentence Retrieval for Twitter \n",
      "🔍 1992/3880 The University of Edinburgh-Uppsala University’s Submission \n",
      "🔍 1993/3880 Continuous Space Translation Models with Neural Networks\n",
      "🔍 1994/3880 Controlling Politeness in Neural Machine Translation via Sid\n",
      "🔍 1995/3880 Discourse-Related Language Contrasts in English-Croatian Hum\n",
      "🔍 1996/3880 Diversify and Combine Improving Word Alignment for Machine T\n",
      "🔍 1997/3880 End-to-End Simultaneous Translation System for IWSLT2020 Usi\n",
      "🔍 1998/3880 English to Hindi Multi-modal Neural Machine Translation and \n",
      "🔍 1999/3880 Evaluating the morphological competence of Machine Translati\n",
      "🔍 2000/3880 Generalizing Back-Translation in Neural Machine Translation\n",
      "🔍 2001/3880 GTCOM Neural Machine Translation Systems for WMT19\n",
      "🔍 2002/3880 Meta Ensemble for Japanese-Chinese Neural Machine Translatio\n",
      "🔍 2003/3880 ParFDA for Fast Deployment of Accurate Statistical Machine T\n",
      "🔍 2004/3880 Private Access to Phrase Tables for Statistical Machine Tran\n",
      "🔍 2005/3880 RTM Stacking Results for Machine Translation Performance Pre\n",
      "🔍 2006/3880 Sentiment Translation through Lexicon Induction\n",
      "🔍 2007/3880 TerrorCat a Translation Error Categorization-based MT Qualit\n",
      "🔍 2008/3880 The CMU Machine Translation Systems at WMT 2013 Syntax, Synt\n",
      "🔍 2009/3880 The University of Edinburgh’s English-Tamil and English-Inuk\n",
      "🔍 2010/3880 Transformer-based Cascaded Multimodal Speech Translation\n",
      "🔍 2011/3880 Transliteration Better than Translation Answering Code-mixed\n",
      "🔍 2012/3880 Unsupervised Neural Machine Translation with Weight Sharing\n",
      "🔍 2013/3880 Word-based Domain Adaptation for Neural Machine Translation\n",
      "🔍 2014/3880 Zero-Shot Neural Machine Translation Russian-Hindi @LoResMT \n",
      "🔍 2015/3880 A Recurrent Neural Networks Approach for Estimating the Qual\n",
      "🔍 2016/3880 Combining Word-Level and Character-Level Models for Machine \n",
      "🔍 2017/3880 Direct Error Rate Minimization for Statistical Machine Trans\n",
      "🔍 2018/3880 EED Extended Edit Distance Measure for Machine Translation\n",
      "🔍 2019/3880 How do Humans Evaluate Machine Translation\n",
      "🔍 2020/3880 JUST System for WMT20 Chat Translation Task\n",
      "🔍 2021/3880 Name-aware Machine Translation\n",
      "🔍 2022/3880 Predicting Translation Performance with Referential Translat\n",
      "🔍 2023/3880 Producing Unseen Morphological Variants in Statistical Machi\n",
      "🔍 2024/3880 Sparse and Constrained Attention for Neural Machine Translat\n",
      "🔍 2025/3880 Spell Checking Techniques for Replacement of Unknown Words a\n",
      "🔍 2026/3880 Trait-Based Hypothesis Selection For Machine Translation\n",
      "🔍 2027/3880 Translation Model Adaptation by Resampling\n",
      "🔍 2028/3880 Unsupervised Adaptation for Statistical Machine Translation\n",
      "🔍 2029/3880 Unsupervised Paraphrasing without Translation\n",
      "🔍 2030/3880 An Empirical Comparison of Features and Tuning for Phrase-ba\n",
      "🔍 2031/3880 Filtering Pseudo-References by Paraphrasing for Automatic Ev\n",
      "🔍 2032/3880 How Grammatical is Character-level Neural Machine Translatio\n",
      "🔍 2033/3880 Improved Reordering for Shallow-n Grammar based Hierarchical\n",
      "🔍 2034/3880 Integration of Multiple Bilingually-Learned Segmentation Sch\n",
      "🔍 2035/3880 Investigations in Exact Inference for Hierarchical Translati\n",
      "🔍 2036/3880 Local System Voting Feature for Machine Translation System C\n",
      "🔍 2037/3880 Modeling Syntactic and Semantic Structures in Hierarchical P\n",
      "🔍 2038/3880 Neural Hidden Markov Model for Machine Translation\n",
      "🔍 2039/3880 Neural System Combination for Machine Translation\n",
      "🔍 2040/3880 Optimization Strategies for Online Large-Margin Learning in \n",
      "🔍 2041/3880 Stacking for Statistical Machine Translation\n",
      "🔍 2042/3880 Statistical Machine Translation with a Factorized Grammar\n",
      "🔍 2043/3880 Tencent AI Lab Machine Translation Systems for WMT20 Chat Tr\n",
      "🔍 2044/3880 Document Translation vs. Query Translation for Cross-Lingual\n",
      "🔍 2045/3880 Language Model Prior for Low-Resource Neural Machine Transla\n",
      "🔍 2046/3880 Detecting Word Sense Disambiguation Biases in Machine Transl\n",
      "🔍 2047/3880 Translation Artifacts in Cross-lingual Transfer Learning\n",
      "🔍 2048/3880 An Empirical Comparison of Domain Adaptation Methods for Neu\n",
      "🔍 2049/3880 Combining Sequence Distillation and Transfer Learning for Ef\n",
      "🔍 2050/3880 DFKI Hybrid Machine Translation System for WMT 2011 - On the\n",
      "🔍 2051/3880 Discriminative Sample Selection for Statistical Machine Tran\n",
      "🔍 2052/3880 Improved Translation with Source Syntax Labels\n",
      "🔍 2053/3880 Improving Machine Translation Quality Estimation with Neural\n",
      "🔍 2054/3880 Naver Labs Europe’s Systems for the WMT19 Machine Translatio\n",
      "🔍 2055/3880 Neural Machine Translation with Recurrent Attention Modeling\n",
      "🔍 2056/3880 Post-ordering by Parsing for Japanese-English Statistical Ma\n",
      "🔍 2057/3880 Gender in Danger Evaluating Speech Translation Technology on\n",
      "🔍 2058/3880 Uncertainty-Aware Curriculum Learning for Neural Machine Tra\n",
      "🔍 2059/3880 Reducing Word Omission Errors in Neural Machine Translation \n",
      "🔍 2060/3880 Exploiting Sentential Context for Neural Machine Translation\n",
      "🔍 2061/3880 An Exploration of Forest-to-String Translation Does Translat\n",
      "🔍 2062/3880 Augmenting String-to-Tree and Tree-to-String Translation wit\n",
      "🔍 2063/3880 Automatically Predicting Sentence Translation Difficulty\n",
      "🔍 2064/3880 Bootstrapping Entity Translation on Weakly Comparable Corpor\n",
      "🔍 2065/3880 Effects of Empty Categories on Machine Translation\n",
      "🔍 2066/3880 NICT’s Supervised Neural Machine Translation Systems for the\n",
      "🔍 2067/3880 Stream-based Translation Models for Statistical Machine Tran\n",
      "🔍 2068/3880 The FLORES Evaluation Datasets for Low-Resource Machine Tran\n",
      "🔍 2069/3880 Decision Trees for Lexical Smoothing in Statistical Machine \n",
      "🔍 2070/3880 Incremental Syntactic Language Models for Phrase-based Trans\n",
      "🔍 2071/3880 Learning Unsupervised Word Translations Without Adversaries\n",
      "🔍 2072/3880 Linear Mixture Models for Robust Machine Translation\n",
      "🔍 2073/3880 Priming Neural Machine Translation\n",
      "🔍 2074/3880 A Multi-Task Architecture on Relevance-based Neural Query Tr\n",
      "🔍 2075/3880 Latent Variable Model for Multi-modal Translation\n",
      "🔍 2076/3880 Effectively pretraining a speech translation decoder with Ma\n",
      "🔍 2077/3880 Bilingual Sentiment Consistency for Statistical Machine Tran\n",
      "🔍 2078/3880 Example-Based Paraphrasing for Improved Phrase-Based Statist\n",
      "🔍 2079/3880 Graph-based Semi-Supervised Learning of Translation Models f\n",
      "🔍 2080/3880 Modeling Source Syntax for Neural Machine Translation\n",
      "🔍 2081/3880 Subword Segmentation and a Single Bridge Language Affect Zer\n",
      "🔍 2082/3880 Unbabel’s Participation in the WMT17 Translation Quality Est\n",
      "🔍 2083/3880 Lattice Transformer for Speech Translation\n",
      "🔍 2084/3880 Distilling Translations with Visual Awareness\n",
      "🔍 2085/3880 Augmenting Translation Models with Simulated Acoustic Confus\n",
      "🔍 2086/3880 Learning Hierarchical Translation Structure with Linguistic \n",
      "🔍 2087/3880 Look It Up Bilingual and Monolingual Dictionaries Improve Ne\n",
      "🔍 2088/3880 NTT’s Machine Translation Systems for WMT19 Robustness Task\n",
      "🔍 2089/3880 Sequence-to-Dependency Neural Machine Translation\n",
      "🔍 2090/3880 Stem Translation with Affix-Based Rule Selection for Aggluti\n",
      "🔍 2091/3880 UAlacant Using Online Machine Translation for Cross-Lingual \n",
      "🔍 2092/3880 Using Discourse Structure Improves Machine Translation Evalu\n",
      "🔍 2093/3880 Speech Translation and the End-to-End Promise Taking Stock o\n",
      "🔍 2094/3880 A Novel Translation Framework Based on Rhetorical Structure \n",
      "🔍 2095/3880 Complete Multilingual Neural Machine Translation\n",
      "🔍 2096/3880 Extraction Programs A Unified Approach to Translation Rule E\n",
      "🔍 2097/3880 Learning Continuous Phrase Representations for Translation M\n",
      "🔍 2098/3880 Phrase-Based Translation Model for Question Retrieval in Com\n",
      "🔍 2099/3880 Adaptive Quality Estimation for Machine Translation\n",
      "🔍 2100/3880 Bayesian Extraction of Minimal SCFG Rules for Hierarchical P\n",
      "🔍 2101/3880 Paraphrase Generation as Zero-Shot Multilingual Translation \n",
      "🔍 2102/3880 Robust Machine Translation with Domain Sensitive Pseudo-Sour\n",
      "🔍 2103/3880 Hard-Coded Gaussian Attention for Neural Machine Translation\n",
      "🔍 2104/3880 In Neural Machine Translation, What Does Transfer Learning T\n",
      "🔍 2105/3880 Bilingual Lexical Cohesion Trigger Model for Document-Level \n",
      "🔍 2106/3880 From n-gram-based to CRF-based Translation Models\n",
      "🔍 2107/3880 Improving On-line Handwritten Recognition using Translation \n",
      "🔍 2108/3880 Improving Robustness of Neural Machine Translation with Mult\n",
      "🔍 2109/3880 When Does Unsupervised Machine Translation Work\n",
      "🔍 2110/3880 Learning a Multi-Domain Curriculum for Neural Machine Transl\n",
      "🔍 2111/3880 Reducing Gender Bias in Neural Machine Translation as a Doma\n",
      "🔍 2112/3880 Translationese as a Language in “Multilingual” NMT\n",
      "🔍 2113/3880 Using Context in Neural Machine Translation Training Objecti\n",
      "🔍 2114/3880 Variational Neural Machine Translation with Normalizing Flow\n",
      "🔍 2115/3880 A Machine Translation Approach for Modernizing Historical Do\n",
      "🔍 2116/3880 An Empirical Evaluation of Noise Contrastive Estimation for \n",
      "🔍 2117/3880 Coreference and Coherence in Neural Machine Translation A St\n",
      "🔍 2118/3880 DiDi Labs’ End-to-end System for the IWSLT 2020 Offline Spee\n",
      "🔍 2119/3880 Domain Differential Adaptation for Neural Machine Translatio\n",
      "🔍 2120/3880 Efficient Path Counting Transducers for Minimum Bayes-Risk D\n",
      "🔍 2121/3880 End-to-end Speech Translation System Description of LIT for \n",
      "🔍 2122/3880 Fast and Scalable Decoding with Language Model Look-Ahead fo\n",
      "🔍 2123/3880 Feature Decay Algorithms for Fast Deployment of Accurate Sta\n",
      "🔍 2124/3880 GTCOM Neural Machine Translation Systems for WMT20\n",
      "🔍 2125/3880 Improving Character-Based Decoding Using Target-Side Morphol\n",
      "🔍 2126/3880 Machine Translation of Arabic Dialects\n",
      "🔍 2127/3880 Machine Translation with parfda, Moses, kenlm, nplm, and PRO\n",
      "🔍 2128/3880 Neural Machine Translation Using Extracted Context Based on \n",
      "🔍 2129/3880 ONTS “Optima” News Translation System\n",
      "🔍 2130/3880 Semantic Structural Decomposition for Neural Machine Transla\n",
      "🔍 2131/3880 Statistical Power and Translationese in Machine Translation \n",
      "🔍 2132/3880 Supervised and Unsupervised Machine Translation for Myanmar-\n",
      "🔍 2133/3880 Syntax-based Rewriting for Simultaneous Machine Translation\n",
      "🔍 2134/3880 Tagged Back-Translation\n",
      "🔍 2135/3880 Target-side Word Segmentation Strategies for Neural Machine \n",
      "🔍 2136/3880 TESLA at WMT 2011 Translation Evaluation and Tunable Metric\n",
      "🔍 2137/3880 Train, Sort, Explain Learning to Diagnose Translation Models\n",
      "🔍 2138/3880 Training on Synthetic Noise Improves Robustness to Natural N\n",
      "🔍 2139/3880 TransRead Designing a Bilingual Reading Experience with Mach\n",
      "🔍 2140/3880 Triangular Architecture for Rare Language Translation\n",
      "🔍 2141/3880 Unbabel’s Participation in the WMT19 Translation Quality Est\n",
      "🔍 2142/3880 Unsupervised Compositional Translation of Multiword Expressi\n",
      "🔍 2143/3880 Unsupervised Search for the Optimal Segmentation for Statist\n",
      "🔍 2144/3880 Systematic Comparison of Professional and Crowdsourced Refer\n",
      "🔍 2145/3880 BART Denoising Sequence-to-Sequence Pre-training for Natural\n",
      "🔍 2146/3880 Towards Multimodal Simultaneous Neural Machine Translation\n",
      "🔍 2147/3880 Can Markov Models Over Minimal Translation Units Help Phrase\n",
      "🔍 2148/3880 Diving Deep into Context-Aware Neural Machine Translation\n",
      "🔍 2149/3880 Domain Adaptation for Machine Translation by Mining Unseen W\n",
      "🔍 2150/3880 Explicit Cross-lingual Pre-training for Unsupervised Machine\n",
      "🔍 2151/3880 A Study of Residual Adapters for Multi-Domain Neural Machine\n",
      "🔍 2152/3880 Cost Optimization in Crowdsourcing Translation Low cost tran\n",
      "🔍 2153/3880 Latent Part-of-Speech Sequences for Neural Machine Translati\n",
      "🔍 2154/3880 Learning Non-linear Features for Machine Translation Using G\n",
      "🔍 2155/3880 Shallow-to-Deep Training for Neural Machine Translation\n",
      "🔍 2156/3880 Unsupervised Multimodal Neural Machine Translation with Pseu\n",
      "🔍 2157/3880 Improving Decoding Generalization for Tree-to-String Transla\n",
      "🔍 2158/3880 Iterative Refinement in the Continuous Space for Non-Autoreg\n",
      "🔍 2159/3880 Language Independent Connectivity Strength Features for Phra\n",
      "🔍 2160/3880 Mitigating Gender Bias in Machine Translation with Target Ge\n",
      "🔍 2161/3880 Discriminative Feature-Tied Mixture Modeling for Statistical\n",
      "🔍 2162/3880 Phrase Training Based Adaptation for Statistical Machine Tra\n",
      "🔍 2163/3880 Semantic Roles for String to Tree Machine Translation\n",
      "🔍 2164/3880 Towards Linear Time Neural Machine Translation with Capsule \n",
      "🔍 2165/3880 Addressing Posterior Collapse with Mutual Information for Im\n",
      "🔍 2166/3880 Balancing Training for Multilingual Neural Machine Translati\n",
      "🔍 2167/3880 Evaluating Robustness to Input Perturbations for Neural Mach\n",
      "🔍 2168/3880 Regularized Context Gates on Transformer for Machine Transla\n",
      "🔍 2169/3880 Ensembling Factored Neural Machine Translation Models for Au\n",
      "🔍 2170/3880 Is Machine Translation Ripe for Cross-Lingual Sentiment Clas\n",
      "🔍 2171/3880 Machine Translation Evaluation Meets Community Question Answ\n",
      "🔍 2172/3880 Multi-task Learning for Multilingual Neural Machine Translat\n",
      "🔍 2173/3880 Translation Acquisition Using Synonym Sets\n",
      "🔍 2174/3880 Findings of the WMT 2020 Biomedical Translation Shared Task \n",
      "🔍 2175/3880 Token-level Adaptive Training for Neural Machine Translation\n",
      "🔍 2176/3880 An Infinite Hierarchical Bayesian Model of Phrasal Translati\n",
      "🔍 2177/3880 Automated Paraphrase Lattice Creation for HyTER Machine Tran\n",
      "🔍 2178/3880 Inducing a Discriminative Parser to Optimize Machine Transla\n",
      "🔍 2179/3880 Multi-Unit Transformers for Neural Machine Translation\n",
      "🔍 2180/3880 Additive Neural Networks for Statistical Machine Translation\n",
      "🔍 2181/3880 Exploiting Semantics in Neural Machine Translation with Grap\n",
      "🔍 2182/3880 Iterative Dual Domain Adaptation for Neural Machine Translat\n",
      "🔍 2183/3880 On the Sparsity of Neural Machine Translation Models\n",
      "🔍 2184/3880 On-line Language Model Biasing for Statistical Machine Trans\n",
      "🔍 2185/3880 Pairwise Neural Machine Translation Evaluation\n",
      "🔍 2186/3880 Sentiment after Translation A Case-Study on Arabic Social Me\n",
      "🔍 2187/3880 The UMD Neural Machine Translation Systems at WMT17 Bandit L\n",
      "🔍 2188/3880 Tree-to-Sequence Attentional Neural Machine Translation\n",
      "🔍 2189/3880 A Large-Scale Test Set for the Evaluation of Context-Aware P\n",
      "🔍 2190/3880 Analyzing Optimization for Statistical Machine Translation M\n",
      "🔍 2191/3880 Building a Non-Trivial Paraphrase Corpus Using Multiple Mach\n",
      "🔍 2192/3880 CASMACAT A Computer-assisted Translation Workbench\n",
      "🔍 2193/3880 DiDi’s Machine Translation System for WMT2020\n",
      "🔍 2194/3880 Filling Gender & Number Gaps in Neural Machine Translation w\n",
      "🔍 2195/3880 Head-Driven Hierarchical Phrase-based Translation\n",
      "🔍 2196/3880 LIUM’s Contributions to the WMT2019 News Translation Task Da\n",
      "🔍 2197/3880 Meteor 1.3 Automatic Metric for Reliable Optimization and Ev\n",
      "🔍 2198/3880 Models and Inference for Prefix-Constrained Machine Translat\n",
      "🔍 2199/3880 Multi-Source Neural Machine Translation with Data Augmentati\n",
      "🔍 2200/3880 Predicting Target Language CCG Supertags Improves Neural Mac\n",
      "🔍 2201/3880 Subword Regularization Improving Neural Network Translation \n",
      "🔍 2202/3880 Syntax-based Statistical Machine Translation using Tree Auto\n",
      "🔍 2203/3880 The KIT-LIMSI Translation System for WMT 2014\n",
      "🔍 2204/3880 The University of Maryland Statistical Machine Translation S\n",
      "🔍 2205/3880 TMU Japanese-English Multimodal Machine Translation System f\n",
      "🔍 2206/3880 A Topic Similarity Model for Hierarchical Phrase-based Trans\n",
      "🔍 2207/3880 Hierarchical Phrase Table Combination for Machine Translatio\n",
      "🔍 2208/3880 Incorporating a Local Translation Mechanism into Non-autoreg\n",
      "🔍 2209/3880 Incremental Decoding and Training Methods for Simultaneous T\n",
      "🔍 2210/3880 Multi-agent Learning for Neural Machine Translation\n",
      "🔍 2211/3880 Neural Machine Translation via Binary Code Prediction\n",
      "🔍 2212/3880 Online Learning for Interactive Statistical Machine Translat\n",
      "🔍 2213/3880 Soft Dependency Constraints for Reordering in Hierarchical P\n",
      "🔍 2214/3880 Why Not Grab a Free Lunch Mining Large Corpora for Parallel \n",
      "🔍 2215/3880 Learning Hidden Unit Contribution for Adapting Neural Machin\n",
      "🔍 2216/3880 Learning Translational and Knowledge-based Similarities from\n",
      "🔍 2217/3880 Mapping the Perfect via Translation Mining\n",
      "🔍 2218/3880 Non-linear Learning for Statistical Machine Translation\n",
      "🔍 2219/3880 Pivot-based Transfer Learning for Neural Machine Translation\n",
      "🔍 2220/3880 Self-Paced Learning for Neural Machine Translation\n",
      "🔍 2221/3880 Shallow Local Multi-Bottom-up Tree Transducers in Statistica\n",
      "🔍 2222/3880 Statistical Machine Translation with Local Language Models\n",
      "🔍 2223/3880 Style Transfer Through Back-Translation\n",
      "🔍 2224/3880 What do Neural Machine Translation Models Learn about Morpho\n",
      "🔍 2225/3880 Context-Aware Monolingual Repair for Neural Machine Translat\n",
      "🔍 2226/3880 Enlisting the Ghost Modeling Empty Categories for Machine Tr\n",
      "🔍 2227/3880 Fast Generation of Translation Forest for Large-Scale SMT Di\n",
      "🔍 2228/3880 Long-Short Term Masking Transformer A Simple but Effective B\n",
      "🔍 2229/3880 Neural Machine Translation Decoding with Terminology Constra\n",
      "🔍 2230/3880 What is Hidden among Translation Rules\n",
      "🔍 2231/3880 A Multi-Domain Translation Model Framework for Statistical M\n",
      "🔍 2232/3880 Converting Continuous-Space Language Models into N-Gram Lang\n",
      "🔍 2233/3880 Generating Diverse Translation from Model Distribution with \n",
      "🔍 2234/3880 Multi-Granularity Self-Attention for Neural Machine Translat\n",
      "🔍 2235/3880 Non-projective Dependency-based Pre-Reordering with Recurren\n",
      "🔍 2236/3880 On the Evaluation of Semantic Phenomena in Neural Machine Tr\n",
      "🔍 2237/3880 Translation Assistance by Translation of L1 Fragments in an \n",
      "🔍 2238/3880 A Corpus Level MIRA Tuning Strategy for Machine Translation\n",
      "🔍 2239/3880 Non-Autoregressive Machine Translation with Latent Alignment\n",
      "🔍 2240/3880 Paraphrasing Revisited with Neural Machine Translation\n",
      "🔍 2241/3880 Part-of-Speech Induction in Dependency Trees for Statistical\n",
      "🔍 2242/3880 Pragmatic Neural Language Modelling in Machine Translation\n",
      "🔍 2243/3880 Response-based Learning for Grounded Machine Translation\n",
      "🔍 2244/3880 Structural and Topical Dimensions in Multi-Task Patent Trans\n",
      "🔍 2245/3880 Using Word Vectors to Improve Word Alignments for Low Resour\n",
      "🔍 2246/3880 Binarized Forest to String Translation\n",
      "🔍 2247/3880 Cache-based Document-level Statistical Machine Translation\n",
      "🔍 2248/3880 Identifying Word Translations from Comparable Corpora Using \n",
      "🔍 2249/3880 Statistical Machine Translation Improves Question Retrieval \n",
      "🔍 2250/3880 When and Why Are Pre-Trained Word Embeddings Useful for Neur\n",
      "🔍 2251/3880 Learning to Transform and Select Elementary Trees for Improv\n",
      "🔍 2252/3880 Minimum Imputed-Risk Unsupervised Discriminative Training fo\n",
      "🔍 2253/3880 One Model to Learn Both Zero Pronoun Prediction and Translat\n",
      "🔍 2254/3880 RTM-DCU Referential Translation Machines for Semantic Simila\n",
      "🔍 2255/3880 Dynamic Past and Future for Neural Machine Translation\n",
      "🔍 2256/3880 Rule Markov Models for Fast Tree-to-String Translation\n",
      "🔍 2257/3880 Elhuyar submission to the Biomedical Translation Task 2020 o\n",
      "🔍 2258/3880 MT Quality Estimation for Computer-assisted Translation Does\n",
      "🔍 2259/3880 Revisit Automatic Error Detection for Wrong and Missing Tran\n",
      "🔍 2260/3880 The NL2KR Platform for building Natural Language Translation\n",
      "🔍 2261/3880 Context-Dependent Translation Selection Using Convolutional \n",
      "🔍 2262/3880 Entropy-based Pruning for Phrase-based Machine Translation\n",
      "🔍 2263/3880 Towards Understanding Neural Machine Translation with Word I\n",
      "🔍 2264/3880 YerevaNN’s Systems for WMT20 Biomedical Translation Task The\n",
      "🔍 2265/3880 Alto Rapid Prototyping for Parsing and Translation\n",
      "🔍 2266/3880 Automatic Machine Translation Evaluation in Many Languages v\n",
      "🔍 2267/3880 BERT Enhanced Neural Machine Translation and Sequence Taggin\n",
      "🔍 2268/3880 Beyond Weight Tying Learning Joint Input-Output Embeddings f\n",
      "🔍 2269/3880 Chimera – Three Heads for English-to-Czech Translation\n",
      "🔍 2270/3880 End-to-End Speech-Translation with Knowledge Distillation FB\n",
      "🔍 2271/3880 Exact Decoding of Syntactic Translation Models through Lagra\n",
      "🔍 2272/3880 Exploiting Linguistic Resources for Neural Machine Translati\n",
      "🔍 2273/3880 Facebook AI’s WMT20 News Translation Task Submission\n",
      "🔍 2274/3880 Improving Multilingual Neural Machine Translation For Low-Re\n",
      "🔍 2275/3880 Jane Open Source Machine Translation System Combination\n",
      "🔍 2276/3880 LetsMT! Cloud-Based Platform for Do-It-Yourself Machine Tran\n",
      "🔍 2277/3880 Modeling Coverage for Neural Machine Translation\n",
      "🔍 2278/3880 Neural Machine Translation between Myanmar (Burmese) and Rak\n",
      "🔍 2279/3880 The Best of Both Worlds Combining Recent Advances in Neural \n",
      "🔍 2280/3880 The Effect of Translationese in Machine Translation Test Set\n",
      "🔍 2281/3880 The IIT Bombay Hindi-English Translation System at WMT 2014\n",
      "🔍 2282/3880 The Karlsruhe Institute of Technology Translation Systems fo\n",
      "🔍 2283/3880 The University of Maryland’s Kazakh-English Neural Machine T\n",
      "🔍 2284/3880 Tied Multitask Learning for Neural Speech Translation\n",
      "🔍 2285/3880 Transfer Learning in Multilingual Neural Machine Translation\n",
      "🔍 2286/3880 Consistent Translation of Repeated Nouns using Syntactic and\n",
      "🔍 2287/3880 Learning Word Reorderings for Hierarchical Phrase-based Stat\n",
      "🔍 2288/3880 Multilingual Neural Machine Translation with Language Cluste\n",
      "🔍 2289/3880 Pretrained Language Models and Backtranslation for English-B\n",
      "🔍 2290/3880 Sentence Embedding for Neural Machine Translation Domain Ada\n",
      "🔍 2291/3880 Data Augmentation for Low-Resource Neural Machine Translatio\n",
      "🔍 2292/3880 Improved Lexically Constrained Decoding for Translation and \n",
      "🔍 2293/3880 Lite Training Strategies for Portuguese-English and English-\n",
      "🔍 2294/3880 Optimizing Segmentation Strategies for Simultaneous Speech T\n",
      "🔍 2295/3880 Translation-Based Projection for Multilingual Coreference Re\n",
      "🔍 2296/3880 Unpaired Sentiment-to-Sentiment Translation A Cycled Reinfor\n",
      "🔍 2297/3880 Assessing Phrase-Based Translation Models with Oracle Decodi\n",
      "🔍 2298/3880 Knowledge-Based Question Answering as Machine Translation\n",
      "🔍 2299/3880 Multi-Pass Decoding With Complex Feature Guidance for Statis\n",
      "🔍 2300/3880 Speeding Up Neural Machine Translation Decoding by Shrinking\n",
      "🔍 2301/3880 The ADAPT’s Submissions to the WMT20 Biomedical Translation \n",
      "🔍 2302/3880 A Hybrid Approach to Skeleton-based Translation\n",
      "🔍 2303/3880 Automatic Evaluation of Translation Quality for Distant Lang\n",
      "🔍 2304/3880 Chunk-Based Bi-Scale Decoder for Neural Machine Translation\n",
      "🔍 2305/3880 FBK Machine Translation Evaluation and Word Similarity metri\n",
      "🔍 2306/3880 FJWU participation for the WMT20 Biomedical Translation Task\n",
      "🔍 2307/3880 What’s in a Domain Analyzing Genre and Topic Differences in \n",
      "🔍 2308/3880 Effective Selection of Translation Model Training Data\n",
      "🔍 2309/3880 Huawei’s Submissions to the WMT20 Biomedical Translation Tas\n",
      "🔍 2310/3880 Addressing Exposure Bias With Document Minimum Risk Training\n",
      "🔍 2311/3880 Improving Pivot Translation by Remembering the Pivot\n",
      "🔍 2312/3880 Refinements to Interactive Translation Prediction Based on S\n",
      "🔍 2313/3880 Sensible L2 Translation Assistance by Emulating the Manual P\n",
      "🔍 2314/3880 An empirical study on the effectiveness of images in Multimo\n",
      "🔍 2315/3880 Challenging Language-Dependent Segmentation for Arabic An Ap\n",
      "🔍 2316/3880 Context-Aware Graph Segmentation for Graph-Based Translation\n",
      "🔍 2317/3880 Modeling the Translation of Predicate-Argument Structure for\n",
      "🔍 2318/3880 SAARSHEFF at SemEval-2016 Task 1 Semantic Textual Similarity\n",
      "🔍 2319/3880 Unsupervised Translation Sense Clustering\n",
      "🔍 2320/3880 UoS Participation in the WMT20 Translation of Biomedical Abs\n",
      "🔍 2321/3880 A Ranking-based Approach to Word Reordering for Statistical \n",
      "🔍 2322/3880 Coverage Embedding Models for Neural Machine Translation\n",
      "🔍 2323/3880 Cross-language and Cross-encyclopedia Article Linking Using \n",
      "🔍 2324/3880 Reranking Translation Candidates Produced by Several Bilingu\n",
      "🔍 2325/3880 Character-Level Machine Translation Evaluation for Languages\n",
      "🔍 2326/3880 Extending Machine Translation Evaluation Metrics with Lexica\n",
      "🔍 2327/3880 Lexicalized Reordering for Left-to-Right Hierarchical Phrase\n",
      "🔍 2328/3880 Tencent AI Lab Machine Translation Systems for the WMT20 Bio\n",
      "🔍 2329/3880 CNGL Grading Student Answers by Acts of Translation\n",
      "🔍 2915/3880 Inquisitive Question Generation for High Level Text Comprehe\n",
      "🔍 2918/3880 Improved Neural Relation Detection for Knowledge Base Questi\n",
      "🔍 2989/3880 Syn-QG Syntactic and Shallow Semantic Rules for Question Gen\n",
      "🔍 3009/3880 QuASE Question-Answer Driven Sentence Encoding\n",
      "🔍 3029/3880 A Question Type Driven and Copy Loss Enhanced Frameworkfor A\n",
      "🔍 3035/3880 Latent Space Embedding for Retrieval in Question-Answer Arch\n",
      "🔍 3051/3880 Recovering Question Answering Errors via Query Revision\n",
      "🔍 3388/3880 Adversarial and Domain-Aware BERT for Cross-Domain Sentiment\n",
      "🔍 3391/3880 Limbic Author-Based Sentiment Aspect Modeling Regularized wi\n",
      "🔍 3461/3880 AKTSKI at SemEval-2016 Task 5 Aspect Based Sentiment Analysi\n",
      "🔍 3481/3880 Capsule Network with Interactive Attention for Aspect-Level \n",
      "🔍 3503/3880 Diversified Multiple Instance Learning for Document-Level Mu\n",
      "🔍 3509/3880 LCCT A Semi-supervised Model for Sentiment Classification\n",
      "🔍 3524/3880 iTac Aspect Based Sentiment Analysis using Sentiment Trees a\n",
      "🔍 3547/3880 Multi-Task Stance Detection with Sentiment and Stance Lexico\n",
      "🔍 3595/3880 NileTMRG at SemEval-2016 Task 7 Deriving Prior Polarities fo\n",
      "🔍 3615/3880 Transformation Networks for Target-Oriented Sentiment Classi\n",
      "🔍 3631/3880 Variational Semi-Supervised Aspect-Term Sentiment Analysis v\n",
      "🔍 3662/3880 Learning Semantic Representations of Users and Products for \n",
      "✅ Match rate: 2221/3880\n",
      "🏗️ Building Knowledge Graph...\n",
      "✅ Knowledge Graph Built Successfully\n",
      "Nodes: 2712\n",
      "Knowledge Edges: 235725\n",
      "Citation Edges: 10918\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:34:42.888792Z",
     "start_time": "2026-02-19T06:34:42.726710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a=pd.read_csv(\"knowledge_edges.csv\")\n",
    "b=pd.read_csv(\"citation_edges.csv\")"
   ],
   "id": "55c81e51a080c94b",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:35:17.783057Z",
     "start_time": "2026-02-19T06:35:17.767900Z"
    }
   },
   "cell_type": "code",
   "source": "a",
   "id": "c55a042e095efbe4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               source      target  predicate\n",
       "0       P_W2986265153  E_37693cfc          0\n",
       "1       P_W2986265153  E_37693cfc          0\n",
       "2       P_W2986265153  E_37693cfc          0\n",
       "3       P_W2986265153  E_093f65e0          0\n",
       "4       P_W2986265153  E_093f65e0          0\n",
       "...               ...         ...        ...\n",
       "235720  P_W3115997577  E_f7e6c855         27\n",
       "235721  P_W3115997577  E_bf822969         27\n",
       "235722  P_W3115997577  E_bf822969         27\n",
       "235723  P_W3115997577  E_bf822969         27\n",
       "235724  P_W3115997577  E_bf822969         27\n",
       "\n",
       "[235725 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>predicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_W2986265153</td>\n",
       "      <td>E_37693cfc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_W2986265153</td>\n",
       "      <td>E_37693cfc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_W2986265153</td>\n",
       "      <td>E_37693cfc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_W2986265153</td>\n",
       "      <td>E_093f65e0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_W2986265153</td>\n",
       "      <td>E_093f65e0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235720</th>\n",
       "      <td>P_W3115997577</td>\n",
       "      <td>E_f7e6c855</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235721</th>\n",
       "      <td>P_W3115997577</td>\n",
       "      <td>E_bf822969</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235722</th>\n",
       "      <td>P_W3115997577</td>\n",
       "      <td>E_bf822969</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235723</th>\n",
       "      <td>P_W3115997577</td>\n",
       "      <td>E_bf822969</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235724</th>\n",
       "      <td>P_W3115997577</td>\n",
       "      <td>E_bf822969</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235725 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:35:11.299201Z",
     "start_time": "2026-02-19T06:35:11.278402Z"
    }
   },
   "cell_type": "code",
   "source": "b",
   "id": "b53963c925191650",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              source         target    year\n",
       "0      P_W2971141904    P_W11511616  2019.0\n",
       "1      P_W2971141904    P_W22168010  2019.0\n",
       "2      P_W2971141904   P_W630532510  2019.0\n",
       "3      P_W2971141904  P_W2102423300  2019.0\n",
       "4      P_W2971141904  P_W2120699290  2019.0\n",
       "...              ...            ...     ...\n",
       "10913  P_W2251292973  P_W2998704965  2015.0\n",
       "10914  P_W2251292973  P_W3104097132  2015.0\n",
       "10915  P_W2251292973  P_W4285719527  2015.0\n",
       "10916  P_W2251292973  P_W4294170691  2015.0\n",
       "10917  P_W2251292973  P_W4385414156  2015.0\n",
       "\n",
       "[10918 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_W2971141904</td>\n",
       "      <td>P_W11511616</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_W2971141904</td>\n",
       "      <td>P_W22168010</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_W2971141904</td>\n",
       "      <td>P_W630532510</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_W2971141904</td>\n",
       "      <td>P_W2102423300</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_W2971141904</td>\n",
       "      <td>P_W2120699290</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10913</th>\n",
       "      <td>P_W2251292973</td>\n",
       "      <td>P_W2998704965</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10914</th>\n",
       "      <td>P_W2251292973</td>\n",
       "      <td>P_W3104097132</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10915</th>\n",
       "      <td>P_W2251292973</td>\n",
       "      <td>P_W4285719527</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10916</th>\n",
       "      <td>P_W2251292973</td>\n",
       "      <td>P_W4294170691</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10917</th>\n",
       "      <td>P_W2251292973</td>\n",
       "      <td>P_W4385414156</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10918 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:36:03.466322Z",
     "start_time": "2026-02-19T06:36:03.448776Z"
    }
   },
   "cell_type": "code",
   "source": "c=pd.read_csv(\"nodes.csv\")",
   "id": "ef70cc2bb230087c",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:36:08.085125Z",
     "start_time": "2026-02-19T06:36:08.066630Z"
    }
   },
   "cell_type": "code",
   "source": "c",
   "id": "5342603205e6ebfe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            node_id node_type    year   domain  split   name\n",
       "0     P_W3173691672     Paper  2021.0  Dia2021  NOVEL    NaN\n",
       "1     P_W3176450677     Paper  2021.0  Dia2021  NOVEL    NaN\n",
       "2     P_W3171266972     Paper  2021.0  Dia2021  NOVEL    NaN\n",
       "3     P_W3166143260     Paper  2021.0  Dia2021  NOVEL    NaN\n",
       "4     P_W3108508534     Paper  2021.0  Dia2021  NOVEL    NaN\n",
       "...             ...       ...     ...      ...    ...    ...\n",
       "2707     E_9b72e31d    Entity     NaN      NaN    NaN  540.0\n",
       "2708     E_285e19f2    Entity     NaN      NaN    NaN  503.0\n",
       "2709     E_8e6b42f1    Entity     NaN      NaN    NaN  471.0\n",
       "2710     E_5878a7ab    Entity     NaN      NaN    NaN  167.0\n",
       "2711     E_115f8950    Entity     NaN      NaN    NaN  223.0\n",
       "\n",
       "[2712 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>node_type</th>\n",
       "      <th>year</th>\n",
       "      <th>domain</th>\n",
       "      <th>split</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_W3173691672</td>\n",
       "      <td>Paper</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>NOVEL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_W3176450677</td>\n",
       "      <td>Paper</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>NOVEL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_W3171266972</td>\n",
       "      <td>Paper</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>NOVEL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_W3166143260</td>\n",
       "      <td>Paper</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>NOVEL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_W3108508534</td>\n",
       "      <td>Paper</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>Dia2021</td>\n",
       "      <td>NOVEL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>E_9b72e31d</td>\n",
       "      <td>Entity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>E_285e19f2</td>\n",
       "      <td>Entity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>E_8e6b42f1</td>\n",
       "      <td>Entity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>E_5878a7ab</td>\n",
       "      <td>Entity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>E_115f8950</td>\n",
       "      <td>Entity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2712 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:40:04.800914Z",
     "start_time": "2026-02-19T06:40:04.789518Z"
    }
   },
   "cell_type": "code",
   "source": "d=c[\"year\"]>=2022",
   "id": "628506e87025d448",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "2707    False\n",
       "2708    False\n",
       "2709    False\n",
       "2710    False\n",
       "2711    False\n",
       "Name: year, Length: 2712, dtype: bool"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T06:40:26.600493Z",
     "start_time": "2026-02-19T06:40:26.582559Z"
    }
   },
   "cell_type": "code",
   "source": "d",
   "id": "362ef5a8579c486b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[89], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m d\n",
      "\u001B[1;31mNameError\u001B[0m: name 'd' is not defined"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "37b650e1abb6865b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
