{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fc956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import hashlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57596d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENALEX_API_KEY = \"gk3pilAfGAop5QnB0IWhRy\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Novelty-Detection-Project\",\n",
    "    \"mailto\": \"ishantk250705@gmail.com\"\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://api.openalex.org/works/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84089f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_global_paper_id(split, domain, paper_id):\n",
    "    return f\"{split}_{domain}_{paper_id}\"\n",
    "\n",
    "def normalize_entity(entity):\n",
    "    return str(entity).strip().lower()\n",
    "\n",
    "def create_entity_id(entity_name):\n",
    "    h = hashlib.md5(entity_name.encode()).hexdigest()[:12]\n",
    "    return f\"E_{h}\"\n",
    "\n",
    "def fetch_openalex_metadata(openalex_id):\n",
    "    url = BASE_URL + openalex_id\n",
    "    params = {\"api_key\": OPENALEX_API_KEY}\n",
    "    response = requests.get(url, headers=HEADERS, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    data = response.json()\n",
    "    return {\n",
    "        \"year\": data.get(\"publication_year\"),\n",
    "        \"abstract\": data.get(\"abstract\"),\n",
    "        \"cited_works\": data.get(\"referenced_works\", [])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c30be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKG_FILES = {\n",
    "    \"Dia\": \"Scientific_Novelty_Detection/Triplets/SKG/Dia_triplets.csv\",\n",
    "    \"MT\": \"Scientific_Novelty_Detection/Triplets/SKG/MT_triplets.csv\",\n",
    "    \"NLI\": \"Scientific_Novelty_Detection/Triplets/SKG/NLI_triplets.csv\",\n",
    "    \"Par\": \"Scientific_Novelty_Detection/Triplets/SKG/Par_triplets.csv\",\n",
    "    \"QA\": \"Scientific_Novelty_Detection/Triplets/SKG/QA_triplets.csv\",\n",
    "    \"SA\": \"Scientific_Novelty_Detection/Triplets/SKG/SA_triplets.csv\",\n",
    "    \"Sum\": \"Scientific_Novelty_Detection/Triplets/SKG/Sum_triplets.csv\",\n",
    "}\n",
    "\n",
    "NOVEL_FILES = {\n",
    "    \"Dia\": \"Scientific_Novelty_Detection/Triplets/Novel_Papers/Dia2021_triplets.csv\",\n",
    "    \"MT\": \"Scientific_Novelty_Detection/Triplets/Novel_Papers/MT2021_triplets.csv\",\n",
    "    \"QA\": \"Scientific_Novelty_Detection/Triplets/Novel_Papers/QA2021_triplets.csv\",\n",
    "    \"SA\": \"Scientific_Novelty_Detection/Triplets/Novel_Papers/SA2021_triplets.csv\",\n",
    "    \"Sum\": \"Scientific_Novelty_Detection/Triplets/Novel_Papers/Sum2021_triplets.csv\",\n",
    "}\n",
    "\n",
    "BLOG_FILES = {\n",
    "    \"Dia\": \"Scientific_Novelty_Detection/Triplets/Blogs/Dia_Blogs_triplets.csv\",\n",
    "    \"MT\": \"Scientific_Novelty_Detection/Triplets/Blogs/MT_Blogs_triplets.csv\",\n",
    "    \"QA\": \"Scientific_Novelty_Detection/Triplets/Blogs/QA_Blogs_triplets.csv\",\n",
    "    \"SA\": \"Scientific_Novelty_Detection/Triplets/Blogs/SA_Blogs_triplets.csv\",\n",
    "    \"Sum\": \"Scientific_Novelty_Detection/Triplets/Blogs/Sum_Blogs_triplets.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f471e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SKG...\n",
      "Loading NOVEL...\n",
      "Loading BLOG...\n",
      "Total triples: 238088\n"
     ]
    }
   ],
   "source": [
    "def load_triplets(file_dict, split_name):\n",
    "    all_rows = []\n",
    "\n",
    "    for domain, path in file_dict.items():\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        required_cols = [\"paper_ID\", \"pred\", \"obj\"]\n",
    "        for col in required_cols:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"{col} not found in {path}\")\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            global_id = create_global_paper_id(\n",
    "                split_name,\n",
    "                domain,\n",
    "                row[\"paper_ID\"]\n",
    "            )\n",
    "\n",
    "            entity = normalize_entity(row[\"obj\"])\n",
    "            entity_id = create_entity_id(entity)\n",
    "\n",
    "            all_rows.append({\n",
    "                \"paper_id\": global_id,\n",
    "                \"domain\": domain,\n",
    "                \"split\": split_name,\n",
    "                \"predicate\": row[\"pred\"],\n",
    "                \"entity_name\": entity,\n",
    "                \"entity_id\": entity_id\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "\n",
    "print(\"Loading SKG...\")\n",
    "skg_df = load_triplets(SKG_FILES, \"SKG\")\n",
    "\n",
    "print(\"Loading NOVEL...\")\n",
    "novel_df = load_triplets(NOVEL_FILES, \"NOVEL\")\n",
    "\n",
    "print(\"Loading BLOG...\")\n",
    "blog_df = load_triplets(BLOG_FILES, \"BLOG\")\n",
    "\n",
    "triplets_df = pd.concat([skg_df, novel_df, blog_df], ignore_index=True)\n",
    "\n",
    "print(\"Total triples:\", len(triplets_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8179f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers: 3965\n"
     ]
    }
   ],
   "source": [
    "paper_nodes = triplets_df[[\"paper_id\", \"domain\", \"split\"]].drop_duplicates()\n",
    "\n",
    "paper_nodes[\"node_type\"] = \"Paper\"\n",
    "paper_nodes[\"year\"] = None\n",
    "paper_nodes[\"name\"] = None\n",
    "\n",
    "paper_nodes = paper_nodes.rename(columns={\"paper_id\": \"node_id\"})\n",
    "\n",
    "print(\"Total papers:\", len(paper_nodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd987892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities: 108362\n"
     ]
    }
   ],
   "source": [
    "entity_nodes = triplets_df[[\"entity_id\", \"entity_name\"]].drop_duplicates()\n",
    "\n",
    "entity_nodes[\"node_type\"] = \"Entity\"\n",
    "entity_nodes[\"year\"] = None\n",
    "entity_nodes[\"domain\"] = None\n",
    "entity_nodes[\"split\"] = None\n",
    "\n",
    "entity_nodes = entity_nodes.rename(columns={\n",
    "    \"entity_id\": \"node_id\",\n",
    "    \"entity_name\": \"name\"\n",
    "})\n",
    "\n",
    "print(\"Total entities:\", len(entity_nodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9168923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1494/3965 [09:51<16:17,  2.53it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m tqdm(paper_nodes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Extract OpenAlex ID from your ID if available\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Modify this depending on your mapping format\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     openalex_id \u001b[38;5;241m=\u001b[39m pid  \u001b[38;5;66;03m# <-- FIX THIS IF NEEDED\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_openalex_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenalex_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m meta:\n\u001b[0;32m     11\u001b[0m         metadata_store[pid] \u001b[38;5;241m=\u001b[39m meta\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mfetch_openalex_metadata\u001b[1;34m(openalex_id)\u001b[0m\n\u001b[0;32m     12\u001b[0m url \u001b[38;5;241m=\u001b[39m BASE_URL \u001b[38;5;241m+\u001b[39m openalex_id\n\u001b[0;32m     13\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: OPENALEX_API_KEY}\n\u001b[1;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:571\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 571\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    574\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metadata_store = {}\n",
    "\n",
    "for pid in tqdm(paper_nodes[\"node_id\"].tolist()):\n",
    "    # Extract OpenAlex ID from your ID if available\n",
    "    # Modify this depending on your mapping format\n",
    "    openalex_id = pid  # <-- FIX THIS IF NEEDED\n",
    "\n",
    "    meta = fetch_openalex_metadata(openalex_id)\n",
    "\n",
    "    if meta:\n",
    "        metadata_store[pid] = meta\n",
    "        time.sleep(0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25274e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplets Dia unique papers:\n",
      "304\n",
      "GROBID Dia files:\n",
      "319\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Triplets Dia unique papers:\")\n",
    "df = pd.read_csv(\"Scientific_Novelty_Detection/Triplets/SKG/Dia_triplets.csv\")\n",
    "print(df[\"paper_ID\"].nunique())\n",
    "\n",
    "print(\"GROBID Dia files:\")\n",
    "print(len(os.listdir(\"Scientific_Novelty_Detection/grobid_files/SKG_Papers/Dia\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e7b4402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.IIT-UHH at SemEval-2017 Task 3 Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification-Grobid-out.txt', '10.PLATO Pre-trained Dialogue Generation Model with Discrete Latent Variable-Grobid-out.txt', '10.Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks-Grobid-out.txt', '10.The Impact of Interpretation Problems on Tutorial Dialogue-Grobid-out.txt', '100.Learning the Information Status of Noun Phrases in Spoken Dialogues-Grobid-out.txt', '103.Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model-Grobid-out.txt', '104.Towards an Automatic Turing Test Learning to Evaluate Dialogue Responses-Grobid-out.txt', '107.Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System-Grobid-out.txt', '11.A Statistical Spoken Dialogue System using Complex User Goals and Value Directed Compression-Grobid-out.txt', '11.Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-Adaptive Spoken Dialogue System-Grobid-out.txt', '11.Representing Movie Characters in Dialogues-Grobid-out.txt', '11.Safe In-vehicle Dialogue Using Learned Predictions of User Utterances-Grobid-out.txt', '11.Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network-Grobid-out.txt', '111.Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems-Grobid-out.txt', '113.Feudal Reinforcement Learning for Dialogue Management in Large Domains-Grobid-out.txt', '120.An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue-Grobid-out.txt', '120.tucSage Grammar Rule Induction for Spoken Dialogue Systems via Probabilistic Candidate Selection-Grobid-out.txt', '121.Chat Detection in an Intelligent Assistant Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems-Grobid-out.txt', '123.A Compare Aggregate Transformer for Understanding Document-grounded Dialogue-Grobid-out.txt', '124.Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue-Grobid-out.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(\"Scientific_Novelty_Detection/grobid_files/SKG_Papers/Dia\")\n",
    "print(files[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd9693a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_dia = pd.read_csv(\"Scientific_Novelty_Detection/Triplets/SKG/Dia_triplets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dde71167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['150k human-human dialogues', 'recently introduced guesswhat ?!', 'pre-trained models', 'reinforce', 'plain stochastic gradient descent ( sgd )', '80 epochs', 'learning rate', 'batch size', '0.001', '64']\n"
     ]
    }
   ],
   "source": [
    "paper_id = 1\n",
    "\n",
    "entities = df_dia[df_dia[\"paper_ID\"] == paper_id][\"obj\"].unique()\n",
    "entities = [e.lower() for e in entities if isinstance(e, str)]\n",
    "\n",
    "print(entities[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e200335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "grobid_path = \"Scientific_Novelty_Detection/grobid_files/SKG_Papers/Dia\"\n",
    "files = os.listdir(grobid_path)\n",
    "\n",
    "def find_best_match(entities):\n",
    "    best_file = None\n",
    "    best_score = 0\n",
    "\n",
    "    for file in files:\n",
    "        with open(os.path.join(grobid_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().lower()\n",
    "\n",
    "        score = sum(1 for e in entities[:5] if e in text)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_file = file\n",
    "\n",
    "    return best_file, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc11b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.PLATO Pre-trained Dialogue Generation Model with Discrete Latent Variable-Grobid-out.txt 2\n"
     ]
    }
   ],
   "source": [
    "match, score = find_best_match(entities)\n",
    "print(match, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd8b7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match_strong(entities, grobid_path, files):\n",
    "    best_file = None\n",
    "    best_score = 0\n",
    "\n",
    "    # Filter entities (keep meaningful ones)\n",
    "    filtered = [\n",
    "        e.lower() for e in entities\n",
    "        if isinstance(e, str) and len(e.split()) > 1 and len(e) > 6\n",
    "    ]\n",
    "\n",
    "    for file in files:\n",
    "        with open(os.path.join(grobid_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().lower()\n",
    "\n",
    "        score = 0\n",
    "        for e in filtered:\n",
    "            if e in text:\n",
    "                score += 1\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_file = file\n",
    "\n",
    "    return best_file, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c911cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260.Deal or No Deal End-to-End Learning of Negotiation Dialogues-Grobid-out.txt 5\n"
     ]
    }
   ],
   "source": [
    "match, score = find_best_match_strong(entities, grobid_path, files)\n",
    "print(match, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0f6435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_triplet_documents(df):\n",
    "    docs = defaultdict(str)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = row[\"paper_ID\"]\n",
    "        text = \" \".join([\n",
    "            str(row[\"sub\"]),\n",
    "            str(row[\"pred\"]),\n",
    "            str(row[\"obj\"])\n",
    "        ])\n",
    "        docs[pid] += \" \" + text.lower()\n",
    "\n",
    "    return docs\n",
    "\n",
    "triplet_docs = build_triplet_documents(df_dia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13b4ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_docs = {}\n",
    "for file in files:\n",
    "    with open(os.path.join(grobid_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        grobid_docs[file] = f.read().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fb88de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def match_paper(pid):\n",
    "    trip_text = triplet_docs[pid]\n",
    "\n",
    "    corpus = [trip_text] + list(grobid_docs.values())\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer=\"char_wb\",\n",
    "        ngram_range=(3,5))\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    sims = cosine_similarity(tfidf[0:1], tfidf[1:]).flatten()\n",
    "\n",
    "    best_idx = sims.argmax()\n",
    "    best_score = sims[best_idx]\n",
    "    best_file = list(grobid_docs.keys())[best_idx]\n",
    "\n",
    "    return best_file, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a88b1e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.A Compare Aggregate Transformer for Understanding Document-grounded Dialogue-Grobid-out.txt 0.4338368957213103\n"
     ]
    }
   ],
   "source": [
    "match, score = match_paper(12)\n",
    "print(match, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee0e4371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done mapping.\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "\n",
    "for pid in triplet_docs.keys():\n",
    "    best_file, best_score = match_paper(pid)\n",
    "    mapping[pid] = {\n",
    "        \"filename\": best_file,\n",
    "        \"similarity\": best_score\n",
    "    }\n",
    "\n",
    "print(\"Done mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "168c463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low confidence matches: 16\n"
     ]
    }
   ],
   "source": [
    "low_conf = {k: v for k, v in mapping.items() if v[\"similarity\"] < 0.25}\n",
    "print(\"Low confidence matches:\", len(low_conf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bee4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_paper_topk(pid, k=3):\n",
    "    trip_text = triplet_docs[pid]\n",
    "    corpus = [trip_text] + list(grobid_docs.values())\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5))\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    sims = cosine_similarity(tfidf[0:1], tfidf[1:]).flatten()\n",
    "\n",
    "    top_idx = sims.argsort()[-k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        results.append((list(grobid_docs.keys())[idx], sims[idx]))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c3af1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [('544.Reading Turn by Turn Hierarchical Attention Architecture for Spoken Dialogue Comprehension-Grobid-out.txt', np.float64(0.43302192358235825)), ('652.doc2dial A Goal-Oriented Document-Grounded Dialogue Dataset-Grobid-out.txt', np.float64(0.42365534019603673)), ('429.Don’t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training-Grobid-out.txt', np.float64(0.41445582261615527))]\n"
     ]
    }
   ],
   "source": [
    "for pid, info in mapping.items():\n",
    "    if info[\"similarity\"] < 0.25:\n",
    "        print(pid, match_paper_topk(pid))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e46fc599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ishan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ishan\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 838.69it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c615ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute grobid embeddings\n",
    "grobid_embeddings = {}\n",
    "for file, text in grobid_docs.items():\n",
    "    grobid_embeddings[file] = model.encode(text, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b467655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('652.doc2dial A Goal-Oriented Document-Grounded Dialogue Dataset-Grobid-out.txt',\n",
       "  0.7339877486228943),\n",
       " ('233.Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences-Grobid-out.txt',\n",
       "  0.6888933777809143),\n",
       " ('123.A Compare Aggregate Transformer for Understanding Document-grounded Dialogue-Grobid-out.txt',\n",
       "  0.6872419118881226)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def semantic_match(pid):\n",
    "    trip_text = triplet_docs[pid]\n",
    "    trip_emb = model.encode(trip_text, convert_to_tensor=True)\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for file, emb in grobid_embeddings.items():\n",
    "        sim = util.cos_sim(trip_emb, emb).item()\n",
    "        scores[file] = sim\n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_scores[:3]\n",
    "\n",
    "semantic_match(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77d2fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_mapping(pid):\n",
    "    # First try TF-IDF\n",
    "    best_file, best_score = match_paper(pid)\n",
    "\n",
    "    if best_score >= 0.30:\n",
    "        return best_file, best_score, \"tfidf\"\n",
    "\n",
    "    # Otherwise use semantic matching\n",
    "    top3 = semantic_match(pid)\n",
    "\n",
    "    top_file, top_score = top3[0]\n",
    "    second_score = top3[1][1]\n",
    "\n",
    "    if top_score >= 0.60 and (top_score - second_score) >= 0.03:\n",
    "        return top_file, top_score, \"semantic\"\n",
    "\n",
    "    return None, None, \"manual\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f51a33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual review needed: 12\n"
     ]
    }
   ],
   "source": [
    "final_mapping = {}\n",
    "\n",
    "for pid in triplet_docs.keys():\n",
    "    file, score, method = resolve_mapping(pid)\n",
    "    final_mapping[pid] = {\n",
    "        \"filename\": file,\n",
    "        \"score\": score,\n",
    "        \"method\": method\n",
    "    }\n",
    "\n",
    "manual_cases = {k:v for k,v in final_mapping.items() if v[\"method\"] == \"manual\"}\n",
    "print(\"Manual review needed:\", len(manual_cases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c62d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
