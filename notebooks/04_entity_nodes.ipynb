{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:19.265172Z",
     "start_time": "2026-02-22T05:52:18.926303Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import os\n",
    "from glob import glob\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:19.296100Z",
     "start_time": "2026-02-22T05:52:19.274071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Change this to your actual triplet folder path\n",
    "TRIPLETS_FOLDER = \"../../Scientific_Novelty_Detection/Triplets/\"\n",
    "\n",
    "# Recursively find all *_triplets.csv files\n",
    "triplet_files = glob(os.path.join(TRIPLETS_FOLDER, \"**\", \"*_triplets.csv\"), recursive=True)\n",
    "\n",
    "print(\"Found\", len(triplet_files), \"triplet files\")\n",
    "for f in triplet_files:\n",
    "    print(f)"
   ],
   "id": "b4cc12995df9d620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 triplet files\n",
      "../../Scientific_Novelty_Detection/Triplets\\Blogs\\Dia_Blogs_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Blogs\\MT_Blogs_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Blogs\\QA_Blogs_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Blogs\\SA_Blogs_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Blogs\\Sum_Blogs_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Novel_Papers\\Dia2021_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Novel_Papers\\MT2021_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Novel_Papers\\QA2021_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Novel_Papers\\SA2021_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\Novel_Papers\\Sum2021_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\SKG\\Dia_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\SKG\\MT_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\SKG\\NLI_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\SKG\\Par_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\SKG\\QA_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\SKG\\SA_triplets.csv\n",
      "../../Scientific_Novelty_Detection/Triplets\\SKG\\Sum_triplets.csv\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:19.935847Z",
     "start_time": "2026-02-22T05:52:19.306064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Merge all triplets\n",
    "all_dfs = []\n",
    "\n",
    "for file in triplet_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if \"sub\" not in df.columns or \"obj\" not in df.columns:\n",
    "        raise ValueError(f\"Missing 'sub' or 'obj' in {file}\")\n",
    "\n",
    "    all_dfs.append(df[[\"sub\", \"obj\"]])\n",
    "\n",
    "triplets_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "print(\"Total triplet rows:\", len(triplets_df))"
   ],
   "id": "6ec5ac61f47853ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triplet rows: 238088\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:19.965751Z",
     "start_time": "2026-02-22T05:52:19.945815Z"
    }
   },
   "cell_type": "code",
   "source": "triplets_df",
   "id": "c925c7c859725b4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   sub                             obj\n",
       "0                     hyper-parameters           more powerful decoder\n",
       "1                more powerful decoder   higher conversational quality\n",
       "2                    ablation analysis                hyper-parameters\n",
       "3                       tuned decoding                       ssa score\n",
       "4                            ssa score                            79 %\n",
       "...                                ...                             ...\n",
       "238083                         results                  proposed model\n",
       "238084                       our model                longer sequences\n",
       "238085                longer sequences  introduction and the sentences\n",
       "238086  introduction and the sentences                       extractor\n",
       "238087                         results                       our model\n",
       "\n",
       "[238088 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyper-parameters</td>\n",
       "      <td>more powerful decoder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>more powerful decoder</td>\n",
       "      <td>higher conversational quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablation analysis</td>\n",
       "      <td>hyper-parameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tuned decoding</td>\n",
       "      <td>ssa score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ssa score</td>\n",
       "      <td>79 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238083</th>\n",
       "      <td>results</td>\n",
       "      <td>proposed model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238084</th>\n",
       "      <td>our model</td>\n",
       "      <td>longer sequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238085</th>\n",
       "      <td>longer sequences</td>\n",
       "      <td>introduction and the sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238086</th>\n",
       "      <td>introduction and the sentences</td>\n",
       "      <td>extractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238087</th>\n",
       "      <td>results</td>\n",
       "      <td>our model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238088 rows Ã— 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:20.510095Z",
     "start_time": "2026-02-22T05:52:20.018823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Normalise Entity Strings\n",
    "def normalize_entity(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "\n",
    "    text = str(text).strip().lower()\n",
    "\n",
    "    # Remove excessive quotes\n",
    "    text = text.replace('\"\"\"', '\"')\n",
    "    text = text.replace(\"''\", \"'\")\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "triplets_df[\"sub\"] = triplets_df[\"sub\"].apply(normalize_entity)\n",
    "triplets_df[\"obj\"] = triplets_df[\"obj\"].apply(normalize_entity)"
   ],
   "id": "bea2840ec70b4fcd",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:20.682115Z",
     "start_time": "2026-02-22T05:52:20.565334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_entities = set(triplets_df[\"sub\"].dropna()) | set(triplets_df[\"obj\"].dropna())\n",
    "print(\"Raw unique entities:\", len(raw_entities))"
   ],
   "id": "e339b87d64b59c28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw unique entities: 119379\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:20.913912Z",
     "start_time": "2026-02-22T05:52:20.902799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_valid_entity(text):\n",
    "    if not text:\n",
    "        return False\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    # Minimum length\n",
    "    if len(text) < 3:\n",
    "        return False\n",
    "\n",
    "    # Remove parenthesis-dominated patterns\n",
    "    if text.startswith(\"(\"):\n",
    "        return False\n",
    "\n",
    "    if re.fullmatch(r\"\\(.*\\)\", text):\n",
    "        return False\n",
    "\n",
    "    # Remove set notation / math blocks\n",
    "    if text.startswith(\"{\") or text.startswith(\"|\"):\n",
    "        return False\n",
    "\n",
    "    if re.search(r\"\\{.*\\}\", text):\n",
    "        return False\n",
    "\n",
    "    # Remove scientific notation\n",
    "    if re.search(r\"\\d+e[-\\s]?\\d+\", text):\n",
    "        return False\n",
    "\n",
    "    # Remove heavy numeric ratio\n",
    "    digit_ratio = sum(c.isdigit() for c in text) / len(text)\n",
    "    if digit_ratio > 0.4:\n",
    "        return False\n",
    "\n",
    "    # Remove hardware/config patterns\n",
    "    if \"gpu\" in text:\n",
    "        return False\n",
    "\n",
    "    if \"units\" in text:\n",
    "        return False\n",
    "\n",
    "    if \"encoder\" in text and \"decoder\" in text:\n",
    "        return False\n",
    "\n",
    "    # Remove statistical fragments\n",
    "    if text.startswith(\"~\"):\n",
    "        return False\n",
    "\n",
    "    if \"%\" in text:\n",
    "        return False\n",
    "\n",
    "    stat_patterns = [\n",
    "        \" more \",\n",
    "        \" lower\",\n",
    "        \" lines\",\n",
    "        \" questions\",\n",
    "        \" entities\",\n",
    "        \" pp\",\n",
    "        \" ratio\"\n",
    "    ]\n",
    "\n",
    "    for p in stat_patterns:\n",
    "        if p in text:\n",
    "            return False\n",
    "\n",
    "    if re.search(r\"\\d+\\s?[km]\", text):\n",
    "        return False\n",
    "\n",
    "    if re.search(r\"\\d+\\.?\\d*x\", text):\n",
    "        return False\n",
    "\n",
    "    # Must contain alphabet\n",
    "    if not re.search(r\"[a-zA-Z]\", text):\n",
    "        return False\n",
    "\n",
    "    # Remove pure punctuation\n",
    "    if re.fullmatch(r\"[\\W_]+\", text):\n",
    "        return False\n",
    "\n",
    "    return True"
   ],
   "id": "961f4115a0e44c1e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:22.064435Z",
     "start_time": "2026-02-22T05:52:21.108378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Extract unique Entities\n",
    "clean_entities = {e for e in raw_entities if is_valid_entity(e)}\n",
    "print(\"Clean unique entities:\", len(clean_entities))"
   ],
   "id": "b68c2a8019703139",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean unique entities: 108816\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:22.606193Z",
     "start_time": "2026-02-22T05:52:22.242565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Assign entity IDs\n",
    "def generate_entity_id(entity_string):\n",
    "    return \"E_\" + hashlib.md5(entity_string.encode(\"utf-8\")).hexdigest()[:10]\n",
    "\n",
    "entity_data = []\n",
    "\n",
    "for entity in sorted(clean_entities):\n",
    "    entity_id = generate_entity_id(entity)\n",
    "    entity_data.append({\n",
    "        \"node_id\": entity_id,\n",
    "        \"node_type\": \"Entity\",\n",
    "        \"name\": entity\n",
    "    })\n",
    "\n",
    "entity_nodes_df = pd.DataFrame(entity_data)"
   ],
   "id": "451096f28543ad9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:22.668723Z",
     "start_time": "2026-02-22T05:52:22.615188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## VERIFY NO HASH COLLISIONS\n",
    "print(\"Duplicate entity IDs:\",\n",
    "      entity_nodes_df[\"node_id\"].duplicated().sum())\n",
    "\n",
    "assert entity_nodes_df[\"node_id\"].is_unique"
   ],
   "id": "e80838323eb4b9e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entity IDs: 0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:52.610390Z",
     "start_time": "2026-02-22T05:52:52.591378Z"
    }
   },
   "cell_type": "code",
   "source": "assert entity_nodes_df[\"name\"].isnull().sum() == 0",
   "id": "8104a042a3f48b7e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:22.917320Z",
     "start_time": "2026-02-22T05:52:22.722614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "entity_nodes_df.to_csv(\"../outputs/entity_nodes.csv\", index=False)\n",
    "\n",
    "print(\"entity_nodes.csv created successfully.\")"
   ],
   "id": "f46ec2f008a15901",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_nodes.csv created successfully.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:52:22.963102Z",
     "start_time": "2026-02-22T05:52:22.936328Z"
    }
   },
   "cell_type": "code",
   "source": "entity_nodes_df.sample(50)",
   "id": "124366ff9ecc2db5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             node_id node_type  \\\n",
       "20086   E_83c45eb71b    Entity   \n",
       "13207   E_940ff42ba7    Entity   \n",
       "35316   E_1d90d19da5    Entity   \n",
       "95403   E_7f1323c006    Entity   \n",
       "17481   E_d79b581f19    Entity   \n",
       "23860   E_92d11b339c    Entity   \n",
       "60371   E_09d4ca34b1    Entity   \n",
       "3024    E_7db35e539a    Entity   \n",
       "48530   E_42744c1f33    Entity   \n",
       "29538   E_2bcd7c0a47    Entity   \n",
       "29076   E_0e63ee1e15    Entity   \n",
       "45969   E_01158e75c6    Entity   \n",
       "27359   E_cd14a6d6d9    Entity   \n",
       "79405   E_36b8fd14e8    Entity   \n",
       "32372   E_81a6d1b533    Entity   \n",
       "37550   E_d856c8e31b    Entity   \n",
       "15154   E_504688f337    Entity   \n",
       "77735   E_3700d9bb3f    Entity   \n",
       "87637   E_7e4ed17afe    Entity   \n",
       "76194   E_11783c48a3    Entity   \n",
       "85459   E_59ed8f873a    Entity   \n",
       "72781   E_0e0db0b505    Entity   \n",
       "85149   E_0602dd664f    Entity   \n",
       "95871   E_be1f3a5e55    Entity   \n",
       "1402    E_b878e78502    Entity   \n",
       "51509   E_461001ae62    Entity   \n",
       "108281  E_d45e2534a0    Entity   \n",
       "490     E_19ca55cfb1    Entity   \n",
       "66214   E_2787ab1947    Entity   \n",
       "26670   E_c4a7f7c2d5    Entity   \n",
       "96845   E_3898686c2c    Entity   \n",
       "16195   E_29d7bbdbf2    Entity   \n",
       "50358   E_663b07099b    Entity   \n",
       "103222  E_2697a2f1a4    Entity   \n",
       "84960   E_4c052b3e99    Entity   \n",
       "8090    E_a7e0e38ef4    Entity   \n",
       "39424   E_a4ceb9381c    Entity   \n",
       "34272   E_5eb4435e7c    Entity   \n",
       "55500   E_8691dd5160    Entity   \n",
       "97431   E_89221cfd74    Entity   \n",
       "92269   E_673a8d5b26    Entity   \n",
       "64095   E_08758a44d7    Entity   \n",
       "104373  E_a42af86ae2    Entity   \n",
       "70350   E_4c42f81e40    Entity   \n",
       "98798   E_da7b2eb26d    Entity   \n",
       "105607  E_631e884d98    Entity   \n",
       "25323   E_da9cc75d73    Entity   \n",
       "38629   E_57a3813573    Entity   \n",
       "83415   E_4e98d691d5    Entity   \n",
       "24213   E_3aa923a0b1    Entity   \n",
       "\n",
       "                                                     name  \n",
       "20086                          chinese- to - english task  \n",
       "13207                    average of the embedding vectors  \n",
       "35316                      existing top performance model  \n",
       "95403                                        system pastr  \n",
       "17481                            bleu and gender accuracy  \n",
       "23860                        conversational kb - qa agent  \n",
       "60371                                        multi-hop qg  \n",
       "3024                                     2.12 bleu points  \n",
       "48530                              joint bpe vocabularies  \n",
       "29538                                   document rotation  \n",
       "29076                distinctive performance improvements  \n",
       "45969                                   in- domain corpus  \n",
       "27359                              desirable translations  \n",
       "79405                                               recap  \n",
       "32372                                embedding clustering  \n",
       "37550                                        fine-tunning  \n",
       "15154   bert or other state- ofthe - art contextual la...  \n",
       "77735                                     query attention  \n",
       "87637                 similar source and target languages  \n",
       "76194                                      promising step  \n",
       "85459                               sentiment calculation  \n",
       "72781                            performance improvements  \n",
       "85149                                      sentence pairs  \n",
       "95871                             target conjunction aber  \n",
       "1402           1 - best and forest - based configurations  \n",
       "51509                                          learned hs  \n",
       "108281                                             x - en  \n",
       "490                                            + 6.3 bleu  \n",
       "66214                              number of unique words  \n",
       "26670                  deep knowledge tracing ( lm - kt )  \n",
       "96845                                         term metric  \n",
       "16195                                        better model  \n",
       "50358                      large domainindependent corpus  \n",
       "103222                                 unigrams & bigrams  \n",
       "84960                     sentence - level repair version  \n",
       "8090                      alignments and summary patterns  \n",
       "39424                                  fully trained cdcm  \n",
       "34272                    entity detection and entity type  \n",
       "55500                       max-overtime pooling approach  \n",
       "97431                                      tgt vocab size  \n",
       "92269   state - of - the - art dependency tree - to - ...  \n",
       "64095                                       nnjm features  \n",
       "104373                               utterance embeddings  \n",
       "70350                       our prompt- based bert models  \n",
       "98798                       top reranked set of sentences  \n",
       "105607                          vmf and syn-margin models  \n",
       "25323                                       cube function  \n",
       "38629                       four kinds of representations  \n",
       "83415                         scale of the source dataset  \n",
       "24213                               corpus-based learning  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>node_type</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20086</th>\n",
       "      <td>E_83c45eb71b</td>\n",
       "      <td>Entity</td>\n",
       "      <td>chinese- to - english task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13207</th>\n",
       "      <td>E_940ff42ba7</td>\n",
       "      <td>Entity</td>\n",
       "      <td>average of the embedding vectors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35316</th>\n",
       "      <td>E_1d90d19da5</td>\n",
       "      <td>Entity</td>\n",
       "      <td>existing top performance model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95403</th>\n",
       "      <td>E_7f1323c006</td>\n",
       "      <td>Entity</td>\n",
       "      <td>system pastr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17481</th>\n",
       "      <td>E_d79b581f19</td>\n",
       "      <td>Entity</td>\n",
       "      <td>bleu and gender accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23860</th>\n",
       "      <td>E_92d11b339c</td>\n",
       "      <td>Entity</td>\n",
       "      <td>conversational kb - qa agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60371</th>\n",
       "      <td>E_09d4ca34b1</td>\n",
       "      <td>Entity</td>\n",
       "      <td>multi-hop qg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>E_7db35e539a</td>\n",
       "      <td>Entity</td>\n",
       "      <td>2.12 bleu points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48530</th>\n",
       "      <td>E_42744c1f33</td>\n",
       "      <td>Entity</td>\n",
       "      <td>joint bpe vocabularies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29538</th>\n",
       "      <td>E_2bcd7c0a47</td>\n",
       "      <td>Entity</td>\n",
       "      <td>document rotation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29076</th>\n",
       "      <td>E_0e63ee1e15</td>\n",
       "      <td>Entity</td>\n",
       "      <td>distinctive performance improvements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45969</th>\n",
       "      <td>E_01158e75c6</td>\n",
       "      <td>Entity</td>\n",
       "      <td>in- domain corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27359</th>\n",
       "      <td>E_cd14a6d6d9</td>\n",
       "      <td>Entity</td>\n",
       "      <td>desirable translations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79405</th>\n",
       "      <td>E_36b8fd14e8</td>\n",
       "      <td>Entity</td>\n",
       "      <td>recap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32372</th>\n",
       "      <td>E_81a6d1b533</td>\n",
       "      <td>Entity</td>\n",
       "      <td>embedding clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37550</th>\n",
       "      <td>E_d856c8e31b</td>\n",
       "      <td>Entity</td>\n",
       "      <td>fine-tunning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15154</th>\n",
       "      <td>E_504688f337</td>\n",
       "      <td>Entity</td>\n",
       "      <td>bert or other state- ofthe - art contextual la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77735</th>\n",
       "      <td>E_3700d9bb3f</td>\n",
       "      <td>Entity</td>\n",
       "      <td>query attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87637</th>\n",
       "      <td>E_7e4ed17afe</td>\n",
       "      <td>Entity</td>\n",
       "      <td>similar source and target languages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76194</th>\n",
       "      <td>E_11783c48a3</td>\n",
       "      <td>Entity</td>\n",
       "      <td>promising step</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85459</th>\n",
       "      <td>E_59ed8f873a</td>\n",
       "      <td>Entity</td>\n",
       "      <td>sentiment calculation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72781</th>\n",
       "      <td>E_0e0db0b505</td>\n",
       "      <td>Entity</td>\n",
       "      <td>performance improvements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85149</th>\n",
       "      <td>E_0602dd664f</td>\n",
       "      <td>Entity</td>\n",
       "      <td>sentence pairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95871</th>\n",
       "      <td>E_be1f3a5e55</td>\n",
       "      <td>Entity</td>\n",
       "      <td>target conjunction aber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>E_b878e78502</td>\n",
       "      <td>Entity</td>\n",
       "      <td>1 - best and forest - based configurations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51509</th>\n",
       "      <td>E_461001ae62</td>\n",
       "      <td>Entity</td>\n",
       "      <td>learned hs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108281</th>\n",
       "      <td>E_d45e2534a0</td>\n",
       "      <td>Entity</td>\n",
       "      <td>x - en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>E_19ca55cfb1</td>\n",
       "      <td>Entity</td>\n",
       "      <td>+ 6.3 bleu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66214</th>\n",
       "      <td>E_2787ab1947</td>\n",
       "      <td>Entity</td>\n",
       "      <td>number of unique words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26670</th>\n",
       "      <td>E_c4a7f7c2d5</td>\n",
       "      <td>Entity</td>\n",
       "      <td>deep knowledge tracing ( lm - kt )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96845</th>\n",
       "      <td>E_3898686c2c</td>\n",
       "      <td>Entity</td>\n",
       "      <td>term metric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16195</th>\n",
       "      <td>E_29d7bbdbf2</td>\n",
       "      <td>Entity</td>\n",
       "      <td>better model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50358</th>\n",
       "      <td>E_663b07099b</td>\n",
       "      <td>Entity</td>\n",
       "      <td>large domainindependent corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103222</th>\n",
       "      <td>E_2697a2f1a4</td>\n",
       "      <td>Entity</td>\n",
       "      <td>unigrams &amp; bigrams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84960</th>\n",
       "      <td>E_4c052b3e99</td>\n",
       "      <td>Entity</td>\n",
       "      <td>sentence - level repair version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>E_a7e0e38ef4</td>\n",
       "      <td>Entity</td>\n",
       "      <td>alignments and summary patterns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39424</th>\n",
       "      <td>E_a4ceb9381c</td>\n",
       "      <td>Entity</td>\n",
       "      <td>fully trained cdcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34272</th>\n",
       "      <td>E_5eb4435e7c</td>\n",
       "      <td>Entity</td>\n",
       "      <td>entity detection and entity type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55500</th>\n",
       "      <td>E_8691dd5160</td>\n",
       "      <td>Entity</td>\n",
       "      <td>max-overtime pooling approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97431</th>\n",
       "      <td>E_89221cfd74</td>\n",
       "      <td>Entity</td>\n",
       "      <td>tgt vocab size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92269</th>\n",
       "      <td>E_673a8d5b26</td>\n",
       "      <td>Entity</td>\n",
       "      <td>state - of - the - art dependency tree - to - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64095</th>\n",
       "      <td>E_08758a44d7</td>\n",
       "      <td>Entity</td>\n",
       "      <td>nnjm features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104373</th>\n",
       "      <td>E_a42af86ae2</td>\n",
       "      <td>Entity</td>\n",
       "      <td>utterance embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70350</th>\n",
       "      <td>E_4c42f81e40</td>\n",
       "      <td>Entity</td>\n",
       "      <td>our prompt- based bert models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98798</th>\n",
       "      <td>E_da7b2eb26d</td>\n",
       "      <td>Entity</td>\n",
       "      <td>top reranked set of sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105607</th>\n",
       "      <td>E_631e884d98</td>\n",
       "      <td>Entity</td>\n",
       "      <td>vmf and syn-margin models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25323</th>\n",
       "      <td>E_da9cc75d73</td>\n",
       "      <td>Entity</td>\n",
       "      <td>cube function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38629</th>\n",
       "      <td>E_57a3813573</td>\n",
       "      <td>Entity</td>\n",
       "      <td>four kinds of representations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83415</th>\n",
       "      <td>E_4e98d691d5</td>\n",
       "      <td>Entity</td>\n",
       "      <td>scale of the source dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24213</th>\n",
       "      <td>E_3aa923a0b1</td>\n",
       "      <td>Entity</td>\n",
       "      <td>corpus-based learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T05:53:20.557721Z",
     "start_time": "2026-02-22T05:53:20.518729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Total entities:\", len(entity_nodes_df))\n",
    "print(\"Duplicate names:\",\n",
    "      entity_nodes_df[\"name\"].duplicated().sum())\n",
    "print(\"Duplicate IDs:\",\n",
    "      entity_nodes_df[\"node_id\"].duplicated().sum())\n",
    "\n",
    "print(\"Entity node checks passed.\")"
   ],
   "id": "317f7b2ad3adf196",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities: 108816\n",
      "Duplicate names: 0\n",
      "Duplicate IDs: 0\n",
      "Entity node checks passed.\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
