global_paper_id,openalex_id,year,cited_by_count,referenced_works,abstract,match_score
NOVEL_DIA_0,https://openalex.org/W3214342458,2021,0,"['https://openalex.org/W242376439', 'https://openalex.org/W1996117323', 'https://openalex.org/W1997161938', 'https://openalex.org/W2098441518', 'https://openalex.org/W2101137458', 'https://openalex.org/W2250539671', 'https://openalex.org/W2784400615', 'https://openalex.org/W2822830299', 'https://openalex.org/W2890140518', 'https://openalex.org/W2893684749', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914204778', 'https://openalex.org/W2916772188', 'https://openalex.org/W2950457956', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963903950', 'https://openalex.org/W2965191666', 'https://openalex.org/W2969574947', 'https://openalex.org/W2971883198', 'https://openalex.org/W2973226110', 'https://openalex.org/W2995969307', 'https://openalex.org/W3034600233']","Recent state-of-the-art approaches in open-domain dialogue include training end-to-end deep-learning models to learn various conversational features like emotional content of response, symbolic transitions of dialogue contexts in a knowledge graph and persona of the agent and the user, among others. While neural models have shown reasonable results, modelling the cognitive processes that humans use when conversing with each other may improve the agent’s quality of responses. A key element of natural conversation is to tailor one’s response such that it accounts for concepts that the speaker and listener may or may not know and the contextual relevance of all prior concepts used in conversation. We show that a rich representation and explicit modeling of these psychological processes can improve predictions made by existing neural network models. In this work, we propose a novel probabilistic approach using Markov Random Fields (MRF) to augment existing deep-learning methods for improved next utterance prediction. Using human and automatic evaluations, we show that our augmentation approach significantly improves the performance of existing state-of-the-art retrieval models for open-domain conversational agents.",0.9906542056074766
NOVEL_DIA_1,https://openalex.org/W3196896228,2021,12,"['https://openalex.org/W222053410', 'https://openalex.org/W1503071992', 'https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133012565', 'https://openalex.org/W2136353104', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250969425', 'https://openalex.org/W2397490041', 'https://openalex.org/W2608029998', 'https://openalex.org/W2669742347', 'https://openalex.org/W2767019613', 'https://openalex.org/W2771249222', 'https://openalex.org/W2794365787', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2904829696', 'https://openalex.org/W2922158773', 'https://openalex.org/W2951039930', 'https://openalex.org/W2952446148', 'https://openalex.org/W2952889708', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962882341', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963686995', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964345285', 'https://openalex.org/W2964669873', 'https://openalex.org/W2970240344', 'https://openalex.org/W2970489252', 'https://openalex.org/W2971287409', 'https://openalex.org/W2971347700', 'https://openalex.org/W2971737394', 'https://openalex.org/W2985067290', 'https://openalex.org/W3012405907', 'https://openalex.org/W3033962033', 'https://openalex.org/W3034351728', 'https://openalex.org/W3034999754', 'https://openalex.org/W3035520602', 'https://openalex.org/W3035629723', 'https://openalex.org/W3099925113', 'https://openalex.org/W3101668578', 'https://openalex.org/W3101683892', 'https://openalex.org/W3103733040', 'https://openalex.org/W3105218667', 'https://openalex.org/W3111706314', 'https://openalex.org/W3113225429', 'https://openalex.org/W3120168417', 'https://openalex.org/W3120749277', 'https://openalex.org/W3120964679', 'https://openalex.org/W3147710239', 'https://openalex.org/W3168420886', 'https://openalex.org/W3173680274', 'https://openalex.org/W3175424132', 'https://openalex.org/W3175870450', 'https://openalex.org/W4385245566', 'https://openalex.org/W4404781009']","Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue characteristics of chat, such as dialogue coherence and speaker personality, are neglected. In this paper, we propose to promote the chat translation by introducing the modeling of dialogue characteristics into the NCT model. To this end, we design four auxiliary tasks including monolingual response generation, cross-lingual response generation, next utterance discrimination, and speaker identification. Together with the main chat translation task, we optimize the enhanced NCT model through the training objectives of all these tasks. By this means, the NCT model can be enhanced by capturing the inherent dialogue characteristics, thus generating more coherent and speaker-relevant translations. Comprehensive experiments on four language directions (English<->German and English<->Chinese) verify the effectiveness and superiority of the proposed approach.",1.0
NOVEL_DIA_2,https://openalex.org/W3173606101,2021,19,"['https://openalex.org/W1522301498', 'https://openalex.org/W2061397531', 'https://openalex.org/W2911489562', 'https://openalex.org/W2944815030', 'https://openalex.org/W2949769095', 'https://openalex.org/W2952468927', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964223283', 'https://openalex.org/W2970119519', 'https://openalex.org/W2970597249', 'https://openalex.org/W2998189980', 'https://openalex.org/W3011411500', 'https://openalex.org/W3034238904', 'https://openalex.org/W3034503989', 'https://openalex.org/W3036362489', 'https://openalex.org/W3098694757', 'https://openalex.org/W3113519452', 'https://openalex.org/W3134155563', 'https://openalex.org/W3153491684', 'https://openalex.org/W4298870559']","Han Wu, Kun Xu, Linfeng Song, Lifeng Jin, Haisong Zhang, Linqi Song. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",1.0
NOVEL_DIA_3,https://openalex.org/W3214623240,2021,5,"['https://openalex.org/W648786980', 'https://openalex.org/W1522301498', 'https://openalex.org/W1861492603', 'https://openalex.org/W1895577753', 'https://openalex.org/W1947481528', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2176263492', 'https://openalex.org/W2268617045', 'https://openalex.org/W2525778437', 'https://openalex.org/W2581637843', 'https://openalex.org/W2607987856', 'https://openalex.org/W2890220768', 'https://openalex.org/W2908747729', 'https://openalex.org/W2938704169', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963163972', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964345285', 'https://openalex.org/W2968297680', 'https://openalex.org/W2979826702', 'https://openalex.org/W2995028880', 'https://openalex.org/W2995404354', 'https://openalex.org/W2996287690', 'https://openalex.org/W3003223181', 'https://openalex.org/W3033166111', 'https://openalex.org/W3098824823', 'https://openalex.org/W3100439847', 'https://openalex.org/W3106118243', 'https://openalex.org/W3176393173', 'https://openalex.org/W4287817357', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566']","Although exposure bias has been widely studied in some NLP tasks, it faces its unique challenges in dialogue response generation, the representative one-to-various generation scenario.In real human dialogue, there are many appropriate responses for the same context, not only with different expressions, but also with different topics. Therefore, due to the much bigger gap between various ground-truth responses and the generated synthetic response, exposure bias is more challenging in dialogue generation task.What’s more, as MLE encourages the model to only learn the common words among different ground-truth responses, but ignores the interesting and specific parts, exposure bias may further lead to the common response generation problem, such as “I don’t know” and “HaHa?” In this paper, we propose a novel adaptive switching mechanism, which learns to automatically transit between ground-truth learning and generated learning regarding the word-level matching score, such as the cosine similarity. Experimental results on both Chinese STC dataset and English Reddit dataset, show that our adaptive method achieves a significant improvement in terms of metric-based evaluation and human evaluation, as compared with the state-of-the-art exposure bias approaches. Further analysis on NMT task also shows that our model can achieve a significant improvement.",1.0
NOVEL_DIA_4,https://openalex.org/W3074476581,2021,6,"['https://openalex.org/W2037789405', 'https://openalex.org/W2106087324', 'https://openalex.org/W2115792525', 'https://openalex.org/W2328886022', 'https://openalex.org/W2728192282', 'https://openalex.org/W2740167620', 'https://openalex.org/W2788330850', 'https://openalex.org/W2796032388', 'https://openalex.org/W2798463315', 'https://openalex.org/W2885421725', 'https://openalex.org/W2889009749', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914204778', 'https://openalex.org/W2914397182', 'https://openalex.org/W2914855263', 'https://openalex.org/W2940154139', 'https://openalex.org/W2946803219', 'https://openalex.org/W2949940827', 'https://openalex.org/W2955539078', 'https://openalex.org/W2962753250', 'https://openalex.org/W2962796276', 'https://openalex.org/W2962805889', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963332597', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963521540', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963695965', 'https://openalex.org/W2963964898', 'https://openalex.org/W2964110616', 'https://openalex.org/W2964202145', 'https://openalex.org/W2965718149', 'https://openalex.org/W2970785611', 'https://openalex.org/W2973049837', 'https://openalex.org/W2979702391', 'https://openalex.org/W2981259322', 'https://openalex.org/W2988647680', 'https://openalex.org/W2989202909', 'https://openalex.org/W2995289474', 'https://openalex.org/W2996287690', 'https://openalex.org/W3000779003', 'https://openalex.org/W3005136412', 'https://openalex.org/W3017860180', 'https://openalex.org/W3021582395', 'https://openalex.org/W3022065131', 'https://openalex.org/W3022592851', 'https://openalex.org/W3023366413', 'https://openalex.org/W3023786569', 'https://openalex.org/W3035068109', 'https://openalex.org/W3035636774']","Dialogue systems pretrained with large language models generate locally coherent responses, but lack the fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. But, current exemplar-based approaches often excessively copy words from the exemplar responses, leading to incoherent replies. We present an Exemplar-based Dialogue Generation model, EDGE, that uses the semantic frames present in exemplar responses to guide generation. We show that controlling dialogue generation based on the semantic frames of exemplars, rather than words in the exemplar itself, improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar responses.",1.0
NOVEL_DIA_5,https://openalex.org/W3212156931,2021,0,"['https://openalex.org/W10957333', 'https://openalex.org/W131533222', 'https://openalex.org/W1599016936', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2130158090', 'https://openalex.org/W2154652894', 'https://openalex.org/W2251939518', 'https://openalex.org/W2328886022', 'https://openalex.org/W2396767181', 'https://openalex.org/W2525127255', 'https://openalex.org/W2894060751', 'https://openalex.org/W2913443447', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963527228', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963854351', 'https://openalex.org/W2964178377', 'https://openalex.org/W2965373594', 'https://openalex.org/W2971164557', 'https://openalex.org/W2972664115', 'https://openalex.org/W2978670439', 'https://openalex.org/W3032834023', 'https://openalex.org/W3034808773', 'https://openalex.org/W3104033643', 'https://openalex.org/W3105604018', 'https://openalex.org/W3117162907', 'https://openalex.org/W3117574848', 'https://openalex.org/W3121541553']","The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Despite the abundance of work done in the field, human judges have to evaluate dialogues quality. As a consequence, performing such evaluations at scale is usually expensive. This work investigates using a deep-learning model trained on the General Language Understanding Evaluation (GLUE) benchmark to serve as a quality indication of open-domain dialogues. The aim is to use the various GLUE tasks as different perspectives on judging the quality of conversation, thus reducing the need for additional training data or responses that serve as quality references. Due to this nature, the method can infer various quality metrics and can derive a component-based overall score. We achieve statistically signif icant correlation coefficients of up to 0.7.",1.0
NOVEL_DIA_6,https://openalex.org/W3202597539,2021,11,"['https://openalex.org/W2270070752', 'https://openalex.org/W2419539795', 'https://openalex.org/W2620558438', 'https://openalex.org/W2898700502', 'https://openalex.org/W2898856000', 'https://openalex.org/W2914120296', 'https://openalex.org/W2914204778', 'https://openalex.org/W2951216772', 'https://openalex.org/W2962738716', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964101860', 'https://openalex.org/W2971277088', 'https://openalex.org/W2973088264', 'https://openalex.org/W2979826702', 'https://openalex.org/W2997214274', 'https://openalex.org/W3011574394', 'https://openalex.org/W3034238904', 'https://openalex.org/W3034255912', 'https://openalex.org/W3034716087', 'https://openalex.org/W3034724424', 'https://openalex.org/W3035016936', 'https://openalex.org/W3035579820', 'https://openalex.org/W3036362489', 'https://openalex.org/W3037879762', 'https://openalex.org/W3098824823', 'https://openalex.org/W3098826124', 'https://openalex.org/W3116343068', 'https://openalex.org/W3121523248', 'https://openalex.org/W3124687886', 'https://openalex.org/W3155682407', 'https://openalex.org/W4235805184', 'https://openalex.org/W4287599851', 'https://openalex.org/W4288288848', 'https://openalex.org/W4288624561', 'https://openalex.org/W4322614701', 'https://openalex.org/W4385245566']","Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English -> Chinese, Chinese -> English) and Multilingual WoZ (English -> German, English -> Italian) datasets. We achieve impressive improvements (> 20% on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10% of the target language task data and zero-shot setup respectively.",1.0
NOVEL_DIA_8,https://openalex.org/W3175994953,2021,18,"['https://openalex.org/W1682403713', 'https://openalex.org/W1924770834', 'https://openalex.org/W2101105183', 'https://openalex.org/W2126204609', 'https://openalex.org/W2788388592', 'https://openalex.org/W2791091755', 'https://openalex.org/W2792760996', 'https://openalex.org/W2891707612', 'https://openalex.org/W2947843732', 'https://openalex.org/W2952555501', 'https://openalex.org/W2962886331', 'https://openalex.org/W2963072899', 'https://openalex.org/W2963412005', 'https://openalex.org/W2963789888', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964588180', 'https://openalex.org/W2970748152', 'https://openalex.org/W2971234316', 'https://openalex.org/W2980994576', 'https://openalex.org/W3034896171', 'https://openalex.org/W3035301094', 'https://openalex.org/W3090415328', 'https://openalex.org/W3102445752', 'https://openalex.org/W3106274079', 'https://openalex.org/W3117865619', 'https://openalex.org/W3176453826', 'https://openalex.org/W3179436402']","Binzong Geng, Fajie Yuan, Qiancheng Xu, Ying Shen, Ruifeng Xu, Min Yang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",1.0
NOVEL_DIA_10,https://openalex.org/W3200036871,2021,14,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2143017621', 'https://openalex.org/W2547875792', 'https://openalex.org/W2578354947', 'https://openalex.org/W2615146352', 'https://openalex.org/W2617566453', 'https://openalex.org/W2752047430', 'https://openalex.org/W2798463315', 'https://openalex.org/W2798931235', 'https://openalex.org/W2801890059', 'https://openalex.org/W2807873315', 'https://openalex.org/W2885305518', 'https://openalex.org/W2885421725', 'https://openalex.org/W2898875342', 'https://openalex.org/W2914204778', 'https://openalex.org/W2914442349', 'https://openalex.org/W2945978556', 'https://openalex.org/W2946393904', 'https://openalex.org/W2952335829', 'https://openalex.org/W2953039584', 'https://openalex.org/W2953320089', 'https://openalex.org/W2962793481', 'https://openalex.org/W2962796276', 'https://openalex.org/W2962851944', 'https://openalex.org/W2962974452', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963366196', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963506530', 'https://openalex.org/W2963667126', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964321064', 'https://openalex.org/W2964352131', 'https://openalex.org/W2964529779', 'https://openalex.org/W2965033324', 'https://openalex.org/W2965718149', 'https://openalex.org/W2970453125', 'https://openalex.org/W2970562804', 'https://openalex.org/W2970680405', 'https://openalex.org/W2971277071', 'https://openalex.org/W2979826702', 'https://openalex.org/W2995197202', 'https://openalex.org/W3004621150', 'https://openalex.org/W3004665584', 'https://openalex.org/W3015322406', 'https://openalex.org/W3023786569', 'https://openalex.org/W3034600233', 'https://openalex.org/W3034720580', 'https://openalex.org/W3035448310', 'https://openalex.org/W3035695271', 'https://openalex.org/W3093059841', 'https://openalex.org/W3098824823', 'https://openalex.org/W3099066892', 'https://openalex.org/W3099852471', 'https://openalex.org/W3100719877', 'https://openalex.org/W3106007100', 'https://openalex.org/W3111711122', 'https://openalex.org/W3114595500', 'https://openalex.org/W3116103312', 'https://openalex.org/W3154272574', 'https://openalex.org/W3155584966', 'https://openalex.org/W3173813266', 'https://openalex.org/W3175039711', 'https://openalex.org/W4287642224', 'https://openalex.org/W4287781290', 'https://openalex.org/W4288624561']","Grounded dialogue models generate responses that are grounded on certain concepts. Limited by the distribution of grounded dialogue data, models trained on such data face the transferability challenges in terms of the data distribution and the type of grounded concepts. To address the challenges, we propose the grounded minimal editing framework, which minimally edits existing responses to be grounded on the given concept. Focusing on personas, we propose Grounded Minimal Editor (GME), which learns to edit by disentangling and recombining persona-related and persona-agnostic parts of the response. To evaluate persona-grounded minimal editing, we present the PersonaMi-nEdit dataset, and experimental results show that GME outperforms competitive baselines by a large margin. To evaluate the transferability, we experiment on the test set of BlendedSkillTalk and show that GME can edit dialogue models' responses to largely improve their persona consistency while preserving the use of knowledge and empathy.",1.0
NOVEL_DIA_11,https://openalex.org/W3171850892,2021,57,"['https://openalex.org/W188088074', 'https://openalex.org/W2001829221', 'https://openalex.org/W2012378416', 'https://openalex.org/W2083365005', 'https://openalex.org/W2101900802', 'https://openalex.org/W2160685721', 'https://openalex.org/W2540646130', 'https://openalex.org/W2595653137', 'https://openalex.org/W2612675303', 'https://openalex.org/W2740168486', 'https://openalex.org/W2744575762', 'https://openalex.org/W2791170418', 'https://openalex.org/W2898875342', 'https://openalex.org/W2942915765', 'https://openalex.org/W2949678053', 'https://openalex.org/W2962974452', 'https://openalex.org/W2962977603', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2968297680', 'https://openalex.org/W2971173235', 'https://openalex.org/W2971883198', 'https://openalex.org/W2979592845', 'https://openalex.org/W2988937804', 'https://openalex.org/W2995404354', 'https://openalex.org/W3000779003', 'https://openalex.org/W3002330681', 'https://openalex.org/W3023786569', 'https://openalex.org/W3030172318', 'https://openalex.org/W3034850762', 'https://openalex.org/W3037528277', 'https://openalex.org/W3082884418', 'https://openalex.org/W3100355250', 'https://openalex.org/W3155584966', 'https://openalex.org/W3157298328', 'https://openalex.org/W4243989635', 'https://openalex.org/W4287900772', 'https://openalex.org/W4288113479', 'https://openalex.org/W4385245566']","Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, Emily Dinan. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_DIA_12,https://openalex.org/W3200506654,2021,1,"['https://openalex.org/W2481265265', 'https://openalex.org/W2604698497', 'https://openalex.org/W2806600904', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963491014', 'https://openalex.org/W2970597249', 'https://openalex.org/W2971737394', 'https://openalex.org/W3100110884']","Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in Natural Language Processing. Previous investigations prove that introducing a further pre-training phase between pre-training and fine-tuning phases to adapt the model on the domain-specific unlabeled data can bring positive effects. However, most of these further pre-training works just keep running the conventional pre-training task, e.g., masked language model, which can be regarded as the domain adaptation to bridge the data distribution gap. After observing diverse downstream tasks, we suggest that different tasks may also need a further pre-training phase with appropriate training tasks to bridge the task formulation gap. To investigate this, we carry out a study for improving multiple task-oriented dialogue downstream tasks through designing various tasks at the further pre-training phase. The experiment shows that different downstream tasks prefer different further pre-training tasks, which have intrinsic correlation and most further pre-training tasks significantly improve certain target tasks rather than all. Our investigation indicates that it is of great importance and effectiveness to design appropriate further pre-training tasks modeling specific information that benefit downstream tasks. Besides, we present multiple constructive empirical conclusions for enhancing task-oriented dialogues.",0.9959183673469388
NOVEL_DIA_13,https://openalex.org/W3201144093,2021,12,"['https://openalex.org/W185375561', 'https://openalex.org/W1522301498', 'https://openalex.org/W1552206080', 'https://openalex.org/W1576632330', 'https://openalex.org/W1940872118', 'https://openalex.org/W1974105891', 'https://openalex.org/W1975244201', 'https://openalex.org/W1983707534', 'https://openalex.org/W1993979041', 'https://openalex.org/W2064675550', 'https://openalex.org/W2097204000', 'https://openalex.org/W2097219834', 'https://openalex.org/W2109878893', 'https://openalex.org/W2110930288', 'https://openalex.org/W2116245940', 'https://openalex.org/W2125447031', 'https://openalex.org/W2132271306', 'https://openalex.org/W2133298256', 'https://openalex.org/W2147880316', 'https://openalex.org/W2157331557', 'https://openalex.org/W2166490941', 'https://openalex.org/W2166700048', 'https://openalex.org/W2252033417', 'https://openalex.org/W2264742718', 'https://openalex.org/W2525032226', 'https://openalex.org/W2571175805', 'https://openalex.org/W2574790321', 'https://openalex.org/W2594726847', 'https://openalex.org/W2595784962', 'https://openalex.org/W2604397416', 'https://openalex.org/W2768661419', 'https://openalex.org/W2769917417', 'https://openalex.org/W2774005037', 'https://openalex.org/W2806117557', 'https://openalex.org/W2835434549', 'https://openalex.org/W2950697717', 'https://openalex.org/W2950801649', 'https://openalex.org/W2951281980', 'https://openalex.org/W2952316487', 'https://openalex.org/W2952589943', 'https://openalex.org/W2953064936', 'https://openalex.org/W2959918500', 'https://openalex.org/W2960994197', 'https://openalex.org/W2962727507', 'https://openalex.org/W2963109634', 'https://openalex.org/W2963170138', 'https://openalex.org/W2963217826', 'https://openalex.org/W2963318456', 'https://openalex.org/W2963907629', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964183327', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964260310', 'https://openalex.org/W2970340522', 'https://openalex.org/W2971077754', 'https://openalex.org/W2998108220', 'https://openalex.org/W3029418112', 'https://openalex.org/W3098158595', 'https://openalex.org/W3104952102', 'https://openalex.org/W3106007100', 'https://openalex.org/W3135812974', 'https://openalex.org/W4249013746', 'https://openalex.org/W4295249402', 'https://openalex.org/W4297795721']","We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner’s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous state of the art for the task, obtaining a 20% relative improvement in successful task completion in self-play evaluations and a 50% relative improvement in success in human evaluations.",1.0
NOVEL_DIA_14,https://openalex.org/W3212307878,2021,1,"['https://openalex.org/W2101105183', 'https://openalex.org/W2154652894', 'https://openalex.org/W2606974598', 'https://openalex.org/W2888302696', 'https://openalex.org/W2912904516', 'https://openalex.org/W2949769095', 'https://openalex.org/W2952855649', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964223283', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970391482', 'https://openalex.org/W2970744242', 'https://openalex.org/W2970960706', 'https://openalex.org/W2970996870', 'https://openalex.org/W3035148359', 'https://openalex.org/W3035566559', 'https://openalex.org/W3089006046', 'https://openalex.org/W3094437659', 'https://openalex.org/W3098694757', 'https://openalex.org/W3100063812', 'https://openalex.org/W3105388824', 'https://openalex.org/W3153491684', 'https://openalex.org/W3163840908', 'https://openalex.org/W3176413694']","The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the model’s outputs may lack fluency. To alleviate this issue, we inject the loss signal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our model over the current state-of-the-art systems when transferring to another dataset.",0.991304347826087
NOVEL_DIA_16,https://openalex.org/W3166704354,2021,37,"['https://openalex.org/W78136081', 'https://openalex.org/W80056832', 'https://openalex.org/W249736312', 'https://openalex.org/W1071251684', 'https://openalex.org/W1506224928', 'https://openalex.org/W1814822231', 'https://openalex.org/W1969501657', 'https://openalex.org/W1997326096', 'https://openalex.org/W2073315954', 'https://openalex.org/W2119769989', 'https://openalex.org/W2145162117', 'https://openalex.org/W2151245229', 'https://openalex.org/W2160685721', 'https://openalex.org/W2340954483', 'https://openalex.org/W2540646130', 'https://openalex.org/W2806344213', 'https://openalex.org/W2896457183', 'https://openalex.org/W2938704169', 'https://openalex.org/W2949089361', 'https://openalex.org/W2949413855', 'https://openalex.org/W2949777421', 'https://openalex.org/W2962768347', 'https://openalex.org/W2963096510', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963667126', 'https://openalex.org/W2964029788', 'https://openalex.org/W2966746916', 'https://openalex.org/W2970395295', 'https://openalex.org/W2971173235', 'https://openalex.org/W2971307358', 'https://openalex.org/W2982756474', 'https://openalex.org/W2988937804', 'https://openalex.org/W2996287690', 'https://openalex.org/W2997195635', 'https://openalex.org/W3013226821', 'https://openalex.org/W3023786569', 'https://openalex.org/W3034937117', 'https://openalex.org/W3035540807', 'https://openalex.org/W3085190015', 'https://openalex.org/W3099246072', 'https://openalex.org/W3099635335', 'https://openalex.org/W3099872554', 'https://openalex.org/W3100355250', 'https://openalex.org/W3155584966', 'https://openalex.org/W4289236291']","Emily Sheng, Kai-Wei Chang, Prem Natarajan, Nanyun Peng. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",0.9923664122137404
NOVEL_DIA_17,https://openalex.org/W3198455100,2021,24,"['https://openalex.org/W1591706642', 'https://openalex.org/W1975879668', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107598941', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143017621', 'https://openalex.org/W2154652894', 'https://openalex.org/W2586847566', 'https://openalex.org/W2606974598', 'https://openalex.org/W2768282280', 'https://openalex.org/W2795571593', 'https://openalex.org/W2891826200', 'https://openalex.org/W2898875342', 'https://openalex.org/W2945260553', 'https://openalex.org/W2949769095', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951508633', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963248455', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964458951', 'https://openalex.org/W2971236040', 'https://openalex.org/W2971274815', 'https://openalex.org/W2972916088', 'https://openalex.org/W2979478117', 'https://openalex.org/W2979826702', 'https://openalex.org/W2982399380', 'https://openalex.org/W2995183464', 'https://openalex.org/W2996227762', 'https://openalex.org/W3007008027', 'https://openalex.org/W3034238904', 'https://openalex.org/W3034758256', 'https://openalex.org/W3034999214', 'https://openalex.org/W3039017601', 'https://openalex.org/W3043859333', 'https://openalex.org/W3098824823', 'https://openalex.org/W3103100151', 'https://openalex.org/W3104123491', 'https://openalex.org/W3104777900', 'https://openalex.org/W3130164045', 'https://openalex.org/W3156789018', 'https://openalex.org/W3172448816', 'https://openalex.org/W4251372957', 'https://openalex.org/W4385245566']","Neural conversation models have shown great potentials towards generating fluent and informative responses by introducing external background knowledge. Nevertheless, it is laborious to construct such knowledge-grounded dialogues, and existing models usually perform poorly when transfer to new domains with limited training samples. Therefore, building a knowledge-grounded dialogue system under the low-resource setting is a still crucial issue. In this paper, we propose a novel three-stage learning framework based on weakly supervised learning which benefits from large scale ungrounded dialogues and unstructured knowledge base. To better cooperate with this framework, we devise a variant of Transformer with decoupled decoder which facilitates the disentangled learning of response generation and knowledge incorporation. Evaluation results on two benchmarks indicate that our approach can outperform other state-of-the-art methods with less training data, and even in zero-resource scenario, our approach still performs well.",1.0
NOVEL_DIA_18,https://openalex.org/W3213758350,2021,15,"['https://openalex.org/W1522301498', 'https://openalex.org/W1965555277', 'https://openalex.org/W1975879668', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2154652894', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548228487', 'https://openalex.org/W2606974598', 'https://openalex.org/W2794557536', 'https://openalex.org/W2799176105', 'https://openalex.org/W2807873315', 'https://openalex.org/W2891103209', 'https://openalex.org/W2891826200', 'https://openalex.org/W2898875342', 'https://openalex.org/W2949769095', 'https://openalex.org/W2949782788', 'https://openalex.org/W2951114218', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963241825', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963945575', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964458951', 'https://openalex.org/W2970971581', 'https://openalex.org/W2995183464', 'https://openalex.org/W2997094605', 'https://openalex.org/W2997260297', 'https://openalex.org/W2997896082', 'https://openalex.org/W3007008027', 'https://openalex.org/W3022187094', 'https://openalex.org/W3032717904', 'https://openalex.org/W3034739704', 'https://openalex.org/W3034758256', 'https://openalex.org/W3034999754', 'https://openalex.org/W3035072597', 'https://openalex.org/W3035148359', 'https://openalex.org/W3035500185', 'https://openalex.org/W3035755323', 'https://openalex.org/W3103100151', 'https://openalex.org/W3103691705', 'https://openalex.org/W3104123491', 'https://openalex.org/W3172448816', 'https://openalex.org/W3176584016', 'https://openalex.org/W3176956817', 'https://openalex.org/W4295312788', 'https://openalex.org/W4385245566', 'https://openalex.org/W4391602018']","Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this task usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects simultaneously in separate yet collaborative latent spaces, so as to capture the inherent correlation between knowledge selection and response generation. During generation, our proposed model firstly draws knowledge candidate from the latent space conditioned on the dialogue context, and then samples a response from another collaborative latent space conditioned on both the context and the selected knowledge. Experimental results on two widely-used knowledge-grounded dialogue datasets show that our model outperforms previous methods on both knowledge selection and response generation.",0.9941520467836256
NOVEL_DIA_20,https://openalex.org/W3158235155,2021,10,"['https://openalex.org/W1801721664', 'https://openalex.org/W1969152782', 'https://openalex.org/W1978347212', 'https://openalex.org/W2250297846', 'https://openalex.org/W2468710617', 'https://openalex.org/W2594229957', 'https://openalex.org/W2784070054', 'https://openalex.org/W2798392716', 'https://openalex.org/W2806482527', 'https://openalex.org/W2806600904', 'https://openalex.org/W2891416139', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898700502', 'https://openalex.org/W2945475330', 'https://openalex.org/W2962831269', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964101860', 'https://openalex.org/W2971737394', 'https://openalex.org/W2979400990', 'https://openalex.org/W2985067290', 'https://openalex.org/W2995289474', 'https://openalex.org/W2997771882', 'https://openalex.org/W3015637204', 'https://openalex.org/W3021016503', 'https://openalex.org/W3023027202', 'https://openalex.org/W3024509506', 'https://openalex.org/W3030754432', 'https://openalex.org/W3034573951', 'https://openalex.org/W3044597194', 'https://openalex.org/W3045689439', 'https://openalex.org/W3045703328', 'https://openalex.org/W3082793928', 'https://openalex.org/W3090305424', 'https://openalex.org/W3097392354', 'https://openalex.org/W3119649668', 'https://openalex.org/W3160818482', 'https://openalex.org/W4287795696', 'https://openalex.org/W4288094254', 'https://openalex.org/W4288288848', 'https://openalex.org/W4322614701']","Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language description for schema in dialog state tracking. Our discussion mainly covers three aspects: encoder architectures, impact of supplementary training, and effective schema description styles. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation.",1.0
NOVEL_DIA_21,https://openalex.org/W3212362289,2021,31,"['https://openalex.org/W26963497', 'https://openalex.org/W1983707534', 'https://openalex.org/W1990334093', 'https://openalex.org/W2044120473', 'https://openalex.org/W2088622183', 'https://openalex.org/W2101210369', 'https://openalex.org/W2111316763', 'https://openalex.org/W2161068821', 'https://openalex.org/W2327037637', 'https://openalex.org/W2530816535', 'https://openalex.org/W2888482885', 'https://openalex.org/W2889326796', 'https://openalex.org/W2889984458', 'https://openalex.org/W2890719433', 'https://openalex.org/W2891602716', 'https://openalex.org/W2898695436', 'https://openalex.org/W2912762447', 'https://openalex.org/W2936695845', 'https://openalex.org/W2940009958', 'https://openalex.org/W2949530332', 'https://openalex.org/W2951970475', 'https://openalex.org/W2952229419', 'https://openalex.org/W2952890017', 'https://openalex.org/W2962369866', 'https://openalex.org/W2963126845', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963600562', 'https://openalex.org/W2963795756', 'https://openalex.org/W2963969878', 'https://openalex.org/W2963983466', 'https://openalex.org/W2963997607', 'https://openalex.org/W2964089584', 'https://openalex.org/W2964159205', 'https://openalex.org/W2970418174', 'https://openalex.org/W2971296908', 'https://openalex.org/W2976223659', 'https://openalex.org/W2989743967', 'https://openalex.org/W2995746049', 'https://openalex.org/W2996403597', 'https://openalex.org/W3006647218', 'https://openalex.org/W3007685714', 'https://openalex.org/W3008323921', 'https://openalex.org/W3010293452', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035160371', 'https://openalex.org/W3035542229', 'https://openalex.org/W3085629518', 'https://openalex.org/W3089659770', 'https://openalex.org/W3092993357', 'https://openalex.org/W3094217371', 'https://openalex.org/W3100560913', 'https://openalex.org/W3100742171', 'https://openalex.org/W3102136264', 'https://openalex.org/W3102903864', 'https://openalex.org/W3104257895', 'https://openalex.org/W3111436535', 'https://openalex.org/W3154981967', 'https://openalex.org/W3162734203', 'https://openalex.org/W3167002470', 'https://openalex.org/W3169565655', 'https://openalex.org/W3171388604', 'https://openalex.org/W3174150157', 'https://openalex.org/W4241891521', 'https://openalex.org/W4287636206', 'https://openalex.org/W4287829148', 'https://openalex.org/W4327656064', 'https://openalex.org/W4392143959']","ive conversation summarization has received growing attention while most current state-of-the-art summarization models heavily rely on human-annotated summaries. To reduce the dependence on labeled summaries, in this work, we present a simple yet effective set of Conversational Data Augmentation (CODA) methods for semi-supervised abstractive conversation summarization, such as random swapping/deletion to perturb the discourse relations inside conversations, dialogue-acts-guided insertion to interrupt the development of conversations, and conditional-generation-based substitution to substitute utterances with their paraphrases generated based on the conversation context. To further utilize unlabeled conversations, we combine CODA with two-stage noisy self-training where we first pre-train the summarization model on unlabeled conversations with pseudo summaries and then fine-tune it on labeled conversations. Experiments conducted on the recent conversation summarization datasets demonstrate the effectiveness of our methods over several state-of-the-art data augmentation baselines.",1.0
NOVEL_DIA_22,https://openalex.org/W3197754599,2021,27,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153579005', 'https://openalex.org/W2154652894', 'https://openalex.org/W2157331557', 'https://openalex.org/W2159637323', 'https://openalex.org/W2507756961', 'https://openalex.org/W2521114121', 'https://openalex.org/W2554952599', 'https://openalex.org/W2767206889', 'https://openalex.org/W2807873315', 'https://openalex.org/W2890560993', 'https://openalex.org/W2891826200', 'https://openalex.org/W2898875342', 'https://openalex.org/W2900629321', 'https://openalex.org/W2920538220', 'https://openalex.org/W2925513271', 'https://openalex.org/W2949644922', 'https://openalex.org/W2949769095', 'https://openalex.org/W2950299257', 'https://openalex.org/W2950457956', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962896208', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964587107', 'https://openalex.org/W2968297680', 'https://openalex.org/W2970832665', 'https://openalex.org/W2985694911', 'https://openalex.org/W2995404354', 'https://openalex.org/W2997219446', 'https://openalex.org/W2997260297', 'https://openalex.org/W2997300509', 'https://openalex.org/W3005768470', 'https://openalex.org/W3034548376', 'https://openalex.org/W3034569646', 'https://openalex.org/W3034696087', 'https://openalex.org/W3034720580', 'https://openalex.org/W3039805635', 'https://openalex.org/W3098295156', 'https://openalex.org/W3114869109', 'https://openalex.org/W3167303745', 'https://openalex.org/W4294170691', 'https://openalex.org/W4322614756', 'https://openalex.org/W4385245566']","Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue, in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The proposed model comprises a multi-concept planning module that learns to identify multiple associated concepts from a concept graph and a customized Insertion Transformer that performs concept-guided non-autoregressive generation to complete a response. The experimental results on two public datasets show that CG-nAR can produce diverse and coherent responses, outperforming state-of-the-art baselines in both automatic and human evaluations with substantially faster inference speed.",0.9953917050691244
NOVEL_DIA_23,https://openalex.org/W3197133421,2021,0,"['https://openalex.org/W2130942839', 'https://openalex.org/W2251058040', 'https://openalex.org/W2606722458', 'https://openalex.org/W2798914047', 'https://openalex.org/W2804010326', 'https://openalex.org/W2808093377', 'https://openalex.org/W2891732163', 'https://openalex.org/W2928941594', 'https://openalex.org/W2945475330', 'https://openalex.org/W2951216772', 'https://openalex.org/W2951392882', 'https://openalex.org/W2962911098', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964101860', 'https://openalex.org/W2988647680', 'https://openalex.org/W3004304303', 'https://openalex.org/W3021016503', 'https://openalex.org/W3021096583', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034715004', 'https://openalex.org/W3044597194', 'https://openalex.org/W3082274269', 'https://openalex.org/W3088415854', 'https://openalex.org/W3099140977', 'https://openalex.org/W3103469330', 'https://openalex.org/W3119649668', 'https://openalex.org/W3135087612', 'https://openalex.org/W3151066410', 'https://openalex.org/W3155308379', 'https://openalex.org/W3156697766', 'https://openalex.org/W3159914836', 'https://openalex.org/W3188964051']","Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this problem from the perspectives of pre-training objectives as well as the formats of context representations. We demonstrate that the choice of pre-training objective makes a significant difference to the state tracking quality. In particular, we find that masked span prediction is more effective than auto-regressive language modeling. We also explore using Pegasus, a span prediction-based pre-training objective for text summarization, for the state tracking model. We found that pre-training for the seemingly distant summarization task works surprisingly well for dialogue state tracking. In addition, we found that while recurrent state context representation works also reasonably well, the model may have a hard time recovering from earlier mistakes. We conducted experiments on the MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.",1.0
NOVEL_DIA_24,https://openalex.org/W3200001502,2021,6,"['https://openalex.org/W860478996', 'https://openalex.org/W1629317078', 'https://openalex.org/W1861492603', 'https://openalex.org/W1986776260', 'https://openalex.org/W2017515822', 'https://openalex.org/W2041945486', 'https://openalex.org/W2130942839', 'https://openalex.org/W2142653990', 'https://openalex.org/W2194775991', 'https://openalex.org/W2429300145', 'https://openalex.org/W2558809543', 'https://openalex.org/W2903036216', 'https://openalex.org/W2938704169', 'https://openalex.org/W2945087694', 'https://openalex.org/W2953251345', 'https://openalex.org/W2962735233', 'https://openalex.org/W2963014658', 'https://openalex.org/W2988617410', 'https://openalex.org/W2996287690', 'https://openalex.org/W2997117909', 'https://openalex.org/W3045998699', 'https://openalex.org/W3107855336', 'https://openalex.org/W3154811600', 'https://openalex.org/W3156736277', 'https://openalex.org/W3184537908', 'https://openalex.org/W4239338037', 'https://openalex.org/W4247986398']","Generating goal-oriented questions in Visual Dialogue tasks is a challenging and longstanding problem. State-Of-The-Art systems are shown to generate questions that, although grammatically correct, often lack an effective strategy and sound unnatural to humans. Inspired by the cognitive literature on information search and cross-situational word learning, we design Confirm-it, a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions that confirm the model{'}s conjecture about the referent. We take the GuessWhat?! game as a case-study. We show that dialogues generated by Confirm-it are more natural and effective than beam search decoding without re-ranking",0.9936305732484076
NOVEL_DIA_25,https://openalex.org/W3170644973,2021,2,"['https://openalex.org/W1191599655', 'https://openalex.org/W1522301498', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W2062175565', 'https://openalex.org/W2117989772', 'https://openalex.org/W2120045257', 'https://openalex.org/W2396229782', 'https://openalex.org/W2408200822', 'https://openalex.org/W2559038528', 'https://openalex.org/W2736601468', 'https://openalex.org/W2739936944', 'https://openalex.org/W2772217324', 'https://openalex.org/W2783543950', 'https://openalex.org/W2810840719', 'https://openalex.org/W2889186204', 'https://openalex.org/W2947212824', 'https://openalex.org/W2949476504', 'https://openalex.org/W2950314731', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963433587', 'https://openalex.org/W2963567240', 'https://openalex.org/W2964121744', 'https://openalex.org/W3016142228', 'https://openalex.org/W3103837004', 'https://openalex.org/W4297806413', 'https://openalex.org/W4306716473']","Training dialogue agents requires a large number of interactions with users: agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages: loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguish useless actions from others and reduce the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80% success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in human evaluation where the agent interactively collects 100 real-user dialogues in training phase.",1.0
NOVEL_DIA_26,https://openalex.org/W3196326437,2021,18,"['https://openalex.org/W1522301498', 'https://openalex.org/W1531174292', 'https://openalex.org/W1544827683', 'https://openalex.org/W1566289585', 'https://openalex.org/W1861492603', 'https://openalex.org/W1882958252', 'https://openalex.org/W1976975519', 'https://openalex.org/W2014571624', 'https://openalex.org/W2015895495', 'https://openalex.org/W2025768430', 'https://openalex.org/W2033839684', 'https://openalex.org/W2125420881', 'https://openalex.org/W2154652894', 'https://openalex.org/W2161068821', 'https://openalex.org/W2187089797', 'https://openalex.org/W2606974598', 'https://openalex.org/W2888482885', 'https://openalex.org/W2908336025', 'https://openalex.org/W2913407944', 'https://openalex.org/W2921522814', 'https://openalex.org/W2940579548', 'https://openalex.org/W2949615363', 'https://openalex.org/W2952809536', 'https://openalex.org/W2952890017', 'https://openalex.org/W2955471745', 'https://openalex.org/W2962704246', 'https://openalex.org/W2962785754', 'https://openalex.org/W2962849707', 'https://openalex.org/W2962965405', 'https://openalex.org/W2962985882', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963521413', 'https://openalex.org/W2963826681', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964288227', 'https://openalex.org/W2970060597', 'https://openalex.org/W2970419734', 'https://openalex.org/W2972916088', 'https://openalex.org/W2982399380', 'https://openalex.org/W2989743967', 'https://openalex.org/W2996264288', 'https://openalex.org/W3034715004', 'https://openalex.org/W3034999214', 'https://openalex.org/W3085629518', 'https://openalex.org/W3094217371', 'https://openalex.org/W3098562101', 'https://openalex.org/W3100560913', 'https://openalex.org/W3102136264', 'https://openalex.org/W3105245805', 'https://openalex.org/W3110659220', 'https://openalex.org/W3111693342', 'https://openalex.org/W3116498179', 'https://openalex.org/W3142970963', 'https://openalex.org/W3169565655', 'https://openalex.org/W3171639395', 'https://openalex.org/W4287636206', 'https://openalex.org/W4385245566']","With the rapid increase in the volume of dialogue data from daily life, there is a growing demand for dialogue summarization. Unfortunately, training a large summarization model is generally infeasible due to the inadequacy of dialogue data with annotated summaries. Most existing works for low-resource dialogue summarization directly pretrain models in other domains, e.g., the news domain, but they generally neglect the huge difference between dialogues and conventional articles. To bridge the gap between out-of-domain pretraining and in-domain fine-tuning, in this work, we propose a multi-source pretraining paradigm to better leverage the external summary data. Specifically, we exploit large-scale in-domain non-summary data to separately pretrain the dialogue encoder and the summary decoder. The combined encoder-decoder model is then pretrained on the out-of-domain summary data using adversarial critics, aiming to facilitate domain-agnostic summarization. The experimental results on two public datasets show that with only limited training data, our approach achieves competitive performance and generalizes well in different dialogue scenarios.",1.0
NOVEL_DIA_28,https://openalex.org/W3200546165,2021,2,"['https://openalex.org/W1598178035', 'https://openalex.org/W1969152782', 'https://openalex.org/W2438667436', 'https://openalex.org/W2557764419', 'https://openalex.org/W2606964149', 'https://openalex.org/W2609826708', 'https://openalex.org/W2784070054', 'https://openalex.org/W2798914047', 'https://openalex.org/W2889787757', 'https://openalex.org/W2908510526', 'https://openalex.org/W2912904516', 'https://openalex.org/W2912924812', 'https://openalex.org/W2945475330', 'https://openalex.org/W2950577311', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963339397', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963925437', 'https://openalex.org/W2965373594', 'https://openalex.org/W2982756474', 'https://openalex.org/W2987442863', 'https://openalex.org/W2988252747', 'https://openalex.org/W2988421999', 'https://openalex.org/W2997771882', 'https://openalex.org/W2998228050', 'https://openalex.org/W3008186200', 'https://openalex.org/W3015637204', 'https://openalex.org/W3016625483', 'https://openalex.org/W3021016503', 'https://openalex.org/W3021096583', 'https://openalex.org/W3030754432', 'https://openalex.org/W3033461281', 'https://openalex.org/W3045703328', 'https://openalex.org/W3082274269', 'https://openalex.org/W3091355780', 'https://openalex.org/W3094479119', 'https://openalex.org/W3099655892', 'https://openalex.org/W3100110884', 'https://openalex.org/W3100128199', 'https://openalex.org/W3115336344', 'https://openalex.org/W3119438769', 'https://openalex.org/W3119649668', 'https://openalex.org/W3119822474', 'https://openalex.org/W3168491067', 'https://openalex.org/W3170632963', 'https://openalex.org/W3189817881']","Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the \textit{cross-task} knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle ""none"" value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained baseline on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains.",1.0
NOVEL_DIA_29,https://openalex.org/W3194113117,2021,49,"['https://openalex.org/W10957333', 'https://openalex.org/W136732505', 'https://openalex.org/W205930466', 'https://openalex.org/W1522301498', 'https://openalex.org/W1840435438', 'https://openalex.org/W2250539671', 'https://openalex.org/W2281420995', 'https://openalex.org/W2505005027', 'https://openalex.org/W2595653137', 'https://openalex.org/W2607700676', 'https://openalex.org/W2741229899', 'https://openalex.org/W2758912220', 'https://openalex.org/W2785615365', 'https://openalex.org/W2791170418', 'https://openalex.org/W2884561390', 'https://openalex.org/W2892217998', 'https://openalex.org/W2898875342', 'https://openalex.org/W2910580498', 'https://openalex.org/W2912102236', 'https://openalex.org/W2913443447', 'https://openalex.org/W2916719435', 'https://openalex.org/W2916772188', 'https://openalex.org/W2938704169', 'https://openalex.org/W2949678053', 'https://openalex.org/W2951583236', 'https://openalex.org/W2951737564', 'https://openalex.org/W2962753250', 'https://openalex.org/W2962875044', 'https://openalex.org/W2962977603', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963351448', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963549959', 'https://openalex.org/W2963691377', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963955897', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970269073', 'https://openalex.org/W2970395295', 'https://openalex.org/W2971150411', 'https://openalex.org/W2971173235', 'https://openalex.org/W2974587494', 'https://openalex.org/W2979826702', 'https://openalex.org/W2982756474', 'https://openalex.org/W2988937804', 'https://openalex.org/W2996287690', 'https://openalex.org/W2997510038', 'https://openalex.org/W3002330681', 'https://openalex.org/W3022992164', 'https://openalex.org/W3034238904', 'https://openalex.org/W3034600233', 'https://openalex.org/W3034937117', 'https://openalex.org/W3034951181', 'https://openalex.org/W3037528277', 'https://openalex.org/W3093233911', 'https://openalex.org/W3098824823', 'https://openalex.org/W3099635335', 'https://openalex.org/W3100355250', 'https://openalex.org/W3100355408', 'https://openalex.org/W3103639864', 'https://openalex.org/W3104152666', 'https://openalex.org/W3106460864', 'https://openalex.org/W3126763054', 'https://openalex.org/W3133423348', 'https://openalex.org/W3155584966', 'https://openalex.org/W3155742828', 'https://openalex.org/W3175432533', 'https://openalex.org/W3177468621', 'https://openalex.org/W3186288536', 'https://openalex.org/W4292779060']","Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.",0.9940828402366864
NOVEL_DIA_30,https://openalex.org/W3198548340,2021,20,"['https://openalex.org/W42510783', 'https://openalex.org/W1487886355', 'https://openalex.org/W1569447338', 'https://openalex.org/W1591607137', 'https://openalex.org/W2101105183', 'https://openalex.org/W2158751803', 'https://openalex.org/W2160204597', 'https://openalex.org/W2182572585', 'https://openalex.org/W2574535369', 'https://openalex.org/W2606974598', 'https://openalex.org/W2781963152', 'https://openalex.org/W2842624112', 'https://openalex.org/W2888482885', 'https://openalex.org/W2896457183', 'https://openalex.org/W2936695845', 'https://openalex.org/W2952138241', 'https://openalex.org/W2962985882', 'https://openalex.org/W2963045354', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963929190', 'https://openalex.org/W2964211782', 'https://openalex.org/W2970419734', 'https://openalex.org/W2970785793', 'https://openalex.org/W2971139875', 'https://openalex.org/W2985237994', 'https://openalex.org/W2989743967', 'https://openalex.org/W2996403597', 'https://openalex.org/W3031414376', 'https://openalex.org/W3035259249', 'https://openalex.org/W3098562101', 'https://openalex.org/W3109862485', 'https://openalex.org/W3110131742', 'https://openalex.org/W3110659220', 'https://openalex.org/W3111693342', 'https://openalex.org/W3116079511', 'https://openalex.org/W3142970963', 'https://openalex.org/W3159259047', 'https://openalex.org/W3164029002', 'https://openalex.org/W3169942382', 'https://openalex.org/W3171639395']","Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer's issues and service progress. These applications require summaries to contain the perspective of a single speaker and have a clear topic flow structure, while neither are available in existing datasets. Therefore, in this paper, we introduce a novel Chinese dataset for Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive summaries in two aspects: (1) In addition to the overall summary for the whole dialogue, role-oriented summaries are also provided to acquire different speakers' viewpoints. (2) All the summaries sum up each topic separately, thus containing the topic-level structure of the dialogue. We define tasks in CSDS as generating the overall summary and different role-oriented summaries for a given dialogue. Next, we compare various summarization methods on CSDS, and experiment results show that existing methods are prone to generate redundant and incoherent summaries. Besides, the performance becomes much worse when analyzing the performance on role-oriented summaries and topic structures. We hope that this study could benchmark Chinese dialogue summarization and benefit further studies.",0.9937106918238994
NOVEL_DIA_31,https://openalex.org/W3199691415,2021,22,"['https://openalex.org/W2563734883', 'https://openalex.org/W2586847566', 'https://openalex.org/W2888302696', 'https://openalex.org/W2891103209', 'https://openalex.org/W2891304738', 'https://openalex.org/W2891826200', 'https://openalex.org/W2898875342', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951508633', 'https://openalex.org/W2951976932', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964223283', 'https://openalex.org/W2964458951', 'https://openalex.org/W2965373594', 'https://openalex.org/W2971008823', 'https://openalex.org/W2979478117', 'https://openalex.org/W2979826702', 'https://openalex.org/W2989312920', 'https://openalex.org/W2995183464', 'https://openalex.org/W3015468748', 'https://openalex.org/W3021582395', 'https://openalex.org/W3023790734', 'https://openalex.org/W3034758256', 'https://openalex.org/W3034951181', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035187010', 'https://openalex.org/W3098824823', 'https://openalex.org/W3099590177', 'https://openalex.org/W3099700870', 'https://openalex.org/W3103691705', 'https://openalex.org/W3104777900', 'https://openalex.org/W3171847983', 'https://openalex.org/W4287824654', 'https://openalex.org/W4299567010', 'https://openalex.org/W4385245566']",Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded conversational datasets and provide analyses showing generalization to unseen documents and long dialogue contexts.,0.9950738916256158
NOVEL_DIA_33,https://openalex.org/W3196636639,2021,8,"['https://openalex.org/W10050918', 'https://openalex.org/W1632484548', 'https://openalex.org/W1821462560', 'https://openalex.org/W2015936967', 'https://openalex.org/W2047335008', 'https://openalex.org/W2055537935', 'https://openalex.org/W2108806737', 'https://openalex.org/W2117555338', 'https://openalex.org/W2119015791', 'https://openalex.org/W2129405869', 'https://openalex.org/W2147933644', 'https://openalex.org/W2154740693', 'https://openalex.org/W2438667436', 'https://openalex.org/W2492505811', 'https://openalex.org/W2736601468', 'https://openalex.org/W2788907134', 'https://openalex.org/W2889424988', 'https://openalex.org/W2896457183', 'https://openalex.org/W2943171176', 'https://openalex.org/W2948194985', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963178340', 'https://openalex.org/W2963215553', 'https://openalex.org/W2963341924', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964059111', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964180249', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970828515', 'https://openalex.org/W2970859221', 'https://openalex.org/W2972777589', 'https://openalex.org/W2995464762', 'https://openalex.org/W2996184071', 'https://openalex.org/W3003199999', 'https://openalex.org/W3006511999', 'https://openalex.org/W3021096583', 'https://openalex.org/W3030754432', 'https://openalex.org/W3034198728', 'https://openalex.org/W3035470414', 'https://openalex.org/W3037879762', 'https://openalex.org/W3088238433', 'https://openalex.org/W3094479119', 'https://openalex.org/W3099140719', 'https://openalex.org/W3103996868', 'https://openalex.org/W3105184920', 'https://openalex.org/W3119649668', 'https://openalex.org/W3156909481', 'https://openalex.org/W3161663089', 'https://openalex.org/W3162462834', 'https://openalex.org/W4288094254', 'https://openalex.org/W4288288848', 'https://openalex.org/W4385245566', 'https://openalex.org/W4386564360']","Carel van Niekerk, Andrey Malinin, Christian Geishauser, Michael Heck, Hsien-chin Lin, Nurul Lubis, Shutong Feng, Milica Gasic. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",1.0
NOVEL_DIA_34,https://openalex.org/W3204697369,2021,39,"['https://openalex.org/W1975244201', 'https://openalex.org/W2009415795', 'https://openalex.org/W2054141820', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133012565', 'https://openalex.org/W2153225416', 'https://openalex.org/W2157881433', 'https://openalex.org/W2158515176', 'https://openalex.org/W2295739661', 'https://openalex.org/W2561529111', 'https://openalex.org/W2605350416', 'https://openalex.org/W2783215745', 'https://openalex.org/W2798914047', 'https://openalex.org/W2808871448', 'https://openalex.org/W2809617427', 'https://openalex.org/W2888515090', 'https://openalex.org/W2891389695', 'https://openalex.org/W2898076813', 'https://openalex.org/W2952215380', 'https://openalex.org/W2955416746', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963201498', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964112275', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964207259', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964309167', 'https://openalex.org/W2970236742', 'https://openalex.org/W2970799419', 'https://openalex.org/W2982904530', 'https://openalex.org/W2988937804', 'https://openalex.org/W2997662139', 'https://openalex.org/W3000779003', 'https://openalex.org/W3022187094', 'https://openalex.org/W3034328025', 'https://openalex.org/W3035098003', 'https://openalex.org/W3035301094', 'https://openalex.org/W3035355914', 'https://openalex.org/W3080122044', 'https://openalex.org/W3094070895', 'https://openalex.org/W3099865390', 'https://openalex.org/W3100790518', 'https://openalex.org/W3101718968', 'https://openalex.org/W3106454043', 'https://openalex.org/W3113741750', 'https://openalex.org/W3115944734', 'https://openalex.org/W3116062118', 'https://openalex.org/W3116455257', 'https://openalex.org/W3121970007', 'https://openalex.org/W3152509363', 'https://openalex.org/W3155584966', 'https://openalex.org/W3162337509', 'https://openalex.org/W3163631185', 'https://openalex.org/W3170262904', 'https://openalex.org/W3174681481', 'https://openalex.org/W3175618100', 'https://openalex.org/W3195061894', 'https://openalex.org/W4287180823', 'https://openalex.org/W4287776017', 'https://openalex.org/W4287900772', 'https://openalex.org/W4289751766', 'https://openalex.org/W4295249402', 'https://openalex.org/W4385245566']","The task of Conversational Recommendation System (CRS), i.e., recommender dialog system, aims to recommend precise items to users through natural language interactions. Though recent end-to-end neural models have shown promising progress on this task, two key challenges still remain. First, the recommended items cannot be always incorporated into the generated response precisely and appropriately. Second, only the items mentioned in the training corpus have a chance to be recommended in the conversation. To tackle these challenges, we introduce a novel framework called NTRD for recommender dialogue system that can decouple the dialogue generation from the item recommendation. NTRD has two key components, i.e., response template generator and item selector. The former adopts an encoder-decoder model to generate a response template with slot locations tied to target items, while the latter fills in slot locations with the proper items using a sufficient attention mechanism. Our approach combines the strengths of both classical slot filling approaches (that are generally controllable) and modern neural NLG approaches (that are generally more natural and accurate). Extensive experiments on the benchmark ReDial show our approach significantly outperforms the previous state-of-the-art methods. Besides, our approach has the unique advantage to produce novel items that do not appear in the training set of dialogue corpus. The code is available at https://github.com/jokieleung/NTRD.",1.0
NOVEL_DIA_35,https://openalex.org/W3171266972,2021,51,"['https://openalex.org/W1902237438', 'https://openalex.org/W2133564696', 'https://openalex.org/W2197546379', 'https://openalex.org/W2798456655', 'https://openalex.org/W2804547589', 'https://openalex.org/W2891416139', 'https://openalex.org/W2908602207', 'https://openalex.org/W2925618549', 'https://openalex.org/W2949446780', 'https://openalex.org/W2951807227', 'https://openalex.org/W2952813980', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963715136', 'https://openalex.org/W2964303773', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970648534', 'https://openalex.org/W2995289474', 'https://openalex.org/W2996428491', 'https://openalex.org/W3021016503', 'https://openalex.org/W3034533785', 'https://openalex.org/W3034975599', 'https://openalex.org/W3083814290', 'https://openalex.org/W3094612274', 'https://openalex.org/W3097517997', 'https://openalex.org/W3176002924', 'https://openalex.org/W3176506090', 'https://openalex.org/W4287795696', 'https://openalex.org/W4300687842', 'https://openalex.org/W4385245566']","Janghoon Han, Taesuk Hong, Byoungjae Kim, Youngjoong Ko, Jungyun Seo. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_DIA_37,https://openalex.org/W3196292458,2021,42,"['https://openalex.org/W110692952', 'https://openalex.org/W1522301498', 'https://openalex.org/W1832693441', 'https://openalex.org/W2093585241', 'https://openalex.org/W2131774270', 'https://openalex.org/W2168248941', 'https://openalex.org/W2181629536', 'https://openalex.org/W2250521169', 'https://openalex.org/W2400672603', 'https://openalex.org/W2513378248', 'https://openalex.org/W2524459589', 'https://openalex.org/W2740550900', 'https://openalex.org/W2749002090', 'https://openalex.org/W2759211898', 'https://openalex.org/W2761590056', 'https://openalex.org/W2914120296', 'https://openalex.org/W2952179106', 'https://openalex.org/W2962830617', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963686995', 'https://openalex.org/W2964015378', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964166731', 'https://openalex.org/W2964300796', 'https://openalex.org/W2965373594', 'https://openalex.org/W2971221499', 'https://openalex.org/W2975998869', 'https://openalex.org/W2982479487', 'https://openalex.org/W2985882473', 'https://openalex.org/W2996428491', 'https://openalex.org/W3021224558', 'https://openalex.org/W3035020961', 'https://openalex.org/W3035053871', 'https://openalex.org/W3035566559', 'https://openalex.org/W3094612274', 'https://openalex.org/W3098556456', 'https://openalex.org/W3099056802', 'https://openalex.org/W3102663935', 'https://openalex.org/W3103836967', 'https://openalex.org/W3112076981', 'https://openalex.org/W3174817334', 'https://openalex.org/W3205498744', 'https://openalex.org/W4385245566']","Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because dialogues have the characteristics of high personal pronoun occurrences and low information density, and since most relational facts in dialogues are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way people understand dialogues. In addition, we propose a novel approach which treats the task of emotion recognition in conversations (ERC) as a dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC datasets demonstrate that our model is very effective in various dialogue-based natural language understanding tasks. In these experiments, TUCORE-GCN outperforms the state-of-the-art models on most of the benchmark datasets. Our code is available at https://github.com/BlackNoodle/TUCORE-GCN.",1.0
NOVEL_DIA_39,https://openalex.org/W3201091014,2021,4,"['https://openalex.org/W138910040', 'https://openalex.org/W2015933299', 'https://openalex.org/W2042689318', 'https://openalex.org/W2057820243', 'https://openalex.org/W2081580037', 'https://openalex.org/W2113242047', 'https://openalex.org/W2114917204', 'https://openalex.org/W2118320923', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140646431', 'https://openalex.org/W2143017621', 'https://openalex.org/W2151373442', 'https://openalex.org/W2155069789', 'https://openalex.org/W2217772701', 'https://openalex.org/W2250701745', 'https://openalex.org/W2331260012', 'https://openalex.org/W2768661419', 'https://openalex.org/W2806042735', 'https://openalex.org/W2892245540', 'https://openalex.org/W2946581011', 'https://openalex.org/W2953320089', 'https://openalex.org/W2963087868', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970550868', 'https://openalex.org/W2970745243', 'https://openalex.org/W2981902456', 'https://openalex.org/W2986536733', 'https://openalex.org/W3011411500', 'https://openalex.org/W3034786666', 'https://openalex.org/W3034797437', 'https://openalex.org/W3152694552', 'https://openalex.org/W4231510805', 'https://openalex.org/W4234682833', 'https://openalex.org/W4249013746', 'https://openalex.org/W4287367772']","Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in daily communications, where speakers may directly use pronouns to refer to some objects present in the environment without introducing the objects first. Although such objects are not mentioned in the dialogue text, they can often be disambiguated by the general topics of the dialogue. Motivated by this, we propose to jointly leverage the local context and global topics of dialogues to solve the out-of-text PCR problem. Extensive experiments demonstrate the effectiveness of adding topic regularization for resolving exophoric pronouns.",1.0
NOVEL_DIA_40,https://openalex.org/W3209879382,2021,1,"['https://openalex.org/W2113063049', 'https://openalex.org/W2115615127', 'https://openalex.org/W2121912514', 'https://openalex.org/W2155707639', 'https://openalex.org/W2803119681', 'https://openalex.org/W2896457183', 'https://openalex.org/W2949600515', 'https://openalex.org/W2963341956', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970597249', 'https://openalex.org/W2998672049', 'https://openalex.org/W3015071427', 'https://openalex.org/W3015297483', 'https://openalex.org/W3015468748', 'https://openalex.org/W3102606416', 'https://openalex.org/W3104997146', 'https://openalex.org/W4287635673', 'https://openalex.org/W4287704453']","Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all reply-to links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a zero-shot dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10% of the data, we achieve nearly the same performance of using the full dataset.",1.0
NOVEL_DIA_41,https://openalex.org/W3173691672,2021,35,"['https://openalex.org/W1522301498', 'https://openalex.org/W1527783480', 'https://openalex.org/W2296073425', 'https://openalex.org/W2468710617', 'https://openalex.org/W2798367796', 'https://openalex.org/W2899628936', 'https://openalex.org/W2945475330', 'https://openalex.org/W2950695840', 'https://openalex.org/W2962721878', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963096017', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964121744', 'https://openalex.org/W2979400990', 'https://openalex.org/W2997657234', 'https://openalex.org/W2998228050', 'https://openalex.org/W2998572029', 'https://openalex.org/W3015974317', 'https://openalex.org/W3021096583', 'https://openalex.org/W3034201598', 'https://openalex.org/W3034284249', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034623328', 'https://openalex.org/W3034938700', 'https://openalex.org/W3035470414', 'https://openalex.org/W3035633461', 'https://openalex.org/W3040352674', 'https://openalex.org/W3091355780', 'https://openalex.org/W3094024803', 'https://openalex.org/W3094479119', 'https://openalex.org/W3099231098', 'https://openalex.org/W3101131512', 'https://openalex.org/W3103753314', 'https://openalex.org/W3106495716', 'https://openalex.org/W3119649668', 'https://openalex.org/W3126918322', 'https://openalex.org/W3142849873', 'https://openalex.org/W3175095351', 'https://openalex.org/W3175678722', 'https://openalex.org/W4285719527', 'https://openalex.org/W4287659415', 'https://openalex.org/W4287749601', 'https://openalex.org/W4287795696', 'https://openalex.org/W4288094254', 'https://openalex.org/W4288288848']","Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Xiaodan Zhu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",0.9950248756218906
NOVEL_DIA_42,https://openalex.org/W3170012708,2021,13,"['https://openalex.org/W1956340063', 'https://openalex.org/W2053154970', 'https://openalex.org/W2101105183', 'https://openalex.org/W2521114121', 'https://openalex.org/W2741323980', 'https://openalex.org/W2761590056', 'https://openalex.org/W2765617518', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898700502', 'https://openalex.org/W2898875342', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914204778', 'https://openalex.org/W2945260553', 'https://openalex.org/W2951583236', 'https://openalex.org/W2962753250', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963188990', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963330684', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963939249', 'https://openalex.org/W2963995063', 'https://openalex.org/W2964042872', 'https://openalex.org/W2964352131', 'https://openalex.org/W2964587107', 'https://openalex.org/W2965373594', 'https://openalex.org/W2965617855', 'https://openalex.org/W2970252402', 'https://openalex.org/W2971274815', 'https://openalex.org/W2972916088', 'https://openalex.org/W2988937804', 'https://openalex.org/W2997892440', 'https://openalex.org/W3022187094', 'https://openalex.org/W3023786569', 'https://openalex.org/W3034337319', 'https://openalex.org/W3035451444', 'https://openalex.org/W3038115231', 'https://openalex.org/W3040352674', 'https://openalex.org/W3082549344', 'https://openalex.org/W3094611204', 'https://openalex.org/W3155584966', 'https://openalex.org/W4287749601', 'https://openalex.org/W4288624561', 'https://openalex.org/W4322614701']","Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer – conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the state-of-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data.",1.0
NOVEL_DIA_43,https://openalex.org/W3204515301,2021,43,"['https://openalex.org/W1513168555', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101390659', 'https://openalex.org/W2125336414', 'https://openalex.org/W2144933361', 'https://openalex.org/W2153190547', 'https://openalex.org/W2154652894', 'https://openalex.org/W2574535369', 'https://openalex.org/W2606974598', 'https://openalex.org/W2612675303', 'https://openalex.org/W2617566453', 'https://openalex.org/W2626778328', 'https://openalex.org/W2735642330', 'https://openalex.org/W2793978524', 'https://openalex.org/W2798139452', 'https://openalex.org/W2889518897', 'https://openalex.org/W2889984458', 'https://openalex.org/W2908510526', 'https://openalex.org/W2949530332', 'https://openalex.org/W2949615363', 'https://openalex.org/W2952138241', 'https://openalex.org/W2962805889', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963126845', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963366196', 'https://openalex.org/W2963607157', 'https://openalex.org/W2963691697', 'https://openalex.org/W2964015378', 'https://openalex.org/W2964222296', 'https://openalex.org/W2968154628', 'https://openalex.org/W2970419734', 'https://openalex.org/W2970745243', 'https://openalex.org/W2970971581', 'https://openalex.org/W2971034336', 'https://openalex.org/W2982399380', 'https://openalex.org/W3008323921', 'https://openalex.org/W3021199973', 'https://openalex.org/W3034863243', 'https://openalex.org/W3034999214', 'https://openalex.org/W3037109418', 'https://openalex.org/W3074476581', 'https://openalex.org/W3098295156', 'https://openalex.org/W3099474967', 'https://openalex.org/W3100439863', 'https://openalex.org/W3100560913', 'https://openalex.org/W3101913037', 'https://openalex.org/W3103031657', 'https://openalex.org/W3104257895', 'https://openalex.org/W3106234277', 'https://openalex.org/W3111372071', 'https://openalex.org/W3158986179', 'https://openalex.org/W3164979202', 'https://openalex.org/W3169117666', 'https://openalex.org/W3170033958', 'https://openalex.org/W3170083118', 'https://openalex.org/W3176770275', 'https://openalex.org/W3193260960', 'https://openalex.org/W4241891521', 'https://openalex.org/W4295312788', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385970303']","In this paper, we propose a controllable neural generation framework that can flexibly guide dialogue summarization with personal named entity planning. The conditional sequences are modulated to decide what types of information or what perspective to focus on when forming summaries to tackle the under-constrained problem in summarization tasks. This framework supports two types of use cases: (1) Comprehensive Perspective, which is a general-purpose case with no user-preference specified, considering summary points from all conversational interlocutors and all mentioned persons; (2) Focus Perspective, positioning the summary based on a user-specified personal named entity, which could be one of the interlocutors or one of the persons mentioned in the conversation. During training, we exploit occurrence planning of personal named entities and coreference information to improve temporal coherence and to minimize hallucination in neural generation. Experimental results show that our proposed framework generates fluent and factually consistent summaries under various planning controls using both objective metrics and human evaluations.",1.0
NOVEL_DIA_44,https://openalex.org/W3200895474,2021,85,"['https://openalex.org/W2251058040', 'https://openalex.org/W2784070054', 'https://openalex.org/W2804010326', 'https://openalex.org/W2888849322', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908510526', 'https://openalex.org/W2945475330', 'https://openalex.org/W2951216772', 'https://openalex.org/W2954492830', 'https://openalex.org/W2955810669', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964057895', 'https://openalex.org/W2965373594', 'https://openalex.org/W2972777589', 'https://openalex.org/W2988252747', 'https://openalex.org/W2988937804', 'https://openalex.org/W2997771882', 'https://openalex.org/W2998228050', 'https://openalex.org/W3000779003', 'https://openalex.org/W3004786215', 'https://openalex.org/W3021016503', 'https://openalex.org/W3021096583', 'https://openalex.org/W3023786569', 'https://openalex.org/W3024509506', 'https://openalex.org/W3034533785', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034999214', 'https://openalex.org/W3044438666', 'https://openalex.org/W3045703328', 'https://openalex.org/W3082274269', 'https://openalex.org/W3099827451', 'https://openalex.org/W3100110884', 'https://openalex.org/W3100128199', 'https://openalex.org/W3102854726', 'https://openalex.org/W3104078590', 'https://openalex.org/W3119649668', 'https://openalex.org/W3119822474', 'https://openalex.org/W3155584966', 'https://openalex.org/W3156909481', 'https://openalex.org/W4287795696', 'https://openalex.org/W4287815000', 'https://openalex.org/W4287900772', 'https://openalex.org/W4288027128', 'https://openalex.org/W4288089799', 'https://openalex.org/W4288094254', 'https://openalex.org/W4292779060']","Task-oriented conversational systems often use dialogue state tracking to represent the user's intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely generative system achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M. The data and code will be available at https://github.com/chiahsuan156/DST-as-Prompting.",1.0
NOVEL_DIA_45,https://openalex.org/W3153046263,2021,72,"['https://openalex.org/W145832685', 'https://openalex.org/W182831726', 'https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1984205520', 'https://openalex.org/W2094728533', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2132453167', 'https://openalex.org/W2148721079', 'https://openalex.org/W2152790380', 'https://openalex.org/W2250930514', 'https://openalex.org/W2252136820', 'https://openalex.org/W2270070752', 'https://openalex.org/W2283196293', 'https://openalex.org/W2295754318', 'https://openalex.org/W2519887557', 'https://openalex.org/W2739046565', 'https://openalex.org/W2739716023', 'https://openalex.org/W2754194354', 'https://openalex.org/W2807873315', 'https://openalex.org/W2909737760', 'https://openalex.org/W2914397182', 'https://openalex.org/W2938704169', 'https://openalex.org/W2949413855', 'https://openalex.org/W2950457956', 'https://openalex.org/W2952523122', 'https://openalex.org/W2962729880', 'https://openalex.org/W2962735233', 'https://openalex.org/W2962816513', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963520511', 'https://openalex.org/W2964015378', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964303773', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970476646', 'https://openalex.org/W2970988759', 'https://openalex.org/W2971236040', 'https://openalex.org/W2972916088', 'https://openalex.org/W2973049837', 'https://openalex.org/W2975059944', 'https://openalex.org/W2979826702', 'https://openalex.org/W2981259322', 'https://openalex.org/W2988937804', 'https://openalex.org/W2995448904', 'https://openalex.org/W2996287690', 'https://openalex.org/W3011411500', 'https://openalex.org/W3015322406', 'https://openalex.org/W3022463379', 'https://openalex.org/W3022814719', 'https://openalex.org/W3034188538', 'https://openalex.org/W3034383590', 'https://openalex.org/W3082549344', 'https://openalex.org/W3098196327', 'https://openalex.org/W3098495697', 'https://openalex.org/W3098824823', 'https://openalex.org/W3099453223', 'https://openalex.org/W3099766584', 'https://openalex.org/W3102659883', 'https://openalex.org/W3106234277', 'https://openalex.org/W3115944734', 'https://openalex.org/W3158244987', 'https://openalex.org/W3176098057', 'https://openalex.org/W4287684041', 'https://openalex.org/W4288091035']","Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose NEURAL PATH HUNTER which follows a generate-then-refine strategy whereby a generated response is amended using the KG. NEURAL PATH HUNTER leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",0.9937888198757764
NOVEL_DIA_47,https://openalex.org/W3171708917,2021,12,"['https://openalex.org/W648947103', 'https://openalex.org/W1522301498', 'https://openalex.org/W1550863320', 'https://openalex.org/W2064675550', 'https://openalex.org/W2077302143', 'https://openalex.org/W2124895976', 'https://openalex.org/W2129554061', 'https://openalex.org/W2137871902', 'https://openalex.org/W2153962611', 'https://openalex.org/W2166293310', 'https://openalex.org/W2267186426', 'https://openalex.org/W2400801499', 'https://openalex.org/W2473329891', 'https://openalex.org/W2534274346', 'https://openalex.org/W2575101493', 'https://openalex.org/W2797625445', 'https://openalex.org/W2803392141', 'https://openalex.org/W2803609229', 'https://openalex.org/W2804945011', 'https://openalex.org/W2891533927', 'https://openalex.org/W2950527759', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962954913', 'https://openalex.org/W2963033987', 'https://openalex.org/W2963066655', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963974889', 'https://openalex.org/W2964121744', 'https://openalex.org/W2971167298', 'https://openalex.org/W4297683418', 'https://openalex.org/W4303633609', 'https://openalex.org/W4385245566']","Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing semantic knowledge. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts: 1. Long-term slot context is not traced effectively, which is crucial for future slot filling. 2. Slot tagging and intent detection could be mutually rewarding, but bi-directional interaction between slot filling and intent detection remains seldom explored. In this paper, we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents. We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before, which are then fed into our decoder for slot tagging. Furthermore, gated memory information is utilized to perform intent detection, mutually improving both tasks through global optimization. Experiments on benchmark ATIS and Snips datasets show that our model achieves state-of-the-art performance and outperforms other methods, especially for the slot filling task.",1.0
NOVEL_DIA_49,https://openalex.org/W3212099586,2021,11,"['https://openalex.org/W117128830', 'https://openalex.org/W1534395135', 'https://openalex.org/W1904365287', 'https://openalex.org/W2021151961', 'https://openalex.org/W2027685064', 'https://openalex.org/W2047057213', 'https://openalex.org/W2056566670', 'https://openalex.org/W2094484179', 'https://openalex.org/W2095705004', 'https://openalex.org/W2099647287', 'https://openalex.org/W2100377190', 'https://openalex.org/W2112707476', 'https://openalex.org/W2145339207', 'https://openalex.org/W2159345153', 'https://openalex.org/W2167362547', 'https://openalex.org/W2201581102', 'https://openalex.org/W2294065713', 'https://openalex.org/W2417401578', 'https://openalex.org/W2436711315', 'https://openalex.org/W2507592741', 'https://openalex.org/W2571927164', 'https://openalex.org/W2594466397', 'https://openalex.org/W2594726847', 'https://openalex.org/W2739936944', 'https://openalex.org/W2740191615', 'https://openalex.org/W2759104452', 'https://openalex.org/W2787841449', 'https://openalex.org/W2798494119', 'https://openalex.org/W2884814595', 'https://openalex.org/W2889186204', 'https://openalex.org/W2891732163', 'https://openalex.org/W2947212824', 'https://openalex.org/W2949252816', 'https://openalex.org/W2949476504', 'https://openalex.org/W2951805158', 'https://openalex.org/W2962996309', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963289713', 'https://openalex.org/W2963433587', 'https://openalex.org/W2963477884', 'https://openalex.org/W2963567240', 'https://openalex.org/W2963692154', 'https://openalex.org/W2963993502', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964044380', 'https://openalex.org/W2964082094', 'https://openalex.org/W2970828515', 'https://openalex.org/W2998458199', 'https://openalex.org/W3008413595', 'https://openalex.org/W3034330559', 'https://openalex.org/W3104546989', 'https://openalex.org/W3105957118', 'https://openalex.org/W3175095351', 'https://openalex.org/W4234134528', 'https://openalex.org/W4239187508', 'https://openalex.org/W4297732320', 'https://openalex.org/W4312609624']","Deep reinforcement learning has shown great potential in training dialogue policies. However, its favorable performance comes at the cost of many rounds of interaction. Most of the existing dialogue policy methods rely on a single learning system, while the human brain has two specialized learning and memory systems, supporting to find good solutions without requiring copious examples. Inspired by the human brain, this paper proposes a novel complementary policy learning (CPL) framework, which exploits the complementary advantages of the episodic memory (EM) policy and the deep Q-network (DQN) policy to achieve fast and effective dialogue policy learning. In order to coordinate between the two policies, we proposed a confidence controller to control the complementary time according to their relative efficacy at different stages. Furthermore, memory connectivity and time pruning are proposed to guarantee the flexible and adaptive generalization of the EM policy in dialog tasks. Experimental results on three dialogue datasets show that our method significantly outperforms existing methods relying on a single learning system.",1.0
NOVEL_DIA_50,https://openalex.org/W3094083263,2021,9,"['https://openalex.org/W1522301498', 'https://openalex.org/W1975244201', 'https://openalex.org/W2171837816', 'https://openalex.org/W2438667436', 'https://openalex.org/W2798914047', 'https://openalex.org/W2808093377', 'https://openalex.org/W2891732163', 'https://openalex.org/W2896457183', 'https://openalex.org/W2949141958', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962886331', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963858333', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970260827', 'https://openalex.org/W2971261034', 'https://openalex.org/W2994673210', 'https://openalex.org/W3013192639', 'https://openalex.org/W3017074538', 'https://openalex.org/W3035301094', 'https://openalex.org/W3082549344', 'https://openalex.org/W3099453223', 'https://openalex.org/W3102521862', 'https://openalex.org/W3106274079', 'https://openalex.org/W3162000275', 'https://openalex.org/W4295838474', 'https://openalex.org/W4297733535', 'https://openalex.org/W4385245566']","Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue systems is challenging, since it requires to properly represent the entity of KB, which is associated with its KB context and dialogue context. The existing works represent the entity with only perceiving a part of its KB context, which can lead to the less effective representation due to the information loss, and adversely favor KB reasoning and response generation. To tackle this issue, we explore to fully contextualize the entity representation by dynamically perceiving all the relevant entities and dialogue history. To achieve this, we propose a COntext-aware Memory Enhanced Transformer framework (COMET), which treats the KB as a sequence and leverages a novel Memory Mask to enforce the entity to only focus on its relevant entities and dialogue history, while avoiding the distraction from the irrelevant entities. Through extensive experiments, we show that our COMET framework can achieve superior performance over the state of the arts.",1.0
NOVEL_DIA_51,https://openalex.org/W3166143260,2021,35,"['https://openalex.org/W1598178035', 'https://openalex.org/W1948566616', 'https://openalex.org/W1993567041', 'https://openalex.org/W2099813784', 'https://openalex.org/W2108806737', 'https://openalex.org/W2250456405', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251235149', 'https://openalex.org/W2464790259', 'https://openalex.org/W2565274151', 'https://openalex.org/W2604698497', 'https://openalex.org/W2611138659', 'https://openalex.org/W2759621817', 'https://openalex.org/W2784070054', 'https://openalex.org/W2891732163', 'https://openalex.org/W2898875342', 'https://openalex.org/W2950457956', 'https://openalex.org/W2951583236', 'https://openalex.org/W2952607215', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963201498', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963969444', 'https://openalex.org/W2963974889', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964210218', 'https://openalex.org/W2965373594', 'https://openalex.org/W2971883198', 'https://openalex.org/W2997771882', 'https://openalex.org/W2998228050', 'https://openalex.org/W2999134550', 'https://openalex.org/W3000779003', 'https://openalex.org/W3002330681', 'https://openalex.org/W3021016503', 'https://openalex.org/W3021096583', 'https://openalex.org/W3023786569', 'https://openalex.org/W3024509506', 'https://openalex.org/W3034337319', 'https://openalex.org/W3034600233', 'https://openalex.org/W3034908682', 'https://openalex.org/W3037528277', 'https://openalex.org/W3082549344', 'https://openalex.org/W3102854726', 'https://openalex.org/W3117238912', 'https://openalex.org/W3155584966', 'https://openalex.org/W4254836149', 'https://openalex.org/W4287795696', 'https://openalex.org/W4287900772', 'https://openalex.org/W4288113479', 'https://openalex.org/W4288288848', 'https://openalex.org/W4289147179', 'https://openalex.org/W4295249402']","Kai Sun, Seungwhan Moon, Paul Crook, Stephen Roller, Becka Silvert, Bing Liu, Zhiguang Wang, Honglei Liu, Eunjoon Cho, Claire Cardie. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_DIA_52,https://openalex.org/W3196675050,2021,18,"['https://openalex.org/W1480981077', 'https://openalex.org/W1574901103', 'https://openalex.org/W1976526581', 'https://openalex.org/W2045812729', 'https://openalex.org/W2144182447', 'https://openalex.org/W2152272511', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251658415', 'https://openalex.org/W2475167333', 'https://openalex.org/W2531327146', 'https://openalex.org/W2575298912', 'https://openalex.org/W2577267675', 'https://openalex.org/W2740149041', 'https://openalex.org/W2758425594', 'https://openalex.org/W2767414122', 'https://openalex.org/W2806753317', 'https://openalex.org/W2889625178', 'https://openalex.org/W2891642103', 'https://openalex.org/W2898856000', 'https://openalex.org/W2904981516', 'https://openalex.org/W2905266130', 'https://openalex.org/W2913352150', 'https://openalex.org/W2923780723', 'https://openalex.org/W2932893307', 'https://openalex.org/W2948367246', 'https://openalex.org/W2951883849', 'https://openalex.org/W2951911250', 'https://openalex.org/W2952409498', 'https://openalex.org/W2959053776', 'https://openalex.org/W2962721878', 'https://openalex.org/W2963238274', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963384319', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963655793', 'https://openalex.org/W2963693742', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963924212', 'https://openalex.org/W2964059111', 'https://openalex.org/W2964201905', 'https://openalex.org/W2965373594', 'https://openalex.org/W2968297680', 'https://openalex.org/W2970418174', 'https://openalex.org/W2970641574', 'https://openalex.org/W2970855255', 'https://openalex.org/W2970946347', 'https://openalex.org/W2971002550', 'https://openalex.org/W2971089712', 'https://openalex.org/W2971296908', 'https://openalex.org/W2985067290', 'https://openalex.org/W2986193249', 'https://openalex.org/W2995188922', 'https://openalex.org/W2995404354', 'https://openalex.org/W2997140799', 'https://openalex.org/W2997212544', 'https://openalex.org/W3014773921', 'https://openalex.org/W3034630076', 'https://openalex.org/W3035441651', 'https://openalex.org/W3094447228', 'https://openalex.org/W3097663391', 'https://openalex.org/W3098341425', 'https://openalex.org/W3099617520', 'https://openalex.org/W3100247553', 'https://openalex.org/W3105190746', 'https://openalex.org/W3105388824', 'https://openalex.org/W3121064530', 'https://openalex.org/W3133589252', 'https://openalex.org/W3165919849', 'https://openalex.org/W4254182148']","Practical dialogue systems require robust methods of detecting out-of-scope (OOS) utterances to avoid conversational breakdowns and related failure modes. Directly training a model with labeled OOS examples yields reasonable performance, but obtaining such data is a resource-intensive process. To tackle this limited-data problem, previous methods focus on better modeling the distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal technique that augments existing data to train better OOS detectors operating in low-data regimes. GOLD generates pseudo-labeled candidates using samples from an auxiliary dataset and keeps only the most beneficial candidates for training through a novel filtering mechanism. In experiments across three target benchmarks, the top GOLD model outperforms all existing methods on all key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median baseline performance. We also analyze the unique properties of OOS data to identify key factors for optimally applying our proposed method.",0.9932885906040269
NOVEL_DIA_53,https://openalex.org/W3212092284,2021,15,"['https://openalex.org/W1902237438', 'https://openalex.org/W2098985784', 'https://openalex.org/W2101105183', 'https://openalex.org/W2127795553', 'https://openalex.org/W2154652894', 'https://openalex.org/W2157331557', 'https://openalex.org/W2304113845', 'https://openalex.org/W2561529111', 'https://openalex.org/W2586847566', 'https://openalex.org/W2606974598', 'https://openalex.org/W2798385473', 'https://openalex.org/W2799037524', 'https://openalex.org/W2804552794', 'https://openalex.org/W2807873315', 'https://openalex.org/W2890969459', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951883832', 'https://openalex.org/W2952420867', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963285578', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963541420', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964207259', 'https://openalex.org/W2970260827', 'https://openalex.org/W2970579055', 'https://openalex.org/W2970988759', 'https://openalex.org/W2971199636', 'https://openalex.org/W2995183464', 'https://openalex.org/W2996227762', 'https://openalex.org/W2997094605', 'https://openalex.org/W2997300509', 'https://openalex.org/W2998083599', 'https://openalex.org/W3011801489', 'https://openalex.org/W3034569646', 'https://openalex.org/W3034606970', 'https://openalex.org/W3034696087', 'https://openalex.org/W3034758256', 'https://openalex.org/W3035072597', 'https://openalex.org/W3035356453', 'https://openalex.org/W3045507162', 'https://openalex.org/W3092288641', 'https://openalex.org/W3093956460', 'https://openalex.org/W3100523370', 'https://openalex.org/W3113225429', 'https://openalex.org/W3113547664', 'https://openalex.org/W3118010541', 'https://openalex.org/W3121541553', 'https://openalex.org/W3187909673', 'https://openalex.org/W4385245566']","Despite achieving remarkable performance, previous knowledge-enhanced works usually only use a single-source homogeneous knowledge base of limited knowledge coverage. Thus, they often degenerate into traditional methods because not all dialogues can be linked with knowledge entries. This paper proposes a novel dialogue generation model, MSKE-Dialog, to solve this issue with three unique advantages: (1) Rather than only one, MSKE-Dialog can simultaneously leverage multiple heterogeneous knowledge sources (it includes but is not limited to commonsense knowledge facts, text knowledge, infobox knowledge) to improve the knowledge coverage; (2) To avoid the topic conflict among the context and different knowledge sources, we propose a Multi-Reference Selection to better select context/knowledge; (3) We propose a Multi-Reference Generation to generate informative responses by referring to multiple generation references at the same time. Extensive evaluations on a Chinese dataset show the superior performance of this work against various state-of-the-art approaches. To our best knowledge, this work is the first to use the multi-source heterogeneous knowledge in the open-domain knowledge-enhanced dialogue generation.",0.9948717948717948
NOVEL_DIA_54,https://openalex.org/W3153659051,2021,3,"['https://openalex.org/W2891732163', 'https://openalex.org/W2945475330', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963858333', 'https://openalex.org/W2988252747', 'https://openalex.org/W2998228050', 'https://openalex.org/W3015974317', 'https://openalex.org/W3021096583', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034956542', 'https://openalex.org/W3035470414', 'https://openalex.org/W3035594326', 'https://openalex.org/W3035633461', 'https://openalex.org/W3088238433', 'https://openalex.org/W3101131512', 'https://openalex.org/W3103753314', 'https://openalex.org/W3119649668', 'https://openalex.org/W3121272494', 'https://openalex.org/W3166401044', 'https://openalex.org/W3174401467']","Dialogue State Tracking is central to multi-domain task-oriented dialogue systems, responsible for extracting information from user utterances. We present a novel hybrid architecture that augments GPT-2 with representations derived from Graph Attention Networks in such a way to allow causal, sequential prediction of slot values. The model architecture captures inter-slot relationships and dependencies across domains that otherwise can be lost in sequential prediction. We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level. We further report detailed analyses to demonstrate the effectiveness of graph models in DST by showing that the proposed graph modules capture inter-slot dependencies and improve the predictions of values that are common to multiple domains.",1.0
NOVEL_DIA_55,https://openalex.org/W3201234832,2021,8,"['https://openalex.org/W2045865594', 'https://openalex.org/W2533180076', 'https://openalex.org/W2745673470', 'https://openalex.org/W2790235966', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964015378', 'https://openalex.org/W2964082993', 'https://openalex.org/W2964165804', 'https://openalex.org/W3016107769', 'https://openalex.org/W3093793648', 'https://openalex.org/W3099700870', 'https://openalex.org/W3156636935', 'https://openalex.org/W3161825643', 'https://openalex.org/W4320803111']","For voice assistants like Alexa, Google Assistant, and Siri, correctly interpreting users’ intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or user errors such as slips of the tongue. Users tend to rephrase their queries until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully utilize the contextual information (e.g. users’ implicit feedback). To this end, we propose a contextual rephrase detection model ContReph to automatically identify rephrases from multi-turn dialogues. We showcase how to leverage the dialogue context and user-agent interaction signals, including the user’s implicit feedback and the time gap between different turns, which can help significantly outperform the pairwise rephrase detection models.",1.0
NOVEL_DIA_56,https://openalex.org/W3169942382,2021,85,"['https://openalex.org/W1880262756', 'https://openalex.org/W1983719983', 'https://openalex.org/W2101234009', 'https://openalex.org/W2154652894', 'https://openalex.org/W2606974598', 'https://openalex.org/W2891732163', 'https://openalex.org/W2945260553', 'https://openalex.org/W2952890017', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963607157', 'https://openalex.org/W2964006684', 'https://openalex.org/W2971274815', 'https://openalex.org/W2978075745', 'https://openalex.org/W2989743967', 'https://openalex.org/W3015818096', 'https://openalex.org/W3023363161', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035259249', 'https://openalex.org/W3085629518', 'https://openalex.org/W3104257895', 'https://openalex.org/W4288095748']","This paper introduces MediaSum, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MediaSum can be used in transfer learning to improve a model’s performance on other dialogue summarization tasks.",0.9931972789115646
NOVEL_DIA_59,https://openalex.org/W3168491067,2021,60,"['https://openalex.org/W1598178035', 'https://openalex.org/W2438667436', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908510526', 'https://openalex.org/W2945475330', 'https://openalex.org/W2951088751', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963578915', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964006684', 'https://openalex.org/W2979400990', 'https://openalex.org/W2988252747', 'https://openalex.org/W2997214274', 'https://openalex.org/W2997771882', 'https://openalex.org/W2998228050', 'https://openalex.org/W3015637204', 'https://openalex.org/W3016625483', 'https://openalex.org/W3021016503', 'https://openalex.org/W3021096583', 'https://openalex.org/W3034284249', 'https://openalex.org/W3034573951', 'https://openalex.org/W3045689439', 'https://openalex.org/W3045703328', 'https://openalex.org/W3049346316', 'https://openalex.org/W3082274269', 'https://openalex.org/W3094479119', 'https://openalex.org/W3097392354', 'https://openalex.org/W3100110884', 'https://openalex.org/W3100128199', 'https://openalex.org/W3119649668', 'https://openalex.org/W3156909481', 'https://openalex.org/W3160818482', 'https://openalex.org/W4285719527', 'https://openalex.org/W4287795696', 'https://openalex.org/W4288027128', 'https://openalex.org/W4288089799', 'https://openalex.org/W4288094254']","Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, Rajen Subba. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_DIA_63,https://openalex.org/W3213435953,2021,19,"['https://openalex.org/W10957333', 'https://openalex.org/W1975879668', 'https://openalex.org/W2406390611', 'https://openalex.org/W2586847566', 'https://openalex.org/W2809213523', 'https://openalex.org/W2898875342', 'https://openalex.org/W2938830017', 'https://openalex.org/W2950220847', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951508633', 'https://openalex.org/W2953356739', 'https://openalex.org/W2962786758', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964458951', 'https://openalex.org/W2970986510', 'https://openalex.org/W2988937804', 'https://openalex.org/W2995183464', 'https://openalex.org/W2996227762', 'https://openalex.org/W3034600233', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035153870', 'https://openalex.org/W3104123491', 'https://openalex.org/W3104415840', 'https://openalex.org/W3104777900', 'https://openalex.org/W3123799706', 'https://openalex.org/W3137695714', 'https://openalex.org/W3155584966', 'https://openalex.org/W4385245566']","Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing methods leverage an external knowledge base to generate appropriate responses. In real-world practical, the entity may not be included by the knowledge base or suffer from the precision of knowledge retrieval. To deal with this problem, instead of introducing knowledge base as the input, we force the model to learn a better semantic representation by predicting the information in the knowledge base, only based on the input context. Specifically, with the help of a knowledge base, we introduce two auxiliary training objectives: 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings.",1.0
NOVEL_DIA_65,https://openalex.org/W3214501429,2021,8,"['https://openalex.org/W1524766440', 'https://openalex.org/W1682403713', 'https://openalex.org/W1821462560', 'https://openalex.org/W1975244201', 'https://openalex.org/W2060277733', 'https://openalex.org/W2108807072', 'https://openalex.org/W2157331557', 'https://openalex.org/W2166344886', 'https://openalex.org/W2250297846', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251235149', 'https://openalex.org/W2473930607', 'https://openalex.org/W2541884796', 'https://openalex.org/W2554863749', 'https://openalex.org/W2560647685', 'https://openalex.org/W2583761661', 'https://openalex.org/W2601450892', 'https://openalex.org/W2737492962', 'https://openalex.org/W2771964490', 'https://openalex.org/W2777054756', 'https://openalex.org/W2798367796', 'https://openalex.org/W2798914047', 'https://openalex.org/W2896457183', 'https://openalex.org/W2945475330', 'https://openalex.org/W2948734064', 'https://openalex.org/W2950635152', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962724315', 'https://openalex.org/W2963038864', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963314614', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963559848', 'https://openalex.org/W2963588172', 'https://openalex.org/W2963733234', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964189064', 'https://openalex.org/W2970404807', 'https://openalex.org/W2971146255', 'https://openalex.org/W2972777589', 'https://openalex.org/W2982410595', 'https://openalex.org/W2986349107', 'https://openalex.org/W2997771882', 'https://openalex.org/W2998228050', 'https://openalex.org/W3021931813', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034898894', 'https://openalex.org/W3035470414', 'https://openalex.org/W3035594326', 'https://openalex.org/W3035633461', 'https://openalex.org/W3035691277', 'https://openalex.org/W3099554319', 'https://openalex.org/W3103819114', 'https://openalex.org/W3109225397', 'https://openalex.org/W3115336344', 'https://openalex.org/W4205340316', 'https://openalex.org/W4295200480', 'https://openalex.org/W4295883599', 'https://openalex.org/W4301163820', 'https://openalex.org/W4318619660', 'https://openalex.org/W4385245566']","Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that KPN effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.",1.0
NOVEL_DIA_66,https://openalex.org/W3163744364,2021,13,"['https://openalex.org/W1495981708', 'https://openalex.org/W1522301498', 'https://openalex.org/W1612381393', 'https://openalex.org/W2050273484', 'https://openalex.org/W2075011505', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130848543', 'https://openalex.org/W2180160918', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250738489', 'https://openalex.org/W2252247041', 'https://openalex.org/W2406055728', 'https://openalex.org/W2507756961', 'https://openalex.org/W2579689822', 'https://openalex.org/W2606974598', 'https://openalex.org/W2921671634', 'https://openalex.org/W2952855649', 'https://openalex.org/W2962769558', 'https://openalex.org/W2963087868', 'https://openalex.org/W2963167649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964222246', 'https://openalex.org/W2970110247', 'https://openalex.org/W2970550868', 'https://openalex.org/W2972873275', 'https://openalex.org/W3011411500', 'https://openalex.org/W3031383011', 'https://openalex.org/W3034797437', 'https://openalex.org/W3171244865', 'https://openalex.org/W4385245566']","Bo-Hsiang Tseng, Shruti Bhargava, Jiarui Lu, Joel Ruben Antony Moniz, Dhivya Piraviperumal, Lin Li, Hong Yu. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",0.9921259842519684
NOVEL_DIA_67,https://openalex.org/W3090998540,2021,5,"['https://openalex.org/W84786028', 'https://openalex.org/W639708223', 'https://openalex.org/W1514535095', 'https://openalex.org/W1522301498', 'https://openalex.org/W1889081078', 'https://openalex.org/W1895577753', 'https://openalex.org/W1933349210', 'https://openalex.org/W2117539524', 'https://openalex.org/W2185175083', 'https://openalex.org/W2546696630', 'https://openalex.org/W2737766105', 'https://openalex.org/W2768661419', 'https://openalex.org/W2890394457', 'https://openalex.org/W2913443447', 'https://openalex.org/W2950178297', 'https://openalex.org/W2950761309', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963467339', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963703197', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963904606', 'https://openalex.org/W2963955897', 'https://openalex.org/W2968124245', 'https://openalex.org/W2970231061', 'https://openalex.org/W2970352191', 'https://openalex.org/W2970608575', 'https://openalex.org/W2971173235', 'https://openalex.org/W2971883198', 'https://openalex.org/W2972119347', 'https://openalex.org/W2982260276', 'https://openalex.org/W2982277189', 'https://openalex.org/W2985799198', 'https://openalex.org/W2988937804', 'https://openalex.org/W2991289756', 'https://openalex.org/W2997517419', 'https://openalex.org/W3000779003', 'https://openalex.org/W3022310886', 'https://openalex.org/W3022579796', 'https://openalex.org/W3023074479', 'https://openalex.org/W3023786569', 'https://openalex.org/W3023893410', 'https://openalex.org/W3034238904', 'https://openalex.org/W3034255912', 'https://openalex.org/W3034600233', 'https://openalex.org/W3034727271', 'https://openalex.org/W3035448310', 'https://openalex.org/W3037528277', 'https://openalex.org/W3037831233', 'https://openalex.org/W3090449556', 'https://openalex.org/W3103639864']","Recent work in open-domain conversational agents has demonstrated that significant improvements in model engagingness and humanness metrics can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of engaging humans in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to engagingness metrics.",1.0
NOVEL_DIA_68,https://openalex.org/W3202125623,2021,26,"['https://openalex.org/W1486649854', 'https://openalex.org/W1566289585', 'https://openalex.org/W1665214252', 'https://openalex.org/W1840435438', 'https://openalex.org/W2155482025', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250790822', 'https://openalex.org/W2413533759', 'https://openalex.org/W2611029872', 'https://openalex.org/W2786464815', 'https://openalex.org/W2790235966', 'https://openalex.org/W2798583685', 'https://openalex.org/W2842511635', 'https://openalex.org/W2884814595', 'https://openalex.org/W2891177506', 'https://openalex.org/W2896457183', 'https://openalex.org/W2923014074', 'https://openalex.org/W2949433733', 'https://openalex.org/W2949446780', 'https://openalex.org/W2953384591', 'https://openalex.org/W2963149412', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963644595', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963918774', 'https://openalex.org/W2970641574', 'https://openalex.org/W3031414376', 'https://openalex.org/W3033406728', 'https://openalex.org/W3034238904', 'https://openalex.org/W3100652389', 'https://openalex.org/W3104033643', 'https://openalex.org/W3104078590', 'https://openalex.org/W3105816068', 'https://openalex.org/W3115295967', 'https://openalex.org/W3131870090', 'https://openalex.org/W3154229486', 'https://openalex.org/W3156636935', 'https://openalex.org/W3164054899', 'https://openalex.org/W3173783447', 'https://openalex.org/W3175362188', 'https://openalex.org/W4252076394', 'https://openalex.org/W4297785815', 'https://openalex.org/W4297808394', 'https://openalex.org/W4298443704', 'https://openalex.org/W4313908941']","Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman’s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.",0.9929078014184396
NOVEL_DIA_69,https://openalex.org/W3213972251,2021,7,"['https://openalex.org/W1522301498', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250297846', 'https://openalex.org/W2798914047', 'https://openalex.org/W2810840719', 'https://openalex.org/W2889448364', 'https://openalex.org/W2891732163', 'https://openalex.org/W2945475330', 'https://openalex.org/W2953071719', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962808855', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963527209', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W2997771882', 'https://openalex.org/W3021096583', 'https://openalex.org/W3034573951', 'https://openalex.org/W3036362489', 'https://openalex.org/W3045689439', 'https://openalex.org/W3100544532', 'https://openalex.org/W3119649668', 'https://openalex.org/W4205671217', 'https://openalex.org/W4288094254', 'https://openalex.org/W4385245566']","Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes severer. Current models are not satisfactory for solving the challenges of ontology integration between domains and out-of-vocabulary problems. To address the problem, we explore the hierarchical semantic of ontology and enhance the interrelation between slots with masked hierarchical attention. In state value decoding stage, we solve the out-of-vocabulary problem by combining generation method and extraction method together. We evaluate the performance of our model on two representative datasets, MultiWOZ in English and CrossWOZ in Chinese. The results show that our model yields a significant performance gain over current state-of-the-art state tracking model and it is more robust to out-of-vocabulary problem compared with other methods.",1.0
NOVEL_DIA_70,https://openalex.org/W3211887611,2021,10,"['https://openalex.org/W1522301498', 'https://openalex.org/W1602500555', 'https://openalex.org/W1933065844', 'https://openalex.org/W2118781169', 'https://openalex.org/W2130463046', 'https://openalex.org/W2194775991', 'https://openalex.org/W2236233024', 'https://openalex.org/W2585954273', 'https://openalex.org/W2627585944', 'https://openalex.org/W2774005037', 'https://openalex.org/W2795911278', 'https://openalex.org/W2805984364', 'https://openalex.org/W2835434549', 'https://openalex.org/W2890902815', 'https://openalex.org/W2896457183', 'https://openalex.org/W2899235916', 'https://openalex.org/W2899771611', 'https://openalex.org/W2908510526', 'https://openalex.org/W2926977875', 'https://openalex.org/W2950697717', 'https://openalex.org/W2951973805', 'https://openalex.org/W2952033963', 'https://openalex.org/W2958574008', 'https://openalex.org/W2959918500', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963726321', 'https://openalex.org/W2963800628', 'https://openalex.org/W2963946945', 'https://openalex.org/W2964043796', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964217371', 'https://openalex.org/W2966715458', 'https://openalex.org/W2967186499', 'https://openalex.org/W2970231061', 'https://openalex.org/W2970340522', 'https://openalex.org/W2970608575', 'https://openalex.org/W2979727876', 'https://openalex.org/W2981601849', 'https://openalex.org/W2981851019', 'https://openalex.org/W2997886377', 'https://openalex.org/W3023306062', 'https://openalex.org/W3029418112', 'https://openalex.org/W3030193665', 'https://openalex.org/W3034253961', 'https://openalex.org/W3034376488', 'https://openalex.org/W3034500398', 'https://openalex.org/W3034578524', 'https://openalex.org/W3034758614', 'https://openalex.org/W3034984114', 'https://openalex.org/W3084181639', 'https://openalex.org/W3090072755', 'https://openalex.org/W3090449556', 'https://openalex.org/W3091588028', 'https://openalex.org/W3100923070', 'https://openalex.org/W3101009265', 'https://openalex.org/W3106571068', 'https://openalex.org/W3108144224', 'https://openalex.org/W3109097593', 'https://openalex.org/W3135367836', 'https://openalex.org/W3166396011', 'https://openalex.org/W4285719527', 'https://openalex.org/W4287603546', 'https://openalex.org/W4297798492']","Communication between human and mobile agents is getting increasingly important as such agents are widely deployed in our daily lives. Vision-and-Dialogue Navigation is one of the tasks that evaluate the agent's ability to interact with humans for assistance and navigate based on natural language responses. In this paper, we explore the Navigation from Dialogue History (NDH) task, which is based on the Cooperative Vision-and-Dialogue Navigation (CVDN) dataset, and present a state-of-the-art model which is built upon Vision-Language transformers. However, despite achieving competitive performance, we find that the agent in the NDH task is not evaluated appropriately by the primary metric – Goal Progress. By analyzing the performance mismatch between Goal Progress and other metrics (e.g., normalized Dynamic Time Warping) from our state-of-the-art model, we show that NDH's sub-path based task setup (i.e., navigating partial trajectory based on its correspondent subset of the full dialogue) does not provide the agent with enough supervision signal towards the goal region. Therefore, we propose a new task setup called NDH-Full which takes the full dialogue and the whole navigation path as one instance. We present a strong baseline model and show initial results on this new task. We further describe several approaches that we try, in order to improve the model performance (based on curriculum learning, pre-training, and data-augmentation), suggesting potential useful training methods on this new NDH-Full task.",0.9934640522875816
NOVEL_DIA_71,https://openalex.org/W3213852610,2021,5,"['https://openalex.org/W1522301498', 'https://openalex.org/W1793121960', 'https://openalex.org/W1975244201', 'https://openalex.org/W2064675550', 'https://openalex.org/W2184957013', 'https://openalex.org/W2797625445', 'https://openalex.org/W2807873315', 'https://openalex.org/W2808093377', 'https://openalex.org/W2896457183', 'https://openalex.org/W2949141958', 'https://openalex.org/W2950457956', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963201498', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218', 'https://openalex.org/W2970110247', 'https://openalex.org/W2970260827', 'https://openalex.org/W3034569646', 'https://openalex.org/W3034879520', 'https://openalex.org/W3035301094', 'https://openalex.org/W3105732730', 'https://openalex.org/W3106274079', 'https://openalex.org/W3117865619', 'https://openalex.org/W4385245566']","Recent years has witnessed the remarkable success in end-to-end task-oriented dialog system, especially when incorporating external knowledge information. However, the quality of most existing models’ generated response is still limited, mainly due to their lack of fine-grained reasoning on deterministic knowledge (w.r.t. conceptual tokens), which makes them difficult to capture the concept shifts and identify user’s real intention in cross-task scenarios. To address these issues, we propose a novel intention mechanism to better model deterministic entity knowledge. Based on such a mechanism, we further propose an intention reasoning network (IR-Net), which consists of joint and multi-hop reasoning, to obtain intention-aware representations of conceptual tokens that can be used to capture the concept shifts involved in task-oriented conversations, so as to effectively identify user’s intention and generate more accurate responses. Experimental results verify the effectiveness of IR-Net, showing that it achieves the state-of-the-art performance on two representative multi-domain dialog datasets.",1.0
SKG_DIA_0,https://openalex.org/W2949782788,2019,46,"['https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W2099471712', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2188365844', 'https://openalex.org/W2250539671', 'https://openalex.org/W2521114121', 'https://openalex.org/W2557436004', 'https://openalex.org/W2565378226', 'https://openalex.org/W2583741591', 'https://openalex.org/W2584220694', 'https://openalex.org/W2593696076', 'https://openalex.org/W2604444020', 'https://openalex.org/W2741323980', 'https://openalex.org/W2753215597', 'https://openalex.org/W2757121784', 'https://openalex.org/W2788932366', 'https://openalex.org/W2798984671', 'https://openalex.org/W2808293489', 'https://openalex.org/W2889502429', 'https://openalex.org/W2890940245', 'https://openalex.org/W2933374552', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963330684', 'https://openalex.org/W2963567641', 'https://openalex.org/W2963594498', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964178377', 'https://openalex.org/W2964222296', 'https://openalex.org/W2964308564', 'https://openalex.org/W3022187094', 'https://openalex.org/W4294294142', 'https://openalex.org/W4320013936']","Due to its potential applications, open-domain dialogue generation has become popular and achieved remarkable progress in recent years, but sometimes suffers from generic responses. Previous models are generally trained based on 1-to-1 mapping from an input query to its response, which actually ignores the nature of 1-to-n mapping in dialogue that there may exist multiple valid responses corresponding to the same query. In this paper, we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. The first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses. Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations.",0.9948186528497408
SKG_DIA_2,https://openalex.org/W3099590177,2020,84,"['https://openalex.org/W150739662', 'https://openalex.org/W1497300277', 'https://openalex.org/W1602839414', 'https://openalex.org/W2079758417', 'https://openalex.org/W2166957049', 'https://openalex.org/W2741802726', 'https://openalex.org/W2757599232', 'https://openalex.org/W2805172814', 'https://openalex.org/W2810015029', 'https://openalex.org/W2888302696', 'https://openalex.org/W2888984656', 'https://openalex.org/W2891304738', 'https://openalex.org/W2923014074', 'https://openalex.org/W2951846787', 'https://openalex.org/W2962690139', 'https://openalex.org/W2962966777', 'https://openalex.org/W2963159690', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963748441', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964223283', 'https://openalex.org/W2971737394', 'https://openalex.org/W2980282514', 'https://openalex.org/W2983409842', 'https://openalex.org/W2985067290', 'https://openalex.org/W3033540108', 'https://openalex.org/W3034624105', 'https://openalex.org/W3037780283', 'https://openalex.org/W3159086726', 'https://openalex.org/W4252076394', 'https://openalex.org/W4287996196', 'https://openalex.org/W4293651473']","We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.",0.9915966386554622
SKG_DIA_3,https://openalex.org/W3025611493,2020,33,"['https://openalex.org/W1958706068', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2172140247', 'https://openalex.org/W2250539671', 'https://openalex.org/W2311783643', 'https://openalex.org/W2427497464', 'https://openalex.org/W2432717477', 'https://openalex.org/W2472819217', 'https://openalex.org/W2601450892', 'https://openalex.org/W2604763608', 'https://openalex.org/W2605133118', 'https://openalex.org/W2741363662', 'https://openalex.org/W2748513770', 'https://openalex.org/W2768195931', 'https://openalex.org/W2768228940', 'https://openalex.org/W2772564276', 'https://openalex.org/W2787911506', 'https://openalex.org/W2791666059', 'https://openalex.org/W2804107505', 'https://openalex.org/W2890719433', 'https://openalex.org/W2896457183', 'https://openalex.org/W2903334135', 'https://openalex.org/W2911327180', 'https://openalex.org/W2914752403', 'https://openalex.org/W2921156067', 'https://openalex.org/W2945367412', 'https://openalex.org/W2945978556', 'https://openalex.org/W2948727657', 'https://openalex.org/W2949529330', 'https://openalex.org/W2949568611', 'https://openalex.org/W2951615109', 'https://openalex.org/W2951980657', 'https://openalex.org/W2952402849', 'https://openalex.org/W2953111196', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963137684', 'https://openalex.org/W2963188990', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341924', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963613359', 'https://openalex.org/W2963643701', 'https://openalex.org/W2963775850', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963866663', 'https://openalex.org/W2963943197', 'https://openalex.org/W2964105864', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964316912', 'https://openalex.org/W2964352131', 'https://openalex.org/W2964588180', 'https://openalex.org/W2970971561', 'https://openalex.org/W3015189211', 'https://openalex.org/W3113529090', 'https://openalex.org/W3159575429', 'https://openalex.org/W4285719527']","Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems. Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task. However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks. In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting. In our approach, each dialogue model consists of a shared module, a gating module, and a private module. The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task. The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity.",1.0
SKG_DIA_4,https://openalex.org/W2963594470,2019,10,"['https://openalex.org/W648947103', 'https://openalex.org/W1522301498', 'https://openalex.org/W1550863320', 'https://openalex.org/W1924770834', 'https://openalex.org/W1972595521', 'https://openalex.org/W2004672974', 'https://openalex.org/W2024632416', 'https://openalex.org/W2094472029', 'https://openalex.org/W2124895976', 'https://openalex.org/W2132508435', 'https://openalex.org/W2166293310', 'https://openalex.org/W2473329891', 'https://openalex.org/W2473965551', 'https://openalex.org/W2575101493', 'https://openalex.org/W2736272491', 'https://openalex.org/W2741361549', 'https://openalex.org/W2786464815', 'https://openalex.org/W2803392141', 'https://openalex.org/W2848493808', 'https://openalex.org/W2888875316', 'https://openalex.org/W2953052971', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963644595', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964121744']","Dialogue contexts are proven helpful in the spoken language understanding (SLU) system and they are typically encoded with explicit memory representations. However, most of the previous models learn the context memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited. In this paper, we propose a new dialogue logistic inference (DLI) task to consolidate the context memory jointly with SLU in the multi-task framework. DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model. Our experimental results show that various popular contextual SLU models can benefit from our approach, and improvements are quite impressive, especially in slot filling.",1.0
SKG_DIA_6,https://openalex.org/W2966371977,2019,1,"['https://openalex.org/W14996564', 'https://openalex.org/W26963497', 'https://openalex.org/W1559297830', 'https://openalex.org/W1579838312', 'https://openalex.org/W2053154970', 'https://openalex.org/W2062946627', 'https://openalex.org/W2070246124', 'https://openalex.org/W2128970689', 'https://openalex.org/W2149706766', 'https://openalex.org/W2164777277', 'https://openalex.org/W2957342141', 'https://openalex.org/W4236137412']","The present study proposes an annotation scheme for classifying the content\nand discourse contribution of question-answer pairs. We propose detailed\nguidelines for using the scheme and apply them to dialogues in English,\nSpanish, and Dutch. Finally, we report on initial machine learning experiments\nfor automatic annotation.\n",0.9937106918238994
SKG_DIA_7,https://openalex.org/W3100963818,2020,5,"['https://openalex.org/W140747314', 'https://openalex.org/W1522301498', 'https://openalex.org/W1793121960', 'https://openalex.org/W1924770834', 'https://openalex.org/W1975244201', 'https://openalex.org/W2117448986', 'https://openalex.org/W2119276199', 'https://openalex.org/W2395389931', 'https://openalex.org/W2560678344', 'https://openalex.org/W2749436976', 'https://openalex.org/W2751448157', 'https://openalex.org/W2768409085', 'https://openalex.org/W2798914047', 'https://openalex.org/W2809324505', 'https://openalex.org/W2891704263', 'https://openalex.org/W2899771611', 'https://openalex.org/W2912624765', 'https://openalex.org/W2949252816', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962713807', 'https://openalex.org/W2962776342', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962886331', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964119254', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218', 'https://openalex.org/W2972571786', 'https://openalex.org/W4288601872', 'https://openalex.org/W4293350112', 'https://openalex.org/W4295249402', 'https://openalex.org/W4301674784']","Despite recent success in neural task-oriented dialogue systems, developing such a real-world system involves accessing large-scale knowledge bases (KBs), which cannot be simply encoded by neural approaches, such as memory network mechanisms. To alleviate the above problem, we propose , an end-to-end trainable text-to-SQL guided framework to learn a neural agent that interacts with KBs using the generated SQL queries. Specifically, the neural agent first learns to ask and confirm the customer's intent during the multi-turn interactions, then dynamically determining when to ground the user constraints into executable SQL queries so as to fetch relevant information from KBs. With the help of our method, the agent can use less but more accurate fetched results to generate useful responses efficiently, instead of incorporating the entire KBs. We evaluate the proposed method on the AirDialogue dataset, a large corpus released by Google, containing the conversations of customers booking flight tickets from the agent. The experimental results show that significantly improves over previous work in terms of accuracy and the BLEU score, which demonstrates not only the ability to achieve the given task but also the good quality of the generated dialogues.",0.9945945945945946
SKG_DIA_8,https://openalex.org/W2952267213,2019,84,"['https://openalex.org/W295828404', 'https://openalex.org/W836999996', 'https://openalex.org/W1532325895', 'https://openalex.org/W1539309091', 'https://openalex.org/W1577202350', 'https://openalex.org/W1591706642', 'https://openalex.org/W1654173042', 'https://openalex.org/W1677182931', 'https://openalex.org/W1948566616', 'https://openalex.org/W1958706068', 'https://openalex.org/W2102531443', 'https://openalex.org/W2115090890', 'https://openalex.org/W2147152072', 'https://openalex.org/W2149489931', 'https://openalex.org/W2152180407', 'https://openalex.org/W2155482025', 'https://openalex.org/W2160458012', 'https://openalex.org/W2183341477', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250473257', 'https://openalex.org/W2250595267', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251861449', 'https://openalex.org/W2252217313', 'https://openalex.org/W2288878529', 'https://openalex.org/W2306229986', 'https://openalex.org/W2318810549', 'https://openalex.org/W2339852062', 'https://openalex.org/W2373570000', 'https://openalex.org/W2413533759', 'https://openalex.org/W2419539795', 'https://openalex.org/W2468484304', 'https://openalex.org/W2534274346', 'https://openalex.org/W2581377246', 'https://openalex.org/W2593751037', 'https://openalex.org/W2604698497', 'https://openalex.org/W2611029872', 'https://openalex.org/W2611669587', 'https://openalex.org/W2620558438', 'https://openalex.org/W2622263826', 'https://openalex.org/W2624413595', 'https://openalex.org/W2766164908', 'https://openalex.org/W2770102447', 'https://openalex.org/W2786983967', 'https://openalex.org/W2794557536', 'https://openalex.org/W2798456655', 'https://openalex.org/W2799042347', 'https://openalex.org/W2806600904', 'https://openalex.org/W2884814595', 'https://openalex.org/W2885421725', 'https://openalex.org/W2886198413', 'https://openalex.org/W2891416139', 'https://openalex.org/W2891732163', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908331278', 'https://openalex.org/W2909544278', 'https://openalex.org/W2914120296', 'https://openalex.org/W2923890923', 'https://openalex.org/W2925618549', 'https://openalex.org/W2949252816', 'https://openalex.org/W2950444459', 'https://openalex.org/W2951807227', 'https://openalex.org/W2952357537', 'https://openalex.org/W2953246132', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962972936', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963149412', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963454111', 'https://openalex.org/W2963469388', 'https://openalex.org/W2963527209', 'https://openalex.org/W2963662719', 'https://openalex.org/W2963691849', 'https://openalex.org/W2963702144', 'https://openalex.org/W2963745931', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963854351', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964210218', 'https://openalex.org/W2972437240', 'https://openalex.org/W3121541553', 'https://openalex.org/W4213009331', 'https://openalex.org/W4252076394', 'https://openalex.org/W4289377895', 'https://openalex.org/W4290742115', 'https://openalex.org/W4294641903', 'https://openalex.org/W4295249402', 'https://openalex.org/W4297736277', 'https://openalex.org/W4297785815', 'https://openalex.org/W4297798436', 'https://openalex.org/W4300125564', 'https://openalex.org/W4300427681', 'https://openalex.org/W4320930577', 'https://openalex.org/W4385245566']","Matthew Henderson, Ivan Vulić, Daniela Gerz, Iñigo Casanueva, Paweł Budzianowski, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrkšić, Pei-Hao Su. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.",1.0
SKG_DIA_9,https://openalex.org/W2889636191,2018,40,"['https://openalex.org/W1518951372', 'https://openalex.org/W1521626219', 'https://openalex.org/W1591706642', 'https://openalex.org/W1714665356', 'https://openalex.org/W1773652845', 'https://openalex.org/W1969152782', 'https://openalex.org/W2077302143', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131876387', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251235149', 'https://openalex.org/W2604698497', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963963856']","The lack of labeled data is one of the main challenges when building a task-oriented dialogue system. Existing dialogue datasets usually rely on human labeling, which is expensive, limited in size, and in low coverage. In this paper, we instead propose our framework auto-dialabel to automatically cluster the dialogue intents and slots. In this framework, we collect a set of context features, leverage an autoencoder for feature assembly, and adapt a dynamic hierarchical clustering method for intent and slot labeling. Experimental results show that our framework can promote human labeling cost to a great extent, achieve good intent clustering accuracy (84.1%), and provide reasonable and instructive slot labeling results.",0.9921259842519684
SKG_DIA_10,https://openalex.org/W3104257895,2020,121,"['https://openalex.org/W34986029', 'https://openalex.org/W1469909552', 'https://openalex.org/W1626945812', 'https://openalex.org/W1654173042', 'https://openalex.org/W2053339128', 'https://openalex.org/W2064675550', 'https://openalex.org/W2106918957', 'https://openalex.org/W2108325777', 'https://openalex.org/W2250749132', 'https://openalex.org/W2251183320', 'https://openalex.org/W2327037637', 'https://openalex.org/W2517028602', 'https://openalex.org/W2606974598', 'https://openalex.org/W2612675303', 'https://openalex.org/W2768957049', 'https://openalex.org/W2772704197', 'https://openalex.org/W2796087471', 'https://openalex.org/W2798821806', 'https://openalex.org/W2807938752', 'https://openalex.org/W2889984458', 'https://openalex.org/W2905137389', 'https://openalex.org/W2912762447', 'https://openalex.org/W2949530332', 'https://openalex.org/W2951855948', 'https://openalex.org/W2952809536', 'https://openalex.org/W2952890017', 'https://openalex.org/W2962704246', 'https://openalex.org/W2962809918', 'https://openalex.org/W2962965405', 'https://openalex.org/W2962985882', 'https://openalex.org/W2963045354', 'https://openalex.org/W2963204221', 'https://openalex.org/W2963926728', 'https://openalex.org/W2963929190', 'https://openalex.org/W2964159778', 'https://openalex.org/W2964262738', 'https://openalex.org/W2970263339', 'https://openalex.org/W2970419734', 'https://openalex.org/W2970641574', 'https://openalex.org/W3008323921', 'https://openalex.org/W3014387963', 'https://openalex.org/W3014468788', 'https://openalex.org/W3015468748', 'https://openalex.org/W3034999214', 'https://openalex.org/W3041280310', 'https://openalex.org/W3100560913', 'https://openalex.org/W4241891521', 'https://openalex.org/W4288089799']","Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations—an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers—remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.",1.0
SKG_DIA_11,https://openalex.org/W3099382298,2020,11,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2507756961', 'https://openalex.org/W2581637843', 'https://openalex.org/W2769906679', 'https://openalex.org/W2789033601', 'https://openalex.org/W2798456655', 'https://openalex.org/W2806935606', 'https://openalex.org/W2807791032', 'https://openalex.org/W2891416139', 'https://openalex.org/W2907042160', 'https://openalex.org/W2908018635', 'https://openalex.org/W2911994530', 'https://openalex.org/W2913471463', 'https://openalex.org/W2951807227', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962896208', 'https://openalex.org/W2963206148', 'https://openalex.org/W2964121744', 'https://openalex.org/W2965453734', 'https://openalex.org/W2972732298', 'https://openalex.org/W2982104868', 'https://openalex.org/W2997793532', 'https://openalex.org/W2998083599']","As an important research topic, customer service dialogue generation tends to generate generic seller responses by leveraging current dialogue information. In this study, we propose a novel and extensible dialogue generation method by leveraging sellers’ historical dialogue information, which can be both accessible and informative. By utilizing innovative historical dialogue representation learning and historical dialogue selection mechanism, the proposed model is capable of detecting most related responses from sellers’ historical dialogues, which can further enhance the current dialogue generation quality. Unlike prior dialogue generation efforts, we treat each seller’s historical dialogues as a list of Customer-Seller utterance pairs and allow the model to measure their different importance, and copy words directly from most relevant pairs. Extensive experimental results show that the proposed approach can generate high-quality responses that cater to specific sellers’ characteristics and exhibit consistent superiority over baselines on a real-world multi-turn customer service dialogue dataset.",1.0
SKG_DIA_12,https://openalex.org/W3098595900,2020,8,"['https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2140679639', 'https://openalex.org/W2153579005', 'https://openalex.org/W2154652894', 'https://openalex.org/W2551396370', 'https://openalex.org/W2586847566', 'https://openalex.org/W2752047430', 'https://openalex.org/W2799176105', 'https://openalex.org/W2807873315', 'https://openalex.org/W2891103209', 'https://openalex.org/W2891826200', 'https://openalex.org/W2898875342', 'https://openalex.org/W2922791555', 'https://openalex.org/W2945052683', 'https://openalex.org/W2945525091', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951225599', 'https://openalex.org/W2951508633', 'https://openalex.org/W2952592807', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963520511', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963945575', 'https://openalex.org/W2964121744', 'https://openalex.org/W2966404868', 'https://openalex.org/W2972664115', 'https://openalex.org/W2986867746', 'https://openalex.org/W2995183464', 'https://openalex.org/W2997300509', 'https://openalex.org/W2998083599', 'https://openalex.org/W3006065545', 'https://openalex.org/W3022187094', 'https://openalex.org/W4294170691', 'https://openalex.org/W4385245566']","Unstructured documents serving as external knowledge of the dialogues help to generate more informative responses. Previous research focused on knowledge selection (KS) in the document with dialogue. However, dialogue history that is not related to the current dialogue may introduce noise in the KS processing. In this paper, we propose a Compare Aggregate Transformer (CAT) to jointly denoise the dialogue context and aggregate the document information for response generation. We designed two different comparison mechanisms to reduce noise (before and during decoding). In addition, we propose two metrics for evaluating document utilization efficiency based on word overlap. Experimental results on the CMU_DoG dataset show that the proposed CAT model outperforms the state-of-the-art approach and strong baselines.",1.0
SKG_DIA_13,https://openalex.org/W3087025499,2020,5,"['https://openalex.org/W1964625659', 'https://openalex.org/W2130942839', 'https://openalex.org/W2154652894', 'https://openalex.org/W2434099711', 'https://openalex.org/W2574535369', 'https://openalex.org/W2606974598', 'https://openalex.org/W2889984458', 'https://openalex.org/W2891022667', 'https://openalex.org/W2898083715', 'https://openalex.org/W2943249692', 'https://openalex.org/W2952890017', 'https://openalex.org/W2963929190', 'https://openalex.org/W2964165364', 'https://openalex.org/W2981456979', 'https://openalex.org/W3019006697', 'https://openalex.org/W3023363161']","Understanding a medical conversation between a patient and a physician poses a unique natural language understanding challenge since it combines elements of standard open ended conversation with very domain specific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice: capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient's medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The model also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80% of the conversations making it a realistic alternative to costly manual summarization by medical experts.",0.9942196531791908
SKG_DIA_14,https://openalex.org/W3092342765,2020,12,"['https://openalex.org/W1522301498', 'https://openalex.org/W1580788756', 'https://openalex.org/W2029031299', 'https://openalex.org/W2092817860', 'https://openalex.org/W2153975459', 'https://openalex.org/W2168490009', 'https://openalex.org/W2280612018', 'https://openalex.org/W2328886022', 'https://openalex.org/W2889963245', 'https://openalex.org/W2913443447', 'https://openalex.org/W2951000191', 'https://openalex.org/W2962729880', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964352131', 'https://openalex.org/W2971883198', 'https://openalex.org/W2990527403', 'https://openalex.org/W2990787438', 'https://openalex.org/W2992347006', 'https://openalex.org/W2996511240', 'https://openalex.org/W3000779003', 'https://openalex.org/W3023786569', 'https://openalex.org/W3034723486']","The lack of time-efficient and reliable evaluation methods hamper the development of conversational dialogue systems (chatbots). Evaluations requiring humans to converse with chatbots are time and cost-intensive, put high cognitive demands on the human judges, and yield low-quality results. In this work, we introduce \emph{Spot The Bot}, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chatbots regarding their ability to mimic the conversational behavior of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chatbot can uphold human-like behavior the longest, i.e., \emph{Survival Analysis}. This metric has the ability to correlate a bot's performance to certain of its characteristics (e.g., \ fluency or sensibleness), yielding interpretable results. The comparably low cost of our framework allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying \emph{Spot The Bot} to three domains, evaluating several state-of-the-art chatbots, and drawing comparisons to related work. The framework is released as a ready-to-use tool.",0.9949748743718592
SKG_DIA_15,https://openalex.org/W2963064439,2016,95,"['https://openalex.org/W218896052', 'https://openalex.org/W744662145', 'https://openalex.org/W1606347560', 'https://openalex.org/W1617082848', 'https://openalex.org/W1746819321', 'https://openalex.org/W1932421248', 'https://openalex.org/W1967071444', 'https://openalex.org/W1975244201', 'https://openalex.org/W1987326241', 'https://openalex.org/W1989996186', 'https://openalex.org/W1996957559', 'https://openalex.org/W2005708641', 'https://openalex.org/W2007221309', 'https://openalex.org/W2047335008', 'https://openalex.org/W2054716580', 'https://openalex.org/W2057244568', 'https://openalex.org/W2064675550', 'https://openalex.org/W2078429476', 'https://openalex.org/W2099201756', 'https://openalex.org/W2119015791', 'https://openalex.org/W2125031621', 'https://openalex.org/W2129297552', 'https://openalex.org/W2140539195', 'https://openalex.org/W2140656735', 'https://openalex.org/W2151814822', 'https://openalex.org/W2153971568', 'https://openalex.org/W2157331557', 'https://openalex.org/W2157826563', 'https://openalex.org/W2158139315', 'https://openalex.org/W2168490009', 'https://openalex.org/W2187089797', 'https://openalex.org/W2214131199', 'https://openalex.org/W2250558341', 'https://openalex.org/W2250679999', 'https://openalex.org/W2251221343', 'https://openalex.org/W2278865652', 'https://openalex.org/W2289601637', 'https://openalex.org/W2296712013', 'https://openalex.org/W2438667436', 'https://openalex.org/W2540392403', 'https://openalex.org/W2903158431', 'https://openalex.org/W2950133940', 'https://openalex.org/W2963252944']","The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.",1.0
SKG_DIA_16,https://openalex.org/W3034330559,2020,17,"['https://openalex.org/W1777239053', 'https://openalex.org/W1786470251', 'https://openalex.org/W1975244201', 'https://openalex.org/W2040123554', 'https://openalex.org/W2062175565', 'https://openalex.org/W2098441518', 'https://openalex.org/W2145339207', 'https://openalex.org/W2158969944', 'https://openalex.org/W2164419340', 'https://openalex.org/W2290053245', 'https://openalex.org/W2397581010', 'https://openalex.org/W2417401578', 'https://openalex.org/W2507592741', 'https://openalex.org/W2571927164', 'https://openalex.org/W2594726847', 'https://openalex.org/W2740191615', 'https://openalex.org/W2759104452', 'https://openalex.org/W2765111838', 'https://openalex.org/W2788862220', 'https://openalex.org/W2798494119', 'https://openalex.org/W2892043231', 'https://openalex.org/W2901893112', 'https://openalex.org/W2949252816', 'https://openalex.org/W2962730405', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963252944', 'https://openalex.org/W2963376229', 'https://openalex.org/W2963692154', 'https://openalex.org/W2964044380', 'https://openalex.org/W2970828515', 'https://openalex.org/W3016143584', 'https://openalex.org/W3104546989', 'https://openalex.org/W4312609624']","Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users. Human demonstrations can be used to accelerate learning progress. However, how to effectively leverage demonstrations to learn dialogue policy remains less explored. In this paper, we present Sˆ2Agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping. We use an imitation model to distill knowledge from demonstrations, based on which policy shaping estimates feedback on how the agent should act in policy space. Reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration. The effectiveness of the proposed Sˆ2Agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation.",1.0
SKG_DIA_17,https://openalex.org/W3103857453,2020,18,"['https://openalex.org/W295828404', 'https://openalex.org/W1518951372', 'https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W2006666561', 'https://openalex.org/W2064675550', 'https://openalex.org/W2094536313', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109814494', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141708418', 'https://openalex.org/W2250539671', 'https://openalex.org/W2296701362', 'https://openalex.org/W2328886022', 'https://openalex.org/W2339852062', 'https://openalex.org/W2396229782', 'https://openalex.org/W2521114121', 'https://openalex.org/W2578354947', 'https://openalex.org/W2581637843', 'https://openalex.org/W2761590056', 'https://openalex.org/W2786983967', 'https://openalex.org/W2799037524', 'https://openalex.org/W2807873315', 'https://openalex.org/W2890969459', 'https://openalex.org/W2892153332', 'https://openalex.org/W2900677074', 'https://openalex.org/W2903571680', 'https://openalex.org/W2946442465', 'https://openalex.org/W2951883832', 'https://openalex.org/W2952993422', 'https://openalex.org/W2962681511', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963559013', 'https://openalex.org/W2963879591', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964238590', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965718149', 'https://openalex.org/W2969379005', 'https://openalex.org/W2970453125', 'https://openalex.org/W2977235550', 'https://openalex.org/W2979702391', 'https://openalex.org/W2985026420', 'https://openalex.org/W2994934025', 'https://openalex.org/W3015812362', 'https://openalex.org/W3106806814', 'https://openalex.org/W4244151341', 'https://openalex.org/W4246855866', 'https://openalex.org/W4289288202', 'https://openalex.org/W4300125564', 'https://openalex.org/W4320013936']","Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken. The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch. Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space. An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.",1.0
SKG_DIA_18,https://openalex.org/W3035044096,2020,141,"['https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2127271556', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2155027007', 'https://openalex.org/W2156718681', 'https://openalex.org/W2688962481', 'https://openalex.org/W2796433937', 'https://openalex.org/W2807791032', 'https://openalex.org/W2890394457', 'https://openalex.org/W2891416139', 'https://openalex.org/W2900227126', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914099135', 'https://openalex.org/W2914204778', 'https://openalex.org/W2945978556', 'https://openalex.org/W2949211412', 'https://openalex.org/W2962852262', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963225934', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963688701', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W2965718149', 'https://openalex.org/W2970794693', 'https://openalex.org/W2970971581', 'https://openalex.org/W2971277071', 'https://openalex.org/W2980282514', 'https://openalex.org/W2983160116', 'https://openalex.org/W3022187094', 'https://openalex.org/W4288624561', 'https://openalex.org/W4295312788', 'https://openalex.org/W4300326073', 'https://openalex.org/W4385245566']","Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.",0.992248062015504
SKG_DIA_21,https://openalex.org/W2964133280,2017,44,"['https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W2125320996', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153653739', 'https://openalex.org/W2337601653', 'https://openalex.org/W2402930259', 'https://openalex.org/W2412715517', 'https://openalex.org/W2417401578', 'https://openalex.org/W2418993857', 'https://openalex.org/W2557436004', 'https://openalex.org/W2558661633', 'https://openalex.org/W2565274151', 'https://openalex.org/W2579486552', 'https://openalex.org/W2604688337', 'https://openalex.org/W2605968924', 'https://openalex.org/W2949252816', 'https://openalex.org/W2949555952', 'https://openalex.org/W2950314731', 'https://openalex.org/W2962707484', 'https://openalex.org/W2962776342', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962886331', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963319085', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963958388', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964352131', 'https://openalex.org/W4241645538', 'https://openalex.org/W4293775587']","We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles.",1.0
SKG_DIA_22,https://openalex.org/W3034930293,2020,18,"['https://openalex.org/W592244745', 'https://openalex.org/W769612788', 'https://openalex.org/W1486649854', 'https://openalex.org/W1786470251', 'https://openalex.org/W1975244201', 'https://openalex.org/W1987326241', 'https://openalex.org/W2108501770', 'https://openalex.org/W2514480375', 'https://openalex.org/W2736601468', 'https://openalex.org/W2746626573', 'https://openalex.org/W2756946152', 'https://openalex.org/W2798494119', 'https://openalex.org/W2806936550', 'https://openalex.org/W2810840719', 'https://openalex.org/W2903396356', 'https://openalex.org/W2915295540', 'https://openalex.org/W2921218568', 'https://openalex.org/W2947212824', 'https://openalex.org/W2949252816', 'https://openalex.org/W2949476504', 'https://openalex.org/W2951021014', 'https://openalex.org/W2951519300', 'https://openalex.org/W2962912551', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963508354', 'https://openalex.org/W2963692154', 'https://openalex.org/W2963712524', 'https://openalex.org/W2963858765', 'https://openalex.org/W2963905903', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964040467', 'https://openalex.org/W2964113870', 'https://openalex.org/W2964158321', 'https://openalex.org/W2970415880', 'https://openalex.org/W2970828515', 'https://openalex.org/W2973721503', 'https://openalex.org/W2976051099', 'https://openalex.org/W2977363161', 'https://openalex.org/W2979805229', 'https://openalex.org/W2998201756', 'https://openalex.org/W3103801215', 'https://openalex.org/W4288322145', 'https://openalex.org/W4288614963', 'https://openalex.org/W4306716473', 'https://openalex.org/W4385245566']","Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues. To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards. This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive. To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning. The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations. The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations. We further propose to learn action embeddings for a better generalization of the reward function. The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.",1.0
SKG_DIA_23,https://openalex.org/W2926587947,2019,45,"['https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W2243397390', 'https://openalex.org/W2251058040', 'https://openalex.org/W2327501763', 'https://openalex.org/W2562979205', 'https://openalex.org/W2603266952', 'https://openalex.org/W2603766943', 'https://openalex.org/W2609368435', 'https://openalex.org/W2735135478', 'https://openalex.org/W2766108848', 'https://openalex.org/W2799194071', 'https://openalex.org/W2891519874', 'https://openalex.org/W2891704263', 'https://openalex.org/W2895033754', 'https://openalex.org/W2962710014', 'https://openalex.org/W2962818281', 'https://openalex.org/W2962852262', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963207607', 'https://openalex.org/W2963217826', 'https://openalex.org/W2963496101', 'https://openalex.org/W2963834268', 'https://openalex.org/W2963857521', 'https://openalex.org/W2963881016', 'https://openalex.org/W2963969878', 'https://openalex.org/W2964153729', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964253222', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964346747', 'https://openalex.org/W2979450790', 'https://openalex.org/W2998277219', 'https://openalex.org/W3013371788', 'https://openalex.org/W3093419064']","Recent research has demonstrated that goal-oriented dialogue agents trained on large datasets can achieve striking performance when interacting with human users. In real world applications, however, it is important to ensure that the agent performs smoothly interacting with not only regular users but also those malicious ones who would attack the system through interactions in order to achieve goals for their own advantage. In this paper, we develop algorithms to evaluate the robustness of a dialogue agent by carefully designed attacks using adversarial agents. Those attacks are performed in both black-box and white-box settings. Furthermore, we demonstrate that adversarial training using our attacks can significantly improve the robustness of a goal-oriented dialogue system. On a case-study of the negotiation agent developed by (Lewis et al., 2017), our attacks reduced the average advantage of rewards between the attacker and the trained RL-based agent from 2.68 to −5.76 on a scale from −10 to 10 for randomized goals. Moreover, with the proposed adversarial training, we are able to improve the robustness of negotiation agents by 1.5 points on average against all our attacks. © 2019 Association for Computational Linguistics",0.9947643979057592
SKG_DIA_24,https://openalex.org/W2739936944,2017,145,"['https://openalex.org/W182831726', 'https://openalex.org/W200223693', 'https://openalex.org/W1492935830', 'https://openalex.org/W1503552041', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W2047335008', 'https://openalex.org/W2119015791', 'https://openalex.org/W2120375264', 'https://openalex.org/W2127838323', 'https://openalex.org/W2159875193', 'https://openalex.org/W2168490009', 'https://openalex.org/W2251058040', 'https://openalex.org/W2288878529', 'https://openalex.org/W2291723583', 'https://openalex.org/W2476025067', 'https://openalex.org/W2508347479', 'https://openalex.org/W2564070522', 'https://openalex.org/W2792382948', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964269028']","Stefan Ultes, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, Dongho Kim, Iñigo Casanueva, Paweł Budzianowski, Nikola Mrkšić, Tsung-Hsien Wen, Milica Gašić, Steve Young. Proceedings of ACL 2017, System Demonstrations. 2017.",0.991304347826087
SKG_DIA_25,https://openalex.org/W2741675028,2017,32,"['https://openalex.org/W77001256', 'https://openalex.org/W1482132414', 'https://openalex.org/W1526096287', 'https://openalex.org/W1612268991', 'https://openalex.org/W1810943226', 'https://openalex.org/W1902237438', 'https://openalex.org/W1911976934', 'https://openalex.org/W1934019294', 'https://openalex.org/W2003458432', 'https://openalex.org/W2006969979', 'https://openalex.org/W2024213584', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2106226466', 'https://openalex.org/W2128970689', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2142416747', 'https://openalex.org/W2146502635', 'https://openalex.org/W2176263492', 'https://openalex.org/W2193413348', 'https://openalex.org/W2312383716', 'https://openalex.org/W2401527985', 'https://openalex.org/W2949640717', 'https://openalex.org/W2964139507', 'https://openalex.org/W2964308564']","We propose a novel hierarchical Recurrent Neural Network (RNN) for learning sequences of Dialogue Acts (DAs). The input in this task is a sequence of utterances (i.e., conversational contributions) comprising a sequence of tokens, and the output is a sequence of DA labels (one label per utterance). Our model leverages the hierarchical nature of dialogue data by using two nested RNNs that capture long-range dependencies at the dialogue level and the utterance level. This model is combined with an attention mechanism that focuses on salient tokens in utterances. Our experimental results show that our model outperforms strong baselines on two popular datasets, Switchboard and MapTask; and our detailed empirical analysis highlights the impact of each aspect of our model.",1.0
SKG_DIA_27,https://openalex.org/W2962989446,2019,247,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1840435438', 'https://openalex.org/W2087451659', 'https://openalex.org/W2130158090', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250790822', 'https://openalex.org/W2608787653', 'https://openalex.org/W2788496822', 'https://openalex.org/W2808633496', 'https://openalex.org/W2890394457', 'https://openalex.org/W2891308403', 'https://openalex.org/W2898658996', 'https://openalex.org/W2923014074', 'https://openalex.org/W2950819771', 'https://openalex.org/W2962736243', 'https://openalex.org/W2962765866', 'https://openalex.org/W2962843521', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963706350', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963918774', 'https://openalex.org/W2963977107', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964321317', 'https://openalex.org/W2964352131']","Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model’s consistency.",1.0
SKG_DIA_29,https://openalex.org/W3106007100,2020,44,"['https://openalex.org/W1522301498', 'https://openalex.org/W1967679121', 'https://openalex.org/W1993979041', 'https://openalex.org/W2107473960', 'https://openalex.org/W2154652894', 'https://openalex.org/W2159056499', 'https://openalex.org/W2165155944', 'https://openalex.org/W2313372168', 'https://openalex.org/W2506483933', 'https://openalex.org/W2574790321', 'https://openalex.org/W2583010282', 'https://openalex.org/W2784400615', 'https://openalex.org/W2805984364', 'https://openalex.org/W2896457183', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914204778', 'https://openalex.org/W2915330159', 'https://openalex.org/W2916772188', 'https://openalex.org/W2921065408', 'https://openalex.org/W2929900303', 'https://openalex.org/W2945978556', 'https://openalex.org/W2951064641', 'https://openalex.org/W2951583236', 'https://openalex.org/W2953044442', 'https://openalex.org/W2962846267', 'https://openalex.org/W2962974452', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963109634', 'https://openalex.org/W2963318456', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963640662', 'https://openalex.org/W2963726321', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964183327', 'https://openalex.org/W2964352131', 'https://openalex.org/W2979826702', 'https://openalex.org/W2983160116', 'https://openalex.org/W2995969307', 'https://openalex.org/W3034720580', 'https://openalex.org/W3035044096', 'https://openalex.org/W3035068109', 'https://openalex.org/W3155584966', 'https://openalex.org/W4288088456', 'https://openalex.org/W4288624561']","We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.",0.9950248756218906
SKG_DIA_30,https://openalex.org/W2970260827,2019,57,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1793121960', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2107598941', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2250265269', 'https://openalex.org/W2251135946', 'https://openalex.org/W2251960799', 'https://openalex.org/W2280798142', 'https://openalex.org/W2534274346', 'https://openalex.org/W2547875792', 'https://openalex.org/W2749436976', 'https://openalex.org/W2808093377', 'https://openalex.org/W2949141958', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963929190', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964308564', 'https://openalex.org/W3106274079', 'https://openalex.org/W4295249402', 'https://openalex.org/W4297728544']","Libo Qin, Yijia Liu, Wanxiang Che, Haoyang Wen, Yangming Li, Ting Liu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_31,https://openalex.org/W2607380417,2017,36,"['https://openalex.org/W64203724', 'https://openalex.org/W1508859482', 'https://openalex.org/W1757796397', 'https://openalex.org/W2037892284', 'https://openalex.org/W2166100419', 'https://openalex.org/W2175723363', 'https://openalex.org/W2231198303', 'https://openalex.org/W2245560598', 'https://openalex.org/W2251854674', 'https://openalex.org/W2257979135', 'https://openalex.org/W2471576823', 'https://openalex.org/W2614087390', 'https://openalex.org/W2963993719', 'https://openalex.org/W4205363819', 'https://openalex.org/W4298857966']","In this paper we present a comparative evaluation of various negotiation strategies within an online version of the game “Settlers of Catan”. The comparison is based on human subjects playing games against artificial game-playingagents (‘bots’) which implement different negotiation dialogue strategies, using a chat dialogue interface to negotiate trades. Our results suggest that a negotiation strategy that uses persuasion, as well as a strategy that is trained from data using Deep Reinforcement Learning, both lead to an improved win rate against humans, compared to previous rule-based and supervised learning baseline dialogue negotiators.",1.0
SKG_DIA_32,https://openalex.org/W2807186566,2018,14,"['https://openalex.org/W6908809', 'https://openalex.org/W1570452348', 'https://openalex.org/W1614298861', 'https://openalex.org/W1665214252', 'https://openalex.org/W1832693441', 'https://openalex.org/W2053994556', 'https://openalex.org/W2081580037', 'https://openalex.org/W2084413241', 'https://openalex.org/W2091494174', 'https://openalex.org/W2131726681', 'https://openalex.org/W2131744502', 'https://openalex.org/W2152814412', 'https://openalex.org/W2158899491', 'https://openalex.org/W2167662839', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251048167', 'https://openalex.org/W2293665254', 'https://openalex.org/W2474466460', 'https://openalex.org/W2508866736', 'https://openalex.org/W2560203405', 'https://openalex.org/W2583010282', 'https://openalex.org/W2741049976', 'https://openalex.org/W2756554273', 'https://openalex.org/W2902541777', 'https://openalex.org/W2952230511', 'https://openalex.org/W2953044442', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963341924', 'https://openalex.org/W4386506836']","When interpreting questions in a virtual patient dialogue system one must inevitably tackle the challenge of a long tail of relatively infrequently asked questions. To make progress on this challenge, we investigate the use of paraphrasing for data augmentation and neural memory-based classification, finding that the two methods work best in combination. In particular, we find that the neural memory-based approach not only outperforms a straight CNN classifier on low frequency questions, but also takes better advantage of the augmented data created by paraphrasing, together yielding a nearly 10% absolute improvement in accuracy on the least frequently asked questions.",1.0
SKG_DIA_34,https://openalex.org/W2798367796,2018,233,"['https://openalex.org/W1497675750', 'https://openalex.org/W1522301498', 'https://openalex.org/W1785674045', 'https://openalex.org/W1814992895', 'https://openalex.org/W1902237438', 'https://openalex.org/W1989996186', 'https://openalex.org/W2030290736', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2119015791', 'https://openalex.org/W2119595900', 'https://openalex.org/W2133013156', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148522164', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251235149', 'https://openalex.org/W2251355666', 'https://openalex.org/W2267186426', 'https://openalex.org/W2438667436', 'https://openalex.org/W2463565445', 'https://openalex.org/W2550821151', 'https://openalex.org/W2551396370', 'https://openalex.org/W2552027021', 'https://openalex.org/W2556468274', 'https://openalex.org/W2606974598', 'https://openalex.org/W2612675303', 'https://openalex.org/W2626792426', 'https://openalex.org/W2740765036', 'https://openalex.org/W2914746235', 'https://openalex.org/W2962847367', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963768805', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963871484', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964222246', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566']","Dialogue state tracking, which estimates user goals and requests given the dialogue context, is an essential part of task-oriented dialogue systems. In this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. Our model uses global modules to shares parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. We show that this significantly improves tracking of rare states. GLAD obtains 88.3% joint goal accuracy and 96.4% request accuracy on the WoZ state tracking task, outperforming prior work by 3.9% and 4.8%. On the DSTC2 task, our model obtains 74.7% joint goal accuracy and 97.3% request accuracy, outperforming prior work by 1.3% and 0.8%",1.0
SKG_DIA_35,https://openalex.org/W3098314733,2020,3,"['https://openalex.org/W1743832209', 'https://openalex.org/W1902237438', 'https://openalex.org/W1985660488', 'https://openalex.org/W2039585592', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2328886022', 'https://openalex.org/W2507756961', 'https://openalex.org/W2521114121', 'https://openalex.org/W2604763608', 'https://openalex.org/W2799037524', 'https://openalex.org/W2804552794', 'https://openalex.org/W2888541716', 'https://openalex.org/W2945978556', 'https://openalex.org/W2946757877', 'https://openalex.org/W2949747155', 'https://openalex.org/W2951980657', 'https://openalex.org/W2952420867', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962788902', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964588180', 'https://openalex.org/W4297813345']","Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning (SML) approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data. Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions. Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions. Code will be public to facilitate the research along this line.",1.0
SKG_DIA_36,https://openalex.org/W2971236040,2019,76,"['https://openalex.org/W1518951372', 'https://openalex.org/W1525961042', 'https://openalex.org/W1591706642', 'https://openalex.org/W1756422141', 'https://openalex.org/W1924770834', 'https://openalex.org/W1958706068', 'https://openalex.org/W2053154970', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101848544', 'https://openalex.org/W2127795553', 'https://openalex.org/W2130942839', 'https://openalex.org/W2164777277', 'https://openalex.org/W2250635077', 'https://openalex.org/W2328886022', 'https://openalex.org/W2521114121', 'https://openalex.org/W2586847566', 'https://openalex.org/W2739716023', 'https://openalex.org/W2740107682', 'https://openalex.org/W2754194354', 'https://openalex.org/W2769099080', 'https://openalex.org/W2775082024', 'https://openalex.org/W2785708181', 'https://openalex.org/W2799037524', 'https://openalex.org/W2807873315', 'https://openalex.org/W2890961898', 'https://openalex.org/W2897513992', 'https://openalex.org/W2950457956', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951008357', 'https://openalex.org/W2953290652', 'https://openalex.org/W2962796276', 'https://openalex.org/W2962881743', 'https://openalex.org/W2962886429', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963371447', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963494889', 'https://openalex.org/W2963544700', 'https://openalex.org/W2963546833', 'https://openalex.org/W2963653601', 'https://openalex.org/W2963858333', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964072618', 'https://openalex.org/W2964152081', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964199361', 'https://openalex.org/W4285719527', 'https://openalex.org/W4297733535', 'https://openalex.org/W4298422451']","Yi-Lin Tuan, Yun-Nung Chen, Hung-yi Lee. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",0.9937106918238994
SKG_DIA_39,https://openalex.org/W3035282664,2020,57,"['https://openalex.org/W1522301498', 'https://openalex.org/W1666146316', 'https://openalex.org/W1958706068', 'https://openalex.org/W1959608418', 'https://openalex.org/W2111362445', 'https://openalex.org/W2133564696', 'https://openalex.org/W2419539795', 'https://openalex.org/W2516907570', 'https://openalex.org/W2521114121', 'https://openalex.org/W2547875792', 'https://openalex.org/W2594978815', 'https://openalex.org/W2595715041', 'https://openalex.org/W2752172973', 'https://openalex.org/W2756978580', 'https://openalex.org/W2761590056', 'https://openalex.org/W2795282075', 'https://openalex.org/W2799176105', 'https://openalex.org/W2808293489', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896457183', 'https://openalex.org/W2904444765', 'https://openalex.org/W2905266130', 'https://openalex.org/W2947375732', 'https://openalex.org/W2950142196', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962721878', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963330684', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963371670', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963443335', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963564796', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963958388', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964137876', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965282611', 'https://openalex.org/W2965718149', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970418174', 'https://openalex.org/W2971252690', 'https://openalex.org/W2982225063', 'https://openalex.org/W2997657234', 'https://openalex.org/W3022187094', 'https://openalex.org/W3035331128', 'https://openalex.org/W4288102237', 'https://openalex.org/W4295253143', 'https://openalex.org/W4385245566']","Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear. This impedes the learning of those data-driven neural dialogue models. Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples. In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously. In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data. Note that, the proposed data manipulation framework is fully data-driven and learnable. It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples. Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments.",0.9958847736625516
SKG_DIA_40,https://openalex.org/W2252199413,2015,3,"['https://openalex.org/W86887328', 'https://openalex.org/W1533642089', 'https://openalex.org/W1548663377', 'https://openalex.org/W2047221353', 'https://openalex.org/W2085337304', 'https://openalex.org/W2100341149', 'https://openalex.org/W2104583100', 'https://openalex.org/W2119821739', 'https://openalex.org/W2123142779', 'https://openalex.org/W2131357087', 'https://openalex.org/W2131753116', 'https://openalex.org/W2139694477', 'https://openalex.org/W2151048449', 'https://openalex.org/W2162638401', 'https://openalex.org/W2181629536', 'https://openalex.org/W2250741050', 'https://openalex.org/W2250869925', 'https://openalex.org/W2949695381', 'https://openalex.org/W4239510810', 'https://openalex.org/W4241676240']","While most previous work onWikification has focused on written texts, this paper presents a Wikification approach for spo-ken dialogues. A set of analyzers are pro-posed to learn dialogue-specific properties along with domain knowledge of conver-sations from Wikipedia. Then, the an-alyzed properties are used as constraints for generating candidates, and the candi-dates are ranked to find the appropriate links. The experimental results show that our proposed approach can significantly improve the performances of the task in human-human dialogues. 1",1.0
SKG_DIA_41,https://openalex.org/W3098379431,2020,32,"['https://openalex.org/W10957333', 'https://openalex.org/W295828404', 'https://openalex.org/W2102531443', 'https://openalex.org/W2108862644', 'https://openalex.org/W2133564696', 'https://openalex.org/W2155482025', 'https://openalex.org/W2170738476', 'https://openalex.org/W2173361515', 'https://openalex.org/W2197546379', 'https://openalex.org/W2338325072', 'https://openalex.org/W2339852062', 'https://openalex.org/W2395531022', 'https://openalex.org/W2561368124', 'https://openalex.org/W2798456655', 'https://openalex.org/W2805077688', 'https://openalex.org/W2891416139', 'https://openalex.org/W2949446780', 'https://openalex.org/W2951287343', 'https://openalex.org/W2951359136', 'https://openalex.org/W2952285288', 'https://openalex.org/W2952813980', 'https://openalex.org/W2962768358', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963521540', 'https://openalex.org/W2963522640', 'https://openalex.org/W2963542836', 'https://openalex.org/W2963735582', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964150246', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964309167', 'https://openalex.org/W2970648534', 'https://openalex.org/W2971190479', 'https://openalex.org/W2985258882', 'https://openalex.org/W4252076394', 'https://openalex.org/W4300125564', 'https://openalex.org/W4300687842']","Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.",0.9945945945945946
SKG_DIA_42,https://openalex.org/W3034950505,2020,41,"['https://openalex.org/W2040975718', 'https://openalex.org/W2761590056', 'https://openalex.org/W2786472963', 'https://openalex.org/W2896457183', 'https://openalex.org/W2904443424', 'https://openalex.org/W2938704169', 'https://openalex.org/W2951000191', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963466651', 'https://openalex.org/W2963527228', 'https://openalex.org/W2963640662', 'https://openalex.org/W2963903950', 'https://openalex.org/W2965373594', 'https://openalex.org/W2966292672', 'https://openalex.org/W2979702391', 'https://openalex.org/W2996428491', 'https://openalex.org/W3011411500', 'https://openalex.org/W3022187094', 'https://openalex.org/W4210764005', 'https://openalex.org/W4288113479', 'https://openalex.org/W4288243162']","Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. However, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. In this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. Experimental results demonstrate that the proposed evaluator achieves a strong correlation (> 0.6) with human judgement and generalizes robustly to diverse responses and corpora. We open-source the code and data in https://github.com/ZHAOTING/dialog-processing.",1.0
SKG_DIA_44,https://openalex.org/W2952100657,2019,16,"['https://openalex.org/W1522301498', 'https://openalex.org/W1526096287', 'https://openalex.org/W1591706642', 'https://openalex.org/W2149484506', 'https://openalex.org/W2157331557', 'https://openalex.org/W2509387270', 'https://openalex.org/W2573626026', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963866450', 'https://openalex.org/W2964106094', 'https://openalex.org/W2964121744', 'https://openalex.org/W4302333880']","Sequence-to-sequence models are a common approach to develop a chatbot. They can train a conversational model in an end-to-end manner. One significant drawback of such a neural network based approach is that the response generation process is a black-box, and how a specific response is generated is unclear. To tackle this problem, an interpretable response generation mechanism is desired. As a step toward this direction, we focus on dialogue-acts (DAs) that may provide insight to understand the response generation process. In particular, we propose a method to predict a DA of the next response based on the history of previous utterances and their DAs. Experiments using a Switch Board Dialogue Act corpus show that compared to the baseline considering only a single utterance, our model achieves 10.8% higher F1-score and 3.0% higher accuracy on DA prediction.",1.0
SKG_DIA_45,https://openalex.org/W3035633461,2020,58,"['https://openalex.org/W1602011256', 'https://openalex.org/W1785674045', 'https://openalex.org/W1975244201', 'https://openalex.org/W2064675550', 'https://openalex.org/W2119015791', 'https://openalex.org/W2250297846', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251235149', 'https://openalex.org/W2251355666', 'https://openalex.org/W2438667436', 'https://openalex.org/W2470673105', 'https://openalex.org/W2798367796', 'https://openalex.org/W2798914047', 'https://openalex.org/W2806482527', 'https://openalex.org/W2808310571', 'https://openalex.org/W2810840719', 'https://openalex.org/W2934890006', 'https://openalex.org/W2945475330', 'https://openalex.org/W2951855948', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963351448', 'https://openalex.org/W2963360026', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963788376', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970404807', 'https://openalex.org/W2972777589', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W2981507717', 'https://openalex.org/W2988252747', 'https://openalex.org/W2996317432', 'https://openalex.org/W3034573951', 'https://openalex.org/W3098057198', 'https://openalex.org/W3119649668', 'https://openalex.org/W4289147179']","Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs. However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history. Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance. In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations. We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training. Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements (+1.24% and +5.98%).",1.0
SKG_DIA_46,https://openalex.org/W2740135329,2017,9,"['https://openalex.org/W129305155', 'https://openalex.org/W1518863056', 'https://openalex.org/W1580807540', 'https://openalex.org/W1588163064', 'https://openalex.org/W1619488417', 'https://openalex.org/W1631260214', 'https://openalex.org/W1741471588', 'https://openalex.org/W1974166643', 'https://openalex.org/W1975994995', 'https://openalex.org/W1995875735', 'https://openalex.org/W1998021300', 'https://openalex.org/W2013056791', 'https://openalex.org/W2035093168', 'https://openalex.org/W2056684507', 'https://openalex.org/W2057563799', 'https://openalex.org/W2058819080', 'https://openalex.org/W2060806362', 'https://openalex.org/W2093230975', 'https://openalex.org/W2097580026', 'https://openalex.org/W2100008374', 'https://openalex.org/W2100750861', 'https://openalex.org/W2101348300', 'https://openalex.org/W2102573550', 'https://openalex.org/W2105672294', 'https://openalex.org/W2107974377', 'https://openalex.org/W2118142207', 'https://openalex.org/W2121706863', 'https://openalex.org/W2123136325', 'https://openalex.org/W2129955048', 'https://openalex.org/W2148545659', 'https://openalex.org/W2159398820', 'https://openalex.org/W2166637769', 'https://openalex.org/W2182998842', 'https://openalex.org/W2292560873', 'https://openalex.org/W2316023651', 'https://openalex.org/W2510737926', 'https://openalex.org/W2518257277', 'https://openalex.org/W2579087420', 'https://openalex.org/W2993383518', 'https://openalex.org/W3122580054', 'https://openalex.org/W3123030186', 'https://openalex.org/W4230644069', 'https://openalex.org/W4247066346', 'https://openalex.org/W6605277767', 'https://openalex.org/W6636811518', 'https://openalex.org/W6637689594', 'https://openalex.org/W6643794096', 'https://openalex.org/W6648982606', 'https://openalex.org/W6649975542', 'https://openalex.org/W6659041405', 'https://openalex.org/W6673748633', 'https://openalex.org/W6674676611', 'https://openalex.org/W6675392920', 'https://openalex.org/W6675861551', 'https://openalex.org/W6677824845', 'https://openalex.org/W6682043691', 'https://openalex.org/W6683469792', 'https://openalex.org/W6686261312', 'https://openalex.org/W6725099393', 'https://openalex.org/W6732333006', 'https://openalex.org/W6845709182', 'https://openalex.org/W7073626071']","We propose a perspective on dialogue that focuses on relative information contributions of conversation partners as a key to successful communication. We predict the success of collaborative task in English and Danish corpora of task-oriented dialogue. Two features are extracted from the frequency domain representations of the lexical entropy series of each interlocutor, power spectrum overlap (PSO) and relative phase (RP). We find that PSO is a negative predictor of task success, while RP is a positive one. An SVM with these features significantly improved on previous task success prediction models. Our findings suggest that the strategic distribution of information density between interlocutors is relevant to task success.",1.0
SKG_DIA_47,https://openalex.org/W3100584004,2020,6,"['https://openalex.org/W251558267', 'https://openalex.org/W1532209448', 'https://openalex.org/W1554274370', 'https://openalex.org/W1591706642', 'https://openalex.org/W1804881260', 'https://openalex.org/W1972711991', 'https://openalex.org/W1976780908', 'https://openalex.org/W1993598708', 'https://openalex.org/W2013489815', 'https://openalex.org/W2080550848', 'https://openalex.org/W2089652186', 'https://openalex.org/W2114030807', 'https://openalex.org/W2133949313', 'https://openalex.org/W2145588856', 'https://openalex.org/W2146277089', 'https://openalex.org/W2160176417', 'https://openalex.org/W2168708086', 'https://openalex.org/W2472161733', 'https://openalex.org/W2524481654', 'https://openalex.org/W2746009407', 'https://openalex.org/W2889134004', 'https://openalex.org/W2889418961', 'https://openalex.org/W2928484296', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963217826', 'https://openalex.org/W2963945964', 'https://openalex.org/W2963952470', 'https://openalex.org/W2970178946', 'https://openalex.org/W2990213592', 'https://openalex.org/W2996699929', 'https://openalex.org/W3032020872', 'https://openalex.org/W3034284720', 'https://openalex.org/W3034319502', 'https://openalex.org/W3106549878', 'https://openalex.org/W3196847085']","Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings. To this end, we propose a Hindi-English human-machine dialogue system that elicits code-switching conversations in a controlled setting. It uses different code-switching agent strategies to understand how users respond and accommodate to the agent's language choice. Through this system, we collect and release a new dataset CommonDost, comprising of 439 human-machine multilingual conversations. We adapt pre-defined metrics to discover linguistic accommodation from users to agents. Finally, we compare these dialogues with Spanish-English dialogues collected in a similar setting, and analyze the impact of linguistic and socio-cultural factors on code-switching patterns across the two language pairs.",1.0
SKG_DIA_49,https://openalex.org/W3022592851,2020,9,"['https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133459682', 'https://openalex.org/W2152790380', 'https://openalex.org/W2294370754', 'https://openalex.org/W2328886022', 'https://openalex.org/W2584220694', 'https://openalex.org/W2604698497', 'https://openalex.org/W2607892599', 'https://openalex.org/W2741333084', 'https://openalex.org/W2885421725', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898658996', 'https://openalex.org/W2949611393', 'https://openalex.org/W2949918260', 'https://openalex.org/W2963527228', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963918774', 'https://openalex.org/W2971883198', 'https://openalex.org/W2978017171']","Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.",1.0
SKG_DIA_50,https://openalex.org/W2945614092,2019,6,"['https://openalex.org/W1520352740', 'https://openalex.org/W1522301498', 'https://openalex.org/W1548663377', 'https://openalex.org/W1566256432', 'https://openalex.org/W1677182931', 'https://openalex.org/W1732222442', 'https://openalex.org/W1789782362', 'https://openalex.org/W1793121960', 'https://openalex.org/W2064675550', 'https://openalex.org/W2085574295', 'https://openalex.org/W2101268022', 'https://openalex.org/W2131357087', 'https://openalex.org/W2141599568', 'https://openalex.org/W2153579005', 'https://openalex.org/W2160654481', 'https://openalex.org/W2292919134', 'https://openalex.org/W2549835527', 'https://openalex.org/W2563734883', 'https://openalex.org/W2631715525', 'https://openalex.org/W2739484150', 'https://openalex.org/W2773354821', 'https://openalex.org/W2799124508', 'https://openalex.org/W2803267010', 'https://openalex.org/W2805807693', 'https://openalex.org/W2807377748', 'https://openalex.org/W2889107415', 'https://openalex.org/W2891698435', 'https://openalex.org/W2950527759', 'https://openalex.org/W2951008357', 'https://openalex.org/W2951976932', 'https://openalex.org/W2962769558', 'https://openalex.org/W2963184844', 'https://openalex.org/W2963290255', 'https://openalex.org/W2963463993', 'https://openalex.org/W2963695529', 'https://openalex.org/W2963751529', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964204621', 'https://openalex.org/W2964222268', 'https://openalex.org/W2989631226', 'https://openalex.org/W3029541414', 'https://openalex.org/W4230980737', 'https://openalex.org/W4294170691', 'https://openalex.org/W4303633609']","Humans use language to refer to entities in the external world. Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed.",0.9943502824858758
SKG_DIA_51,https://openalex.org/W2970866659,2019,8,"['https://openalex.org/W1504739411', 'https://openalex.org/W1522301498', 'https://openalex.org/W1530049455', 'https://openalex.org/W1998070348', 'https://openalex.org/W2021151961', 'https://openalex.org/W2047335008', 'https://openalex.org/W2122493499', 'https://openalex.org/W2129405869', 'https://openalex.org/W2130942839', 'https://openalex.org/W2150775217', 'https://openalex.org/W2157331557', 'https://openalex.org/W2251062710', 'https://openalex.org/W2412715517', 'https://openalex.org/W2594726847', 'https://openalex.org/W2798914047', 'https://openalex.org/W2805406810', 'https://openalex.org/W2884814595', 'https://openalex.org/W2890832667', 'https://openalex.org/W2899908862', 'https://openalex.org/W2952164680', 'https://openalex.org/W2952798561', 'https://openalex.org/W2963134326', 'https://openalex.org/W2964044380', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964308564', 'https://openalex.org/W3099293669']","Lei Shu, Hu Xu, Bing Liu, Piero Molino. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_53,https://openalex.org/W3101223450,2020,152,"['https://openalex.org/W565398442', 'https://openalex.org/W1522301498', 'https://openalex.org/W1580898361', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133512280', 'https://openalex.org/W2194775991', 'https://openalex.org/W2328886022', 'https://openalex.org/W2886305736', 'https://openalex.org/W2890969459', 'https://openalex.org/W2896457183', 'https://openalex.org/W2903928064', 'https://openalex.org/W2951883832', 'https://openalex.org/W2963096510', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964121744', 'https://openalex.org/W2988937804', 'https://openalex.org/W2992922049', 'https://openalex.org/W2997281057', 'https://openalex.org/W2998186887', 'https://openalex.org/W3025853514', 'https://openalex.org/W3025997466', 'https://openalex.org/W3034999214', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang, Penghui Zhu, Shu Chen, Pengtao Xie. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.",0.9894736842105264
SKG_DIA_54,https://openalex.org/W2804780446,2018,53,"['https://openalex.org/W648947103', 'https://openalex.org/W1514535095', 'https://openalex.org/W1551803577', 'https://openalex.org/W1793121960', 'https://openalex.org/W1972595521', 'https://openalex.org/W2004672974', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133564696', 'https://openalex.org/W2250539671', 'https://openalex.org/W2274861244', 'https://openalex.org/W2293453011', 'https://openalex.org/W2335262272', 'https://openalex.org/W2464790259', 'https://openalex.org/W2473329891', 'https://openalex.org/W2473965551', 'https://openalex.org/W2520305281', 'https://openalex.org/W2551571666', 'https://openalex.org/W2575321326', 'https://openalex.org/W2756320212', 'https://openalex.org/W2759621817', 'https://openalex.org/W2761412636', 'https://openalex.org/W2949252816', 'https://openalex.org/W2951008357', 'https://openalex.org/W2963031169', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963974338', 'https://openalex.org/W2964156885', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964308564', 'https://openalex.org/W4285719527', 'https://openalex.org/W4295249402']","Shang-Yu Su, Pei-Chieh Yuan, Yun-Nung Chen. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",0.9952153110047848
SKG_DIA_55,https://openalex.org/W3102260352,2020,7,"['https://openalex.org/W165283731', 'https://openalex.org/W1651093245', 'https://openalex.org/W1686810756', 'https://openalex.org/W2064675550', 'https://openalex.org/W2184540135', 'https://openalex.org/W2194775991', 'https://openalex.org/W2493916176', 'https://openalex.org/W2577171178', 'https://openalex.org/W2583186419', 'https://openalex.org/W2584723080', 'https://openalex.org/W2767263466', 'https://openalex.org/W2767415038', 'https://openalex.org/W2768661419', 'https://openalex.org/W2775491667', 'https://openalex.org/W2795571593', 'https://openalex.org/W2798685833', 'https://openalex.org/W2807844885', 'https://openalex.org/W2835434549', 'https://openalex.org/W2883284130', 'https://openalex.org/W2892245540', 'https://openalex.org/W2896457183', 'https://openalex.org/W2897182555', 'https://openalex.org/W2962749469', 'https://openalex.org/W2962835968', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963351776', 'https://openalex.org/W2963748384', 'https://openalex.org/W2963904606', 'https://openalex.org/W2964213933', 'https://openalex.org/W2980282514']","In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In this paper, we propose a visually-grounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents’ verbal and non-verbal responses. We present experimental results obtained using the proposed VFD dataset and recent neural network models (e.g., BERT, ResNet). The results demonstrate that first-person vision helps neural network models correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available.",1.0
SKG_DIA_57,https://openalex.org/W2740191615,2017,17,"['https://openalex.org/W121023703', 'https://openalex.org/W950880443', 'https://openalex.org/W1539975474', 'https://openalex.org/W1778387566', 'https://openalex.org/W1975244201', 'https://openalex.org/W1990671169', 'https://openalex.org/W1999874108', 'https://openalex.org/W2054795804', 'https://openalex.org/W2062175565', 'https://openalex.org/W2074056782', 'https://openalex.org/W2115101920', 'https://openalex.org/W2121110499', 'https://openalex.org/W2145339207', 'https://openalex.org/W2164419340', 'https://openalex.org/W2166493072', 'https://openalex.org/W2168359464', 'https://openalex.org/W2250456405', 'https://openalex.org/W2251058040', 'https://openalex.org/W2412715517', 'https://openalex.org/W2417401578', 'https://openalex.org/W2438667436', 'https://openalex.org/W2963993502']","Lu Chen, Runzhe Yang, Cheng Chang, Zihao Ye, Xiang Zhou, Kai Yu. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. 2017.",1.0
SKG_DIA_58,https://openalex.org/W2963567240,2018,150,"['https://openalex.org/W140747314', 'https://openalex.org/W178897730', 'https://openalex.org/W1522301498', 'https://openalex.org/W1931877416', 'https://openalex.org/W1975244201', 'https://openalex.org/W2021151961', 'https://openalex.org/W2037897789', 'https://openalex.org/W2047335008', 'https://openalex.org/W2062175565', 'https://openalex.org/W2119717200', 'https://openalex.org/W2137871902', 'https://openalex.org/W2142641780', 'https://openalex.org/W2250297846', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251235149', 'https://openalex.org/W2412715517', 'https://openalex.org/W2435467204', 'https://openalex.org/W2473329891', 'https://openalex.org/W2534274346', 'https://openalex.org/W2583816737', 'https://openalex.org/W2594726847', 'https://openalex.org/W2749436976', 'https://openalex.org/W2772001136', 'https://openalex.org/W2806600904', 'https://openalex.org/W2949252816', 'https://openalex.org/W2962776342', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962957031', 'https://openalex.org/W2963043030', 'https://openalex.org/W2963050422', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964044380', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964352131', 'https://openalex.org/W3104546989', 'https://openalex.org/W4295249402']","Bing Liu, Gokhan Tür, Dilek Hakkani-Tür, Pararth Shah, Larry Heck. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",1.0
SKG_DIA_60,https://openalex.org/W3105184920,2020,20,"['https://openalex.org/W10548402', 'https://openalex.org/W648786980', 'https://openalex.org/W1959608418', 'https://openalex.org/W2047335008', 'https://openalex.org/W2062175565', 'https://openalex.org/W2099471712', 'https://openalex.org/W2119717200', 'https://openalex.org/W2145339207', 'https://openalex.org/W2396229782', 'https://openalex.org/W2438667436', 'https://openalex.org/W2547875792', 'https://openalex.org/W2571927164', 'https://openalex.org/W2594726847', 'https://openalex.org/W2736601468', 'https://openalex.org/W2765111838', 'https://openalex.org/W2798494119', 'https://openalex.org/W2806936550', 'https://openalex.org/W2889186204', 'https://openalex.org/W2949252816', 'https://openalex.org/W2962996309', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963277051', 'https://openalex.org/W2963692154', 'https://openalex.org/W2963712524', 'https://openalex.org/W2964044380', 'https://openalex.org/W2964268978', 'https://openalex.org/W2970828515', 'https://openalex.org/W3104546989', 'https://openalex.org/W3121541553', 'https://openalex.org/W4312609624', 'https://openalex.org/W4320013936']","Reinforcement learning methods have emerged as a popular choice for training an efficient and effective dialogue policy. However, these methods suffer from sparse and unstable reward signals returned by a user simulator only when a dialogue finishes. Besides, the reward signal is manually designed by human experts, which requires domain knowledge. Recently, a number of adversarial learning methods have been proposed to learn the reward function together with the dialogue policy. However, to alternatively update the dialogue policy and the reward model on the fly, we are limited to policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the alternating training of a dialogue agent and the reward model can easily get stuck in local optima or result in mode collapse. To overcome the listed issues, we propose to decompose the adversarial training into two steps. First, we train the discriminator with an auxiliary dialogue generator and then incorporate a derived reward model into a common reinforcement learning method to guide the dialogue policy learning. This approach is applicable to both on-policy and off-policy reinforcement learning methods. Based on our extensive experimentation, we can conclude the proposed method: (1) achieves a remarkable task success rate using both on-policy and off-policy reinforcement learning methods; and (2) has potential to transfer knowledge from existing domains to a new domain.",1.0
SKG_DIA_61,https://openalex.org/W3034956542,2020,30,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591801644', 'https://openalex.org/W1836465849', 'https://openalex.org/W1924770834', 'https://openalex.org/W1975244201', 'https://openalex.org/W2108806737', 'https://openalex.org/W2119717200', 'https://openalex.org/W2176263492', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250539671', 'https://openalex.org/W2472819217', 'https://openalex.org/W2550182557', 'https://openalex.org/W2578206533', 'https://openalex.org/W2604763608', 'https://openalex.org/W2612675303', 'https://openalex.org/W2798367796', 'https://openalex.org/W2888541716', 'https://openalex.org/W2891732163', 'https://openalex.org/W2896457183', 'https://openalex.org/W2912438391', 'https://openalex.org/W2945475330', 'https://openalex.org/W2945978556', 'https://openalex.org/W2951980657', 'https://openalex.org/W2963084599', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963768805', 'https://openalex.org/W2963866663', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2968831808', 'https://openalex.org/W2969301397', 'https://openalex.org/W2970404807', 'https://openalex.org/W2972777589', 'https://openalex.org/W2973230427', 'https://openalex.org/W3104402684', 'https://openalex.org/W4300971732']","A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system. Tremendous progress has been made in recent years. However, the major challenges remain. The state-of-the-art accuracy for DST is below 50% for a multi-domain dialogue task. A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch. In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET). Our first contribution is to improve the DST accuracy. We enhance a neural model based DST generator with a reward manager, which is built on policy gradient reinforcement learning (RL) to fine-tune the generator. With this change, we are able to improve the joint accuracy of DST from 48.79% to 50.91% on the MultiWOZ corpus. Second, we explore to train a DST meta-learning model with a few domains as source domains and a new domain as target domain. We apply the model-agnostic meta-learning algorithm (MAML) to DST and the obtained meta-learning model is used for new domain adaptation. Our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain.",1.0
SKG_DIA_62,https://openalex.org/W2964331476,2019,39,"['https://openalex.org/W109922458', 'https://openalex.org/W182831726', 'https://openalex.org/W836999996', 'https://openalex.org/W1522301498', 'https://openalex.org/W1594982765', 'https://openalex.org/W1841959837', 'https://openalex.org/W1961463694', 'https://openalex.org/W1965394545', 'https://openalex.org/W1971094734', 'https://openalex.org/W1976847947', 'https://openalex.org/W1991002414', 'https://openalex.org/W1993378086', 'https://openalex.org/W2009419346', 'https://openalex.org/W2051576505', 'https://openalex.org/W2053072887', 'https://openalex.org/W2102582267', 'https://openalex.org/W2115613106', 'https://openalex.org/W2128970689', 'https://openalex.org/W2149129894', 'https://openalex.org/W2154608152', 'https://openalex.org/W2161466446', 'https://openalex.org/W2250539671', 'https://openalex.org/W2265658025', 'https://openalex.org/W2397246247', 'https://openalex.org/W2509943063', 'https://openalex.org/W2551396370', 'https://openalex.org/W2599618731', 'https://openalex.org/W2603612888', 'https://openalex.org/W2740747242', 'https://openalex.org/W2740811869', 'https://openalex.org/W2748463297', 'https://openalex.org/W2884561390', 'https://openalex.org/W2891331025', 'https://openalex.org/W2912279724', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963351448', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963955897', 'https://openalex.org/W2964121744', 'https://openalex.org/W3018382390', 'https://openalex.org/W3125532913', 'https://openalex.org/W4236521339', 'https://openalex.org/W4237138749', 'https://openalex.org/W4385245566']","Automatically analyzing dialogue can help understand and guide behavior in domains such as counseling, where interactions are largely mediated by conversation. In this paper, we study modeling behavioral codes used to asses a psychotherapy treatment style called Motivational Interviewing (MI), which is effective for addressing substance abuse and related problems. Specifically, we address the problem of providing real-time guidance to therapists with a dialogue observer that (1) categorizes therapist and client MI behavioral codes and, (2) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist. For both tasks, we define neural network models that build upon recent successes in dialogue modeling. Our experiments demonstrate that our models can outperform several baselines for both tasks. We also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue.",0.9933774834437086
SKG_DIA_64,https://openalex.org/W2250353434,2014,8,"['https://openalex.org/W123366752', 'https://openalex.org/W1497300277', 'https://openalex.org/W1533917153', 'https://openalex.org/W1570771333', 'https://openalex.org/W1787105636', 'https://openalex.org/W1816599501', 'https://openalex.org/W1993027114', 'https://openalex.org/W2028651375', 'https://openalex.org/W2040512654', 'https://openalex.org/W2087347434', 'https://openalex.org/W2088622183', 'https://openalex.org/W2096765155', 'https://openalex.org/W2096968458', 'https://openalex.org/W2101534792', 'https://openalex.org/W2112729630', 'https://openalex.org/W2128970689', 'https://openalex.org/W2138260386', 'https://openalex.org/W2145451908', 'https://openalex.org/W2147196093', 'https://openalex.org/W2151295812', 'https://openalex.org/W2156953626', 'https://openalex.org/W2158794898', 'https://openalex.org/W2166957049', 'https://openalex.org/W2168356304', 'https://openalex.org/W2176475152', 'https://openalex.org/W2250281182', 'https://openalex.org/W2251426493', 'https://openalex.org/W2465041517', 'https://openalex.org/W2788850978', 'https://openalex.org/W2949089885', 'https://openalex.org/W4249258521']","Understanding the actionable outcomes of a dialogue requires effectively modeling situational roles of dialogue participants, the structure of the dialogue and the relevance of each utterance to an eventual action.We develop a latent-variable model that can capture these notions and apply it in the context of courtroom dialogues, in which the objection speech act is used as binary supervision to drive the learning process.We demonstrate quantitatively and qualitatively that our model is able to uncover natural discourse structure from this distant supervision.",0.9705882352941176
SKG_DIA_65,https://openalex.org/W3101068439,2020,21,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133512280', 'https://openalex.org/W2133564696', 'https://openalex.org/W2525778437', 'https://openalex.org/W2742079690', 'https://openalex.org/W2742947407', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898875342', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914204778', 'https://openalex.org/W2922709902', 'https://openalex.org/W2953039584', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963854351', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W2988647680', 'https://openalex.org/W2988937804', 'https://openalex.org/W2997892440', 'https://openalex.org/W3034999214', 'https://openalex.org/W4288624561', 'https://openalex.org/W4385245566']","Large-scale pretrained language models have achieved outstanding performance on natural language understanding tasks. However, it is still under investigating how to apply them to dialogue generation tasks, especially those with responses conditioned on multiple sources. Previous work simply concatenates all input sources or averages information from different input sources. In this work, we study dialogue models with multiple input sources adapted from the pretrained language model GPT2. We explore various methods to fuse multiple separate attention information corresponding to different sources. Our experimental results show that proper fusion methods deliver higher relevance with dialogue history than simple fusion baselines.",1.0
SKG_DIA_66,https://openalex.org/W2962896208,2019,126,"['https://openalex.org/W1899504021', 'https://openalex.org/W1975879668', 'https://openalex.org/W1993378086', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2267186426', 'https://openalex.org/W2413794162', 'https://openalex.org/W2418993857', 'https://openalex.org/W2521114121', 'https://openalex.org/W2561368124', 'https://openalex.org/W2581637843', 'https://openalex.org/W2584185835', 'https://openalex.org/W2612675303', 'https://openalex.org/W2741363662', 'https://openalex.org/W2789033601', 'https://openalex.org/W2798984671', 'https://openalex.org/W2807791032', 'https://openalex.org/W2862781886', 'https://openalex.org/W2891416139', 'https://openalex.org/W2949446780', 'https://openalex.org/W2962707484', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963360026', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964189376', 'https://openalex.org/W2964308564', 'https://openalex.org/W3022187094', 'https://openalex.org/W4385245566']","In multi-turn dialogue generation, response is usually related with only a few contexts. Therefore, an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly. However, the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately, which may hurt the following response generation process. Some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts, but they suffer from either insufficient relevance assumption or position bias problem. In this paper, we propose a new model, named ReCoSa, to tackle this problem. Firstly, a word level LSTM encoder is conducted to obtain the initial representation of each context. Then, the self-attention mechanism is utilized to update both the context and masked response representation. Finally, the attention weights between each context and response representations are computed and used in the further decoding process. Experimental results on both Chinese customer services dataset and English Ubuntu dialogue dataset show that ReCoSa significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on attention shows that the detected relevant contexts by ReCoSa are highly coherent with human’s understanding, validating the correctness and interpretability of ReCoSa.",0.9946524064171124
SKG_DIA_68,https://openalex.org/W2756640946,2017,9,"['https://openalex.org/W598680636', 'https://openalex.org/W1508927691', 'https://openalex.org/W1535960497', 'https://openalex.org/W1541437296', 'https://openalex.org/W1573993894', 'https://openalex.org/W1576632330', 'https://openalex.org/W1654173042', 'https://openalex.org/W1895273801', 'https://openalex.org/W1913627016', 'https://openalex.org/W2009490757', 'https://openalex.org/W2010532052', 'https://openalex.org/W2082484572', 'https://openalex.org/W2101231682', 'https://openalex.org/W2138615112', 'https://openalex.org/W2141230360', 'https://openalex.org/W2163074454', 'https://openalex.org/W2167212741', 'https://openalex.org/W2251183320', 'https://openalex.org/W2270699505', 'https://openalex.org/W2583461040', 'https://openalex.org/W2963308744', 'https://openalex.org/W2963681578', 'https://openalex.org/W3037265734']","We present an unsupervised model of dialogue act sequences in conversation. By modeling topical themes as transitioning more slowly than dialogue acts in conversation, our model de-emphasizes content-related words in order to focus on conversational function words that signal dialogue acts. We also incorporate speaker tendencies to use some acts more than others as an additional predictor of dialogue act prevalence beyond temporal dependencies. According to the evaluation presented on two dissimilar corpora, the CNET forum and NPS Chat corpus, the effectiveness of each modeling assumption is found to vary depending on characteristics of the data. De-emphasizing content-related words yields improvement on the CNET corpus, while utilizing speaker tendencies is advantageous on the NPS corpus. The components of our model complement one another to achieve robust performance on both corpora and outperform state-of-the-art baseline models.",1.0
SKG_DIA_69,https://openalex.org/W3035194816,2020,22,"['https://openalex.org/W2250539671', 'https://openalex.org/W2251058040', 'https://openalex.org/W2556468274', 'https://openalex.org/W2798367796', 'https://openalex.org/W2891732163', 'https://openalex.org/W2945475330', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963527209', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964101860', 'https://openalex.org/W2970404807', 'https://openalex.org/W2970705401', 'https://openalex.org/W2988252747', 'https://openalex.org/W2996317432', 'https://openalex.org/W3008966357', 'https://openalex.org/W3034573951', 'https://openalex.org/W4288027128', 'https://openalex.org/W4289147179']","Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation. By enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long. In our experiments, our proposed model achieves a 7.03% relative improvement over the baseline, establishing a new state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset.",1.0
SKG_DIA_70,https://openalex.org/W3033475175,2020,2,"['https://openalex.org/W10957333', 'https://openalex.org/W73128518', 'https://openalex.org/W1557757161', 'https://openalex.org/W1585586388', 'https://openalex.org/W1728737950', 'https://openalex.org/W2015933299', 'https://openalex.org/W2115615127', 'https://openalex.org/W2117417596', 'https://openalex.org/W2140676672', 'https://openalex.org/W2141236834', 'https://openalex.org/W2152052831', 'https://openalex.org/W2159128165', 'https://openalex.org/W2159757335', 'https://openalex.org/W2160133507', 'https://openalex.org/W2167702024', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251761864', 'https://openalex.org/W2280612018', 'https://openalex.org/W2328886022', 'https://openalex.org/W2408567386', 'https://openalex.org/W2586847566', 'https://openalex.org/W2740181799', 'https://openalex.org/W2794509261', 'https://openalex.org/W2807791032', 'https://openalex.org/W2808636610', 'https://openalex.org/W2889765309', 'https://openalex.org/W2913443447', 'https://openalex.org/W2949918260', 'https://openalex.org/W2953272915', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963640662', 'https://openalex.org/W2963677766', 'https://openalex.org/W2963712524', 'https://openalex.org/W2963866450', 'https://openalex.org/W2964089584', 'https://openalex.org/W2964134121', 'https://openalex.org/W2964609953']","Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances is limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence.",1.0
SKG_DIA_72,https://openalex.org/W3105218667,2020,70,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2154652894', 'https://openalex.org/W2328886022', 'https://openalex.org/W2561529111', 'https://openalex.org/W2584220694', 'https://openalex.org/W2761590056', 'https://openalex.org/W2896457183', 'https://openalex.org/W2913443447', 'https://openalex.org/W2916772188', 'https://openalex.org/W2936695845', 'https://openalex.org/W2950299257', 'https://openalex.org/W2951490152', 'https://openalex.org/W2951583236', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962767366', 'https://openalex.org/W2962786758', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963285578', 'https://openalex.org/W2963527228', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963858333', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964178377', 'https://openalex.org/W2964207259', 'https://openalex.org/W2970252402', 'https://openalex.org/W2988937804', 'https://openalex.org/W2996268457', 'https://openalex.org/W2996403597', 'https://openalex.org/W2998563994', 'https://openalex.org/W3000779003', 'https://openalex.org/W3023786569', 'https://openalex.org/W3034715226', 'https://openalex.org/W3035252911', 'https://openalex.org/W3037026762', 'https://openalex.org/W3113148688', 'https://openalex.org/W3155584966', 'https://openalex.org/W4287900772', 'https://openalex.org/W4294558607', 'https://openalex.org/W4297733535']","Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.",0.994535519125683
SKG_DIA_73,https://openalex.org/W3035451444,2020,227,"['https://openalex.org/W1522301498', 'https://openalex.org/W1566289585', 'https://openalex.org/W1591706642', 'https://openalex.org/W1889081078', 'https://openalex.org/W1958706068', 'https://openalex.org/W2133012565', 'https://openalex.org/W2143177362', 'https://openalex.org/W2157331557', 'https://openalex.org/W2328886022', 'https://openalex.org/W2525778437', 'https://openalex.org/W2761590056', 'https://openalex.org/W2805005636', 'https://openalex.org/W2807873315', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898875342', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914204778', 'https://openalex.org/W2916898195', 'https://openalex.org/W2945260553', 'https://openalex.org/W2948336019', 'https://openalex.org/W2951583236', 'https://openalex.org/W2951697502', 'https://openalex.org/W2953039584', 'https://openalex.org/W2962717182', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963330684', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964213933', 'https://openalex.org/W2964587107', 'https://openalex.org/W2970597249', 'https://openalex.org/W2970682219', 'https://openalex.org/W2973049837', 'https://openalex.org/W2988937804', 'https://openalex.org/W4247864677', 'https://openalex.org/W4288246040', 'https://openalex.org/W4288624561']","Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.",0.9931972789115646
SKG_DIA_75,https://openalex.org/W3105732730,2020,33,"['https://openalex.org/W2127795553', 'https://openalex.org/W2130942839', 'https://openalex.org/W2157331557', 'https://openalex.org/W2432356473', 'https://openalex.org/W2728059831', 'https://openalex.org/W2769099080', 'https://openalex.org/W2774837955', 'https://openalex.org/W2799176105', 'https://openalex.org/W2807873315', 'https://openalex.org/W2889344053', 'https://openalex.org/W2891501508', 'https://openalex.org/W2898987665', 'https://openalex.org/W2900252255', 'https://openalex.org/W2950457956', 'https://openalex.org/W2951105272', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963432357', 'https://openalex.org/W2963520511', 'https://openalex.org/W2963858333', 'https://openalex.org/W2964116313', 'https://openalex.org/W2970236742', 'https://openalex.org/W2970283086', 'https://openalex.org/W2970971581', 'https://openalex.org/W2970988759', 'https://openalex.org/W2971933740', 'https://openalex.org/W2982879526', 'https://openalex.org/W2996428491', 'https://openalex.org/W4295312788', 'https://openalex.org/W4297733535', 'https://openalex.org/W4297895859', 'https://openalex.org/W4385245566']","Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance. To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows. Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context. Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset. Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.",0.9948717948717948
SKG_DIA_76,https://openalex.org/W2970960706,2019,53,"['https://openalex.org/W1522301498', 'https://openalex.org/W2099517310', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108724304', 'https://openalex.org/W2115691521', 'https://openalex.org/W2148952635', 'https://openalex.org/W2154652894', 'https://openalex.org/W2179359904', 'https://openalex.org/W2251344114', 'https://openalex.org/W2565926555', 'https://openalex.org/W2566267116', 'https://openalex.org/W2579689822', 'https://openalex.org/W2584185835', 'https://openalex.org/W2741363662', 'https://openalex.org/W2742113702', 'https://openalex.org/W2798456655', 'https://openalex.org/W2808293489', 'https://openalex.org/W2891416139', 'https://openalex.org/W2896457183', 'https://openalex.org/W2949446780', 'https://openalex.org/W2952723239', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963217826', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963360026', 'https://openalex.org/W3128714985', 'https://openalex.org/W4298159411']","Zhufeng Pan, Kun Bai, Yan Wang, Lianqiang Zhou, Xiaojiang Liu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_77,https://openalex.org/W2889186204,2018,74,"['https://openalex.org/W1491843047', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W2062175565', 'https://openalex.org/W2109038907', 'https://openalex.org/W2117989772', 'https://openalex.org/W2145339207', 'https://openalex.org/W2290354866', 'https://openalex.org/W2295072214', 'https://openalex.org/W2412899141', 'https://openalex.org/W2417401578', 'https://openalex.org/W2473329891', 'https://openalex.org/W2507592741', 'https://openalex.org/W2567374473', 'https://openalex.org/W2571927164', 'https://openalex.org/W2594726847', 'https://openalex.org/W2765111838', 'https://openalex.org/W2783543950', 'https://openalex.org/W2949252816', 'https://openalex.org/W2950471160', 'https://openalex.org/W2962776342', 'https://openalex.org/W2962996309', 'https://openalex.org/W2963043030', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963140401', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964077562', 'https://openalex.org/W2964080167', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964210218', 'https://openalex.org/W3021208093', 'https://openalex.org/W3104546989', 'https://openalex.org/W4293396018', 'https://openalex.org/W4295249402']","This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving the effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed framework that extends the Dyna-Q algorithm to integrate planning for task-completion dialogue policy learning. To obviate DDQ's high dependency on the quality of simulated experiences, we incorporate an RNN-based discriminator in D3Q to differentiate simulated experience from real user experience in order to control the quality of training data. Experiments show that D3Q significantly outperforms DDQ by controlling the quality of simulated experience used for planning. The effectiveness and robustness of D3Q is further demonstrated in a domain extension setting, where the agent's capability of adapting to a changing environment is tested.",0.993006993006993
SKG_DIA_78,https://openalex.org/W2963983466,2017,24,"['https://openalex.org/W35388109', 'https://openalex.org/W142805243', 'https://openalex.org/W225641137', 'https://openalex.org/W1526096287', 'https://openalex.org/W1591607137', 'https://openalex.org/W1752174299', 'https://openalex.org/W1923455183', 'https://openalex.org/W1969700396', 'https://openalex.org/W1982722967', 'https://openalex.org/W1989115577', 'https://openalex.org/W2017187090', 'https://openalex.org/W2044120473', 'https://openalex.org/W2097606805', 'https://openalex.org/W2104367213', 'https://openalex.org/W2105685762', 'https://openalex.org/W2107875902', 'https://openalex.org/W2112383723', 'https://openalex.org/W2125336414', 'https://openalex.org/W2125420881', 'https://openalex.org/W2128970689', 'https://openalex.org/W2130472671', 'https://openalex.org/W2131780215', 'https://openalex.org/W2145588856', 'https://openalex.org/W2146769536', 'https://openalex.org/W2150815390', 'https://openalex.org/W2150824314', 'https://openalex.org/W2158699754', 'https://openalex.org/W2161068821', 'https://openalex.org/W2165887782', 'https://openalex.org/W2171986392', 'https://openalex.org/W2181235373', 'https://openalex.org/W2250749132', 'https://openalex.org/W2251023158', 'https://openalex.org/W2251442452', 'https://openalex.org/W2251521080', 'https://openalex.org/W2338342029', 'https://openalex.org/W2469477431', 'https://openalex.org/W2964036636', 'https://openalex.org/W2964106094', 'https://openalex.org/W4253555784']",We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform state-of-the-art approaches for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members’ understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.,1.0
SKG_DIA_84,https://openalex.org/W2956901422,2019,37,"['https://openalex.org/W1654173042', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132339004', 'https://openalex.org/W2581637843', 'https://openalex.org/W2604698497', 'https://openalex.org/W2611029872', 'https://openalex.org/W2798914047', 'https://openalex.org/W2891732163', 'https://openalex.org/W2898658996', 'https://openalex.org/W2914204778', 'https://openalex.org/W2915295540', 'https://openalex.org/W2938704169', 'https://openalex.org/W2947480709', 'https://openalex.org/W2948110372', 'https://openalex.org/W2950444459', 'https://openalex.org/W2953039584', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963706742', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2972437240', 'https://openalex.org/W2984520708', 'https://openalex.org/W3088987271']","Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision making, and language generation from absurdly small amounts of task-specific data. In this paper, we demonstrate that recent progress in language modeling pre-training and transfer learning shows promise to overcome this problem. We propose a task-oriented dialogue model that operates solely on text input: it effectively bypasses explicit policy and language generation modules. Building on top of the TransferTransfo framework and generative model pre-training, we validate the approach on complex multi-domain task-oriented dialogues from the MultiWOZ dataset. Our automatic and human evaluations show that the proposed model is on par with a strong task-specific neural baseline. In the long run, our approach holds promise to mitigate the data scarcity problem, and to support the construction of more engaging and more eloquent task-oriented conversational agents.",0.99581589958159
SKG_DIA_86,https://openalex.org/W2508101854,2016,4,"['https://openalex.org/W35388109', 'https://openalex.org/W1513873506', 'https://openalex.org/W1516111018', 'https://openalex.org/W1516554472', 'https://openalex.org/W1654173042', 'https://openalex.org/W1853745982', 'https://openalex.org/W1985514943', 'https://openalex.org/W2042096436', 'https://openalex.org/W2044120473', 'https://openalex.org/W2089285937', 'https://openalex.org/W2096192494', 'https://openalex.org/W2100002341', 'https://openalex.org/W2102409316', 'https://openalex.org/W2116064496', 'https://openalex.org/W2116825644', 'https://openalex.org/W2118370253', 'https://openalex.org/W2144100511', 'https://openalex.org/W2146447174', 'https://openalex.org/W2169218343', 'https://openalex.org/W2170323078', 'https://openalex.org/W2251949648', 'https://openalex.org/W2294861638', 'https://openalex.org/W2567948266', 'https://openalex.org/W2949952668', 'https://openalex.org/W4237840503', 'https://openalex.org/W4239181501', 'https://openalex.org/W4293503257']","We propose a new unsupervised learning model, hidden softmax sequence model (HSSM), based on Boltzmann machine for dialogue structure analysis.The model employs three types of units in the hidden layer to discovery dialogue latent structures: softmax units which represent latent states of utterances; binary units which represent latent topics specified by dialogues; and a binary unit that represents the global general topic shared across the whole dialogue corpus.In addition, the model contains extra connections between adjacent hidden softmax units to formulate the dependency between latent states.Two different kinds of real world dialogue corpora, Twitter-Post and AirTicketBooking, are utilized for extensive comparing experiments, and the results illustrate that the proposed model outperforms sate-ofthe-art popular approaches.",1.0
SKG_DIA_87,https://openalex.org/W2620635248,2017,19,[],"Natural language generation (NLG) is a critical component in a spoken\ndialogue system. This paper presents a Recurrent Neural Network based\nEncoder-Decoder architecture, in which an LSTM-based decoder is introduced to\nselect, aggregate semantic elements produced by an attention mechanism over the\ninput elements, and to produce the required utterances. The proposed generator\ncan be jointly trained both sentence planning and surface realization to\nproduce natural language sentences. The proposed model was extensively\nevaluated on four different NLG datasets. The experimental results showed that\nthe proposed generators not only consistently outperform the previous methods\nacross all the NLG domains but also show an ability to generalize from a new,\nunseen domain and learn from multi-domain datasets.\n",1.0
SKG_DIA_88,https://openalex.org/W3037879762,2020,67,"['https://openalex.org/W1948566616', 'https://openalex.org/W2062175565', 'https://openalex.org/W2119717200', 'https://openalex.org/W2159875193', 'https://openalex.org/W2251058040', 'https://openalex.org/W2736601468', 'https://openalex.org/W2739936944', 'https://openalex.org/W2772604077', 'https://openalex.org/W2798914047', 'https://openalex.org/W2810840719', 'https://openalex.org/W2891732163', 'https://openalex.org/W2899908862', 'https://openalex.org/W2915295540', 'https://openalex.org/W2945475330', 'https://openalex.org/W2953071719', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962831269', 'https://openalex.org/W2962852262', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963692154', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2970828515', 'https://openalex.org/W2997108628', 'https://openalex.org/W2997771882', 'https://openalex.org/W3000027512', 'https://openalex.org/W3036362489', 'https://openalex.org/W3104546989', 'https://openalex.org/W4293459943', 'https://openalex.org/W4306716473']","Qi Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi Takanobu, Jinchao Li, Baolin Peng, Jianfeng Gao, Xiaoyan Zhu, Minlie Huang. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2020.",0.994475138121547
SKG_DIA_89,https://openalex.org/W2898790713,2018,14,"['https://openalex.org/W592244745', 'https://openalex.org/W648786980', 'https://openalex.org/W1753530705', 'https://openalex.org/W1947758080', 'https://openalex.org/W1948566616', 'https://openalex.org/W1959608418', 'https://openalex.org/W2161181481', 'https://openalex.org/W2188365844', 'https://openalex.org/W2291723583', 'https://openalex.org/W2510842514', 'https://openalex.org/W2586756136', 'https://openalex.org/W2749498836', 'https://openalex.org/W2756946152', 'https://openalex.org/W2951176429', 'https://openalex.org/W2962800561', 'https://openalex.org/W2962814609', 'https://openalex.org/W2962956378', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963600562', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963735467', 'https://openalex.org/W3100380967']","Recent deep learning models have shown improving results to natural language generation (NLG) irrespective of providing sufficient annotated data. However, a modest training data may harm such models’ performance. Thus, how to build a generator that can utilize as much of knowledge from a low-resource setting data is a crucial issue in NLG. This paper presents a variational neural-based generation model to tackle the NLG problem of having limited labeled dataset, in which we integrate a variational inference into an encoder-decoder generator and introduce a novel auxiliary auto-encoding with an effective training procedure. Experiments showed that the proposed methods not only outperform the previous models when having sufficient training dataset but also demonstrate strong ability to work acceptably well when the training data is scarce.",1.0
SKG_DIA_90,https://openalex.org/W2949413855,2019,74,"['https://openalex.org/W40565524', 'https://openalex.org/W122915500', 'https://openalex.org/W1522301498', 'https://openalex.org/W1574901103', 'https://openalex.org/W1733954365', 'https://openalex.org/W1948566616', 'https://openalex.org/W1978078764', 'https://openalex.org/W2045738181', 'https://openalex.org/W2059857707', 'https://openalex.org/W2064296938', 'https://openalex.org/W2064675550', 'https://openalex.org/W2080656168', 'https://openalex.org/W2095705004', 'https://openalex.org/W2099542783', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107298945', 'https://openalex.org/W2114544007', 'https://openalex.org/W2116716943', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133341864', 'https://openalex.org/W2133564696', 'https://openalex.org/W2164259714', 'https://openalex.org/W2168814553', 'https://openalex.org/W2250539671', 'https://openalex.org/W2291723583', 'https://openalex.org/W2293959043', 'https://openalex.org/W2295512956', 'https://openalex.org/W2429300145', 'https://openalex.org/W2507756961', 'https://openalex.org/W2561658355', 'https://openalex.org/W2613904329', 'https://openalex.org/W2739046565', 'https://openalex.org/W2806532810', 'https://openalex.org/W2889009749', 'https://openalex.org/W2902141901', 'https://openalex.org/W2903428882', 'https://openalex.org/W2914397182', 'https://openalex.org/W2935206035', 'https://openalex.org/W2962729880', 'https://openalex.org/W2962801572', 'https://openalex.org/W2962905474', 'https://openalex.org/W2963084773', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963592583', 'https://openalex.org/W2963672599', 'https://openalex.org/W2963910262', 'https://openalex.org/W2964116568', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3124386704', 'https://openalex.org/W4254255610', 'https://openalex.org/W4288375073']","Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.",1.0
SKG_DIA_96,https://openalex.org/W2970688662,2019,12,"['https://openalex.org/W10957333', 'https://openalex.org/W295828404', 'https://openalex.org/W635530177', 'https://openalex.org/W1591607137', 'https://openalex.org/W2048101965', 'https://openalex.org/W2120333797', 'https://openalex.org/W2128970689', 'https://openalex.org/W2220374841', 'https://openalex.org/W2250539671', 'https://openalex.org/W2328886022', 'https://openalex.org/W2593751037', 'https://openalex.org/W2761590056', 'https://openalex.org/W2786983967', 'https://openalex.org/W2836574416', 'https://openalex.org/W2884970917', 'https://openalex.org/W2889793905', 'https://openalex.org/W2891416139', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963765493', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963903950', 'https://openalex.org/W3121541553', 'https://openalex.org/W4300125564']","Harshit Kumar, Arvind Agarwal, Sachindra Joshi. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_97,https://openalex.org/W3098694757,2020,21,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W2061397531', 'https://openalex.org/W2103076621', 'https://openalex.org/W2123442489', 'https://openalex.org/W2126851059', 'https://openalex.org/W2151170651', 'https://openalex.org/W2507756961', 'https://openalex.org/W2606974598', 'https://openalex.org/W2734443755', 'https://openalex.org/W2806070455', 'https://openalex.org/W2914204778', 'https://openalex.org/W2915816387', 'https://openalex.org/W2936215830', 'https://openalex.org/W2945260553', 'https://openalex.org/W2949769095', 'https://openalex.org/W2952855649', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962803243', 'https://openalex.org/W2963022746', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963560594', 'https://openalex.org/W2963691697', 'https://openalex.org/W2963790827', 'https://openalex.org/W2964121744', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970960706', 'https://openalex.org/W2970996870', 'https://openalex.org/W2971274815', 'https://openalex.org/W2997349160', 'https://openalex.org/W3022187094', 'https://openalex.org/W4288373939', 'https://openalex.org/W4288624561', 'https://openalex.org/W4298870559']","For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words. In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.",1.0
SKG_DIA_98,https://openalex.org/W3102521862,2020,46,"['https://openalex.org/W1522301498', 'https://openalex.org/W1793121960', 'https://openalex.org/W1902237438', 'https://openalex.org/W1975244201', 'https://openalex.org/W1979299372', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2157331557', 'https://openalex.org/W2335875860', 'https://openalex.org/W2396229782', 'https://openalex.org/W2427764808', 'https://openalex.org/W2438667436', 'https://openalex.org/W2473965551', 'https://openalex.org/W2514480375', 'https://openalex.org/W2564070522', 'https://openalex.org/W2594726847', 'https://openalex.org/W2612364175', 'https://openalex.org/W2746626573', 'https://openalex.org/W2797625445', 'https://openalex.org/W2798367796', 'https://openalex.org/W2798494119', 'https://openalex.org/W2798914047', 'https://openalex.org/W2889186204', 'https://openalex.org/W2889224519', 'https://openalex.org/W2896457183', 'https://openalex.org/W2945475330', 'https://openalex.org/W2951008357', 'https://openalex.org/W2953071719', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963020213', 'https://openalex.org/W2963201498', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963924362', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218', 'https://openalex.org/W2998201756', 'https://openalex.org/W3022187094', 'https://openalex.org/W3034930293', 'https://openalex.org/W4285719527', 'https://openalex.org/W4295249402', 'https://openalex.org/W4385245566']","End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.",0.9942196531791908
SKG_DIA_99,https://openalex.org/W3099088459,2020,21,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1604102475', 'https://openalex.org/W1785674045', 'https://openalex.org/W1924770834', 'https://openalex.org/W2016589492', 'https://openalex.org/W2115101920', 'https://openalex.org/W2119015791', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250539671', 'https://openalex.org/W2556468274', 'https://openalex.org/W2560376840', 'https://openalex.org/W2561293850', 'https://openalex.org/W2586719289', 'https://openalex.org/W2592094563', 'https://openalex.org/W2804010326', 'https://openalex.org/W2945475330', 'https://openalex.org/W2962831269', 'https://openalex.org/W2962847367', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963790827', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W2998228050', 'https://openalex.org/W3022187094', 'https://openalex.org/W3034956542', 'https://openalex.org/W3119649668', 'https://openalex.org/W4288094254', 'https://openalex.org/W4288288848', 'https://openalex.org/W4289147179']","The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.",1.0
SKG_DIA_101,https://openalex.org/W3100995786,2020,56,"['https://openalex.org/W1948566616', 'https://openalex.org/W1956340063', 'https://openalex.org/W2012561700', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133512280', 'https://openalex.org/W2154652894', 'https://openalex.org/W2291723583', 'https://openalex.org/W2429300145', 'https://openalex.org/W2518570122', 'https://openalex.org/W2748261613', 'https://openalex.org/W2786660442', 'https://openalex.org/W2798542795', 'https://openalex.org/W2879018339', 'https://openalex.org/W2891732163', 'https://openalex.org/W2949760630', 'https://openalex.org/W2952013107', 'https://openalex.org/W2953071719', 'https://openalex.org/W2963018920', 'https://openalex.org/W2963091658', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963578915', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963912046', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964588180', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970444947', 'https://openalex.org/W2970597249', 'https://openalex.org/W2973049837', 'https://openalex.org/W2994963504', 'https://openalex.org/W2996176596', 'https://openalex.org/W2997771882', 'https://openalex.org/W3014391559', 'https://openalex.org/W3026997957', 'https://openalex.org/W3034881347', 'https://openalex.org/W3035565536', 'https://openalex.org/W3082274269', 'https://openalex.org/W3102854726', 'https://openalex.org/W3115264425', 'https://openalex.org/W4288089799']","Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs. First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language. Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.",1.0
SKG_DIA_102,https://openalex.org/W3100110884,2020,207,"['https://openalex.org/W1566289585', 'https://openalex.org/W2251058040', 'https://openalex.org/W2604698497', 'https://openalex.org/W2784070054', 'https://openalex.org/W2806600904', 'https://openalex.org/W2884814595', 'https://openalex.org/W2891732163', 'https://openalex.org/W2896457183', 'https://openalex.org/W2899663614', 'https://openalex.org/W2908510526', 'https://openalex.org/W2923014074', 'https://openalex.org/W2943737083', 'https://openalex.org/W2945260553', 'https://openalex.org/W2945475330', 'https://openalex.org/W2948110372', 'https://openalex.org/W2951577137', 'https://openalex.org/W2951583236', 'https://openalex.org/W2952267213', 'https://openalex.org/W2963149412', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964101860', 'https://openalex.org/W2965373594', 'https://openalex.org/W2971274815', 'https://openalex.org/W2971737394', 'https://openalex.org/W2972930415', 'https://openalex.org/W2973049837', 'https://openalex.org/W2973230427', 'https://openalex.org/W2977304374', 'https://openalex.org/W2979400990', 'https://openalex.org/W2985067290', 'https://openalex.org/W2986193249', 'https://openalex.org/W2986292373', 'https://openalex.org/W2988647680', 'https://openalex.org/W2988937804', 'https://openalex.org/W2997771882', 'https://openalex.org/W3035451444', 'https://openalex.org/W3082274269', 'https://openalex.org/W3102854726', 'https://openalex.org/W3104078590', 'https://openalex.org/W4288089799', 'https://openalex.org/W4288351520', 'https://openalex.org/W4288624561', 'https://openalex.org/W4385245566']","The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.",0.9936305732484076
SKG_DIA_103,https://openalex.org/W3035356453,2020,103,"['https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W1965555277', 'https://openalex.org/W1996121629', 'https://openalex.org/W2098985784', 'https://openalex.org/W2127795553', 'https://openalex.org/W2130942839', 'https://openalex.org/W2157331557', 'https://openalex.org/W2304113845', 'https://openalex.org/W2328886022', 'https://openalex.org/W2557436004', 'https://openalex.org/W2561529111', 'https://openalex.org/W2586847566', 'https://openalex.org/W2741363662', 'https://openalex.org/W2754194354', 'https://openalex.org/W2757121784', 'https://openalex.org/W2775082024', 'https://openalex.org/W2799037524', 'https://openalex.org/W2799176105', 'https://openalex.org/W2807873315', 'https://openalex.org/W2828237020', 'https://openalex.org/W2887253986', 'https://openalex.org/W2950902819', 'https://openalex.org/W2952420867', 'https://openalex.org/W2962717182', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963285578', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963520511', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963939249', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964207259', 'https://openalex.org/W2964238590', 'https://openalex.org/W2970988759', 'https://openalex.org/W3022187094', 'https://openalex.org/W3121541553']","Generative dialogue systems tend to produce generic responses, which often leads to boring conversations. For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context. Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations. To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI. We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI. We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation. Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments.",1.0
SKG_DIA_104,https://openalex.org/W2104300420,2010,29,"['https://openalex.org/W28861688', 'https://openalex.org/W1515851193', 'https://openalex.org/W1583748115', 'https://openalex.org/W1604986327', 'https://openalex.org/W1860920781', 'https://openalex.org/W1875550513', 'https://openalex.org/W1964725106', 'https://openalex.org/W1986532700', 'https://openalex.org/W2001050921', 'https://openalex.org/W2038550763', 'https://openalex.org/W2045804781', 'https://openalex.org/W2048008777', 'https://openalex.org/W2067097374', 'https://openalex.org/W2088847709', 'https://openalex.org/W2101308260', 'https://openalex.org/W2101533859', 'https://openalex.org/W2107726111', 'https://openalex.org/W2111248079', 'https://openalex.org/W2121528300', 'https://openalex.org/W2132997613', 'https://openalex.org/W2150609911', 'https://openalex.org/W2153190547', 'https://openalex.org/W2161296357', 'https://openalex.org/W2161345458', 'https://openalex.org/W2170809941', 'https://openalex.org/W2911283634', 'https://openalex.org/W2914656440']","Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dialogues, and supports the improvement of mixed-initiative interaction. 1",1.0
SKG_DIA_105,https://openalex.org/W3101131512,2020,11,"['https://openalex.org/W1522301498', 'https://openalex.org/W1785674045', 'https://openalex.org/W1924770834', 'https://openalex.org/W1975244201', 'https://openalex.org/W1989996186', 'https://openalex.org/W2118434577', 'https://openalex.org/W2119015791', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251355666', 'https://openalex.org/W2438667436', 'https://openalex.org/W2556468274', 'https://openalex.org/W2567525733', 'https://openalex.org/W2600702321', 'https://openalex.org/W2798367796', 'https://openalex.org/W2798914047', 'https://openalex.org/W2808293684', 'https://openalex.org/W2892094955', 'https://openalex.org/W2905224888', 'https://openalex.org/W2945475330', 'https://openalex.org/W2951231735', 'https://openalex.org/W2951682790', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963224980', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963548995', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963662654', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964015378', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2979318799', 'https://openalex.org/W2988252747', 'https://openalex.org/W2996317432', 'https://openalex.org/W2998228050', 'https://openalex.org/W2998432370', 'https://openalex.org/W3008966357', 'https://openalex.org/W3021096583', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034908682', 'https://openalex.org/W4288027128', 'https://openalex.org/W4288094254', 'https://openalex.org/W4289147179']","As an essential component of task-oriented dialogue systems, Dialogue State Tracking (DST) takes charge of estimating user intentions and requests in dialogue contexts and extracting substantial goals (states) from user utterances to help the downstream modules to determine the next actions of dialogue systems. For practical usages, a major challenge to constructing a robust DST model is to process a conversation with multi-domain states. However, most existing approaches trained DST on a single domain independently, ignoring the information across domains. To tackle the multi-domain DST task, we first construct a dialogue state graph to transfer structured features among related domain-slot pairs across domains. Then, we encode the graph information of dialogue states by graph convolutional networks and utilize a hard copy mechanism to directly copy historical states from the previous conversation. Experimental results show that our model improves the performances of the multi-domain DST baseline (TRADE) with the absolute joint accuracy of 2.0% and 1.0% on the MultiWOZ 2.0 and 2.1 dialogue datasets, respectively.",0.9933774834437086
SKG_DIA_106,https://openalex.org/W2984342455,2019,12,"['https://openalex.org/W168564468', 'https://openalex.org/W1503441655', 'https://openalex.org/W1525961042', 'https://openalex.org/W1602711325', 'https://openalex.org/W1614298861', 'https://openalex.org/W1662133657', 'https://openalex.org/W2101234009', 'https://openalex.org/W2131744502', 'https://openalex.org/W2134036914', 'https://openalex.org/W2140945425', 'https://openalex.org/W2153579005', 'https://openalex.org/W2158899491', 'https://openalex.org/W2163814903', 'https://openalex.org/W2163922914', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250861254', 'https://openalex.org/W2251846762', 'https://openalex.org/W2252211741', 'https://openalex.org/W2278717175', 'https://openalex.org/W2295759824', 'https://openalex.org/W2461267643', 'https://openalex.org/W2475151947', 'https://openalex.org/W2563734883', 'https://openalex.org/W2566509507', 'https://openalex.org/W2739967154', 'https://openalex.org/W2751916302', 'https://openalex.org/W2773798543', 'https://openalex.org/W2804433558', 'https://openalex.org/W2949952998', 'https://openalex.org/W2950577311', 'https://openalex.org/W2951976932', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962795068', 'https://openalex.org/W2963529986', 'https://openalex.org/W2963541336', 'https://openalex.org/W2963781647', 'https://openalex.org/W2963890755', 'https://openalex.org/W2963925965', 'https://openalex.org/W2964352131', 'https://openalex.org/W4294170691', 'https://openalex.org/W4297805485']","We introduce a new embedding model to represent movie characters and their interactions in a dialogue by encoding in the same representation the language used by these characters as well as information about the other participants in the dialogue. We evaluate the performance of these new character embeddings on two tasks: (1) character relatedness, using a dataset we introduce consisting of a dense character interaction matrix for 4,378 unique character pairs over 22 hours of dialogue from eighteen movies; and (2) character relation classification, for fine- and coarse-grained relations, as well as sentiment relations. Our experiments show that our model significantly outperforms the traditional Word2Vec continuous bag-of-words and skip-gram models, demonstrating the effectiveness of the character embeddings we introduce. We further show how these embeddings can be used in conjunction with a visual question answering system to improve over previous results.",1.0
SKG_DIA_107,https://openalex.org/W3100532709,2020,20,"['https://openalex.org/W1546503963', 'https://openalex.org/W2150593711', 'https://openalex.org/W2162833336', 'https://openalex.org/W2515741950', 'https://openalex.org/W2593864460', 'https://openalex.org/W2624448691', 'https://openalex.org/W2886305736', 'https://openalex.org/W2891732163', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908510526', 'https://openalex.org/W2914204778', 'https://openalex.org/W2945475330', 'https://openalex.org/W2946359678', 'https://openalex.org/W2962776659', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963844597', 'https://openalex.org/W2964006684', 'https://openalex.org/W2965373594', 'https://openalex.org/W2978017171', 'https://openalex.org/W2980282514', 'https://openalex.org/W2986193249', 'https://openalex.org/W2988647680', 'https://openalex.org/W2988937804', 'https://openalex.org/W2996428491', 'https://openalex.org/W3013571468', 'https://openalex.org/W3016625483', 'https://openalex.org/W3035305735', 'https://openalex.org/W3035451444', 'https://openalex.org/W3037026762', 'https://openalex.org/W3099668342', 'https://openalex.org/W3100110884', 'https://openalex.org/W3100128199', 'https://openalex.org/W3104078590', 'https://openalex.org/W4287824654', 'https://openalex.org/W4288624561', 'https://openalex.org/W4299585995', 'https://openalex.org/W4300672471', 'https://openalex.org/W4385245566']","This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.",1.0
SKG_DIA_108,https://openalex.org/W3104546989,2017,153,"['https://openalex.org/W16046748', 'https://openalex.org/W137851535', 'https://openalex.org/W638006253', 'https://openalex.org/W1489525520', 'https://openalex.org/W1492935830', 'https://openalex.org/W1494114146', 'https://openalex.org/W1552182777', 'https://openalex.org/W1584761227', 'https://openalex.org/W1592847719', 'https://openalex.org/W1700691926', 'https://openalex.org/W1763968285', 'https://openalex.org/W1948566616', 'https://openalex.org/W1956340063', 'https://openalex.org/W1975244201', 'https://openalex.org/W1982897610', 'https://openalex.org/W2021618504', 'https://openalex.org/W2024632416', 'https://openalex.org/W2062175565', 'https://openalex.org/W2062295023', 'https://openalex.org/W2078745840', 'https://openalex.org/W2078861931', 'https://openalex.org/W2098651115', 'https://openalex.org/W2107587102', 'https://openalex.org/W2109910161', 'https://openalex.org/W2118462278', 'https://openalex.org/W2120045257', 'https://openalex.org/W2121325257', 'https://openalex.org/W2121517924', 'https://openalex.org/W2123891489', 'https://openalex.org/W2128856065', 'https://openalex.org/W2132997613', 'https://openalex.org/W2145339207', 'https://openalex.org/W2149327368', 'https://openalex.org/W2154652894', 'https://openalex.org/W2158548602', 'https://openalex.org/W2161181481', 'https://openalex.org/W2164777277', 'https://openalex.org/W2204900930', 'https://openalex.org/W2250530145', 'https://openalex.org/W2251165062', 'https://openalex.org/W2251251208', 'https://openalex.org/W2251291469', 'https://openalex.org/W2251818274', 'https://openalex.org/W2291723583', 'https://openalex.org/W2294065713', 'https://openalex.org/W2417401578', 'https://openalex.org/W2427764808', 'https://openalex.org/W2473329891', 'https://openalex.org/W2483402000', 'https://openalex.org/W2512152365', 'https://openalex.org/W2518570122', 'https://openalex.org/W2530291685', 'https://openalex.org/W2558661633', 'https://openalex.org/W2564590796', 'https://openalex.org/W2571927164', 'https://openalex.org/W2578330760', 'https://openalex.org/W2594726847', 'https://openalex.org/W2604688337', 'https://openalex.org/W2745039414', 'https://openalex.org/W2949252816', 'https://openalex.org/W2951577137', 'https://openalex.org/W2962682659', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963262099', 'https://openalex.org/W2963503801', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963912046', 'https://openalex.org/W4302353911']","The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.",1.0
SKG_DIA_109,https://openalex.org/W2429300145,2016,60,[],"We present a natural language generator based on the sequence-to-sequence\napproach that can be trained to produce natural language strings as well as\ndeep syntax dependency trees from input dialogue acts, and we use it to\ndirectly compare two-step generation with separate sentence planning and\nsurface realization stages to a joint, one-step approach. We were able to train\nboth setups successfully using very little training data. The joint setup\noffers better performance, surpassing state-of-the-art with regards to\nn-gram-based scores while providing more relevant outputs.\n",1.0
SKG_DIA_111,https://openalex.org/W2964226791,2019,15,"['https://openalex.org/W100948856', 'https://openalex.org/W941608337', 'https://openalex.org/W1969634691', 'https://openalex.org/W2040960947', 'https://openalex.org/W2123442489', 'https://openalex.org/W2131861279', 'https://openalex.org/W2135112693', 'https://openalex.org/W2152197045', 'https://openalex.org/W2153365547', 'https://openalex.org/W2163074454', 'https://openalex.org/W2164567676', 'https://openalex.org/W2166957049', 'https://openalex.org/W2251579496', 'https://openalex.org/W2251939518', 'https://openalex.org/W2252182164', 'https://openalex.org/W2294607529', 'https://openalex.org/W2564875206', 'https://openalex.org/W2759361123', 'https://openalex.org/W2781846447', 'https://openalex.org/W2783549597', 'https://openalex.org/W2798990287', 'https://openalex.org/W2824107965', 'https://openalex.org/W2864258299', 'https://openalex.org/W2889581899', 'https://openalex.org/W2890802255', 'https://openalex.org/W2907283777', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963209355', 'https://openalex.org/W2963299810', 'https://openalex.org/W2963391789', 'https://openalex.org/W2963448047', 'https://openalex.org/W4237453526', 'https://openalex.org/W4294338648', 'https://openalex.org/W4301488752']","Discourse relation identification has been an active area of research for many years, and the challenge of identifying implicit relations remains largely an unsolved task, especially in the context of an open-domain dialogue system. Previous work primarily relies on a corpora of formal text which is inherently non-dialogic, i.e., news and journals. This data however is not suitable to handle the nuances of informal dialogue nor is it capable of navigating the plethora of valid topics present in open-domain dialogue. In this paper, we designed a novel discourse relation identification pipeline specifically tuned for open-domain dialogue systems. We firstly propose a method to automatically extract the implicit discourse relation argument pairs and labels from a dataset of dialogic turns, resulting in a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. Moreover, we have taken the first steps to leverage the dialogue features unique to our task to further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model.",1.0
SKG_DIA_112,https://openalex.org/W3099605929,2020,10,"['https://openalex.org/W50296447', 'https://openalex.org/W183472599', 'https://openalex.org/W1491843047', 'https://openalex.org/W1625390266', 'https://openalex.org/W1681299129', 'https://openalex.org/W1758031947', 'https://openalex.org/W2062175565', 'https://openalex.org/W2115211925', 'https://openalex.org/W2121863487', 'https://openalex.org/W2145339207', 'https://openalex.org/W2155027007', 'https://openalex.org/W2173564293', 'https://openalex.org/W2257979135', 'https://openalex.org/W2396961959', 'https://openalex.org/W2571927164', 'https://openalex.org/W2766447205', 'https://openalex.org/W2915295540', 'https://openalex.org/W2951091467', 'https://openalex.org/W2952546452', 'https://openalex.org/W2962996309', 'https://openalex.org/W2963170138', 'https://openalex.org/W2963433587', 'https://openalex.org/W2970028737']","We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors. Such idea arises naturally in human behaviors, e.g. predicting others' responses and then deciding our own actions. In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.",1.0
SKG_DIA_113,https://openalex.org/W2971222961,2019,62,"['https://openalex.org/W1530785623', 'https://openalex.org/W1551039219', 'https://openalex.org/W1662382123', 'https://openalex.org/W1665214252', 'https://openalex.org/W1793121960', 'https://openalex.org/W1832693441', 'https://openalex.org/W1924770834', 'https://openalex.org/W2064675550', 'https://openalex.org/W2092206588', 'https://openalex.org/W2116341502', 'https://openalex.org/W2132555391', 'https://openalex.org/W2146334809', 'https://openalex.org/W2250539671', 'https://openalex.org/W2519887557', 'https://openalex.org/W2553397501', 'https://openalex.org/W2578994755', 'https://openalex.org/W2604314403', 'https://openalex.org/W2606780347', 'https://openalex.org/W2740550900', 'https://openalex.org/W2788420618', 'https://openalex.org/W2791506524', 'https://openalex.org/W2792352584', 'https://openalex.org/W2805662932', 'https://openalex.org/W2891359673', 'https://openalex.org/W2926337278', 'https://openalex.org/W2962718314', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963686995', 'https://openalex.org/W2963710346', 'https://openalex.org/W2963873807', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964300796', 'https://openalex.org/W2964321699']","Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources.In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC.We leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition.Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods.We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets.",0.9943502824858758
SKG_DIA_115,https://openalex.org/W3100955355,2020,30,"['https://openalex.org/W932413789', 'https://openalex.org/W1522301498', 'https://openalex.org/W2053154970', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2120861206', 'https://openalex.org/W2133564696', 'https://openalex.org/W2138204974', 'https://openalex.org/W2138621090', 'https://openalex.org/W2155482025', 'https://openalex.org/W2419539795', 'https://openalex.org/W2493916176', 'https://openalex.org/W2521114121', 'https://openalex.org/W2581637843', 'https://openalex.org/W2584185835', 'https://openalex.org/W2752461516', 'https://openalex.org/W2784400615', 'https://openalex.org/W2798456655', 'https://openalex.org/W2807791032', 'https://openalex.org/W2821503932', 'https://openalex.org/W2842511635', 'https://openalex.org/W2890969459', 'https://openalex.org/W2891416139', 'https://openalex.org/W2908747729', 'https://openalex.org/W2944870985', 'https://openalex.org/W2951883832', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963084471', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963330684', 'https://openalex.org/W2963360026', 'https://openalex.org/W2963360726', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963879591', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963958388', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964134121', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970648534', 'https://openalex.org/W2996068536', 'https://openalex.org/W2997657234', 'https://openalex.org/W3005680577', 'https://openalex.org/W3015322406', 'https://openalex.org/W3022187094', 'https://openalex.org/W3035044096', 'https://openalex.org/W4252076394', 'https://openalex.org/W4287824654', 'https://openalex.org/W4297808394', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566']","Neural dialogue response generation has gained much popularity in recent years. Maximum Likelihood Estimation (MLE) objective is widely adopted in existing dialogue model learning. However, models trained with MLE objective function are plagued by the low-diversity issue when it comes to the open-domain conversational setting. Inspired by the observation that humans not only learn from the positive signals but also benefit from correcting behaviors of undesirable actions, in this work, we introduce contrastive learning into dialogue generation, where the model explicitly perceives the difference between the well-chosen positive and negative utterances. Specifically, we employ a pretrained baseline model as a reference. During contrastive learning, the target dialogue model is trained to give higher conditional probabilities for the positive samples, and lower conditional probabilities for those negative samples, compared to the reference model. To manage the multi-mapping relations prevalent in human conversation, we augment contrastive dialogue learning with group-wise dual sampling. Extensive experimental results show that the proposed group-wise contrastive learning framework is suited for training a wide range of neural dialogue generation models with very favorable performance over the baseline training approaches.",1.0
SKG_DIA_116,https://openalex.org/W3101471490,2020,31,"['https://openalex.org/W1566289585', 'https://openalex.org/W1574440611', 'https://openalex.org/W2339852062', 'https://openalex.org/W2579198228', 'https://openalex.org/W2582886532', 'https://openalex.org/W2593751037', 'https://openalex.org/W2608787653', 'https://openalex.org/W2785414597', 'https://openalex.org/W2798456655', 'https://openalex.org/W2890394457', 'https://openalex.org/W2891416139', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908602207', 'https://openalex.org/W2949446780', 'https://openalex.org/W2949600515', 'https://openalex.org/W2952576857', 'https://openalex.org/W2952813980', 'https://openalex.org/W2953071719', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962952612', 'https://openalex.org/W2963041453', 'https://openalex.org/W2963341956', 'https://openalex.org/W2967674528', 'https://openalex.org/W2969574947', 'https://openalex.org/W2970404807', 'https://openalex.org/W2972437240', 'https://openalex.org/W2973054254', 'https://openalex.org/W2985258882', 'https://openalex.org/W2985686011', 'https://openalex.org/W2986292373', 'https://openalex.org/W2986683329', 'https://openalex.org/W2988937804', 'https://openalex.org/W3015071427', 'https://openalex.org/W3181035168']","Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.",1.0
SKG_DIA_119,https://openalex.org/W3103837004,2020,6,"['https://openalex.org/W1491843047', 'https://openalex.org/W1522301498', 'https://openalex.org/W1758031947', 'https://openalex.org/W1778387566', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W2000936041', 'https://openalex.org/W2035934535', 'https://openalex.org/W2062175565', 'https://openalex.org/W2117989772', 'https://openalex.org/W2120045257', 'https://openalex.org/W2290354866', 'https://openalex.org/W2396229782', 'https://openalex.org/W2408200822', 'https://openalex.org/W2559038528', 'https://openalex.org/W2567374473', 'https://openalex.org/W2736601468', 'https://openalex.org/W2739936944', 'https://openalex.org/W2759567155', 'https://openalex.org/W2772217324', 'https://openalex.org/W2783543950', 'https://openalex.org/W2810840719', 'https://openalex.org/W2889186204', 'https://openalex.org/W2947212824', 'https://openalex.org/W2949476504', 'https://openalex.org/W2950314731', 'https://openalex.org/W2950471160', 'https://openalex.org/W2950517718', 'https://openalex.org/W2962872206', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963095800', 'https://openalex.org/W2963433587', 'https://openalex.org/W2963567240', 'https://openalex.org/W2963674921', 'https://openalex.org/W2963993502', 'https://openalex.org/W2964044380', 'https://openalex.org/W2964077562', 'https://openalex.org/W2964080167', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964220198', 'https://openalex.org/W2970028737', 'https://openalex.org/W2970828515', 'https://openalex.org/W3016142228', 'https://openalex.org/W3021208093', 'https://openalex.org/W4293396018', 'https://openalex.org/W4297789121', 'https://openalex.org/W4297806413', 'https://openalex.org/W4306716473']","In order to improve the sample-efficiency of deep reinforcement learning (DRL), we implemented imagination augmented agent (I2A) in spoken dialogue systems (SDS). Although I2A achieves a higher success rate than baselines by augmenting predicted future into a policy network, its complicated architecture introduces unwanted instability. In this work, we propose actor-double-critic (ADC) to improve the stability and overall performance of I2A. ADC simplifies the architecture of I2A to reduce excessive parameters and hyper-parameters. More importantly, a separate model-based critic shares parameters between actions and makes back-propagation explicit. In our experiments on Cambridge Restaurant Booking task, ADC enhances success rates considerably and shows robustness to imperfect environment models. In addition, ADC exhibits the stability and sample-efficiency as significantly reducing the baseline standard deviation of success rates and reaching the 80% success rate with half training data.",0.9942857142857144
SKG_DIA_120,https://openalex.org/W2799176105,2018,181,"['https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W1552847225', 'https://openalex.org/W1591706642', 'https://openalex.org/W1993378086', 'https://openalex.org/W2039133703', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2157331557', 'https://openalex.org/W2557436004', 'https://openalex.org/W2583741591', 'https://openalex.org/W2586847566', 'https://openalex.org/W2593696076', 'https://openalex.org/W2740107682', 'https://openalex.org/W2754194354', 'https://openalex.org/W2769934148', 'https://openalex.org/W2789033601', 'https://openalex.org/W2949555952', 'https://openalex.org/W2950902819', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963546833', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963957489', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964121744', 'https://openalex.org/W3022187094']","End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion, the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset prove that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly.",1.0
SKG_DIA_123,https://openalex.org/W4288104350,2019,1,[],"Hierarchical neural networks are often used to model inherent structures\nwithin dialogues. For goal-oriented dialogues, these models miss a mechanism\nadhering to the goals and neglect the distinct conversational patterns between\ntwo interlocutors. In this work, we propose Goal-Embedded Dual Hierarchical\nAttentional Encoder-Decoder (G-DuHA) able to center around goals and capture\ninterlocutor-level disparity while modeling goal-oriented dialogues.\nExperiments on dialogue generation, response generation, and human evaluations\ndemonstrate that the proposed model successfully generates higher-quality, more\ndiverse and goal-centric dialogues. Moreover, we apply data augmentation via\ngoal-oriented dialogue generation for task-oriented dialog systems with better\nperformance achieved.\n",0.986842105263158
SKG_DIA_124,https://openalex.org/W3101469874,2020,30,"['https://openalex.org/W1522301498', 'https://openalex.org/W1948566616', 'https://openalex.org/W2095705004', 'https://openalex.org/W2130942839', 'https://openalex.org/W2183341477', 'https://openalex.org/W2250297846', 'https://openalex.org/W2251058040', 'https://openalex.org/W2534274346', 'https://openalex.org/W2594726847', 'https://openalex.org/W2749436976', 'https://openalex.org/W2798367796', 'https://openalex.org/W2798494119', 'https://openalex.org/W2798914047', 'https://openalex.org/W2804491889', 'https://openalex.org/W2915295540', 'https://openalex.org/W2945475330', 'https://openalex.org/W2949252816', 'https://openalex.org/W2953071719', 'https://openalex.org/W2954492830', 'https://openalex.org/W2956474441', 'https://openalex.org/W2962831269', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962886331', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963201498', 'https://openalex.org/W2963412005', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964077278', 'https://openalex.org/W2969708789', 'https://openalex.org/W2970260827', 'https://openalex.org/W2970404807', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W2979478117', 'https://openalex.org/W2979520138', 'https://openalex.org/W2979813451', 'https://openalex.org/W2988647680', 'https://openalex.org/W2997108628', 'https://openalex.org/W3008966357', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034656193', 'https://openalex.org/W3104546989', 'https://openalex.org/W3106274079', 'https://openalex.org/W4288027128', 'https://openalex.org/W4295249402', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only. Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users. Unlike the existing approaches that are often designed to train each module separately, we propose “UniConv” - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously. We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines.",0.9946524064171124
SKG_DIA_126,https://openalex.org/W3035148359,2020,102,"['https://openalex.org/W13682356', 'https://openalex.org/W836999996', 'https://openalex.org/W1482214997', 'https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W1654173042', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975879668', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140679639', 'https://openalex.org/W2157331557', 'https://openalex.org/W2166637769', 'https://openalex.org/W2271840356', 'https://openalex.org/W2294078178', 'https://openalex.org/W2295754318', 'https://openalex.org/W2575772232', 'https://openalex.org/W2583741591', 'https://openalex.org/W2586847566', 'https://openalex.org/W2754194354', 'https://openalex.org/W2799176105', 'https://openalex.org/W2804552794', 'https://openalex.org/W2807873315', 'https://openalex.org/W2888541716', 'https://openalex.org/W2891103209', 'https://openalex.org/W2891416139', 'https://openalex.org/W2891826200', 'https://openalex.org/W2898875342', 'https://openalex.org/W2899771611', 'https://openalex.org/W2938704169', 'https://openalex.org/W2938830017', 'https://openalex.org/W2945367412', 'https://openalex.org/W2948519922', 'https://openalex.org/W2949769095', 'https://openalex.org/W2950457956', 'https://openalex.org/W2950902819', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963945575', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964588180', 'https://openalex.org/W2971236040', 'https://openalex.org/W2972664115', 'https://openalex.org/W2996287690', 'https://openalex.org/W3004150797', 'https://openalex.org/W4285719527', 'https://openalex.org/W4385245566']","The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available.",0.9947643979057592
SKG_DIA_127,https://openalex.org/W2899035231,2018,0,"['https://openalex.org/W10957333', 'https://openalex.org/W40565524', 'https://openalex.org/W295828404', 'https://openalex.org/W1591706642', 'https://openalex.org/W1646707810', 'https://openalex.org/W2064675550', 'https://openalex.org/W2096145771', 'https://openalex.org/W2102531443', 'https://openalex.org/W2107878631', 'https://openalex.org/W2118434577', 'https://openalex.org/W2121725465', 'https://openalex.org/W2157331557', 'https://openalex.org/W2173361515', 'https://openalex.org/W2197546379', 'https://openalex.org/W2251235149', 'https://openalex.org/W2295512956', 'https://openalex.org/W2306229986', 'https://openalex.org/W2339852062', 'https://openalex.org/W2346566861', 'https://openalex.org/W2395531022', 'https://openalex.org/W2561368124', 'https://openalex.org/W2621404689', 'https://openalex.org/W2785414597', 'https://openalex.org/W2787462651', 'https://openalex.org/W2891416139', 'https://openalex.org/W2899771611', 'https://openalex.org/W2952566282', 'https://openalex.org/W2962776342', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963522640', 'https://openalex.org/W2963542836', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218']","Building systems that can communicate with humans is a core problem in Artificial Intelligence. This work proposes a novel neural network architecture for response selection in an end-to-end multi-turn conversational dialogue setting. The architecture applies context level attention and incorporates additional external knowledge provided by descriptions of domain-specific words. It uses a bi-directional Gated Recurrent Unit (GRU) for encoding context and responses and learns to attend over the context words given the latent response representation and vice versa.In addition, it incorporates external domain specific information using another GRU for encoding the domain keyword descriptions. This allows better representation of domain-specific keywords in responses and hence improves the overall performance. Experimental results show that our model outperforms all other state-of-the-art methods for response selection in multi-turn conversations.",1.0
SKG_DIA_128,https://openalex.org/W2963825865,2018,1157,"['https://openalex.org/W836999996', 'https://openalex.org/W1518951372', 'https://openalex.org/W1591706642', 'https://openalex.org/W1627609287', 'https://openalex.org/W1958706068', 'https://openalex.org/W1975244201', 'https://openalex.org/W2086511124', 'https://openalex.org/W2106111649', 'https://openalex.org/W2130942839', 'https://openalex.org/W2139575250', 'https://openalex.org/W2160219061', 'https://openalex.org/W2161466446', 'https://openalex.org/W2220374841', 'https://openalex.org/W2250539671', 'https://openalex.org/W2328886022', 'https://openalex.org/W2550893117', 'https://openalex.org/W2688962481', 'https://openalex.org/W2751124354', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962779279', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964119254', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964352131', 'https://openalex.org/W3022187094', 'https://openalex.org/W4295249402', 'https://openalex.org/W4297809080', 'https://openalex.org/W4300326073']","Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018.",0.9846153846153848
SKG_DIA_129,https://openalex.org/W2970603474,2019,18,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1880262756', 'https://openalex.org/W1959608418', 'https://openalex.org/W2016944307', 'https://openalex.org/W2089150068', 'https://openalex.org/W2125320996', 'https://openalex.org/W2140679639', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250645967', 'https://openalex.org/W2804766086', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963879591', 'https://openalex.org/W2963916785', 'https://openalex.org/W2964121744', 'https://openalex.org/W3022187094', 'https://openalex.org/W4231510805', 'https://openalex.org/W4285719527']","Min Zeng, Yisen Wang, Yuan Luo. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_137,https://openalex.org/W2963170138,2018,128,"['https://openalex.org/W1480477506', 'https://openalex.org/W1518951372', 'https://openalex.org/W1521219245', 'https://openalex.org/W1830940278', 'https://openalex.org/W1920532145', 'https://openalex.org/W1975244201', 'https://openalex.org/W2001050921', 'https://openalex.org/W2136152952', 'https://openalex.org/W2146502635', 'https://openalex.org/W2166100419', 'https://openalex.org/W2174703168', 'https://openalex.org/W2175723363', 'https://openalex.org/W2246008130', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251854674', 'https://openalex.org/W2405601855', 'https://openalex.org/W2468710617', 'https://openalex.org/W2579198228', 'https://openalex.org/W2581637843', 'https://openalex.org/W2593696076', 'https://openalex.org/W2593751037', 'https://openalex.org/W2594726847', 'https://openalex.org/W2607380417', 'https://openalex.org/W2769917417', 'https://openalex.org/W2886444114', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962766710', 'https://openalex.org/W2962852262', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963217826', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963993719', 'https://openalex.org/W2964210218', 'https://openalex.org/W4230563027', 'https://openalex.org/W4295249402', 'https://openalex.org/W6600669541']","We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing $50) and the execution of that strategy (e.g., generating “The bike is brand new. Selling for just $50!”). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse dialogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.",1.0
SKG_DIA_139,https://openalex.org/W2945978556,2019,185,"['https://openalex.org/W99485931', 'https://openalex.org/W1522301498', 'https://openalex.org/W1598377843', 'https://openalex.org/W1627609287', 'https://openalex.org/W2089217417', 'https://openalex.org/W2101105183', 'https://openalex.org/W2140804329', 'https://openalex.org/W2250539671', 'https://openalex.org/W2328886022', 'https://openalex.org/W2472819217', 'https://openalex.org/W2604763608', 'https://openalex.org/W2608787653', 'https://openalex.org/W2688962481', 'https://openalex.org/W2753160622', 'https://openalex.org/W2787501667', 'https://openalex.org/W2797625445', 'https://openalex.org/W2810840719', 'https://openalex.org/W2888541716', 'https://openalex.org/W2890394457', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898178263', 'https://openalex.org/W2898658996', 'https://openalex.org/W2913443447', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963225934', 'https://openalex.org/W2963341924', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963775850', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963866663', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964316912', 'https://openalex.org/W2964352131', 'https://openalex.org/W2970192826', 'https://openalex.org/W2979478117', 'https://openalex.org/W3099023595', 'https://openalex.org/W3106274079', 'https://openalex.org/W4288631924', 'https://openalex.org/W4300326073', 'https://openalex.org/W4306716473', 'https://openalex.org/W4385245566']","Existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-metalearning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency.",1.0
SKG_DIA_141,https://openalex.org/W2888093771,2018,38,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2136939460', 'https://openalex.org/W2181347294', 'https://openalex.org/W2251241778', 'https://openalex.org/W2286300105', 'https://openalex.org/W2418993857', 'https://openalex.org/W2536015822', 'https://openalex.org/W2551884415', 'https://openalex.org/W2581637843', 'https://openalex.org/W2604698497', 'https://openalex.org/W2761590056', 'https://openalex.org/W2774104751', 'https://openalex.org/W2807278718', 'https://openalex.org/W2888396831', 'https://openalex.org/W2892153332', 'https://openalex.org/W2951359136', 'https://openalex.org/W2962676842', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963970666', 'https://openalex.org/W2964183117', 'https://openalex.org/W4295249402', 'https://openalex.org/W4385245566']","Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models.",1.0
SKG_DIA_143,https://openalex.org/W2983931018,2020,14,"['https://openalex.org/W2483215953', 'https://openalex.org/W2528130257', 'https://openalex.org/W2550893117', 'https://openalex.org/W2573660794', 'https://openalex.org/W2604487042', 'https://openalex.org/W2615146352', 'https://openalex.org/W2741179140', 'https://openalex.org/W2768716007', 'https://openalex.org/W2770618123', 'https://openalex.org/W2774008708', 'https://openalex.org/W2798583685', 'https://openalex.org/W2798664956', 'https://openalex.org/W2799258637', 'https://openalex.org/W2804377263', 'https://openalex.org/W2889624842', 'https://openalex.org/W2891389695', 'https://openalex.org/W2898081668', 'https://openalex.org/W2899501643', 'https://openalex.org/W2899513582', 'https://openalex.org/W2908709331', 'https://openalex.org/W2913222130', 'https://openalex.org/W2916772188', 'https://openalex.org/W2926555354', 'https://openalex.org/W2941985495', 'https://openalex.org/W2942580297', 'https://openalex.org/W2947283491', 'https://openalex.org/W2949744925', 'https://openalex.org/W2950866572', 'https://openalex.org/W2950997402', 'https://openalex.org/W2951864292', 'https://openalex.org/W2952328691', 'https://openalex.org/W2952355715', 'https://openalex.org/W2953029503', 'https://openalex.org/W2953245949', 'https://openalex.org/W2962990575', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963526187', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963919731', 'https://openalex.org/W2964352131', 'https://openalex.org/W2969117066', 'https://openalex.org/W2969574947', 'https://openalex.org/W2970252402', 'https://openalex.org/W2970387952', 'https://openalex.org/W2970408399', 'https://openalex.org/W2970712718', 'https://openalex.org/W2970723691', 'https://openalex.org/W2970730986', 'https://openalex.org/W2970800693', 'https://openalex.org/W2970879410', 'https://openalex.org/W2970948593', 'https://openalex.org/W2971015127', 'https://openalex.org/W2971307358', 'https://openalex.org/W2971883198', 'https://openalex.org/W2972051251', 'https://openalex.org/W2972654913', 'https://openalex.org/W2972668795', 'https://openalex.org/W2979463066', 'https://openalex.org/W2982277189', 'https://openalex.org/W2982560432', 'https://openalex.org/W2987440089', 'https://openalex.org/W2997607995', 'https://openalex.org/W3023029948', 'https://openalex.org/W3034515982', 'https://openalex.org/W3036997889', 'https://openalex.org/W3037132330', 'https://openalex.org/W3037286488', 'https://openalex.org/W3037378535', 'https://openalex.org/W3037541370', 'https://openalex.org/W3037696302', 'https://openalex.org/W3037697022', 'https://openalex.org/W3042791954']","Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses.",0.9928057553956836
SKG_DIA_144,https://openalex.org/W3099289136,2020,3,"['https://openalex.org/W1522301498', 'https://openalex.org/W1604102475', 'https://openalex.org/W1785674045', 'https://openalex.org/W2004858782', 'https://openalex.org/W2055537935', 'https://openalex.org/W2080045301', 'https://openalex.org/W2107559689', 'https://openalex.org/W2119015791', 'https://openalex.org/W2137813581', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250456405', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251058040', 'https://openalex.org/W2438667436', 'https://openalex.org/W2556468274', 'https://openalex.org/W2560376840', 'https://openalex.org/W2561293850', 'https://openalex.org/W2586719289', 'https://openalex.org/W2592094563', 'https://openalex.org/W2788635322', 'https://openalex.org/W2804010326', 'https://openalex.org/W2896457183', 'https://openalex.org/W2945475330', 'https://openalex.org/W2962831269', 'https://openalex.org/W2962847367', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964218959', 'https://openalex.org/W2970404807', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W2988252747', 'https://openalex.org/W2998228050', 'https://openalex.org/W3034956542', 'https://openalex.org/W4288027128', 'https://openalex.org/W4289147179']","Dialogue state tracking (DST) is an important part of a spoken dialogue system. Existing DST models either ignore temporal feature dependencies across dialogue turns or fail to explicitly model temporal state dependencies in a dialogue. In this work, we propose Temporally Expressive Networks (TEN) to jointly model the two types of temporal dependencies in DST. The TEN model utilizes the power of recurrent networks and probabilistic graphical models. Evaluating on standard datasets, TEN is demonstrated to improve the accuracy of turn-level-state prediction and the state aggregation.",1.0
SKG_DIA_145,https://openalex.org/W3104777900,2020,150,"['https://openalex.org/W10957333', 'https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1975879668', 'https://openalex.org/W2130942839', 'https://openalex.org/W2155027007', 'https://openalex.org/W2328886022', 'https://openalex.org/W2521114121', 'https://openalex.org/W2583186419', 'https://openalex.org/W2584185835', 'https://openalex.org/W2613904329', 'https://openalex.org/W2795571593', 'https://openalex.org/W2798888952', 'https://openalex.org/W2807873315', 'https://openalex.org/W2807880213', 'https://openalex.org/W2891826200', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898875342', 'https://openalex.org/W2899513582', 'https://openalex.org/W2914120296', 'https://openalex.org/W2914204778', 'https://openalex.org/W2916772188', 'https://openalex.org/W2923978210', 'https://openalex.org/W2938224028', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945260553', 'https://openalex.org/W2950457956', 'https://openalex.org/W2951508633', 'https://openalex.org/W2952420867', 'https://openalex.org/W2952468927', 'https://openalex.org/W2954278700', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962785754', 'https://openalex.org/W2962796276', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962896208', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963167649', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963360026', 'https://openalex.org/W2963395792', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963904606', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964082993', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964352131', 'https://openalex.org/W2964458951', 'https://openalex.org/W2965373594', 'https://openalex.org/W2966715458', 'https://openalex.org/W2969876226', 'https://openalex.org/W2970597249', 'https://openalex.org/W2970608575', 'https://openalex.org/W2971236040', 'https://openalex.org/W2971274815', 'https://openalex.org/W2972664115', 'https://openalex.org/W2981851019', 'https://openalex.org/W2988937804', 'https://openalex.org/W2995183464', 'https://openalex.org/W2996227762', 'https://openalex.org/W2997300509', 'https://openalex.org/W3000779003', 'https://openalex.org/W3022187094', 'https://openalex.org/W3034999214', 'https://openalex.org/W3082274269', 'https://openalex.org/W4287900772', 'https://openalex.org/W4288089799', 'https://openalex.org/W4288624561', 'https://openalex.org/W4385245566']","We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.",1.0
SKG_DIA_146,https://openalex.org/W2759567155,2017,16,"['https://openalex.org/W1529399279', 'https://openalex.org/W1539975474', 'https://openalex.org/W1589064538', 'https://openalex.org/W1757796397', 'https://openalex.org/W1778387566', 'https://openalex.org/W1785674045', 'https://openalex.org/W1947758080', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W1989996186', 'https://openalex.org/W2054716580', 'https://openalex.org/W2062175565', 'https://openalex.org/W2117989772', 'https://openalex.org/W2119015791', 'https://openalex.org/W2121110499', 'https://openalex.org/W2168359464', 'https://openalex.org/W2186615578', 'https://openalex.org/W2250456405', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251221343', 'https://openalex.org/W2291723583', 'https://openalex.org/W2438667436', 'https://openalex.org/W2563829177', 'https://openalex.org/W2740191615', 'https://openalex.org/W2913340405', 'https://openalex.org/W2951176429', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963993502', 'https://openalex.org/W4298857966']","The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical. But in reality, due to the scarcity of real interaction data, the dialogue system usually grows slowly. Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is unsustainable. To accurately depict this, two quantitative metrics are proposed to assess safety and efficiency issues. For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher. Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.",1.0
SKG_DIA_147,https://openalex.org/W2970182362,2019,10,"['https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1847211030', 'https://openalex.org/W1909320841', 'https://openalex.org/W1959608418', 'https://openalex.org/W2080094394', 'https://openalex.org/W2108501770', 'https://openalex.org/W2125320996', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133012565', 'https://openalex.org/W2156876426', 'https://openalex.org/W2188365844', 'https://openalex.org/W2271840356', 'https://openalex.org/W2328886022', 'https://openalex.org/W2560512785', 'https://openalex.org/W2586756136', 'https://openalex.org/W2587284713', 'https://openalex.org/W2594978815', 'https://openalex.org/W2756540778', 'https://openalex.org/W2789033601', 'https://openalex.org/W2796335507', 'https://openalex.org/W2953046278', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963018920', 'https://openalex.org/W2963081964', 'https://openalex.org/W2963090522', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963443335', 'https://openalex.org/W2963600562', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964042872', 'https://openalex.org/W2964121744', 'https://openalex.org/W3022187094', 'https://openalex.org/W3121541553', 'https://openalex.org/W4297940714']","Jinxin Chang, Ruifang He, Longbiao Wang, Xiangyu Zhao, Ting Yang, Ruifang Wang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_149,https://openalex.org/W2951287343,2019,29,"['https://openalex.org/W295828404', 'https://openalex.org/W1522301498', 'https://openalex.org/W1527783480', 'https://openalex.org/W1591706642', 'https://openalex.org/W1975879668', 'https://openalex.org/W2100913937', 'https://openalex.org/W2102531443', 'https://openalex.org/W2128892113', 'https://openalex.org/W2153579005', 'https://openalex.org/W2170738476', 'https://openalex.org/W2256388387', 'https://openalex.org/W2296073425', 'https://openalex.org/W2339852062', 'https://openalex.org/W2418993857', 'https://openalex.org/W2521114121', 'https://openalex.org/W2561368124', 'https://openalex.org/W2767802162', 'https://openalex.org/W2786471719', 'https://openalex.org/W2798033004', 'https://openalex.org/W2798456655', 'https://openalex.org/W2807880213', 'https://openalex.org/W2809210859', 'https://openalex.org/W2885593519', 'https://openalex.org/W2891416139', 'https://openalex.org/W2892359699', 'https://openalex.org/W2908331278', 'https://openalex.org/W2949446780', 'https://openalex.org/W2951359136', 'https://openalex.org/W2962707484', 'https://openalex.org/W2962768358', 'https://openalex.org/W2962796276', 'https://openalex.org/W2962838727', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963386594', 'https://openalex.org/W2963780286', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963826056', 'https://openalex.org/W2963962154', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964092386', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964150246', 'https://openalex.org/W2964309167', 'https://openalex.org/W2964352131', 'https://openalex.org/W4294170691', 'https://openalex.org/W4300125564']","We study learning of a matching model for response selection in retrieval-based dialogue systems. The problem is equally important with designing the architecture of a model, but is less explored in existing literature. To learn a robust matching model from noisy training data, we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum. Under the framework, we simultaneously learn two matching models with independent training sets. In each iteration, one model transfers the knowledge learned from its training set to the other model, and at the same time receives the guide from the other model on how to overcome noise in training. Through being both a teacher and a student, the two models learn from each other and get improved together. Evaluation results on two public data sets indicate that the proposed learning approach can generally and significantly improve the performance of existing matching models.",1.0
SKG_DIA_150,https://openalex.org/W2963521540,2019,63,"['https://openalex.org/W295828404', 'https://openalex.org/W1518951372', 'https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W1958706068', 'https://openalex.org/W1970026646', 'https://openalex.org/W2095705004', 'https://openalex.org/W2099471712', 'https://openalex.org/W2119717200', 'https://openalex.org/W2133564696', 'https://openalex.org/W2159640018', 'https://openalex.org/W2161466446', 'https://openalex.org/W2170738476', 'https://openalex.org/W2399880602', 'https://openalex.org/W2521114121', 'https://openalex.org/W2539809671', 'https://openalex.org/W2586847566', 'https://openalex.org/W2593696076', 'https://openalex.org/W2605246398', 'https://openalex.org/W2611714756', 'https://openalex.org/W2740258984', 'https://openalex.org/W2795601629', 'https://openalex.org/W2798463315', 'https://openalex.org/W2798970679', 'https://openalex.org/W2801454967', 'https://openalex.org/W2807873315', 'https://openalex.org/W2885421725', 'https://openalex.org/W2897139265', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951359136', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962985882', 'https://openalex.org/W2963034998', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963958388', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W3022187094', 'https://openalex.org/W4230563027', 'https://openalex.org/W4300125564', 'https://openalex.org/W6635590879', 'https://openalex.org/W6642850889', 'https://openalex.org/W6698480918', 'https://openalex.org/W6727329663']","Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, Shuming Shi. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",0.9925925925925926
SKG_DIA_151,https://openalex.org/W3100938283,2020,2,"['https://openalex.org/W1522301498', 'https://openalex.org/W1975244201', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2250539671', 'https://openalex.org/W2293363371', 'https://openalex.org/W2604698497', 'https://openalex.org/W2604763608', 'https://openalex.org/W2798914047', 'https://openalex.org/W2808093377', 'https://openalex.org/W2889448364', 'https://openalex.org/W2889494558', 'https://openalex.org/W2949141958', 'https://openalex.org/W2962886331', 'https://openalex.org/W2963201498', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963789888', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970182362', 'https://openalex.org/W2970260827', 'https://openalex.org/W2970529793', 'https://openalex.org/W2997108628', 'https://openalex.org/W2998572029', 'https://openalex.org/W3034956542', 'https://openalex.org/W3101893606', 'https://openalex.org/W3102854726']","In this paper, we propose a meta-learning based semi-supervised explicit dialogue state tracker (SEDST) for neural dialogue generation, denoted as MEDST. Our main motivation is to further bridge the chasm between the need for high accuracy dialogue state tracker and the common reality that only scarce annotated data is available for most real-life dialogue tasks. Specifically, MEDST has two core steps: meta-training with adequate unlabelled data in an automatic way and meta-testing with a few annotated data by supervised learning. In particular, we enhance SEDST via entropy regularization, and investigate semi-supervised learning frameworks based on model-agnostic meta-learning (MAML) that are able to reduce the amount of required intermediate state labelling. We find that by leveraging un-annotated data in meta-way instead, the amount of dialogue state annotations can be reduced below 10% while maintaining equivalent system performance. Experimental results show MEDST outperforms SEDST substantially by 18.7% joint goal accuracy and 14.3% entity match rate on the KVRET corpus with 2% labelled data in semi-supervision.",1.0
SKG_DIA_153,https://openalex.org/W2963412005,2016,73,"['https://openalex.org/W179875071', 'https://openalex.org/W196214544', 'https://openalex.org/W1514535095', 'https://openalex.org/W1544827683', 'https://openalex.org/W1552182777', 'https://openalex.org/W1591706642', 'https://openalex.org/W1793121960', 'https://openalex.org/W1810943226', 'https://openalex.org/W1947758080', 'https://openalex.org/W1948566616', 'https://openalex.org/W1954715867', 'https://openalex.org/W1969152782', 'https://openalex.org/W1975244201', 'https://openalex.org/W1987326241', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107598941', 'https://openalex.org/W2115101920', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132679783', 'https://openalex.org/W2142086811', 'https://openalex.org/W2171928131', 'https://openalex.org/W2220374841', 'https://openalex.org/W2250297846', 'https://openalex.org/W2291723583', 'https://openalex.org/W2295953541', 'https://openalex.org/W2296073425', 'https://openalex.org/W2296712013', 'https://openalex.org/W2950178297', 'https://openalex.org/W2950635152', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962905474', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963295373', 'https://openalex.org/W2963606038', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964119254', 'https://openalex.org/W2964267515', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964325845', 'https://openalex.org/W2964352131']","Tsung-Hsien Wen, Milica Gašić, Nikola Mrkšić, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, Steve Young. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016.",1.0
SKG_DIA_154,https://openalex.org/W3035337525,2020,45,"['https://openalex.org/W1522301498', 'https://openalex.org/W1948566616', 'https://openalex.org/W2101105183', 'https://openalex.org/W2594726847', 'https://openalex.org/W2612675303', 'https://openalex.org/W2749436976', 'https://openalex.org/W2751124354', 'https://openalex.org/W2804010326', 'https://openalex.org/W2804491889', 'https://openalex.org/W2888779557', 'https://openalex.org/W2888849322', 'https://openalex.org/W2892248135', 'https://openalex.org/W2896457183', 'https://openalex.org/W2915295540', 'https://openalex.org/W2945475330', 'https://openalex.org/W2953071719', 'https://openalex.org/W2954492830', 'https://openalex.org/W2956474441', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963442512', 'https://openalex.org/W2963677766', 'https://openalex.org/W2963699608', 'https://openalex.org/W2963768805', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964044380', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218', 'https://openalex.org/W2979520138', 'https://openalex.org/W2988536330', 'https://openalex.org/W2988937804', 'https://openalex.org/W4288025838', 'https://openalex.org/W4295249402', 'https://openalex.org/W4385245566']","Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two shortcomings with such approaches. First, the inherent structures of multi-domain dialogue acts are neglected. Second, the semantic associations between acts and responses are not taken into account for response generation. To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently. Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed. We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively. Extensive experiments are conducted on the large-scale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.",1.0
SKG_DIA_155,https://openalex.org/W3092134391,2020,0,"['https://openalex.org/W1895577753', 'https://openalex.org/W1970260754', 'https://openalex.org/W2043909051', 'https://openalex.org/W2062812566', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112006025', 'https://openalex.org/W2130995195', 'https://openalex.org/W2154652894', 'https://openalex.org/W2327805699', 'https://openalex.org/W2507756961', 'https://openalex.org/W2548441712', 'https://openalex.org/W2562522356', 'https://openalex.org/W2606974598', 'https://openalex.org/W2612675303', 'https://openalex.org/W2688962481', 'https://openalex.org/W2739836857', 'https://openalex.org/W2800044557', 'https://openalex.org/W2810840719', 'https://openalex.org/W2890771450', 'https://openalex.org/W2892037875', 'https://openalex.org/W2900220550', 'https://openalex.org/W2900227126', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914204778', 'https://openalex.org/W2916898195', 'https://openalex.org/W2962717047', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963672599', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963945575', 'https://openalex.org/W2971274815', 'https://openalex.org/W2996287690', 'https://openalex.org/W2997020034', 'https://openalex.org/W3034758820', 'https://openalex.org/W3082274269', 'https://openalex.org/W3100111242', 'https://openalex.org/W3102649892', 'https://openalex.org/W3103944220', 'https://openalex.org/W3117055973']","In the context of chit-chat dialogues it has been shown that endowing systems with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such personas has thus far been limited to a fact-based representation (e.g. ""I have two cats.""). We argue that these representations remain superficial w.r.t. the complexity of human personality. In this work, we propose to make a step forward and investigate stance-based persona, trying to grasp more profound characteristics, such as opinions, values, and beliefs to drive language generation. To this end, we introduce a novel dataset allowing to explore different stance-based persona representations and their impact on claim generation, showing that they are able to grasp abstract and profound aspects of the author persona.",1.0
SKG_DIA_156,https://openalex.org/W2886305736,2018,197,"['https://openalex.org/W1808652302', 'https://openalex.org/W1839682376', 'https://openalex.org/W1975244201', 'https://openalex.org/W2062175565', 'https://openalex.org/W2145339207', 'https://openalex.org/W2168490009', 'https://openalex.org/W2169767637', 'https://openalex.org/W2175723363', 'https://openalex.org/W2417401578', 'https://openalex.org/W2518582440', 'https://openalex.org/W2529481617', 'https://openalex.org/W2604580630', 'https://openalex.org/W2623759943', 'https://openalex.org/W2765111838', 'https://openalex.org/W2772001136', 'https://openalex.org/W2772633986', 'https://openalex.org/W2788477667', 'https://openalex.org/W2798161840', 'https://openalex.org/W2949252816', 'https://openalex.org/W2950722229', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963993719', 'https://openalex.org/W3104546989']","Zhongyu Wei, Qianlong Liu, Baolin Peng, Huaixiao Tou, Ting Chen, Xuanjing Huang, Kam-fai Wong, Xiangying Dai. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2018.",1.0
SKG_DIA_158,https://openalex.org/W2510141903,2016,69,"['https://openalex.org/W169052826', 'https://openalex.org/W1551909886', 'https://openalex.org/W1576520375', 'https://openalex.org/W1842080548', 'https://openalex.org/W1973914601', 'https://openalex.org/W2008217938', 'https://openalex.org/W2024011160', 'https://openalex.org/W2038634595', 'https://openalex.org/W2038997150', 'https://openalex.org/W2064594469', 'https://openalex.org/W2077018496', 'https://openalex.org/W2097726431', 'https://openalex.org/W2098437222', 'https://openalex.org/W2099653665', 'https://openalex.org/W2101234009', 'https://openalex.org/W2104146993', 'https://openalex.org/W2127467141', 'https://openalex.org/W2138260386', 'https://openalex.org/W2139686264', 'https://openalex.org/W2142112646', 'https://openalex.org/W2157961599', 'https://openalex.org/W2158349948', 'https://openalex.org/W2162860143', 'https://openalex.org/W2165044314', 'https://openalex.org/W2250247764', 'https://openalex.org/W2250489604', 'https://openalex.org/W2251920663', 'https://openalex.org/W2251971374', 'https://openalex.org/W2263859238', 'https://openalex.org/W2294058101', 'https://openalex.org/W2405202646', 'https://openalex.org/W2419539795', 'https://openalex.org/W2489406233', 'https://openalex.org/W2949600092', 'https://openalex.org/W4231096565']","This paper is a novel study that views sarcasm detection in dialogue as a sequence labeling task, where a dialogue is made up of a sequence of utterances.We create a manuallylabeled dataset of dialogue from TV series 'Friends' annotated with sarcasm.Our goal is to predict sarcasm in each utterance, using sequential nature of a scene.We show performance gain using sequence labeling as compared to classification-based approaches.Our experiments are based on three sets of features, one is derived from information in our dataset, the other two are from past works.Two sequence labeling algorithms (SVM-HMM and SEARN) outperform three classification algorithms (SVM, Naive Bayes) for all these feature sets, with an increase in F-score of around 4%.Our observations highlight the viability of sequence labeling techniques for sarcasm detection of dialogue.",0.9770114942528736
SKG_DIA_159,https://openalex.org/W3087176213,2020,29,"['https://openalex.org/W138607541', 'https://openalex.org/W140467209', 'https://openalex.org/W592244745', 'https://openalex.org/W962073815', 'https://openalex.org/W1654173042', 'https://openalex.org/W1831406492', 'https://openalex.org/W1877570817', 'https://openalex.org/W1889081078', 'https://openalex.org/W1924770834', 'https://openalex.org/W1959608418', 'https://openalex.org/W1978470410', 'https://openalex.org/W1993378086', 'https://openalex.org/W2005900670', 'https://openalex.org/W2064675550', 'https://openalex.org/W2115615127', 'https://openalex.org/W2121738076', 'https://openalex.org/W2167702024', 'https://openalex.org/W2169218343', 'https://openalex.org/W2170323078', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251949648', 'https://openalex.org/W2930865789', 'https://openalex.org/W2934842096', 'https://openalex.org/W2947622595', 'https://openalex.org/W2949847915', 'https://openalex.org/W2951004968', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963790827', 'https://openalex.org/W2970726176', 'https://openalex.org/W3004162225', 'https://openalex.org/W3022187094', 'https://openalex.org/W3037738964', 'https://openalex.org/W4245826738', 'https://openalex.org/W4288345685', 'https://openalex.org/W4385245566']","Inducing a meaningful structural representation from one or a set of\ndialogues is a crucial but challenging task in computational linguistics.\nAdvancement made in this area is critical for dialogue system design and\ndiscourse analysis. It can also be extended to solve grammatical inference. In\nthis work, we propose to incorporate structured attention layers into a\nVariational Recurrent Neural Network (VRNN) model with discrete latent states\nto learn dialogue structure in an unsupervised fashion. Compared to a vanilla\nVRNN, structured attention enables a model to focus on different parts of the\nsource sentence embeddings while enforcing a structural inductive bias.\nExperiments show that on two-party dialogue datasets, VRNN with structured\nattention learns semantic structures that are similar to templates used to\ngenerate this dialogue corpus. While on multi-party dialogue datasets, our\nmodel learns an interactive structure demonstrating its capability of\ndistinguishing speakers or addresses, automatically disentangling dialogues\nwithout explicit human annotation.\n",1.0
SKG_DIA_160,https://openalex.org/W2889844776,2018,20,"['https://openalex.org/W10957333', 'https://openalex.org/W62710299', 'https://openalex.org/W154525918', 'https://openalex.org/W836999996', 'https://openalex.org/W1518951372', 'https://openalex.org/W1544827683', 'https://openalex.org/W1548360242', 'https://openalex.org/W1591607137', 'https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W1948566616', 'https://openalex.org/W1973674431', 'https://openalex.org/W1975244201', 'https://openalex.org/W1995945562', 'https://openalex.org/W2014571624', 'https://openalex.org/W2016522586', 'https://openalex.org/W2041065123', 'https://openalex.org/W2059857707', 'https://openalex.org/W2099603084', 'https://openalex.org/W2106547558', 'https://openalex.org/W2108366050', 'https://openalex.org/W2113850638', 'https://openalex.org/W2120804913', 'https://openalex.org/W2126891226', 'https://openalex.org/W2133459682', 'https://openalex.org/W2139501017', 'https://openalex.org/W2144717869', 'https://openalex.org/W2144960104', 'https://openalex.org/W2153501885', 'https://openalex.org/W2153579005', 'https://openalex.org/W2154652894', 'https://openalex.org/W2154764394', 'https://openalex.org/W2155898337', 'https://openalex.org/W2160219061', 'https://openalex.org/W2161466446', 'https://openalex.org/W2163068732', 'https://openalex.org/W2288380077', 'https://openalex.org/W2335122196', 'https://openalex.org/W2396614874', 'https://openalex.org/W2418993857', 'https://openalex.org/W2550936021', 'https://openalex.org/W2551396370', 'https://openalex.org/W2558809543', 'https://openalex.org/W2562335618', 'https://openalex.org/W2571175805', 'https://openalex.org/W2575870198', 'https://openalex.org/W2583186419', 'https://openalex.org/W2583816737', 'https://openalex.org/W2597655663', 'https://openalex.org/W2606982687', 'https://openalex.org/W2756923502', 'https://openalex.org/W2768661419', 'https://openalex.org/W2769997045', 'https://openalex.org/W2774005037', 'https://openalex.org/W2784820185', 'https://openalex.org/W2796084947', 'https://openalex.org/W2949615363', 'https://openalex.org/W2950697717', 'https://openalex.org/W2952982889', 'https://openalex.org/W2962684798', 'https://openalex.org/W2962707484', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963109634', 'https://openalex.org/W2963386218', 'https://openalex.org/W2963545239', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963800628', 'https://openalex.org/W2963834699', 'https://openalex.org/W2963890755', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964352131', 'https://openalex.org/W3022187094', 'https://openalex.org/W4294170691', 'https://openalex.org/W4310299640', 'https://openalex.org/W4317607572']","Current dialogue systems focus more on textual and speech context knowledge and are usually based on two speakers. Some recent work has investigated static image-based dialogue. However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers. To move closer towards such multimodal conversational skills and visually-situated applications, we introduce a new video-context, many-speaker dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv. This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history. For strong baselines, we also present several discriminative and generative models, e.g., based on tridirectional attention flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic phrase-matching metrics, as well as human evaluation studies. We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.",1.0
SKG_DIA_162,https://openalex.org/W2625113742,2017,51,"['https://openalex.org/W10957333', 'https://openalex.org/W1591706642', 'https://openalex.org/W1625390266', 'https://openalex.org/W1681299129', 'https://openalex.org/W1830940278', 'https://openalex.org/W1920532145', 'https://openalex.org/W1958706068', 'https://openalex.org/W2040123554', 'https://openalex.org/W2043985309', 'https://openalex.org/W2104067325', 'https://openalex.org/W2167149475', 'https://openalex.org/W2175723363', 'https://openalex.org/W2188321399', 'https://openalex.org/W2251058040', 'https://openalex.org/W2257979135', 'https://openalex.org/W2328886022', 'https://openalex.org/W2340944142', 'https://openalex.org/W2403702038', 'https://openalex.org/W2410983263', 'https://openalex.org/W2410985346', 'https://openalex.org/W2598220062', 'https://openalex.org/W2602275733', 'https://openalex.org/W2608175740', 'https://openalex.org/W2886444114', 'https://openalex.org/W2953119472', 'https://openalex.org/W2953158660', 'https://openalex.org/W2964119254', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564']","Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).",0.9917355371900828
SKG_DIA_164,https://openalex.org/W2970579055,2019,58,"['https://openalex.org/W295828404', 'https://openalex.org/W1518951372', 'https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W2025768430', 'https://openalex.org/W2064675550', 'https://openalex.org/W2133564696', 'https://openalex.org/W2161466446', 'https://openalex.org/W2170738476', 'https://openalex.org/W2183341477', 'https://openalex.org/W2521114121', 'https://openalex.org/W2539809671', 'https://openalex.org/W2584220694', 'https://openalex.org/W2586847566', 'https://openalex.org/W2611029872', 'https://openalex.org/W2616969219', 'https://openalex.org/W2739046565', 'https://openalex.org/W2788330850', 'https://openalex.org/W2798463315', 'https://openalex.org/W2803267010', 'https://openalex.org/W2807873315', 'https://openalex.org/W2808293489', 'https://openalex.org/W2885421725', 'https://openalex.org/W2888213795', 'https://openalex.org/W2889009749', 'https://openalex.org/W2890274659', 'https://openalex.org/W2949747155', 'https://openalex.org/W2949782788', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951359136', 'https://openalex.org/W2951824008', 'https://openalex.org/W2952723239', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963018920', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963939249', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964178377', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W4236521339', 'https://openalex.org/W4300125564', 'https://openalex.org/W4385245566']","Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Shuming Shi. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_165,https://openalex.org/W3100355408,2020,54,"['https://openalex.org/W1522301498', 'https://openalex.org/W2054192854', 'https://openalex.org/W2143017621', 'https://openalex.org/W2157331557', 'https://openalex.org/W2187089797', 'https://openalex.org/W2483215953', 'https://openalex.org/W2510955516', 'https://openalex.org/W2547875792', 'https://openalex.org/W2565378226', 'https://openalex.org/W2581637843', 'https://openalex.org/W2618825949', 'https://openalex.org/W2758912220', 'https://openalex.org/W2760062370', 'https://openalex.org/W2890719433', 'https://openalex.org/W2940009958', 'https://openalex.org/W2950018712', 'https://openalex.org/W2953320089', 'https://openalex.org/W2962917899', 'https://openalex.org/W2963078909', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963363122', 'https://openalex.org/W2963524705', 'https://openalex.org/W2963526187', 'https://openalex.org/W2963612262', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964119254', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964352131', 'https://openalex.org/W2972766508', 'https://openalex.org/W3013520104', 'https://openalex.org/W3015001695', 'https://openalex.org/W3031668985', 'https://openalex.org/W3103639864', 'https://openalex.org/W3104617516', 'https://openalex.org/W3117655171', 'https://openalex.org/W3181414820', 'https://openalex.org/W4285790115', 'https://openalex.org/W4288104702', 'https://openalex.org/W4294294142']","Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people’s gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.",1.0
SKG_DIA_166,https://openalex.org/W2798914047,2018,350,"['https://openalex.org/W10050918', 'https://openalex.org/W1767747848', 'https://openalex.org/W1905882502', 'https://openalex.org/W1969152782', 'https://openalex.org/W1975244201', 'https://openalex.org/W1993567041', 'https://openalex.org/W2061495585', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108806737', 'https://openalex.org/W2115101920', 'https://openalex.org/W2175723363', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251149342', 'https://openalex.org/W2251235149', 'https://openalex.org/W2464790259', 'https://openalex.org/W2470040910', 'https://openalex.org/W2552839021', 'https://openalex.org/W2594726847', 'https://openalex.org/W2759621817', 'https://openalex.org/W2919873176', 'https://openalex.org/W2949558714', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962905474', 'https://openalex.org/W2963043030', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963412005', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963993719', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964268978', 'https://openalex.org/W3121541553', 'https://openalex.org/W4385245566']",10.18653/v1/P18-1133,0.9949748743718592
SKG_DIA_167,https://openalex.org/W2760623195,2017,12,"['https://openalex.org/W77001256', 'https://openalex.org/W648786980', 'https://openalex.org/W1526096287', 'https://openalex.org/W1810943226', 'https://openalex.org/W2064675550', 'https://openalex.org/W2106226466', 'https://openalex.org/W2128970689', 'https://openalex.org/W2166637769', 'https://openalex.org/W2176263492', 'https://openalex.org/W2312383716', 'https://openalex.org/W2401527985', 'https://openalex.org/W2741675028', 'https://openalex.org/W2950304420', 'https://openalex.org/W2964036636', 'https://openalex.org/W2964139507']","This paper introduces a novel training/decoding strategy for sequence labeling.<br/>Instead of greedily choosing a label at each time step, and using it for<br/>the next prediction, we retain the probability distribution over the current label,<br/>and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural models, achieving state-of-the-art results on<br/>the MapTask and Switchboard corpora.",1.0
SKG_DIA_168,https://openalex.org/W2951450739,2019,29,"['https://openalex.org/W10957333', 'https://openalex.org/W1518951372', 'https://openalex.org/W1533861849', 'https://openalex.org/W1591706642', 'https://openalex.org/W1686810756', 'https://openalex.org/W1902237438', 'https://openalex.org/W1975244201', 'https://openalex.org/W1975879668', 'https://openalex.org/W2016589492', 'https://openalex.org/W2062989416', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133512280', 'https://openalex.org/W2133564696', 'https://openalex.org/W2154652894', 'https://openalex.org/W2157331557', 'https://openalex.org/W2183341477', 'https://openalex.org/W2293453011', 'https://openalex.org/W2296712013', 'https://openalex.org/W2412393473', 'https://openalex.org/W2558809543', 'https://openalex.org/W2583186419', 'https://openalex.org/W2603266952', 'https://openalex.org/W2605246398', 'https://openalex.org/W2729046720', 'https://openalex.org/W2740747242', 'https://openalex.org/W2768661419', 'https://openalex.org/W2785523195', 'https://openalex.org/W2808503835', 'https://openalex.org/W2897182555', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962764403', 'https://openalex.org/W2962814079', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963049308', 'https://openalex.org/W2963150162', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963287297', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963579811', 'https://openalex.org/W2963717374', 'https://openalex.org/W2963748384', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963917086', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964011461', 'https://openalex.org/W2964042872', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W2969576497', 'https://openalex.org/W3022187094', 'https://openalex.org/W3101313921', 'https://openalex.org/W4385245566']","Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems. The state-of-the-art dialogue systems are primarily based on unimodal sources, predominantly the text, and hence cannot capture the information present in the other sources such as videos, audios, images etc. With the availability of large scale multimodal dialogue dataset (MMD) (Saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user. Without capturing the information from both the text and image, the system will be incapable of generating correct and desirable responses. In this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance. Our evaluation shows that the proposed model can generate appropriate responses while preserving the position and attribute information. Experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics.",1.0
SKG_DIA_169,https://openalex.org/W3034446185,2020,114,"['https://openalex.org/W1599016936', 'https://openalex.org/W2086511124', 'https://openalex.org/W2561529111', 'https://openalex.org/W2606964149', 'https://openalex.org/W2608787653', 'https://openalex.org/W2742320045', 'https://openalex.org/W2747329762', 'https://openalex.org/W2794325560', 'https://openalex.org/W2798456655', 'https://openalex.org/W2804897457', 'https://openalex.org/W2891416139', 'https://openalex.org/W2912904516', 'https://openalex.org/W2919420119', 'https://openalex.org/W2925618549', 'https://openalex.org/W2948947170', 'https://openalex.org/W2949446780', 'https://openalex.org/W2950444459', 'https://openalex.org/W2952813980', 'https://openalex.org/W2962838727', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963159690', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963520511', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963995027', 'https://openalex.org/W2964207259', 'https://openalex.org/W2964223283', 'https://openalex.org/W2964309167', 'https://openalex.org/W2965373594', 'https://openalex.org/W2967674528', 'https://openalex.org/W2970780738', 'https://openalex.org/W2970960706', 'https://openalex.org/W2971034336', 'https://openalex.org/W2972324944', 'https://openalex.org/W3099023595', 'https://openalex.org/W3105492289', 'https://openalex.org/W4385245566']","Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams. Compared to previous benchmarks for non-task oriented dialogue systems, MuTual is much more challenging since it requires a model that be able to handle various reasoning problems. Empirical results show that state-of-the-art methods only reach 71%, which is far behind human performance of 94%, indicating that there is ample room for improving reasoning ability.",0.99009900990099
SKG_DIA_173,https://openalex.org/W2789327587,2018,8,"['https://openalex.org/W1757796397', 'https://openalex.org/W2040123554', 'https://openalex.org/W2154740693', 'https://openalex.org/W2160371091', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250558341', 'https://openalex.org/W2251058040', 'https://openalex.org/W2312609093', 'https://openalex.org/W2417401578', 'https://openalex.org/W2558661633', 'https://openalex.org/W2772217324', 'https://openalex.org/W2887703723', 'https://openalex.org/W2964044380', 'https://openalex.org/W6701898310']","Reinforcement learning (RL) is a promising approach to solve dialogue policy optimisation. Traditional RL algorithms, however, fail to scale to large domains due to the curse of dimensionality. We propose a novel Dialogue Management architecture, based on Feudal RL, which decomposes the decision into two steps; a first step where a master policy selects a subset of primitive actions, and a second step where a primitive action is chosen from the selected subset. The structural information included in the domain ontology is used to abstract the dialogue state space, taking the decisions at each step using different parts of the abstracted state. This, combined with an information sharing mechanism between slots, increases the scalability to large domains. We show that an implementation of this approach, based on Deep-Q Networks, significantly outperforms previous state of the art in several dialogue domains and environments, without the need of any additional reward signal.",1.0
SKG_DIA_177,https://openalex.org/W3102037269,2020,29,"['https://openalex.org/W1514535095', 'https://openalex.org/W1522301498', 'https://openalex.org/W1522734439', 'https://openalex.org/W1533861849', 'https://openalex.org/W1575833922', 'https://openalex.org/W1933349210', 'https://openalex.org/W1947481528', 'https://openalex.org/W1956340063', 'https://openalex.org/W1957740064', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2131774270', 'https://openalex.org/W2154652894', 'https://openalex.org/W2156303437', 'https://openalex.org/W2183341477', 'https://openalex.org/W2194775991', 'https://openalex.org/W2337252826', 'https://openalex.org/W2342662179', 'https://openalex.org/W2507756961', 'https://openalex.org/W2508429489', 'https://openalex.org/W2526050071', 'https://openalex.org/W2526449353', 'https://openalex.org/W2549139847', 'https://openalex.org/W2560730294', 'https://openalex.org/W2565656701', 'https://openalex.org/W2606982687', 'https://openalex.org/W2810643877', 'https://openalex.org/W2810840719', 'https://openalex.org/W2891732163', 'https://openalex.org/W2904291752', 'https://openalex.org/W2904452845', 'https://openalex.org/W2905141912', 'https://openalex.org/W2937305817', 'https://openalex.org/W2938555542', 'https://openalex.org/W2948048211', 'https://openalex.org/W2951390634', 'https://openalex.org/W2954199749', 'https://openalex.org/W2956387449', 'https://openalex.org/W2962934715', 'https://openalex.org/W2962949233', 'https://openalex.org/W2963383024', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963524571', 'https://openalex.org/W2963529931', 'https://openalex.org/W2963541336', 'https://openalex.org/W2963576560', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963823251', 'https://openalex.org/W2963890755', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964213933', 'https://openalex.org/W2964218959', 'https://openalex.org/W2968553732', 'https://openalex.org/W2972745026', 'https://openalex.org/W2997344006', 'https://openalex.org/W2997805943', 'https://openalex.org/W2998166190', 'https://openalex.org/W2998476971', 'https://openalex.org/W3016211260', 'https://openalex.org/W3034730770', 'https://openalex.org/W3099388488', 'https://openalex.org/W4306716473', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.",0.9932885906040269
SKG_DIA_178,https://openalex.org/W2970168865,2019,23,"['https://openalex.org/W184147902', 'https://openalex.org/W1614298861', 'https://openalex.org/W1980287119', 'https://openalex.org/W2064675550', 'https://openalex.org/W2083306472', 'https://openalex.org/W2126581182', 'https://openalex.org/W2133288557', 'https://openalex.org/W2235475559', 'https://openalex.org/W2464050240', 'https://openalex.org/W2470673105', 'https://openalex.org/W2568951855', 'https://openalex.org/W2741609276', 'https://openalex.org/W2757541972', 'https://openalex.org/W2767329425', 'https://openalex.org/W2769436106', 'https://openalex.org/W2788618579', 'https://openalex.org/W2798600398', 'https://openalex.org/W2798966390', 'https://openalex.org/W2808182015', 'https://openalex.org/W2892217184', 'https://openalex.org/W2899455119', 'https://openalex.org/W2950883469', 'https://openalex.org/W2963240575', 'https://openalex.org/W2964164368', 'https://openalex.org/W2964300796', 'https://openalex.org/W2964871493', 'https://openalex.org/W2965510113']","Customers ask questions and customer service staffs answer their questions, which is the basic service model via multi-turn customer service (CS) dialogues on E-commerce platforms. Existing studies fail to provide comprehensive service satisfaction analysis, namely satisfaction polarity classification (e.g., well satisfied, met and unsatisfied) and sentimental utterance identification (e.g., positive, neutral and negative). In this paper, we conduct a pilot study on the task of service satisfaction analysis (SSA) based on multi-turn CS dialogues. We propose an extensible Context-Assisted Multiple Instance Learning (CAMIL) model to predict the sentiments of all the customer utterances and then aggregate those sentiments into service satisfaction polarity. After that, we propose a novel Context Clue Matching Mechanism (CCMM) to enhance the representations of all customer utterances with their matched context clues, i.e., sentiment and reasoning clues. We construct two CS dialogue datasets from a top E-commerce platform. Extensive experimental results are presented and contrasted against a few previous models to demonstrate the efficacy of our model.",1.0
SKG_DIA_179,https://openalex.org/W2963140401,2018,39,"['https://openalex.org/W1492935830', 'https://openalex.org/W1556824961', 'https://openalex.org/W1586944634', 'https://openalex.org/W2022677886', 'https://openalex.org/W2062175565', 'https://openalex.org/W2064675550', 'https://openalex.org/W2090170171', 'https://openalex.org/W2109910161', 'https://openalex.org/W2114451917', 'https://openalex.org/W2132997613', 'https://openalex.org/W2133458291', 'https://openalex.org/W2135909747', 'https://openalex.org/W2140016149', 'https://openalex.org/W2143435603', 'https://openalex.org/W2145339207', 'https://openalex.org/W2153385324', 'https://openalex.org/W2160808139', 'https://openalex.org/W2288878529', 'https://openalex.org/W2312609093', 'https://openalex.org/W2412899141', 'https://openalex.org/W2516334389', 'https://openalex.org/W2550612212', 'https://openalex.org/W2558661633', 'https://openalex.org/W2571927164', 'https://openalex.org/W2592215206', 'https://openalex.org/W2592647456', 'https://openalex.org/W2594726847', 'https://openalex.org/W2604698497', 'https://openalex.org/W2765111838', 'https://openalex.org/W2772001136', 'https://openalex.org/W2783543950', 'https://openalex.org/W2807142242', 'https://openalex.org/W2949252816', 'https://openalex.org/W2949267040', 'https://openalex.org/W2950040888', 'https://openalex.org/W2962776342', 'https://openalex.org/W2963043030', 'https://openalex.org/W2963077280', 'https://openalex.org/W2963262099', 'https://openalex.org/W2964080167', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964227312', 'https://openalex.org/W3029062788', 'https://openalex.org/W3104546989', 'https://openalex.org/W4214717370']","Developing agents to engage in complex goal-oriented dialogues is challenging partly because the main learning signals are very sparse in long conversations. In this paper, we propose a divide-and-conquer approach that discovers and exploits the hidden structure of the task to enable efficient policy learning. First, given successful example dialogues, we propose the Subgoal Discovery Network (SDN) to divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised fashion. We then use these subgoals to learn a multi-level policy by hierarchical reinforcement learning. We demonstrate our method by building a dialogue agent for the composite task of travel planning. Experiments with simulated and real users show that our approach performs competitively against a state-of-the-art method that requires human-defined subgoals. Moreover, we show that the learned subgoals are often human comprehensible.",1.0
SKG_DIA_180,https://openalex.org/W3105049371,2020,7,"['https://openalex.org/W2014415866', 'https://openalex.org/W2085030399', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104049510', 'https://openalex.org/W2123301721', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131876387', 'https://openalex.org/W2136189984', 'https://openalex.org/W2339852062', 'https://openalex.org/W2539671052', 'https://openalex.org/W2547875792', 'https://openalex.org/W2586847566', 'https://openalex.org/W2610935556', 'https://openalex.org/W2741363662', 'https://openalex.org/W2885421725', 'https://openalex.org/W2891103209', 'https://openalex.org/W2898875342', 'https://openalex.org/W2936695845', 'https://openalex.org/W2950902819', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964458951', 'https://openalex.org/W2986867746', 'https://openalex.org/W2995183464', 'https://openalex.org/W3011801489', 'https://openalex.org/W4299606006', 'https://openalex.org/W4385245566']","This paper is concerned with improving dialogue generation models through injection of knowledge, e.g., content relevant to the post that can increase the quality of responses. Past research extends the training of the generative models by incorporating statistical properties of posts, responses and related knowledge, without explicitly assessing the knowledge quality. In our work, we demonstrate the importance of knowledge relevance and adopt a two-phase approach. We first apply a novel method, Transformer & Post based Posterior Approximation (TPPA) to select knowledge, and then use the Transformer with Expanded Decoder (TED) model to generate responses from both the post and the knowledge. TPPA method processes posts, post related knowledge, and response related knowledge at both word and sentence level. Our experiments with the TED generative model demonstrate the effectiveness of TPPA as it outperforms a set of strong baseline models. Our TPPA method is extendable and supports further optimization of knowledge retrieval and injection.",1.0
SKG_DIA_182,https://openalex.org/W2110633879,2012,37,"['https://openalex.org/W47495561', 'https://openalex.org/W102921666', 'https://openalex.org/W178161411', 'https://openalex.org/W200223693', 'https://openalex.org/W1487640415', 'https://openalex.org/W1515851193', 'https://openalex.org/W1584761227', 'https://openalex.org/W1598875788', 'https://openalex.org/W1630833108', 'https://openalex.org/W1681299129', 'https://openalex.org/W1769776978', 'https://openalex.org/W1944068894', 'https://openalex.org/W1974821254', 'https://openalex.org/W1986532700', 'https://openalex.org/W2021151961', 'https://openalex.org/W2023612782', 'https://openalex.org/W2024785476', 'https://openalex.org/W2026431544', 'https://openalex.org/W2067097374', 'https://openalex.org/W2079833689', 'https://openalex.org/W2102573550', 'https://openalex.org/W2115101920', 'https://openalex.org/W2119631826', 'https://openalex.org/W2121517924', 'https://openalex.org/W2129955048', 'https://openalex.org/W2132997613', 'https://openalex.org/W2134466368', 'https://openalex.org/W2140188190', 'https://openalex.org/W2149029524', 'https://openalex.org/W2153385324', 'https://openalex.org/W2157134106', 'https://openalex.org/W2157365695', 'https://openalex.org/W2160219061', 'https://openalex.org/W2169430966', 'https://openalex.org/W2312609093', 'https://openalex.org/W2914679068', 'https://openalex.org/W3099293669']","Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%. 1",1.0
SKG_DIA_185,https://openalex.org/W3099140977,2020,26,"['https://openalex.org/W1522301498', 'https://openalex.org/W1785674045', 'https://openalex.org/W2250297846', 'https://openalex.org/W2251355666', 'https://openalex.org/W2741361549', 'https://openalex.org/W2798367796', 'https://openalex.org/W2891732163', 'https://openalex.org/W2945475330', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2979400990', 'https://openalex.org/W2988252747', 'https://openalex.org/W3021096583', 'https://openalex.org/W3119649668', 'https://openalex.org/W4288288848', 'https://openalex.org/W4385245566']","Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of ontology, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30% ontology is used, VN can also contribute to our model.",1.0
SKG_DIA_186,https://openalex.org/W3034573951,2020,187,"['https://openalex.org/W1522301498', 'https://openalex.org/W2095705004', 'https://openalex.org/W2157331557', 'https://openalex.org/W2251058040', 'https://openalex.org/W2606974598', 'https://openalex.org/W2779809129', 'https://openalex.org/W2798367796', 'https://openalex.org/W2798914047', 'https://openalex.org/W2895976713', 'https://openalex.org/W2896457183', 'https://openalex.org/W2945475330', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2970404807', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W2996317432', 'https://openalex.org/W3005419354', 'https://openalex.org/W3008966357', 'https://openalex.org/W3121541553', 'https://openalex.org/W4288094254', 'https://openalex.org/W4289147179']","Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.",1.0
SKG_DIA_187,https://openalex.org/W3102445752,2020,21,"['https://openalex.org/W1522301498', 'https://openalex.org/W1793121960', 'https://openalex.org/W1821462560', 'https://openalex.org/W1902237438', 'https://openalex.org/W1924770834', 'https://openalex.org/W1975244201', 'https://openalex.org/W1975879668', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113207845', 'https://openalex.org/W2148952635', 'https://openalex.org/W2153579005', 'https://openalex.org/W2294370754', 'https://openalex.org/W2438667436', 'https://openalex.org/W2559094423', 'https://openalex.org/W2711861986', 'https://openalex.org/W2729046720', 'https://openalex.org/W2743289088', 'https://openalex.org/W2788836009', 'https://openalex.org/W2798914047', 'https://openalex.org/W2950009015', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963084599', 'https://openalex.org/W2963248455', 'https://openalex.org/W2963412005', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963789888', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218', 'https://openalex.org/W2970260827', 'https://openalex.org/W2975451647', 'https://openalex.org/W3106274079', 'https://openalex.org/W4294170691', 'https://openalex.org/W4295249402', 'https://openalex.org/W4297728544']","The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a ""Two-Teacher One-Student"" learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously. TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network). Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network. Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student. The usage of discriminators relaxes the rigid coupling between the student and teachers. Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods.",1.0
SKG_DIA_188,https://openalex.org/W2970418174,2019,36,"['https://openalex.org/W4919037', 'https://openalex.org/W267862395', 'https://openalex.org/W836999996', 'https://openalex.org/W1982477054', 'https://openalex.org/W2016522586', 'https://openalex.org/W2099471712', 'https://openalex.org/W2119717200', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141125852', 'https://openalex.org/W2155027007', 'https://openalex.org/W2156163116', 'https://openalex.org/W2163605009', 'https://openalex.org/W2170240176', 'https://openalex.org/W2176263492', 'https://openalex.org/W2251658415', 'https://openalex.org/W2418993857', 'https://openalex.org/W2604262106', 'https://openalex.org/W2733239165', 'https://openalex.org/W2740149041', 'https://openalex.org/W2751049198', 'https://openalex.org/W2751134959', 'https://openalex.org/W2767019926', 'https://openalex.org/W2770173563', 'https://openalex.org/W2775795276', 'https://openalex.org/W2786273134', 'https://openalex.org/W2796084947', 'https://openalex.org/W2804047946', 'https://openalex.org/W2888519496', 'https://openalex.org/W2890719433', 'https://openalex.org/W2951751045', 'https://openalex.org/W2962707484', 'https://openalex.org/W2962721878', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963012544', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963352069', 'https://openalex.org/W2963552443', 'https://openalex.org/W2963655793', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963774520', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963858765', 'https://openalex.org/W2964081807', 'https://openalex.org/W2964222296', 'https://openalex.org/W2964308564', 'https://openalex.org/W3022187094', 'https://openalex.org/W4295727797', 'https://openalex.org/W4320013936']","Tong Niu, Mohit Bansal. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_191,https://openalex.org/W3035470414,2020,46,"['https://openalex.org/W1522301498', 'https://openalex.org/W1785674045', 'https://openalex.org/W1924770834', 'https://openalex.org/W2095705004', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251355666', 'https://openalex.org/W2507756961', 'https://openalex.org/W2556468274', 'https://openalex.org/W2798367796', 'https://openalex.org/W2934890006', 'https://openalex.org/W2945475330', 'https://openalex.org/W2962847367', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963970400', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2972892266', 'https://openalex.org/W4289147179']","Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information's interference and improve long dialogue context tracking. Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module. Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset.",0.9933774834437086
SKG_DIA_193,https://openalex.org/W2988157054,2019,57,"['https://openalex.org/W150462035', 'https://openalex.org/W192515608', 'https://openalex.org/W783082550', 'https://openalex.org/W1503312748', 'https://openalex.org/W1522301498', 'https://openalex.org/W1526096287', 'https://openalex.org/W1542146969', 'https://openalex.org/W1880262756', 'https://openalex.org/W1991133427', 'https://openalex.org/W2008689742', 'https://openalex.org/W2038043464', 'https://openalex.org/W2128970689', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148830595', 'https://openalex.org/W2167212741', 'https://openalex.org/W2171986392', 'https://openalex.org/W2250539671', 'https://openalex.org/W2252245646', 'https://openalex.org/W2573626026', 'https://openalex.org/W2624871570', 'https://openalex.org/W2757599232', 'https://openalex.org/W2761590056', 'https://openalex.org/W2803437449', 'https://openalex.org/W2884240866', 'https://openalex.org/W2931751229', 'https://openalex.org/W2962902328', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963550483', 'https://openalex.org/W2963765493', 'https://openalex.org/W2964036636', 'https://openalex.org/W2964089584', 'https://openalex.org/W2964106094', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964331270', 'https://openalex.org/W4212995409']","Recognising dialogue acts (DA) is important for many natural language processing tasks such as dialogue generation and intention recognition. In this paper, we propose a dual-attention hierarchical recurrent neural network for DA classification. Our model is partially inspired by the observation that conversational utterances are normally associated with both a DA and a topic, where the former captures the social act and the latter describes the subject matter. However, such a dependency between DAs and topics has not been utilised by most existing systems for DA classification. With a novel dual task-specific attention mechanism, our model is able, for utterances, to capture information about both DAs and topics, as well as information about the interactions between them. Experimental results show that by modelling topic as an auxiliary task, our model can significantly improve DA classification, yielding better or comparable performance to the state-of-the-art method on three public datasets.",1.0
SKG_DIA_196,https://openalex.org/W2971066408,2019,48,"['https://openalex.org/W12821309', 'https://openalex.org/W1533861849', 'https://openalex.org/W1808652302', 'https://openalex.org/W1940872118', 'https://openalex.org/W1991133427', 'https://openalex.org/W2045016337', 'https://openalex.org/W2053769007', 'https://openalex.org/W2064675550', 'https://openalex.org/W2120757740', 'https://openalex.org/W2133012565', 'https://openalex.org/W2134036914', 'https://openalex.org/W2147880316', 'https://openalex.org/W2158899491', 'https://openalex.org/W2160379455', 'https://openalex.org/W2169099542', 'https://openalex.org/W2798161840', 'https://openalex.org/W2851886341', 'https://openalex.org/W2884705011', 'https://openalex.org/W2886305736', 'https://openalex.org/W2925613093', 'https://openalex.org/W2949952998', 'https://openalex.org/W2952087486', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962902328', 'https://openalex.org/W2963672540']","Xinzhu Lin, Xiahui He, Qin Chen, Huaixiao Tou, Zhongyu Wei, Ting Chen. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_197,https://openalex.org/W3038061735,2020,22,"['https://openalex.org/W131229082', 'https://openalex.org/W1521764462', 'https://openalex.org/W1522301498', 'https://openalex.org/W1540372287', 'https://openalex.org/W1548953971', 'https://openalex.org/W2009434747', 'https://openalex.org/W2044249691', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123159101', 'https://openalex.org/W2125320996', 'https://openalex.org/W2133553328', 'https://openalex.org/W2163640453', 'https://openalex.org/W2165079349', 'https://openalex.org/W2193006414', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251591847', 'https://openalex.org/W2294475924', 'https://openalex.org/W2553193462', 'https://openalex.org/W2620761940', 'https://openalex.org/W2740716165', 'https://openalex.org/W2759361123', 'https://openalex.org/W2785847546', 'https://openalex.org/W2886305736', 'https://openalex.org/W2936695845', 'https://openalex.org/W2941035428', 'https://openalex.org/W2962849532', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964046741', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964652672', 'https://openalex.org/W2996403597', 'https://openalex.org/W2999995091', 'https://openalex.org/W3098682828', 'https://openalex.org/W3103120164', 'https://openalex.org/W3103692324', 'https://openalex.org/W4235418491', 'https://openalex.org/W4285719527', 'https://openalex.org/W4295249402']","One-to-one tutoring is often an effective means to help students learn, and recent experiments with neural conversation systems are promising. However, large open datasets of tutoring conversations are lacking. To remedy this, we propose a novel asynchronous method for collecting tutoring dialogue via crowdworkers that is both amenable to the needs of deep learning algorithms and reflective of pedagogical concerns. In this approach, extended conversations are obtained between crowdworkers role-playing as both students and tutors. The CIMA collection, which we make publicly available, is novel in that students are exposed to overlapping grounded concepts between exercises and multiple relevant tutoring responses are collected for the same input. CIMA contains several compelling properties from an educational perspective: student role-players complete exercises in fewer turns during the course of the conversation and tutor players adopt strategies that conform with some educational conversational norms, such as providing hints versus asking questions in appropriate contexts. The dataset enables a model to be trained to generate the next tutoring utterance in a conversation, conditioned on a provided action strategy.",0.9908256880733946
SKG_DIA_200,https://openalex.org/W2970377674,2019,8,"['https://openalex.org/W8550301', 'https://openalex.org/W1607983314', 'https://openalex.org/W2053154970', 'https://openalex.org/W2108598243', 'https://openalex.org/W2113207145', 'https://openalex.org/W2251058040', 'https://openalex.org/W2889470921', 'https://openalex.org/W2891732163', 'https://openalex.org/W2907283777', 'https://openalex.org/W2964006684']","Edward Collins, Nikolai Rozanov, Bingbing Zhang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations. 2019.",0.9894736842105264
SKG_DIA_202,https://openalex.org/W2931751229,2019,60,"['https://openalex.org/W27394811', 'https://openalex.org/W1486649854', 'https://openalex.org/W1576632330', 'https://openalex.org/W2128970689', 'https://openalex.org/W2131744502', 'https://openalex.org/W2146785422', 'https://openalex.org/W2250539671', 'https://openalex.org/W2401527985', 'https://openalex.org/W2470673105', 'https://openalex.org/W2573626026', 'https://openalex.org/W2576526989', 'https://openalex.org/W2741675028', 'https://openalex.org/W2757599232', 'https://openalex.org/W2786396726', 'https://openalex.org/W2901101385', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962803631', 'https://openalex.org/W2962902328', 'https://openalex.org/W2963386218', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963550483', 'https://openalex.org/W2963625095', 'https://openalex.org/W2963765493', 'https://openalex.org/W2964036636', 'https://openalex.org/W2964106094', 'https://openalex.org/W2964139507', 'https://openalex.org/W2964189376', 'https://openalex.org/W2964331270', 'https://openalex.org/W2988157054']",Recent work in Dialogue Act classification has treated the task as a sequence labeling problem using hierarchical deep neural networks. We build on this prior work by leveraging the effectiveness of a context-aware self-attention mechanism coupled with a hierarchical recurrent neural network. We conduct extensive evaluations on standard Dialogue Act classification datasets and show significant improvement over state-of-the-art results on the Switchboard Dialogue Act (SwDA) Corpus. We also investigate the impact of different utterance-level representation learning methods and show that our method is effective at capturing utterance-level semantic text representations while maintaining high accuracy.,1.0
SKG_DIA_203,https://openalex.org/W2945554776,2019,21,"['https://openalex.org/W635530177', 'https://openalex.org/W759515131', 'https://openalex.org/W1518951372', 'https://openalex.org/W1976434636', 'https://openalex.org/W1999374445', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111305191', 'https://openalex.org/W2123301721', 'https://openalex.org/W2130942839', 'https://openalex.org/W2149741699', 'https://openalex.org/W2154652894', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250571530', 'https://openalex.org/W2261586594', 'https://openalex.org/W2411447339', 'https://openalex.org/W2463562195', 'https://openalex.org/W2474824677', 'https://openalex.org/W2525450212', 'https://openalex.org/W2557436004', 'https://openalex.org/W2573519875', 'https://openalex.org/W2581637843', 'https://openalex.org/W2604444020', 'https://openalex.org/W2734443755', 'https://openalex.org/W2784400615', 'https://openalex.org/W2787711783', 'https://openalex.org/W2798888952', 'https://openalex.org/W2805005636', 'https://openalex.org/W2806120502', 'https://openalex.org/W2807791032', 'https://openalex.org/W2890274659', 'https://openalex.org/W2890276793', 'https://openalex.org/W2962883166', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963161084', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963283805', 'https://openalex.org/W2963353834', 'https://openalex.org/W2963371447', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963918774', 'https://openalex.org/W2964042872', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964134121', 'https://openalex.org/W2964137876', 'https://openalex.org/W2964178377', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W2972052008']","Wei-Jen Ko, Greg Durrett, Junyi Jessy Li. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_DIA_204,https://openalex.org/W2565875961,2016,126,"['https://openalex.org/W44815768', 'https://openalex.org/W147964346', 'https://openalex.org/W1492025072', 'https://openalex.org/W1501669607', 'https://openalex.org/W1524333225', 'https://openalex.org/W1832693441', 'https://openalex.org/W1932847118', 'https://openalex.org/W2085662862', 'https://openalex.org/W2120615054', 'https://openalex.org/W2153579005', 'https://openalex.org/W2153635508', 'https://openalex.org/W2158899491', 'https://openalex.org/W2251321385', 'https://openalex.org/W2402040266', 'https://openalex.org/W2423024114', 'https://openalex.org/W2489406233', 'https://openalex.org/W2607154244', 'https://openalex.org/W2611669587', 'https://openalex.org/W2951714314', 'https://openalex.org/W2952230511', 'https://openalex.org/W2963921497', 'https://openalex.org/W3120421331', 'https://openalex.org/W4294170691', 'https://openalex.org/W4301100068']","In this paper, we describe our approach of enabling an interactive dialogue system to recognize user emotion and sentiment in realtime. These modules allow otherwise conventional dialogue systems to have “empathy” and answer to the user while being aware of their emotion and intent. Emotion recognition from speech previously consists of feature engineering and machine learning where the first stage causes delay in decoding time. We describe a CNN model to extract emotion from raw speech input without feature engineering. This approach even achieves an impressive average of 65.7% accuracy on six emotion categories, a 4.5% improvement when compared to the conventional feature based SVM classification. A separate, CNN-based sentiment analysis module recognizes sentiments from speech recognition results, with 82.5 Fmeasure on human-machine dialogues when trained with out-of-domain data.",1.0
SKG_DIA_206,https://openalex.org/W3034408002,2020,48,"['https://openalex.org/W1940872118', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2133012565', 'https://openalex.org/W2153579005', 'https://openalex.org/W2158899491', 'https://openalex.org/W2168041406', 'https://openalex.org/W2515682654', 'https://openalex.org/W2538374209', 'https://openalex.org/W2551396370', 'https://openalex.org/W2798456655', 'https://openalex.org/W2798858969', 'https://openalex.org/W2806237610', 'https://openalex.org/W2891416139', 'https://openalex.org/W2962902328', 'https://openalex.org/W2963108794', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2971066408']","Electronic Medical Records (EMRs) have become key components of modern medical care systems. Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation. We then propose a Medical Information Extractor (MIE) towards medical dialogues. MIE is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status. To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account. The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues.",0.991869918699187
SKG_DIA_209,https://openalex.org/W2157134106,2010,43,"['https://openalex.org/W160067033', 'https://openalex.org/W163988823', 'https://openalex.org/W192597414', 'https://openalex.org/W605575788', 'https://openalex.org/W1494346129', 'https://openalex.org/W1515851193', 'https://openalex.org/W1536009700', 'https://openalex.org/W1584761227', 'https://openalex.org/W1601121220', 'https://openalex.org/W1695997110', 'https://openalex.org/W1974821254', 'https://openalex.org/W1980340273', 'https://openalex.org/W1986957457', 'https://openalex.org/W1997319212', 'https://openalex.org/W2000189111', 'https://openalex.org/W2025989261', 'https://openalex.org/W2030721126', 'https://openalex.org/W2062175565', 'https://openalex.org/W2107726111', 'https://openalex.org/W2108654132', 'https://openalex.org/W2117989772', 'https://openalex.org/W2124003719', 'https://openalex.org/W2149029524', 'https://openalex.org/W2154124305', 'https://openalex.org/W2166490941', 'https://openalex.org/W2168490009', 'https://openalex.org/W2911283634', 'https://openalex.org/W2914656440']","We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon ’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the system learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6 % average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise. 1",0.9946524064171124
SKG_DIA_210,https://openalex.org/W1796449931,2010,28,"['https://openalex.org/W1502293651', 'https://openalex.org/W1529752812', 'https://openalex.org/W1631260214', 'https://openalex.org/W1970961429', 'https://openalex.org/W1985270055', 'https://openalex.org/W2007261869', 'https://openalex.org/W2020755048', 'https://openalex.org/W2031871242', 'https://openalex.org/W2037442428', 'https://openalex.org/W2042585112', 'https://openalex.org/W2044268022', 'https://openalex.org/W2056513832', 'https://openalex.org/W2086458350', 'https://openalex.org/W2105024318', 'https://openalex.org/W2113123608', 'https://openalex.org/W2113272343', 'https://openalex.org/W2119609990', 'https://openalex.org/W2126956557', 'https://openalex.org/W2137066512', 'https://openalex.org/W2138476326', 'https://openalex.org/W2152907450', 'https://openalex.org/W2168041474', 'https://openalex.org/W2594610113']","In situated dialogue humans often utter linguistic expressions that refer to extralinguistic entities in the environment. Correctly resolving these references is critical yet challenging for artificial agents partly due to their limited speech recognition and language understanding capabilities. Motivated by psycholinguistic studies demonstrating a tight link between language production and human eye gaze, we have developed approaches that integrate naturally occurring human eye gaze with speech recognition hypotheses to resolve exophoric references in situated dialogue in a virtual world. In addition to incorporating eye gaze with the best recognized spoken hypothesis, we developed an algorithm to also handle multiple hypotheses modeled as word confusion networks. Our empirical results demonstrate that incorporating eye gaze with recognition hypotheses consistently outperforms the results obtained from processing recognition hypotheses alone. Incorporating eye gaze with word confusion networks further improves performance. 1",1.0
SKG_DIA_211,https://openalex.org/W2251949648,2014,56,"['https://openalex.org/W190230800', 'https://openalex.org/W194972339', 'https://openalex.org/W962073815', 'https://openalex.org/W1483307070', 'https://openalex.org/W1568327918', 'https://openalex.org/W1654173042', 'https://openalex.org/W1681299129', 'https://openalex.org/W1831406492', 'https://openalex.org/W1880262756', 'https://openalex.org/W1974654183', 'https://openalex.org/W1980340273', 'https://openalex.org/W1985125789', 'https://openalex.org/W1985514943', 'https://openalex.org/W2001082470', 'https://openalex.org/W2033593667', 'https://openalex.org/W2042096436', 'https://openalex.org/W2048064382', 'https://openalex.org/W2080972498', 'https://openalex.org/W2085623775', 'https://openalex.org/W2099873701', 'https://openalex.org/W2101308260', 'https://openalex.org/W2103339462', 'https://openalex.org/W2106918957', 'https://openalex.org/W2115979064', 'https://openalex.org/W2118370253', 'https://openalex.org/W2124585778', 'https://openalex.org/W2128970689', 'https://openalex.org/W2132997613', 'https://openalex.org/W2137636595', 'https://openalex.org/W2137877387', 'https://openalex.org/W2138742901', 'https://openalex.org/W2144100511', 'https://openalex.org/W2165599843', 'https://openalex.org/W2169218343', 'https://openalex.org/W2170323078', 'https://openalex.org/W2171802301', 'https://openalex.org/W2172125983', 'https://openalex.org/W2172135926', 'https://openalex.org/W2340541798', 'https://openalex.org/W3037265734']","A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue, since it provides a basis for analysing, evaluating, and building conversational systems. We propose three new unsupervised models to discover latent structures in task-oriented dialogues. Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states). We apply them to two real, non-trivial datasets: human-computer spoken dialogues in bus query service, and humanhuman text-based chats from a live technical support service. We show that our models extract meaningful state representations and dialogue structures consistent with human annotations. Quantitatively, we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task.",1.0
SKG_DIA_213,https://openalex.org/W2110450943,2011,53,"['https://openalex.org/W24147957', 'https://openalex.org/W131993176', 'https://openalex.org/W138117123', 'https://openalex.org/W169684159', 'https://openalex.org/W180179361', 'https://openalex.org/W180870983', 'https://openalex.org/W1524941034', 'https://openalex.org/W1530785623', 'https://openalex.org/W1544484417', 'https://openalex.org/W1568422853', 'https://openalex.org/W1588539311', 'https://openalex.org/W1595126664', 'https://openalex.org/W1736979814', 'https://openalex.org/W1966621938', 'https://openalex.org/W1983906381', 'https://openalex.org/W2009936685', 'https://openalex.org/W2090920598', 'https://openalex.org/W2097480711', 'https://openalex.org/W2098326211', 'https://openalex.org/W2102518401', 'https://openalex.org/W2102953093', 'https://openalex.org/W2106180965', 'https://openalex.org/W2117645142', 'https://openalex.org/W2118163921', 'https://openalex.org/W2122147877', 'https://openalex.org/W2127462305', 'https://openalex.org/W2128970689', 'https://openalex.org/W2133990480', 'https://openalex.org/W2134031328', 'https://openalex.org/W2135995390', 'https://openalex.org/W2144910574', 'https://openalex.org/W2145511413', 'https://openalex.org/W2146111747', 'https://openalex.org/W2152627593', 'https://openalex.org/W2155120841', 'https://openalex.org/W2156503193', 'https://openalex.org/W2161926933', 'https://openalex.org/W2164385461', 'https://openalex.org/W2166835339', 'https://openalex.org/W2168816626', 'https://openalex.org/W2187872264', 'https://openalex.org/W2743036880', 'https://openalex.org/W2955331414']","Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affect-enriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification.",1.0
SKG_DIA_214,https://openalex.org/W3105781833,2020,18,"['https://openalex.org/W10548402', 'https://openalex.org/W1525961042', 'https://openalex.org/W1598178035', 'https://openalex.org/W2047335008', 'https://openalex.org/W2151814822', 'https://openalex.org/W2157331557', 'https://openalex.org/W2161466446', 'https://openalex.org/W2288878529', 'https://openalex.org/W2396229782', 'https://openalex.org/W2438667436', 'https://openalex.org/W2547875792', 'https://openalex.org/W2594726847', 'https://openalex.org/W2736601468', 'https://openalex.org/W2765111838', 'https://openalex.org/W2798494119', 'https://openalex.org/W2806936550', 'https://openalex.org/W2889186204', 'https://openalex.org/W2949252816', 'https://openalex.org/W2950483141', 'https://openalex.org/W2962996309', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963095800', 'https://openalex.org/W2963277051', 'https://openalex.org/W2963692154', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964044380', 'https://openalex.org/W2970828515', 'https://openalex.org/W3015731157', 'https://openalex.org/W3104546989', 'https://openalex.org/W3121541553', 'https://openalex.org/W4236521339', 'https://openalex.org/W4312609624', 'https://openalex.org/W4394670483']","Dialogue policy learning for task-oriented dialogue systems has enjoyed great progress recently mostly through employing reinforcement learning methods. However, these approaches have become very sophisticated. It is time to re-evaluate it. Are we really making progress developing dialogue agents only based on reinforcement learning? We demonstrate how (1) traditional supervised learning together with (2) a simulator-free adversarial learning method can be used to achieve performance comparable to state-of-the-art reinforcement learning-based methods. First, we introduce a simple dialogue action decoder to predict the appropriate actions. Then, the traditional multi-label classification solution for dialogue policy learning is extended by adding dense layers to improve the dialogue agent performance. Finally, we employ the Gumbel-Softmax estimator to alternatively train the dialogue agent and the dialogue reward model without using reinforcement learning. Based on our extensive experimentation, we can conclude the proposed methods can achieve more stable and higher performance with fewer efforts, such as the domain knowledge required to design a user simulator and the intractable parameter tuning in reinforcement learning. Our main goal is not to beat RL with supervised learning, but to demonstrate the value of rethinking the role of reinforcement learning and supervised learning in optimizing task-oriented dialogue systems.",1.0
SKG_DIA_216,https://openalex.org/W2581637843,2017,765,"['https://openalex.org/W10957333', 'https://openalex.org/W1518951372', 'https://openalex.org/W1591706642', 'https://openalex.org/W1604792744', 'https://openalex.org/W1847211030', 'https://openalex.org/W1902237438', 'https://openalex.org/W1958706068', 'https://openalex.org/W2046765929', 'https://openalex.org/W2064675550', 'https://openalex.org/W2099471712', 'https://openalex.org/W2115613106', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145482038', 'https://openalex.org/W2173520492', 'https://openalex.org/W2176263492', 'https://openalex.org/W2210838531', 'https://openalex.org/W2257979135', 'https://openalex.org/W2335122196', 'https://openalex.org/W2395531022', 'https://openalex.org/W2413332972', 'https://openalex.org/W2417401578', 'https://openalex.org/W2418993857', 'https://openalex.org/W2542835211', 'https://openalex.org/W2550893117', 'https://openalex.org/W2557436004', 'https://openalex.org/W2562579542', 'https://openalex.org/W2565274151', 'https://openalex.org/W2570431255', 'https://openalex.org/W2583679610', 'https://openalex.org/W2583741591', 'https://openalex.org/W2951523806', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963226019', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963373786', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963527228', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963684088', 'https://openalex.org/W2963729324', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964039645', 'https://openalex.org/W2964268978', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W2964352247', 'https://openalex.org/W3022187094', 'https://openalex.org/W4234099752', 'https://openalex.org/W4294149591', 'https://openalex.org/W4297809080', 'https://openalex.org/W4302353911', 'https://openalex.org/W4307979480', 'https://openalex.org/W4320013936']","We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines",1.0
SKG_DIA_218,https://openalex.org/W3035594326,2020,45,"['https://openalex.org/W2250297846', 'https://openalex.org/W2556468274', 'https://openalex.org/W2798914047', 'https://openalex.org/W2891732163', 'https://openalex.org/W2945475330', 'https://openalex.org/W2954492830', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963788376', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970404807', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990']","Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. Experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets.",1.0
SKG_DIA_219,https://openalex.org/W2805662932,2018,441,"['https://openalex.org/W2402700', 'https://openalex.org/W67701990', 'https://openalex.org/W1504610641', 'https://openalex.org/W1522734439', 'https://openalex.org/W1555767263', 'https://openalex.org/W1601218598', 'https://openalex.org/W1793121960', 'https://openalex.org/W1832693441', 'https://openalex.org/W1923034539', 'https://openalex.org/W1973453096', 'https://openalex.org/W1985867508', 'https://openalex.org/W1994518960', 'https://openalex.org/W2012372415', 'https://openalex.org/W2033702744', 'https://openalex.org/W2047024757', 'https://openalex.org/W2053101950', 'https://openalex.org/W2064675550', 'https://openalex.org/W2085662862', 'https://openalex.org/W2097998348', 'https://openalex.org/W2114399139', 'https://openalex.org/W2120615054', 'https://openalex.org/W2131494463', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140211181', 'https://openalex.org/W2143350951', 'https://openalex.org/W2146334809', 'https://openalex.org/W2149465541', 'https://openalex.org/W2149940198', 'https://openalex.org/W2157331557', 'https://openalex.org/W2158335606', 'https://openalex.org/W2163685610', 'https://openalex.org/W2376179408', 'https://openalex.org/W2405274704', 'https://openalex.org/W2406223855', 'https://openalex.org/W2518826259', 'https://openalex.org/W2533262878', 'https://openalex.org/W2544767710', 'https://openalex.org/W2565875961', 'https://openalex.org/W2578994755', 'https://openalex.org/W2584561145', 'https://openalex.org/W2610961739', 'https://openalex.org/W2740550900', 'https://openalex.org/W2742947407', 'https://openalex.org/W2754573465', 'https://openalex.org/W2766718178', 'https://openalex.org/W2767461737', 'https://openalex.org/W2772633765', 'https://openalex.org/W2786411768', 'https://openalex.org/W2788967885', 'https://openalex.org/W2950527759', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962709202', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963520511', 'https://openalex.org/W2964010806', 'https://openalex.org/W2964091467', 'https://openalex.org/W2964260444', 'https://openalex.org/W2964308564', 'https://openalex.org/W3144863038', 'https://openalex.org/W4300985914', 'https://openalex.org/W4303633609']","Emotion recognition in conversations is crucial for the development of empathetic machines. Present methods mostly ignore the role of inter-speaker dependency relations while classifying emotions in conversations. In this paper, we address recognizing utterance-level emotions in dyadic conversational videos. We propose a deep neural framework, termed conversational memory network, which leverages contextual information from the conversation history. The framework takes a multimodal approach comprising audio, visual and textual features with gated recurrent units to model past utterances of each speaker into memories. Such memories are then merged using attention-based hops to capture inter-speaker dependencies. Experiments show an accuracy improvement of 3-4% over the state of the art.",1.0
SKG_DIA_220,https://openalex.org/W2169349402,2013,2,"['https://openalex.org/W1545498002', 'https://openalex.org/W1552062513', 'https://openalex.org/W1596967103', 'https://openalex.org/W1704101508', 'https://openalex.org/W1858542621', 'https://openalex.org/W2146393546', 'https://openalex.org/W2153190884', 'https://openalex.org/W2167277498', 'https://openalex.org/W2187436616', 'https://openalex.org/W2251052362', 'https://openalex.org/W2963375312']","This paper explores to what extent lemmatisation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode.",1.0
SKG_DIA_222,https://openalex.org/W2798753108,2018,76,"['https://openalex.org/W1496189301', 'https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W1993378086', 'https://openalex.org/W2062175565', 'https://openalex.org/W2107618763', 'https://openalex.org/W2111742432', 'https://openalex.org/W2123442489', 'https://openalex.org/W2145618437', 'https://openalex.org/W2154268919', 'https://openalex.org/W2157331557', 'https://openalex.org/W2163274265', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251079237', 'https://openalex.org/W2252136820', 'https://openalex.org/W2269738476', 'https://openalex.org/W2296712013', 'https://openalex.org/W2342096063', 'https://openalex.org/W2402144811', 'https://openalex.org/W2559038528', 'https://openalex.org/W2561148973', 'https://openalex.org/W2566402689', 'https://openalex.org/W2751448157', 'https://openalex.org/W2759477115', 'https://openalex.org/W2762513422', 'https://openalex.org/W2768409085', 'https://openalex.org/W2782031709', 'https://openalex.org/W2786472750', 'https://openalex.org/W2950314731', 'https://openalex.org/W2953384591', 'https://openalex.org/W2962682659', 'https://openalex.org/W2962885446', 'https://openalex.org/W2963794306', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964271186', 'https://openalex.org/W3022187094', 'https://openalex.org/W3159064672', 'https://openalex.org/W4246375690']","The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries. However, further improvement of the existing approaches turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction. DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3% to 69.0% using only 2.4 validation questions per dialogue.",0.99009900990099
SKG_DIA_225,https://openalex.org/W2971160427,2019,76,"['https://openalex.org/W2064675550', 'https://openalex.org/W2342045095', 'https://openalex.org/W2493916176', 'https://openalex.org/W2561995736', 'https://openalex.org/W2741602058', 'https://openalex.org/W2791169651', 'https://openalex.org/W2885323099', 'https://openalex.org/W2891424355', 'https://openalex.org/W2891896107', 'https://openalex.org/W2898856000', 'https://openalex.org/W2954447110', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962738716', 'https://openalex.org/W2962934384', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963993537', 'https://openalex.org/W2970126578', 'https://openalex.org/W2970876710', 'https://openalex.org/W3106003309', 'https://openalex.org/W4289744173', 'https://openalex.org/W4299579390', 'https://openalex.org/W4302343710']","Despite the surging demands for multilingual task-oriented dialog systems (e.g., Alexa, Google Home), there has been less research done in multilingual or cross-lingual scenarios. Hence, we propose a zero-shot adaptation of task-oriented dialogue system to low-resource languages. To tackle this challenge, we first use a set of very few parallel word pairs to refine the aligned cross-lingual word-level representations. We then employ a latent variable model to cope with the variance of similar sentences across different languages, which is induced by imperfect cross-lingual alignments and inherent differences in languages. Finally, the experimental results show that even though we utilize much less external resources, our model achieves better adaptation performance for natural language understanding task (i.e., the intent detection and slot filling) compared to the current state-of-the-art model in the zero-shot scenario.",1.0
SKG_DIA_226,https://openalex.org/W3034720580,2020,85,"['https://openalex.org/W1591706642', 'https://openalex.org/W1840435438', 'https://openalex.org/W1959608418', 'https://openalex.org/W2025768430', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2220374841', 'https://openalex.org/W2415204069', 'https://openalex.org/W2578354947', 'https://openalex.org/W2581637843', 'https://openalex.org/W2608787653', 'https://openalex.org/W2625977873', 'https://openalex.org/W2756386045', 'https://openalex.org/W2885421725', 'https://openalex.org/W2898875342', 'https://openalex.org/W2911994530', 'https://openalex.org/W2947375732', 'https://openalex.org/W2951008357', 'https://openalex.org/W2952855649', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962863107', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963521540', 'https://openalex.org/W2963544700', 'https://openalex.org/W2963640662', 'https://openalex.org/W2963719234', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W2965718149', 'https://openalex.org/W2970579055', 'https://openalex.org/W2979478117', 'https://openalex.org/W2983160116', 'https://openalex.org/W2997662139', 'https://openalex.org/W2997892440', 'https://openalex.org/W3035044096', 'https://openalex.org/W3099023595', 'https://openalex.org/W3168988646', 'https://openalex.org/W4385245566']","Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance.",0.9954337899543378
SKG_DIA_227,https://openalex.org/W2970110247,2019,49,"['https://openalex.org/W140747314', 'https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1672330139', 'https://openalex.org/W1988912276', 'https://openalex.org/W2051948206', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117448986', 'https://openalex.org/W2130607791', 'https://openalex.org/W2133564696', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250738489', 'https://openalex.org/W2250947630', 'https://openalex.org/W2512180100', 'https://openalex.org/W2579689822', 'https://openalex.org/W2606974598', 'https://openalex.org/W2798914047', 'https://openalex.org/W2889448364', 'https://openalex.org/W2962886331', 'https://openalex.org/W2963087868', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963167649', 'https://openalex.org/W2963412005', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964222246', 'https://openalex.org/W2964308564']","Jun Quan, Deyi Xiong, Bonnie Webber, Changjian Hu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",0.9950248756218906
SKG_DIA_228,https://openalex.org/W2739484150,2017,29,"['https://openalex.org/W1965693266', 'https://openalex.org/W2064770078', 'https://openalex.org/W2098345921', 'https://openalex.org/W2099115159', 'https://openalex.org/W2108806737', 'https://openalex.org/W2111032703', 'https://openalex.org/W2131357087', 'https://openalex.org/W2139694477', 'https://openalex.org/W2151048449', 'https://openalex.org/W2155069789', 'https://openalex.org/W2250738489', 'https://openalex.org/W2251035762', 'https://openalex.org/W2251064706', 'https://openalex.org/W2251591847', 'https://openalex.org/W2336260055', 'https://openalex.org/W2464790259', 'https://openalex.org/W2493916176', 'https://openalex.org/W2566645459', 'https://openalex.org/W2602447579', 'https://openalex.org/W2759621817', 'https://openalex.org/W2963167649', 'https://openalex.org/W2963184844', 'https://openalex.org/W2963695529']","This paper presents a novel approach to character identification, that is an entity linking task that maps mentions to characters in dialogues from TV show transcripts. We first augment and correct several cases of annotation errors in an existing corpus so the corpus is clearer and cleaner for statistical learning. We also introduce the agglomerative convolutional neural network that takes groups of features and learns mention and mention-pair embeddings for coreference resolution. We then propose another neural model that employs the embeddings learned and creates cluster embeddings for entity linking. Our coreference resolution model shows comparable results to other state-of-the-art systems. Our entity linking model significantly outperforms the previous work, showing the F1 score of 86.76% and the accuracy of 95.30% for character identification.",0.9954337899543378
SKG_DIA_230,https://openalex.org/W3089529423,2020,7,"['https://openalex.org/W1591706642', 'https://openalex.org/W1975244201', 'https://openalex.org/W1979299372', 'https://openalex.org/W2012056301', 'https://openalex.org/W2052569240', 'https://openalex.org/W2062175565', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104544334', 'https://openalex.org/W2132339004', 'https://openalex.org/W2132997613', 'https://openalex.org/W2340944142', 'https://openalex.org/W2438667436', 'https://openalex.org/W2550893117', 'https://openalex.org/W2604763608', 'https://openalex.org/W2620558438', 'https://openalex.org/W2772001136', 'https://openalex.org/W2798494119', 'https://openalex.org/W2798779216', 'https://openalex.org/W2798914047', 'https://openalex.org/W2804047045', 'https://openalex.org/W2806600904', 'https://openalex.org/W2808093377', 'https://openalex.org/W2810840719', 'https://openalex.org/W2898856000', 'https://openalex.org/W2910795780', 'https://openalex.org/W2914204778', 'https://openalex.org/W2940744433', 'https://openalex.org/W2945475330', 'https://openalex.org/W2946411231', 'https://openalex.org/W2947480709', 'https://openalex.org/W2948110372', 'https://openalex.org/W2950457956', 'https://openalex.org/W2951088751', 'https://openalex.org/W2951091467', 'https://openalex.org/W2952562169', 'https://openalex.org/W2962682659', 'https://openalex.org/W2962738716', 'https://openalex.org/W2962881743', 'https://openalex.org/W2963043030', 'https://openalex.org/W2963248455', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963433587', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963578915', 'https://openalex.org/W2963887424', 'https://openalex.org/W2963984224', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964210218', 'https://openalex.org/W2970401203', 'https://openalex.org/W2970476646', 'https://openalex.org/W2971160427', 'https://openalex.org/W2972352645', 'https://openalex.org/W2979308242', 'https://openalex.org/W2981852735', 'https://openalex.org/W2982482202', 'https://openalex.org/W2985008383', 'https://openalex.org/W2989692108', 'https://openalex.org/W2994811754', 'https://openalex.org/W2998385486', 'https://openalex.org/W2999134550', 'https://openalex.org/W2999524812', 'https://openalex.org/W3000514857', 'https://openalex.org/W3005441132', 'https://openalex.org/W3007894275', 'https://openalex.org/W3008917945', 'https://openalex.org/W3013192639', 'https://openalex.org/W3014521650', 'https://openalex.org/W3015731157', 'https://openalex.org/W3016625483', 'https://openalex.org/W3017796738', 'https://openalex.org/W3020629500', 'https://openalex.org/W3021016503', 'https://openalex.org/W3021533447', 'https://openalex.org/W3021813138', 'https://openalex.org/W3034533785', 'https://openalex.org/W3035301094', 'https://openalex.org/W3037026762', 'https://openalex.org/W3049346316', 'https://openalex.org/W3088453957', 'https://openalex.org/W3092049183', 'https://openalex.org/W3099231098', 'https://openalex.org/W3101469874', 'https://openalex.org/W3102659883', 'https://openalex.org/W3106274079', 'https://openalex.org/W3151929433', 'https://openalex.org/W3189817881']","Task-oriented dialogue systems are either modularized with separate dialogue state tracking (DST) and management steps or end-to-end trainable. In either case, the knowledge base (KB) plays an essential role in fulfilling user requests. Modularized systems rely on DST to interact with the KB, which is expensive in terms of annotation and inference time. End-to-end systems use the KB directly as input, but they cannot scale when the KB is larger than a few hundred entries. In this paper, we propose a method to embed the KB, of any size, directly into the model parameters. The resulting model does not require any DST or template responses, nor the KB as input, and it can dynamically update its KB via fine-tuning. We evaluate our solution in five task-oriented dialogue datasets with small, medium, and large KB size. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all evaluated datasets.",1.0
SKG_DIA_231,https://openalex.org/W2160458012,2012,175,"['https://openalex.org/W966700856', 'https://openalex.org/W1572676935', 'https://openalex.org/W1604513301', 'https://openalex.org/W1956559956', 'https://openalex.org/W1978394996', 'https://openalex.org/W2025265669', 'https://openalex.org/W2094383466', 'https://openalex.org/W2144211451', 'https://openalex.org/W2147308966', 'https://openalex.org/W2156517809', 'https://openalex.org/W2161466446', 'https://openalex.org/W2163723844', 'https://openalex.org/W2165612380', 'https://openalex.org/W2250895046', 'https://openalex.org/W2574874554', 'https://openalex.org/W2579447366']","This system demonstration paper presents IRIS (Informal Response Interactive System), a chat-oriented dialogue system based on the vector space model framework. The system belongs to the class of examplebased dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples. Additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed. 1",0.9927007299270072
SKG_DIA_232,https://openalex.org/W3100523370,2020,17,"['https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W2127795553', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2561529111', 'https://openalex.org/W2586847566', 'https://openalex.org/W2606974598', 'https://openalex.org/W2754194354', 'https://openalex.org/W2757121784', 'https://openalex.org/W2807873315', 'https://openalex.org/W2885421725', 'https://openalex.org/W2887253986', 'https://openalex.org/W2950902819', 'https://openalex.org/W2952420867', 'https://openalex.org/W2952723239', 'https://openalex.org/W2962717182', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963521540', 'https://openalex.org/W2963548348', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964207259', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970260827', 'https://openalex.org/W2971199636', 'https://openalex.org/W2997094605', 'https://openalex.org/W2998083599', 'https://openalex.org/W3034696087', 'https://openalex.org/W3035356453', 'https://openalex.org/W3121541553', 'https://openalex.org/W4385245566']","Incorporating commonsense knowledge can alleviate the issue of generating generic responses in open-domain generative dialogue systems. However, selecting knowledge facts for the dialogue context is still a challenge. The widely used approach Entity Name Matching always retrieves irrelevant facts from the view of local entity words. This paper proposes a novel knowledge selection approach, Prototype-KR, and a knowledge-aware generative model, Prototype-KRG. Given a query, our approach first retrieves a set of prototype dialogues that are relevant to the query. We find knowledge facts used in prototype dialogues usually are highly relevant to the current query; thus, Prototype-KR ranks such knowledge facts based on the semantic similarity and then selects the most appropriate facts. Subsequently, Prototype-KRG can generate an informative response using the selected knowledge facts. Experiments demonstrate that our approach has achieved notable improvements on the most metrics, compared to generative baselines. Meanwhile, compared to IR(Retrieval)-based baselines, responses generated by our approach are more relevant to the context and have comparable informativeness.",1.0
SKG_DIA_233,https://openalex.org/W2131354864,2011,8,"['https://openalex.org/W10376690', 'https://openalex.org/W174903682', 'https://openalex.org/W1527005685', 'https://openalex.org/W1531374185', 'https://openalex.org/W1586162438', 'https://openalex.org/W1594389717', 'https://openalex.org/W2045738181', 'https://openalex.org/W2101426635', 'https://openalex.org/W2109730238', 'https://openalex.org/W2109888564', 'https://openalex.org/W2120764338', 'https://openalex.org/W2122130215', 'https://openalex.org/W2122151248', 'https://openalex.org/W2130523331', 'https://openalex.org/W2136228803', 'https://openalex.org/W2152921782', 'https://openalex.org/W2158570207', 'https://openalex.org/W2160736451', 'https://openalex.org/W2165804688', 'https://openalex.org/W2186590411']","This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.",1.0
SKG_DIA_234,https://openalex.org/W2949476504,2019,29,"['https://openalex.org/W1533861849', 'https://openalex.org/W1687382548', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W2062175565', 'https://openalex.org/W2108738385', 'https://openalex.org/W2117989772', 'https://openalex.org/W2145339207', 'https://openalex.org/W2408200822', 'https://openalex.org/W2417401578', 'https://openalex.org/W2473329891', 'https://openalex.org/W2507592741', 'https://openalex.org/W2571927164', 'https://openalex.org/W2592808142', 'https://openalex.org/W2594726847', 'https://openalex.org/W2705882626', 'https://openalex.org/W2759567155', 'https://openalex.org/W2783543950', 'https://openalex.org/W2808007596', 'https://openalex.org/W2810840719', 'https://openalex.org/W2884814595', 'https://openalex.org/W2889186204', 'https://openalex.org/W2949252816', 'https://openalex.org/W2962776342', 'https://openalex.org/W2963007936', 'https://openalex.org/W2963043030', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963433587', 'https://openalex.org/W2963567240', 'https://openalex.org/W2963993502', 'https://openalex.org/W2964080167', 'https://openalex.org/W2964101860', 'https://openalex.org/W4289760729', 'https://openalex.org/W4294446480', 'https://openalex.org/W4306716473']","This paper presents a new approach that extends Deep Dyna-Q (DDQ) by incorporating a Budget-Conscious Scheduling (BCS) to best utilize a fixed, small amount of user interactions (budget) for learning task-oriented dialogue agents. BCS consists of (1) a Poisson-based global scheduler to allocate budget over different stages of training; (2) a controller to decide at each training step whether the agent is trained using real or simulated experiences; (3) a user goal sampling module to generate the experiences that are most effective for policy learning. Experiments on a movie-ticket booking task with simulated and real users show that our approach leads to significant improvements in success rate over the state-of-the-art baselines given the fixed budget.",1.0
SKG_DIA_235,https://openalex.org/W2741802726,2017,25,"['https://openalex.org/W77001256', 'https://openalex.org/W1482132414', 'https://openalex.org/W1526096287', 'https://openalex.org/W1810943226', 'https://openalex.org/W1902237438', 'https://openalex.org/W1924770834', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2106226466', 'https://openalex.org/W2128970689', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146502635', 'https://openalex.org/W2312383716', 'https://openalex.org/W2401527985', 'https://openalex.org/W2511929605', 'https://openalex.org/W2964036636', 'https://openalex.org/W2964139507', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564']","We propose a novel generative neural network architecture for Dialogue Act classification. Building upon the Recurrent Neural Network framework, our model incorporates a novel attentional technique and a label to label connection for sequence learning, akin to Hidden Markov Models. The experiments show that both of these innovations lead our model to outperform strong baselines for dialogue act classification on MapTask and Switchboard corpora. We further empirically analyse the effectiveness of each of the new innovations.",1.0
SKG_DIA_236,https://openalex.org/W2990563493,2020,69,"['https://openalex.org/W10957333', 'https://openalex.org/W1518951372', 'https://openalex.org/W1539562568', 'https://openalex.org/W1591706642', 'https://openalex.org/W2003458432', 'https://openalex.org/W2038721957', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140679639', 'https://openalex.org/W2154652894', 'https://openalex.org/W2328886022', 'https://openalex.org/W2584220694', 'https://openalex.org/W2586847566', 'https://openalex.org/W2761590056', 'https://openalex.org/W2787711783', 'https://openalex.org/W2807278718', 'https://openalex.org/W2900260828', 'https://openalex.org/W2914204778', 'https://openalex.org/W2950902819', 'https://openalex.org/W2962786758', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963283805', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963466651', 'https://openalex.org/W2963527228', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964178377', 'https://openalex.org/W2964308564', 'https://openalex.org/W2996068536', 'https://openalex.org/W3022187094', 'https://openalex.org/W4285719527', 'https://openalex.org/W4288624561', 'https://openalex.org/W4385245566']","Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.",1.0
SKG_DIA_237,https://openalex.org/W2997416967,2020,4,"['https://openalex.org/W771469340', 'https://openalex.org/W1523385540', 'https://openalex.org/W1591706642', 'https://openalex.org/W1883346539', 'https://openalex.org/W1902237438', 'https://openalex.org/W1949478088', 'https://openalex.org/W1959608418', 'https://openalex.org/W2025341678', 'https://openalex.org/W2089150068', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116516955', 'https://openalex.org/W2560512785', 'https://openalex.org/W2593768305', 'https://openalex.org/W2604444020', 'https://openalex.org/W2733128608', 'https://openalex.org/W2734443755', 'https://openalex.org/W2761590056', 'https://openalex.org/W2783130359', 'https://openalex.org/W2784400615', 'https://openalex.org/W2798888952', 'https://openalex.org/W2799016112', 'https://openalex.org/W2888093771', 'https://openalex.org/W2890274659', 'https://openalex.org/W2896457183', 'https://openalex.org/W2904683980', 'https://openalex.org/W2914204778', 'https://openalex.org/W2916898195', 'https://openalex.org/W2938704169', 'https://openalex.org/W2945554776', 'https://openalex.org/W2949782788', 'https://openalex.org/W2953046278', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963096510', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963330684', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963903950', 'https://openalex.org/W2964134121', 'https://openalex.org/W2996287690', 'https://openalex.org/W4237723258', 'https://openalex.org/W4288624561', 'https://openalex.org/W4294408270']","Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on vocabulary. We learn the pair relationship between the prompts and responses as a regression task on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.",1.0
SKG_DIA_238,https://openalex.org/W2805005636,2018,143,"['https://openalex.org/W10957333', 'https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W1566256432', 'https://openalex.org/W1591706642', 'https://openalex.org/W1902237438', 'https://openalex.org/W1966797434', 'https://openalex.org/W1975827232', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153579005', 'https://openalex.org/W2419539795', 'https://openalex.org/W2550821151', 'https://openalex.org/W2563351168', 'https://openalex.org/W2581637843', 'https://openalex.org/W2590513900', 'https://openalex.org/W2618843390', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963790827', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W3022187094', 'https://openalex.org/W4249243325', 'https://openalex.org/W4294170691']","Chenyang Huang, Osmar Zaïane, Amine Trabelsi, Nouha Dziri. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 2018.",1.0
SKG_DIA_243,https://openalex.org/W2891744372,2018,31,"['https://openalex.org/W10957333', 'https://openalex.org/W222053410', 'https://openalex.org/W1516111018', 'https://openalex.org/W1518951372', 'https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1603615955', 'https://openalex.org/W1614298861', 'https://openalex.org/W1779483307', 'https://openalex.org/W1828163288', 'https://openalex.org/W1958706068', 'https://openalex.org/W1959608418', 'https://openalex.org/W1975879668', 'https://openalex.org/W2077846610', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2122262818', 'https://openalex.org/W2130942839', 'https://openalex.org/W2136144249', 'https://openalex.org/W2142112143', 'https://openalex.org/W2157331557', 'https://openalex.org/W2171421863', 'https://openalex.org/W2188365844', 'https://openalex.org/W2210838531', 'https://openalex.org/W2250645967', 'https://openalex.org/W2328886022', 'https://openalex.org/W2419501139', 'https://openalex.org/W2546938941', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548228487', 'https://openalex.org/W2557436004', 'https://openalex.org/W2560512785', 'https://openalex.org/W2581637843', 'https://openalex.org/W2587284713', 'https://openalex.org/W2598482664', 'https://openalex.org/W2602076750', 'https://openalex.org/W2604178507', 'https://openalex.org/W2612675303', 'https://openalex.org/W2661761953', 'https://openalex.org/W2761590056', 'https://openalex.org/W2788277448', 'https://openalex.org/W2789543585', 'https://openalex.org/W2803832867', 'https://openalex.org/W2899771611', 'https://openalex.org/W2950306824', 'https://openalex.org/W2952264928', 'https://openalex.org/W2953046278', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962738009', 'https://openalex.org/W2962871754', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963226019', 'https://openalex.org/W2963275229', 'https://openalex.org/W2963332597', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963768805', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963800509', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963958388', 'https://openalex.org/W2964026424', 'https://openalex.org/W2964042872', 'https://openalex.org/W2964121744', 'https://openalex.org/W3022187094', 'https://openalex.org/W3136591341', 'https://openalex.org/W4237840503']","Sequence-to-Sequence (seq2seq) models have become overwhelmingly popular in building end-to-end trainable dialogue systems. Though highly efficient in learning the backbone of human-computer communications, they suffer from the problem of strongly favoring short generic responses. In this paper, we argue that a good response should smoothly connect both the preceding dialogue history and the following conversations. We strengthen this connection by mutual information maximization. To sidestep the non-differentiability of discrete natural language tokens, we introduce an auxiliary continuous code space and map such code space to a learnable prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the generated responses are closely related to the dialogue context and lead to more interactive conversations.",0.9937106918238994
SKG_DIA_244,https://openalex.org/W3020990315,2020,5,"['https://openalex.org/W1525961042', 'https://openalex.org/W1598178035', 'https://openalex.org/W1785674045', 'https://openalex.org/W1969152782', 'https://openalex.org/W2055537935', 'https://openalex.org/W2077302143', 'https://openalex.org/W2130942839', 'https://openalex.org/W2251957808', 'https://openalex.org/W2468710617', 'https://openalex.org/W2561715562', 'https://openalex.org/W2745934983', 'https://openalex.org/W2751448157', 'https://openalex.org/W2759998906', 'https://openalex.org/W2784070054', 'https://openalex.org/W2798367796', 'https://openalex.org/W2892248135', 'https://openalex.org/W2906637185', 'https://openalex.org/W2945475330', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963267799', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963518342', 'https://openalex.org/W2964101860', 'https://openalex.org/W2971034786', 'https://openalex.org/W2972571786', 'https://openalex.org/W2988252747', 'https://openalex.org/W2999837696', 'https://openalex.org/W3102961474', 'https://openalex.org/W3188964051']",Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%.,1.0
SKG_DIA_245,https://openalex.org/W2508347479,2016,73,"['https://openalex.org/W1569268747', 'https://openalex.org/W1941338968', 'https://openalex.org/W1963612627', 'https://openalex.org/W1975244201', 'https://openalex.org/W1979982232', 'https://openalex.org/W2043496750', 'https://openalex.org/W2084950741', 'https://openalex.org/W2101308260', 'https://openalex.org/W2112924667', 'https://openalex.org/W2127838323']","We present a new release of OpenDial, an open-source toolkit for building and evaluating spoken dialogue systems.The toolkit relies on an information-state architecture where the dialogue state is represented as a Bayesian network and acts as a shared memory for all system modules.The domain models are specified via probabilistic rules encoded in XML.Open-Dial has been deployed in several application domains such as human-robot interaction, intelligent tutoring systems and multi-modal in-car driver assistants.",0.993939393939394
SKG_DIA_247,https://openalex.org/W4288162065,2019,6,[],"Grounding a pronoun to a visual object it refers to requires complex\nreasoning from various information sources, especially in conversational\nscenarios. For example, when people in a conversation talk about something all\nspeakers can see, they often directly use pronouns (e.g., it) to refer to it\nwithout previous introduction. This fact brings a huge challenge for modern\nnatural language understanding systems, particularly conventional context-based\npronoun coreference models. To tackle this challenge, in this paper, we\nformally define the task of visual-aware pronoun coreference resolution (PCR)\nand introduce VisPro, a large-scale dialogue PCR dataset, to investigate\nwhether and how the visual information can help resolve pronouns in dialogues.\nWe then propose a novel visual-aware PCR model, VisCoref, for this task and\nconduct comprehensive experiments and case studies on our dataset. Results\ndemonstrate the importance of the visual information in this PCR case and show\nthe effectiveness of the proposed model.\n",0.9813664596273292
SKG_DIA_248,https://openalex.org/W3034520363,2020,24,"['https://openalex.org/W2557764419', 'https://openalex.org/W2558203065', 'https://openalex.org/W2612228435', 'https://openalex.org/W2805206884', 'https://openalex.org/W2888302696', 'https://openalex.org/W2896457183', 'https://openalex.org/W2912904516', 'https://openalex.org/W2914120296', 'https://openalex.org/W2951534261', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963339397', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963963993', 'https://openalex.org/W2964120615', 'https://openalex.org/W2964223283', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970049541', 'https://openalex.org/W2970597249', 'https://openalex.org/W2975059944', 'https://openalex.org/W2979928633', 'https://openalex.org/W2990138404', 'https://openalex.org/W2996428491', 'https://openalex.org/W4288614645', 'https://openalex.org/W4385245566']","We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts. Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA). Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.",1.0
SKG_DIA_251,https://openalex.org/W2432549722,2017,73,"['https://openalex.org/W1441091828', 'https://openalex.org/W1524766440', 'https://openalex.org/W1587506928', 'https://openalex.org/W1785674045', 'https://openalex.org/W1814992895', 'https://openalex.org/W1989996186', 'https://openalex.org/W1993567041', 'https://openalex.org/W2024632416', 'https://openalex.org/W2055537935', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101138947', 'https://openalex.org/W2115101920', 'https://openalex.org/W2118463056', 'https://openalex.org/W2166293310', 'https://openalex.org/W2187089797', 'https://openalex.org/W2189256702', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250456405', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251355666', 'https://openalex.org/W2282031862', 'https://openalex.org/W2372621665', 'https://openalex.org/W2397579082', 'https://openalex.org/W2438667436', 'https://openalex.org/W2442323289', 'https://openalex.org/W2468710617', 'https://openalex.org/W2471178169', 'https://openalex.org/W2534274346', 'https://openalex.org/W2561293850', 'https://openalex.org/W2575101493', 'https://openalex.org/W2586719289', 'https://openalex.org/W2949300694', 'https://openalex.org/W2949697461', 'https://openalex.org/W2951642317', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962847367', 'https://openalex.org/W2963050422', 'https://openalex.org/W2963775726', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963974889', 'https://openalex.org/W2964121744']","One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.",0.991304347826087
SKG_DIA_253,https://openalex.org/W2158162461,2011,1,"['https://openalex.org/W194972339', 'https://openalex.org/W1495817119']","This paper describes Dico II+, an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogueand a speech cursor which enables menu navigation as well as browsing long list using haptic input and spoken output.",1.0
SKG_DIA_254,https://openalex.org/W3087232873,2020,19,"['https://openalex.org/W1840435438', 'https://openalex.org/W1879966306', 'https://openalex.org/W2053154970', 'https://openalex.org/W2164777277', 'https://openalex.org/W2608787653', 'https://openalex.org/W2741631785', 'https://openalex.org/W2754831410', 'https://openalex.org/W2788496822', 'https://openalex.org/W2797206276', 'https://openalex.org/W2911994530', 'https://openalex.org/W2914204778', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963530300', 'https://openalex.org/W2963544700', 'https://openalex.org/W2963640662', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963846996', 'https://openalex.org/W2964309167', 'https://openalex.org/W2965718149', 'https://openalex.org/W2971822538', 'https://openalex.org/W2972324944', 'https://openalex.org/W2983160116', 'https://openalex.org/W2988937804', 'https://openalex.org/W2996657743', 'https://openalex.org/W2997306177', 'https://openalex.org/W2997892440', 'https://openalex.org/W3034720580', 'https://openalex.org/W3037026762', 'https://openalex.org/W3099023595', 'https://openalex.org/W4288624561', 'https://openalex.org/W4294338648']","Maintaining a consistent attribute profile is crucial for dialogue agents to\nnaturally converse with humans. Existing studies on improving attribute\nconsistency mainly explored how to incorporate attribute information in the\nresponses, but few efforts have been made to identify the consistency relations\nbetween response and attribute profile. To facilitate the study of profile\nconsistency identification, we create a large-scale human-annotated dataset\nwith over 110K single-turn conversations and their key-value attribute\nprofiles. Explicit relation between response and profile is manually labeled.\nWe also propose a key-value structure information enriched BERT model to\nidentify the profile consistency, and it gained improvements over strong\nbaselines. Further evaluations on downstream tasks demonstrate that the profile\nconsistency identification model is conducive for improving dialogue\nconsistency.\n",1.0
SKG_DIA_256,https://openalex.org/W2593696076,2017,50,"['https://openalex.org/W6908809', 'https://openalex.org/W36903255', 'https://openalex.org/W630532510', 'https://openalex.org/W889023230', 'https://openalex.org/W1574486308', 'https://openalex.org/W1958706068', 'https://openalex.org/W1959608418', 'https://openalex.org/W2011925159', 'https://openalex.org/W2108501770', 'https://openalex.org/W2159640018', 'https://openalex.org/W2172140247', 'https://openalex.org/W2173681125', 'https://openalex.org/W2188365844', 'https://openalex.org/W2210838531', 'https://openalex.org/W2311783643', 'https://openalex.org/W2328886022', 'https://openalex.org/W2384495648', 'https://openalex.org/W2399880602', 'https://openalex.org/W2552838200', 'https://openalex.org/W2949416428', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963773425', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964352131', 'https://openalex.org/W3022187094']","We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the ‘boring output’ issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.",1.0
SKG_DIA_257,https://openalex.org/W2970444947,2019,33,"['https://openalex.org/W1522301498', 'https://openalex.org/W1917215959', 'https://openalex.org/W1947758080', 'https://openalex.org/W1948566616', 'https://openalex.org/W2060833990', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108239140', 'https://openalex.org/W2124445791', 'https://openalex.org/W2130942839', 'https://openalex.org/W2157331557', 'https://openalex.org/W2169818249', 'https://openalex.org/W2291723583', 'https://openalex.org/W2891439916', 'https://openalex.org/W2951176429', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963206148', 'https://openalex.org/W2964121744', 'https://openalex.org/W3100380967', 'https://openalex.org/W4292763141', 'https://openalex.org/W4299612276', 'https://openalex.org/W4300532998']","Chenguang Zhu, Michael Zeng, Xuedong Huang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_259,https://openalex.org/W2798494119,2018,184,"['https://openalex.org/W16046748', 'https://openalex.org/W52170320', 'https://openalex.org/W1491843047', 'https://openalex.org/W1515851193', 'https://openalex.org/W1564312725', 'https://openalex.org/W1681299129', 'https://openalex.org/W1758031947', 'https://openalex.org/W1778387566', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W2035934535', 'https://openalex.org/W2048226872', 'https://openalex.org/W2062175565', 'https://openalex.org/W2109038907', 'https://openalex.org/W2112483970', 'https://openalex.org/W2117989772', 'https://openalex.org/W2145339207', 'https://openalex.org/W2257979135', 'https://openalex.org/W2290354866', 'https://openalex.org/W2295072214', 'https://openalex.org/W2396229782', 'https://openalex.org/W2412899141', 'https://openalex.org/W2417401578', 'https://openalex.org/W2473329891', 'https://openalex.org/W2507592741', 'https://openalex.org/W2567374473', 'https://openalex.org/W2571927164', 'https://openalex.org/W2594726847', 'https://openalex.org/W2765111838', 'https://openalex.org/W2766447205', 'https://openalex.org/W2949252816', 'https://openalex.org/W2950314731', 'https://openalex.org/W2950471160', 'https://openalex.org/W2950517718', 'https://openalex.org/W2962776342', 'https://openalex.org/W2962996309', 'https://openalex.org/W2963043030', 'https://openalex.org/W2963064439', 'https://openalex.org/W2963068985', 'https://openalex.org/W2964077562', 'https://openalex.org/W2964080167', 'https://openalex.org/W2964101860', 'https://openalex.org/W3021208093', 'https://openalex.org/W3104546989', 'https://openalex.org/W4229706854', 'https://openalex.org/W4245108548', 'https://openalex.org/W4293396018']","Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.",0.9935483870967742
SKG_DIA_260,https://openalex.org/W2564880510,2016,24,"['https://openalex.org/W1536313757', 'https://openalex.org/W1669985085', 'https://openalex.org/W1875231349', 'https://openalex.org/W1970251998', 'https://openalex.org/W1971505647', 'https://openalex.org/W1980511580', 'https://openalex.org/W1991820721', 'https://openalex.org/W1992711317', 'https://openalex.org/W2024715881', 'https://openalex.org/W2032325851', 'https://openalex.org/W2050125829', 'https://openalex.org/W2082178985', 'https://openalex.org/W2087900392', 'https://openalex.org/W2089652186', 'https://openalex.org/W2092196438', 'https://openalex.org/W2093268611', 'https://openalex.org/W2101416990', 'https://openalex.org/W2101532847', 'https://openalex.org/W2120314390', 'https://openalex.org/W2123027949', 'https://openalex.org/W2125336414', 'https://openalex.org/W2130703925', 'https://openalex.org/W2135243376', 'https://openalex.org/W2143302363', 'https://openalex.org/W2145588856', 'https://openalex.org/W2145747781', 'https://openalex.org/W2147154374', 'https://openalex.org/W2148445705', 'https://openalex.org/W2154522120', 'https://openalex.org/W2182998842', 'https://openalex.org/W2252271018', 'https://openalex.org/W2404124622', 'https://openalex.org/W2952377244', 'https://openalex.org/W2964195122', 'https://openalex.org/W3122271111', 'https://openalex.org/W4243968218', 'https://openalex.org/W4249803144', 'https://openalex.org/W4251603968', 'https://openalex.org/W4256327895', 'https://openalex.org/W4317471952']","When interacting individuals entrain, they begin to speak more like each other.To support research on entrainment in cooperative multi-party dialogues, we have created a corpus where teams of three or four speakers play two rounds of a cooperative board game.We describe the experimental design and technical infrastructure used to collect our corpus, which consists of audio, video, transcriptions, and questionnaire data for 63 teams (47 hours of audio).We illustrate the use of our corpus as a novel resource for studying team entrainment by 1) developing and evaluating teamlevel acoustic-prosodic entrainment measures that extend existing dyad measures, and 2) investigating relationships between team entrainment and participation dominance.",1.0
SKG_DIA_261,https://openalex.org/W2952728748,2019,53,[],"Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is\nconducted based on visual and audio aspects of a given video, is significantly\nmore challenging than traditional image or text-grounded dialogue systems\nbecause (1) feature space of videos span across multiple picture frames, making\nit difficult to obtain semantic information; and (2) a dialogue agent must\nperceive and process information from different modalities (audio, video,\ncaption, etc.) to obtain a comprehensive understanding. Most existing work is\nbased on RNNs and sequence-to-sequence architectures, which are not very\neffective for capturing complex long-term dependencies (like in videos). To\novercome this, we propose Multimodal Transformer Networks (MTN) to encode\nvideos and incorporate information from different modalities. We also propose\nquery-aware attention through an auto-encoder to extract query-aware features\nfrom non-text modalities. We develop a training procedure to simulate\ntoken-level decoding to improve the quality of generated responses during\ninference. We get state of the art performance on Dialogue System Technology\nChallenge 7 (DSTC7). Our model also generalizes to another multimodal\nvisual-grounded dialogue task, and obtains promising performance. We\nimplemented our models using PyTorch and the code is released at\nhttps://github.com/henryhungle/MTN.\n",1.0
SKG_DIA_262,https://openalex.org/W3120948726,2020,2,"['https://openalex.org/W2064675550', 'https://openalex.org/W2117489143', 'https://openalex.org/W2130942839', 'https://openalex.org/W2250539671', 'https://openalex.org/W2293453011', 'https://openalex.org/W2296712013', 'https://openalex.org/W2840098622', 'https://openalex.org/W2950527759', 'https://openalex.org/W2963210342', 'https://openalex.org/W2964119254', 'https://openalex.org/W2964308564']","The Differentiable Neural Computer (DNC), a neural network model with an addressable external memory, can solve algorithmic and question answering tasks. There are various improved versions of DNC, such as rsDNC and DNC-DMS. However, how to integrate structured knowledge into these DNC models remains a challenging research question. We incorporate an architecture for knowledge into such DNC models, i.e. DNC, rsDNC and DNC-DMS, to improve the ability to generate correct responses using both contextual information and structured knowledge. Our improved rsDNC model improves the mean accuracy by approximately 20% to the original rsDNC on tasks requiring knowledge in the dialog bAbI tasks. In addition, our improved rsDNC and DNC-DMS models also yield better performance than their original models in the Movie Dialog dataset.",1.0
SKG_DIA_263,https://openalex.org/W3098258760,2020,63,"['https://openalex.org/W105848778', 'https://openalex.org/W1914571458', 'https://openalex.org/W2019502441', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106242970', 'https://openalex.org/W2111910266', 'https://openalex.org/W2130041324', 'https://openalex.org/W2138621090', 'https://openalex.org/W2147453867', 'https://openalex.org/W2155482025', 'https://openalex.org/W2264742718', 'https://openalex.org/W2761590056', 'https://openalex.org/W2890969459', 'https://openalex.org/W2906579211', 'https://openalex.org/W2916898195', 'https://openalex.org/W2936695845', 'https://openalex.org/W2949555952', 'https://openalex.org/W2951883832', 'https://openalex.org/W2953206424', 'https://openalex.org/W2962717182', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963667505', 'https://openalex.org/W2963825865', 'https://openalex.org/W2969574947', 'https://openalex.org/W2970785611', 'https://openalex.org/W2972437240', 'https://openalex.org/W2988937804', 'https://openalex.org/W2996403597', 'https://openalex.org/W2998563994', 'https://openalex.org/W3000779003', 'https://openalex.org/W3005680577', 'https://openalex.org/W3023786569', 'https://openalex.org/W3024436954', 'https://openalex.org/W3035252911', 'https://openalex.org/W3037969532', 'https://openalex.org/W3098708719', 'https://openalex.org/W3104078590', 'https://openalex.org/W3155584966', 'https://openalex.org/W4252076394', 'https://openalex.org/W4287900772']","Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.",1.0
SKG_DIA_264,https://openalex.org/W3105205406,2020,18,"['https://openalex.org/W648786980', 'https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1690739335', 'https://openalex.org/W1821462560', 'https://openalex.org/W1975879668', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2419539795', 'https://openalex.org/W2521114121', 'https://openalex.org/W2581637843', 'https://openalex.org/W2584185835', 'https://openalex.org/W2586847566', 'https://openalex.org/W2761590056', 'https://openalex.org/W2767206889', 'https://openalex.org/W2784400615', 'https://openalex.org/W2807880213', 'https://openalex.org/W2890969459', 'https://openalex.org/W2891744372', 'https://openalex.org/W2892153332', 'https://openalex.org/W2916898195', 'https://openalex.org/W2950142196', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951883832', 'https://openalex.org/W2952729433', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962896208', 'https://openalex.org/W2962915948', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963330684', 'https://openalex.org/W2963360026', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963544536', 'https://openalex.org/W2963712524', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963959597', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964118293', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964134121', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970454332', 'https://openalex.org/W2976965654', 'https://openalex.org/W2997955173', 'https://openalex.org/W3022187094', 'https://openalex.org/W3093329015', 'https://openalex.org/W4288256350', 'https://openalex.org/W4299971819', 'https://openalex.org/W4385245566']","Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge. More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference. Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge.",1.0
SKG_DIA_265,https://openalex.org/W3035314827,2020,26,"['https://openalex.org/W1933349210', 'https://openalex.org/W2194775991', 'https://openalex.org/W2331128040', 'https://openalex.org/W2337252826', 'https://openalex.org/W2549139847', 'https://openalex.org/W2563399268', 'https://openalex.org/W2606982687', 'https://openalex.org/W2810643877', 'https://openalex.org/W2914204778', 'https://openalex.org/W2952686080', 'https://openalex.org/W2962934715', 'https://openalex.org/W2963174698', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963446712', 'https://openalex.org/W2965373594', 'https://openalex.org/W2966715458', 'https://openalex.org/W2967674528', 'https://openalex.org/W2969876226', 'https://openalex.org/W2970608575', 'https://openalex.org/W2970869018', 'https://openalex.org/W2972777589', 'https://openalex.org/W2975501350', 'https://openalex.org/W2981851019', 'https://openalex.org/W2988647680', 'https://openalex.org/W2996428491', 'https://openalex.org/W2998356391', 'https://openalex.org/W3015746101', 'https://openalex.org/W3099388488', 'https://openalex.org/W4288624561', 'https://openalex.org/W4297800839', 'https://openalex.org/W4385245566']","Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network. Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.",1.0
SKG_DIA_266,https://openalex.org/W2945475330,2019,412,"['https://openalex.org/W1522301498', 'https://openalex.org/W1785674045', 'https://openalex.org/W1924770834', 'https://openalex.org/W2047335008', 'https://openalex.org/W2119015791', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251355666', 'https://openalex.org/W2407905223', 'https://openalex.org/W2426267443', 'https://openalex.org/W2438667436', 'https://openalex.org/W2507756961', 'https://openalex.org/W2550821151', 'https://openalex.org/W2554616628', 'https://openalex.org/W2556468274', 'https://openalex.org/W2560647685', 'https://openalex.org/W2561696860', 'https://openalex.org/W2583761661', 'https://openalex.org/W2594726847', 'https://openalex.org/W2604763608', 'https://openalex.org/W2605043629', 'https://openalex.org/W2606974598', 'https://openalex.org/W2777054756', 'https://openalex.org/W2798367796', 'https://openalex.org/W2798914047', 'https://openalex.org/W2809324505', 'https://openalex.org/W2810840719', 'https://openalex.org/W2888541716', 'https://openalex.org/W2949995560', 'https://openalex.org/W2962724315', 'https://openalex.org/W2962934384', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963578915', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963866663', 'https://openalex.org/W2963924212', 'https://openalex.org/W2963936679', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964077278', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964316912', 'https://openalex.org/W3103800629', 'https://openalex.org/W4289147179', 'https://openalex.org/W4293350112', 'https://openalex.org/W4295200480', 'https://openalex.org/W4295883599', 'https://openalex.org/W4306716473', 'https://openalex.org/W4319988532']","Over-dependence on domain ontology and lack of knowledge sharing across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short in tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a TRAnsferable Dialogue staff, generator (TRADE) that generates dialogue states from utterances using a copy mechanism, facilitating knowledge transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art joint goal accuracy of 48.62% for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show its transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.",1.0
SKG_DIA_267,https://openalex.org/W2291723583,2016,172,"['https://openalex.org/W119177792', 'https://openalex.org/W179875071', 'https://openalex.org/W1542311097', 'https://openalex.org/W1552182777', 'https://openalex.org/W1606347560', 'https://openalex.org/W1627331591', 'https://openalex.org/W1644652583', 'https://openalex.org/W1834646128', 'https://openalex.org/W1947758080', 'https://openalex.org/W1948566616', 'https://openalex.org/W1971111363', 'https://openalex.org/W1973310094', 'https://openalex.org/W1975244201', 'https://openalex.org/W1989549063', 'https://openalex.org/W1999965501', 'https://openalex.org/W2005076803', 'https://openalex.org/W2005874308', 'https://openalex.org/W2008652694', 'https://openalex.org/W2012918311', 'https://openalex.org/W2018116724', 'https://openalex.org/W2020073413', 'https://openalex.org/W2050523636', 'https://openalex.org/W2063473970', 'https://openalex.org/W2064675550', 'https://openalex.org/W2093973850', 'https://openalex.org/W2096557251', 'https://openalex.org/W2099542783', 'https://openalex.org/W2100969003', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108239140', 'https://openalex.org/W2120354757', 'https://openalex.org/W2122514299', 'https://openalex.org/W2127838323', 'https://openalex.org/W2137387514', 'https://openalex.org/W2139079654', 'https://openalex.org/W2146871184', 'https://openalex.org/W2150355110', 'https://openalex.org/W2157716519', 'https://openalex.org/W2161166090', 'https://openalex.org/W2161181481', 'https://openalex.org/W2163302275', 'https://openalex.org/W2165698076', 'https://openalex.org/W2251071050', 'https://openalex.org/W2288878529', 'https://openalex.org/W2294684023', 'https://openalex.org/W2399550240', 'https://openalex.org/W2953265577', 'https://openalex.org/W2963788376', 'https://openalex.org/W4285719527', 'https://openalex.org/W4292763141']","Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain.",1.0
SKG_DIA_268,https://openalex.org/W2970799419,2019,81,"['https://openalex.org/W1509030783', 'https://openalex.org/W1522301498', 'https://openalex.org/W2000031724', 'https://openalex.org/W2054141820', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2127480961', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144487656', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250777971', 'https://openalex.org/W2605350416', 'https://openalex.org/W2755957574', 'https://openalex.org/W2766447205', 'https://openalex.org/W2780155557', 'https://openalex.org/W2784400615', 'https://openalex.org/W2891389695', 'https://openalex.org/W2896457183', 'https://openalex.org/W2952215380', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962852262', 'https://openalex.org/W2962879001', 'https://openalex.org/W2962974452', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963170138', 'https://openalex.org/W2963217826', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963537482', 'https://openalex.org/W2963626623', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964308564', 'https://openalex.org/W4239272157', 'https://openalex.org/W4295249402']","Dongyeop Kang, Anusha Balakrishnan, Pararth Shah, Paul Crook, Y-Lan Boureau, Jason Weston. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",0.994475138121547
SKG_DIA_270,https://openalex.org/W3019549080,2020,7,"['https://openalex.org/W1491843047', 'https://openalex.org/W1975244201', 'https://openalex.org/W2062175565', 'https://openalex.org/W2064290760', 'https://openalex.org/W2097714468', 'https://openalex.org/W2117989772', 'https://openalex.org/W2141538250', 'https://openalex.org/W2145339207', 'https://openalex.org/W2155027007', 'https://openalex.org/W2168490009', 'https://openalex.org/W2177156214', 'https://openalex.org/W2250464714', 'https://openalex.org/W2257979135', 'https://openalex.org/W2288878529', 'https://openalex.org/W2410985346', 'https://openalex.org/W2412899141', 'https://openalex.org/W2417401578', 'https://openalex.org/W2438667436', 'https://openalex.org/W2513380446', 'https://openalex.org/W2558661633', 'https://openalex.org/W2571927164', 'https://openalex.org/W2594726847', 'https://openalex.org/W2736601468', 'https://openalex.org/W2789327587', 'https://openalex.org/W2797760463', 'https://openalex.org/W2798494119', 'https://openalex.org/W2804047045', 'https://openalex.org/W2889186204', 'https://openalex.org/W2891732163', 'https://openalex.org/W2899771611', 'https://openalex.org/W2899908862', 'https://openalex.org/W2947212824', 'https://openalex.org/W2949252816', 'https://openalex.org/W2949868354', 'https://openalex.org/W2951091467', 'https://openalex.org/W2962682659', 'https://openalex.org/W2962852262', 'https://openalex.org/W2962879001', 'https://openalex.org/W2962996309', 'https://openalex.org/W2963043030', 'https://openalex.org/W2963433587', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970028737', 'https://openalex.org/W3104546989']","Most existing approaches for goal-oriented dialogue policy learning used reinforcement learning, which focuses on the target agent policy and simply treat the opposite agent policy as part of the environment. While in real-world scenarios, the behavior of an opposite agent often exhibits certain patterns or underlies hidden policies, which can be inferred and utilized by the target agent to facilitate its own decision making. This strategy is common in human mental simulation by first imaging a specific action and the probable results before really acting it. We therefore propose an opposite behavior aware framework for policy learning in goal-oriented dialogues. We estimate the opposite agent's policy from its behavior and use this estimation to improve the target agent by regarding it as part of the target policy. We evaluate our model on both cooperative and competitive dialogue tasks, showing superior performance over state-of-the-art baselines.",1.0
SKG_DIA_271,https://openalex.org/W2934890006,2019,12,"['https://openalex.org/W1522301498', 'https://openalex.org/W1989996186', 'https://openalex.org/W1993567041', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2108806737', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250539671', 'https://openalex.org/W2473965551', 'https://openalex.org/W2556468274', 'https://openalex.org/W2597655663', 'https://openalex.org/W2604698497', 'https://openalex.org/W2624448691', 'https://openalex.org/W2749436976', 'https://openalex.org/W2798367796', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963844597', 'https://openalex.org/W2963970400', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W4289147179']","Sanuj Sharma, Prafulla Kumar Choubey, Ruihong Huang. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_DIA_273,https://openalex.org/W3035473672,2020,62,"['https://openalex.org/W155915536', 'https://openalex.org/W1503312748', 'https://openalex.org/W1517858959', 'https://openalex.org/W1526096287', 'https://openalex.org/W1913627016', 'https://openalex.org/W2004864148', 'https://openalex.org/W2016730668', 'https://openalex.org/W2032254851', 'https://openalex.org/W2038570053', 'https://openalex.org/W2045565604', 'https://openalex.org/W2064675550', 'https://openalex.org/W2079725295', 'https://openalex.org/W2085662862', 'https://openalex.org/W2098689807', 'https://openalex.org/W2098872617', 'https://openalex.org/W2108598243', 'https://openalex.org/W2109020278', 'https://openalex.org/W2110450943', 'https://openalex.org/W2122563357', 'https://openalex.org/W2124141504', 'https://openalex.org/W2128970689', 'https://openalex.org/W2146334809', 'https://openalex.org/W2166637769', 'https://openalex.org/W2194775991', 'https://openalex.org/W2250539671', 'https://openalex.org/W2397375888', 'https://openalex.org/W2556418146', 'https://openalex.org/W2573626026', 'https://openalex.org/W2703895418', 'https://openalex.org/W2739871800', 'https://openalex.org/W2740550900', 'https://openalex.org/W2757599232', 'https://openalex.org/W2772633765', 'https://openalex.org/W2796830519', 'https://openalex.org/W2805005636', 'https://openalex.org/W2809286861', 'https://openalex.org/W2883409523', 'https://openalex.org/W2917937435', 'https://openalex.org/W2918189006', 'https://openalex.org/W2931751229', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963349408', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963686995', 'https://openalex.org/W2963765493', 'https://openalex.org/W2964089584', 'https://openalex.org/W2971770322', 'https://openalex.org/W2978127603', 'https://openalex.org/W3100146208', 'https://openalex.org/W3163340360', 'https://openalex.org/W4239031955', 'https://openalex.org/W4239072543', 'https://openalex.org/W4385245566']","The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of both multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.",1.0
SKG_DIA_275,https://openalex.org/W2296330515,2015,18,"['https://openalex.org/W108437174', 'https://openalex.org/W113130591', 'https://openalex.org/W158043429', 'https://openalex.org/W204341599', 'https://openalex.org/W239563548', 'https://openalex.org/W1581527173', 'https://openalex.org/W1931054966', 'https://openalex.org/W2088911157', 'https://openalex.org/W2111889471', 'https://openalex.org/W2124741472', 'https://openalex.org/W2133258739', 'https://openalex.org/W2135819134', 'https://openalex.org/W2136925175', 'https://openalex.org/W2158349948', 'https://openalex.org/W2250555572', 'https://openalex.org/W2251229550', 'https://openalex.org/W2397240125', 'https://openalex.org/W2962957031']","We take a novel approach to zero pronoun resolution in Chinese: our model explicitly tracks the flow of focus in a discourse. Our approach, which generalizes to deictic references, is not reliant on the presence of overt noun phrase antecedents to resolve to, and allows us to address the large percentage of “non-anaphoric” pronouns filtered out in other approaches. We furthermore train our model using readily available parallel Chinese/English corpora, allowing for training without hand-annotated data. Our results demonstrate improvements on two test sets, as well as the usefulness of linguistically motivated features.",1.0
SKG_DIA_276,https://openalex.org/W2970404807,2019,99,"['https://openalex.org/W1522301498', 'https://openalex.org/W1677182931', 'https://openalex.org/W1793121960', 'https://openalex.org/W1975244201', 'https://openalex.org/W2064675550', 'https://openalex.org/W2157331557', 'https://openalex.org/W2251058040', 'https://openalex.org/W2785523195', 'https://openalex.org/W2804010326', 'https://openalex.org/W2896457183', 'https://openalex.org/W2945475330', 'https://openalex.org/W2950635152', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962847367', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963248507', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963907629', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101377', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W4289147179']","Liliang Ren, Jianmo Ni, Julian McAuley. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_277,https://openalex.org/W2971190479,2019,33,"['https://openalex.org/W295828404', 'https://openalex.org/W1551842868', 'https://openalex.org/W2086511124', 'https://openalex.org/W2102531443', 'https://openalex.org/W2339852062', 'https://openalex.org/W2395531022', 'https://openalex.org/W2539338396', 'https://openalex.org/W2561368124', 'https://openalex.org/W2748668722', 'https://openalex.org/W2774267535', 'https://openalex.org/W2782368579', 'https://openalex.org/W2798456655', 'https://openalex.org/W2891416139', 'https://openalex.org/W2949446780', 'https://openalex.org/W2952813980', 'https://openalex.org/W2962768358', 'https://openalex.org/W2962854379', 'https://openalex.org/W2963161823', 'https://openalex.org/W2977304374', 'https://openalex.org/W4300125564']","Jia Li, Chongyang Tao, Wei Wu, Yansong Feng, Dongyan Zhao, Rui Yan. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_DIA_279,https://openalex.org/W2977149219,2019,9,"['https://openalex.org/W1522301498', 'https://openalex.org/W1959608418', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2187089797', 'https://openalex.org/W2250539671', 'https://openalex.org/W2328886022', 'https://openalex.org/W2399880602', 'https://openalex.org/W2435450765', 'https://openalex.org/W2521114121', 'https://openalex.org/W2593696076', 'https://openalex.org/W2606712314', 'https://openalex.org/W2615146352', 'https://openalex.org/W2621133045', 'https://openalex.org/W2626778328', 'https://openalex.org/W2743149734', 'https://openalex.org/W2757121784', 'https://openalex.org/W2789033601', 'https://openalex.org/W2798385473', 'https://openalex.org/W2804255934', 'https://openalex.org/W2888456631', 'https://openalex.org/W2940154139', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950037544', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962755817', 'https://openalex.org/W2962810352', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963332597', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963469850', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963958388', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964119254', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3022187094', 'https://openalex.org/W4385245566']","Neural conversation systems generate responses based on the sequence-to-sequence (SEQ2SEQ) paradigm. Typically, the model is equipped with a single set of learned parameters to generate responses for given input contexts. When confronting diverse conversations, its adaptability is rather limited and the model is hence prone to generate generic responses. In this work, we propose an {\bf Ada}ptive {\bf N}eural {\bf D}ialogue generation model, \textsc{AdaND}, which manages various conversations with conversation-specific parameterization. For each conversation, the model generates parameters of the encoder-decoder by referring to the input context. In particular, we propose two adaptive parameterization mechanisms: a context-aware and a topic-aware parameterization mechanism. The context-aware parameterization directly generates the parameters by capturing local semantics of the given context. The topic-aware parameterization enables parameter sharing among conversations with similar topics by first inferring the latent topics of the given context and then generating the parameters with respect to the distributional topics. Extensive experiments conducted on a large-scale real-world conversational dataset show that our model achieves superior performance in terms of both quantitative metrics and human evaluations.",1.0
SKG_DIA_280,https://openalex.org/W2890719433,2018,62,"['https://openalex.org/W1525961042', 'https://openalex.org/W1645476387', 'https://openalex.org/W1673923490', 'https://openalex.org/W1945616565', 'https://openalex.org/W1995945562', 'https://openalex.org/W2016522586', 'https://openalex.org/W2112507308', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144960104', 'https://openalex.org/W2155250282', 'https://openalex.org/W2180612164', 'https://openalex.org/W2251882135', 'https://openalex.org/W2293844262', 'https://openalex.org/W2328886022', 'https://openalex.org/W2402144811', 'https://openalex.org/W2418993857', 'https://openalex.org/W2561498661', 'https://openalex.org/W2570685808', 'https://openalex.org/W2571175805', 'https://openalex.org/W2594590228', 'https://openalex.org/W2603766943', 'https://openalex.org/W2606974598', 'https://openalex.org/W2609368435', 'https://openalex.org/W2612675303', 'https://openalex.org/W2619479788', 'https://openalex.org/W2735135478', 'https://openalex.org/W2766108848', 'https://openalex.org/W2767899794', 'https://openalex.org/W2770626128', 'https://openalex.org/W2772621923', 'https://openalex.org/W2774961896', 'https://openalex.org/W2796084947', 'https://openalex.org/W2953384591', 'https://openalex.org/W2962707484', 'https://openalex.org/W2962713901', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962790689', 'https://openalex.org/W2962818281', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963109634', 'https://openalex.org/W2963123621', 'https://openalex.org/W2963126845', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963207607', 'https://openalex.org/W2963217826', 'https://openalex.org/W2963564844', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963768805', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963919731', 'https://openalex.org/W2963969878', 'https://openalex.org/W2964153729', 'https://openalex.org/W2964308564', 'https://openalex.org/W3022187094', 'https://openalex.org/W3193304830', 'https://openalex.org/W4294410794', 'https://openalex.org/W4310299640']","We present two categories of model-agnostic adversarial strategies that reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change strategies that evaluate over-sensitivity to small and semantics-preserving edits, as well as Should-Change strategies that test if a model is over-stable against subtle yet semantics-changing modifications. We next perform adversarial training with each strategy, employing a max-margin approach for negative generative examples. This not only makes the target dialogue model more robust to the adversarial inputs, but also helps it perform significantly better on the original inputs. Moreover, training on all strategies combined achieves further improvements, achieving a new state-of-the-art performance on the original task (also verified via human evaluation). In addition to adversarial training, we also address the robustness task at the model-level, by feeding it subword units as both inputs and outputs, and show that the resulting model is equally competitive, requires only 1/4 of the original vocabulary size, and is robust to one of the adversarial strategies (to which the original model is vulnerable) even without adversarial training.",1.0
SKG_DIA_282,https://openalex.org/W2951855948,2019,23,"['https://openalex.org/W1522301498', 'https://openalex.org/W1533504578', 'https://openalex.org/W1544827683', 'https://openalex.org/W1902237438', 'https://openalex.org/W2095705004', 'https://openalex.org/W2097127032', 'https://openalex.org/W2131774270', 'https://openalex.org/W2153190547', 'https://openalex.org/W2250539671', 'https://openalex.org/W2470673105', 'https://openalex.org/W2507756961', 'https://openalex.org/W2516930406', 'https://openalex.org/W2551396370', 'https://openalex.org/W2562607067', 'https://openalex.org/W2740747242', 'https://openalex.org/W2741263286', 'https://openalex.org/W2804243436', 'https://openalex.org/W2896457183', 'https://openalex.org/W2912904516', 'https://openalex.org/W2949615363', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963080779', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963545005', 'https://openalex.org/W2963547127', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963795756', 'https://openalex.org/W2963963993', 'https://openalex.org/W2963969878', 'https://openalex.org/W2964121744', 'https://openalex.org/W4248634141', 'https://openalex.org/W4288614645', 'https://openalex.org/W4385245566']","Comprehending multi-turn spoken conversations is an emerging research area, presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers. Unlike passages, where sentences are often the default semantic modeling unit, in multi-turn conversations, a turn is a topically coherent unit embodied with immediately relevant context, making it a linguistically intuitive segment for computationally modeling verbal interactions. Therefore, in this work, we propose a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance. Experiments are conducted on a multi-turn conversation dataset, where nurses inquire and discuss symptom information with patients. We empirically show that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples.",0.994475138121547
SKG_DIA_283,https://openalex.org/W3100128199,2020,136,"['https://openalex.org/W1975244201', 'https://openalex.org/W2101105183', 'https://openalex.org/W2438667436', 'https://openalex.org/W2798367796', 'https://openalex.org/W2798914047', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2914204778', 'https://openalex.org/W2923014074', 'https://openalex.org/W2945260553', 'https://openalex.org/W2945475330', 'https://openalex.org/W2953071719', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962831269', 'https://openalex.org/W2962886331', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964210218', 'https://openalex.org/W2965373594', 'https://openalex.org/W2969708789', 'https://openalex.org/W2970404807', 'https://openalex.org/W2970597249', 'https://openalex.org/W2971274815', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W2979520138', 'https://openalex.org/W2980207396', 'https://openalex.org/W2980282514', 'https://openalex.org/W2982482202', 'https://openalex.org/W2988252747', 'https://openalex.org/W2988647680', 'https://openalex.org/W2988937804', 'https://openalex.org/W2996317432', 'https://openalex.org/W2997108628', 'https://openalex.org/W2997771882', 'https://openalex.org/W2998228050', 'https://openalex.org/W2999134550', 'https://openalex.org/W3000779003', 'https://openalex.org/W3008966357', 'https://openalex.org/W3011411500', 'https://openalex.org/W3016625483', 'https://openalex.org/W3021016503', 'https://openalex.org/W3023786569', 'https://openalex.org/W3024509506', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034999214', 'https://openalex.org/W3082274269', 'https://openalex.org/W3082549344', 'https://openalex.org/W3100110884', 'https://openalex.org/W3103616906', 'https://openalex.org/W3119649668', 'https://openalex.org/W3155584966', 'https://openalex.org/W4287795696', 'https://openalex.org/W4287900772', 'https://openalex.org/W4288027128', 'https://openalex.org/W4288087450', 'https://openalex.org/W4288089799', 'https://openalex.org/W4288094254', 'https://openalex.org/W4288624561', 'https://openalex.org/W4289147179', 'https://openalex.org/W4295249402']","In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to “carryover” the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20% training data, and 3) Lev greatly improves the inference efficiency.",0.9928057553956836
SKG_DIA_284,https://openalex.org/W2952813980,2019,146,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1924770834', 'https://openalex.org/W2097117768', 'https://openalex.org/W2102531443', 'https://openalex.org/W2153579005', 'https://openalex.org/W2163605009', 'https://openalex.org/W2170738476', 'https://openalex.org/W2194775991', 'https://openalex.org/W2197546379', 'https://openalex.org/W2338325072', 'https://openalex.org/W2339852062', 'https://openalex.org/W2395531022', 'https://openalex.org/W2521114121', 'https://openalex.org/W2561368124', 'https://openalex.org/W2581637843', 'https://openalex.org/W2767802162', 'https://openalex.org/W2770970123', 'https://openalex.org/W2786983967', 'https://openalex.org/W2798456655', 'https://openalex.org/W2807880213', 'https://openalex.org/W2809210859', 'https://openalex.org/W2889581211', 'https://openalex.org/W2891416139', 'https://openalex.org/W2896457183', 'https://openalex.org/W2949446780', 'https://openalex.org/W2951359136', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962768358', 'https://openalex.org/W2962796276', 'https://openalex.org/W2962838727', 'https://openalex.org/W2962854379', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963522640', 'https://openalex.org/W2963542836', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963986868', 'https://openalex.org/W2964046515', 'https://openalex.org/W2964082993', 'https://openalex.org/W2964092386', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964150246', 'https://openalex.org/W2964309167', 'https://openalex.org/W2964352131', 'https://openalex.org/W4294170691', 'https://openalex.org/W4300687842', 'https://openalex.org/W4394666973']","Currently, researchers have paid great attention to retrieval-based dialogues in open-domain. In particular, people study the problem by investigating context-response matching for multi-turn response selection based on publicly recognized benchmark data sets. State-of-the-art methods require a response to interact with each utterance in a context from the beginning, but the interaction is performed in a shallow way. In this work, we let utterance-response interaction go deep by proposing an interaction-over-interaction network (IoI). The model performs matching by stacking multiple interaction blocks in which residual information from one time of interaction initiates the interaction process again. Thus, matching information within an utterance-response pair is extracted from the interaction of the pair in an iterative fashion, and the information flows along the chain of the blocks via representations. Evaluation results on three benchmark data sets indicate that IoI can significantly outperform state-of-the-art methods in terms of various matching metrics. Through further analysis, we also unveil how the depth of interaction affects the performance of IoI.",0.9961685823754788
SKG_DIA_285,https://openalex.org/W2963283951,2018,149,"['https://openalex.org/W648947103', 'https://openalex.org/W1522301498', 'https://openalex.org/W1904365287', 'https://openalex.org/W2115090890', 'https://openalex.org/W2250297846', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251163406', 'https://openalex.org/W2251235149', 'https://openalex.org/W2396928039', 'https://openalex.org/W2412715517', 'https://openalex.org/W2507756961', 'https://openalex.org/W2516930406', 'https://openalex.org/W2606974598', 'https://openalex.org/W2772001136', 'https://openalex.org/W2949252816', 'https://openalex.org/W2962776342', 'https://openalex.org/W2962847367', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963068985', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964210218', 'https://openalex.org/W3037881859', 'https://openalex.org/W4295249402']","We highlight a practical yet rarely discussed problem in dialogue state tracking (DST), namely handling unknown slot values. Previous approaches generally assume predefined candidate lists and thus are not designed to output unknown values, especially when the spoken language understanding (SLU) module is absent as in many end-to-end (E2E) systems. We describe in this paper an E2E architecture based on the pointer network (PtrNet) that can effectively extract unknown slot values while still obtains state-of-the-art accuracy on the standard DSTC2 benchmark. We also provide extensive empirical evidence to show that tracking unknown values can be challenging and our approach can bring significant improvement with the help of an effective feature dropout technique.",1.0
SKG_DIA_286,https://openalex.org/W3088955686,2020,0,"['https://openalex.org/W1645937837', 'https://openalex.org/W1887590894', 'https://openalex.org/W1956340063', 'https://openalex.org/W1975244201', 'https://openalex.org/W2033334263', 'https://openalex.org/W2057452878', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109878893', 'https://openalex.org/W2111140836', 'https://openalex.org/W2119930118', 'https://openalex.org/W2123301721', 'https://openalex.org/W2123442489', 'https://openalex.org/W2127979034', 'https://openalex.org/W2128076956', 'https://openalex.org/W2128970689', 'https://openalex.org/W2137076533', 'https://openalex.org/W2141403362', 'https://openalex.org/W2153190547', 'https://openalex.org/W2154652894', 'https://openalex.org/W2328886022', 'https://openalex.org/W2479110610', 'https://openalex.org/W2498260772', 'https://openalex.org/W2594125959', 'https://openalex.org/W2617907584', 'https://openalex.org/W2729046720', 'https://openalex.org/W2739874095', 'https://openalex.org/W2774104751', 'https://openalex.org/W2785702304', 'https://openalex.org/W2793978524', 'https://openalex.org/W2798664956', 'https://openalex.org/W2874826521', 'https://openalex.org/W2890581450', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914204778', 'https://openalex.org/W2935206035', 'https://openalex.org/W2949417144', 'https://openalex.org/W2951560313', 'https://openalex.org/W2952098430', 'https://openalex.org/W2952607215', 'https://openalex.org/W2953039584', 'https://openalex.org/W2962676842', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963170138', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963592583', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964352131', 'https://openalex.org/W2967853777', 'https://openalex.org/W2970072000', 'https://openalex.org/W2970252402', 'https://openalex.org/W2970597249', 'https://openalex.org/W2973049837', 'https://openalex.org/W2985891764', 'https://openalex.org/W2990141119', 'https://openalex.org/W2996287690', 'https://openalex.org/W2996403597', 'https://openalex.org/W2996614149', 'https://openalex.org/W3007723711', 'https://openalex.org/W3032777814', 'https://openalex.org/W3102195370']","Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation (NLG) are typically construed as end-to-end architectures that do not adequately model human generation processes. To investigate, we decouple generation into two separate phases: planning and realization. In the planning phase, we train two planners to generate plans for response utterances. The realization phase uses response plans to produce an appropriate response. Through rigorous evaluations, both automated and human, we demonstrate that decoupling the process into planning and realization performs better than an end-to-end approach.",1.0
SKG_DIA_287,https://openalex.org/W2803728898,2018,155,"['https://openalex.org/W1832693441', 'https://openalex.org/W2024490156', 'https://openalex.org/W2057900969', 'https://openalex.org/W2123442489', 'https://openalex.org/W2251058040', 'https://openalex.org/W2293004735', 'https://openalex.org/W2404126548', 'https://openalex.org/W2562335618', 'https://openalex.org/W2594726847', 'https://openalex.org/W2812722839', 'https://openalex.org/W2899771611', 'https://openalex.org/W2951642864', 'https://openalex.org/W2962886331', 'https://openalex.org/W2962974452', 'https://openalex.org/W2963140597', 'https://openalex.org/W2963625095', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964263551', 'https://openalex.org/W3121541553', 'https://openalex.org/W4295249402']","Mikhail Burtsev, Alexander Seliverstov, Rafael Airapetyan, Mikhail Arkhipov, Dilyara Baymurzina, Nickolay Bushkov, Olga Gureenkova, Taras Khakhulin, Yuri Kuratov, Denis Kuznetsov, Alexey Litinsky, Varvara Logacheva, Alexey Lymar, Valentin Malykh, Maxim Petrov, Vadim Polulyakh, Leonid Pugachev, Alexey Sorokin, Maria Vikhreva, Marat Zaynutdinov. Proceedings of ACL 2018, System Demonstrations. 2018.",0.9902912621359224
SKG_DIA_288,https://openalex.org/W2176271827,2011,14,"['https://openalex.org/W101214240', 'https://openalex.org/W1547546052', 'https://openalex.org/W1567277581', 'https://openalex.org/W1570601904', 'https://openalex.org/W1576504150', 'https://openalex.org/W1576520375', 'https://openalex.org/W1585753929', 'https://openalex.org/W1965206833', 'https://openalex.org/W1981082061', 'https://openalex.org/W1982246600', 'https://openalex.org/W2029639339', 'https://openalex.org/W2042609668', 'https://openalex.org/W2067982155', 'https://openalex.org/W2075011505', 'https://openalex.org/W2083905610', 'https://openalex.org/W2089654579', 'https://openalex.org/W2098345921', 'https://openalex.org/W2107232048', 'https://openalex.org/W2113310984', 'https://openalex.org/W2116786260', 'https://openalex.org/W2124700572', 'https://openalex.org/W2127713198', 'https://openalex.org/W2129665406', 'https://openalex.org/W2130031580', 'https://openalex.org/W2131340601', 'https://openalex.org/W2131554698', 'https://openalex.org/W2144087279', 'https://openalex.org/W2145020046', 'https://openalex.org/W2164455818', 'https://openalex.org/W2169789327', 'https://openalex.org/W2171828761', 'https://openalex.org/W2222512263', 'https://openalex.org/W2295547137', 'https://openalex.org/W2475711339', 'https://openalex.org/W3011367699', 'https://openalex.org/W3023431232', 'https://openalex.org/W3135387542']","An entity in a dialogue may be old, new, or mediated/inferrable with respect to the hearer’s beliefs. Knowing the information status of the entities participating in a dialogue can therefore facilitate its interpretation. We address the under-investigated problem of automatically determining the information status of discourse entities. Specifically, we extend Nissim’s (2006) machine learning approach to information-status determination with lexical and structured features, and exploit learned knowledge of the information status of each discourse entity for coreference resolution. Experimental results on a set of Switchboard dialogues reveal that (1) incorporating our proposed features into Nissim’s feature set enables our system to achieve stateof-the-art performance on information-status classification, and (2) the resulting information can be used to improve the performance of learning-based coreference resolvers. 1",1.0
SKG_DIA_289,https://openalex.org/W3099890447,2020,25,"['https://openalex.org/W112197792', 'https://openalex.org/W222053410', 'https://openalex.org/W1591706642', 'https://openalex.org/W1821462560', 'https://openalex.org/W2128892113', 'https://openalex.org/W2130942839', 'https://openalex.org/W2586847566', 'https://openalex.org/W2611029872', 'https://openalex.org/W2804047946', 'https://openalex.org/W2807873315', 'https://openalex.org/W2808437126', 'https://openalex.org/W2889681790', 'https://openalex.org/W2896457183', 'https://openalex.org/W2904444765', 'https://openalex.org/W2911994530', 'https://openalex.org/W2950902819', 'https://openalex.org/W2953039584', 'https://openalex.org/W2962796276', 'https://openalex.org/W2963201498', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963521540', 'https://openalex.org/W2963544700', 'https://openalex.org/W2963564796', 'https://openalex.org/W2963691849', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963825865', 'https://openalex.org/W2964309167', 'https://openalex.org/W2971296908', 'https://openalex.org/W2997892440', 'https://openalex.org/W3014773921', 'https://openalex.org/W3015322406', 'https://openalex.org/W3035072597', 'https://openalex.org/W3035148359', 'https://openalex.org/W3035282664', 'https://openalex.org/W3093956460', 'https://openalex.org/W3097517997', 'https://openalex.org/W3104078590', 'https://openalex.org/W4295253143', 'https://openalex.org/W4295727797', 'https://openalex.org/W4385245566']","Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.",0.9933774834437086
SKG_DIA_293,https://openalex.org/W3117170115,2020,1,"['https://openalex.org/W1832693441', 'https://openalex.org/W2051840895', 'https://openalex.org/W2092206588', 'https://openalex.org/W2146334809', 'https://openalex.org/W2163605009', 'https://openalex.org/W2194775991', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250966211', 'https://openalex.org/W2470673105', 'https://openalex.org/W2563010554', 'https://openalex.org/W2740550900', 'https://openalex.org/W2805662932', 'https://openalex.org/W2886757387', 'https://openalex.org/W2887030499', 'https://openalex.org/W2891359673', 'https://openalex.org/W2905807898', 'https://openalex.org/W2951008357', 'https://openalex.org/W2953739332', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963520511', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963920114', 'https://openalex.org/W2964230360', 'https://openalex.org/W2964300796', 'https://openalex.org/W3205498744']","Unlike non-conversation scenes, emotion recognition in dialogues (ERD) poses more complicated challenges due to its interactive nature and intricate contextual information. All present methods model historical utterances without considering the content of the target utterance. However, different parts of a historical utterance may contribute differently to emotion inference of different target utterances. Therefore we propose Fine-grained Extraction and Reasoning Network (FERNet) to generate target-specific historical utterance representations. The reasoning module effectively handles both local and global sequential dependencies to reason over context, and updates target utterance representations to more informed vectors. Experiments on two benchmarks show that our method achieves competitive performance compared with previous methods.",0.994413407821229
SKG_DIA_297,https://openalex.org/W3101319477,2020,17,"['https://openalex.org/W1494910745', 'https://openalex.org/W1533861849', 'https://openalex.org/W1591706642', 'https://openalex.org/W1686810756', 'https://openalex.org/W1902237438', 'https://openalex.org/W1975879668', 'https://openalex.org/W1993378086', 'https://openalex.org/W2016589492', 'https://openalex.org/W2062989416', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2154652894', 'https://openalex.org/W2157331557', 'https://openalex.org/W2183341477', 'https://openalex.org/W2250539671', 'https://openalex.org/W2558809543', 'https://openalex.org/W2583186419', 'https://openalex.org/W2768661419', 'https://openalex.org/W2785523195', 'https://openalex.org/W2871584425', 'https://openalex.org/W2891732163', 'https://openalex.org/W2897182555', 'https://openalex.org/W2944887439', 'https://openalex.org/W2948742685', 'https://openalex.org/W2950009015', 'https://openalex.org/W2951450739', 'https://openalex.org/W2951980657', 'https://openalex.org/W2952723239', 'https://openalex.org/W2953071719', 'https://openalex.org/W2953251345', 'https://openalex.org/W2956125353', 'https://openalex.org/W2962764403', 'https://openalex.org/W2962814079', 'https://openalex.org/W2962835968', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963248455', 'https://openalex.org/W2963287297', 'https://openalex.org/W2963748384', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964011461', 'https://openalex.org/W2964588180', 'https://openalex.org/W2969576497', 'https://openalex.org/W2971261034', 'https://openalex.org/W2972603547', 'https://openalex.org/W2985891764', 'https://openalex.org/W2988647680', 'https://openalex.org/W3022187094', 'https://openalex.org/W3106274079', 'https://openalex.org/W4286749334', 'https://openalex.org/W4297728544']","In the recent past, dialogue systems have gained immense popularity and have become ubiquitous. During conversations, humans not only rely on languages but seek contextual information through visual contents as well. In every task-oriented dialogue system, the user is guided by the different aspects of a product or service that regulates the conversation towards selecting the product or service. In this work, we present a multi-modal conversational framework for a task-oriented dialogue setup that generates the responses following the different aspects of a product or service to cater to the user’s needs. We show that the responses guided by the aspect information provide more interactive and informative responses for better communication between the agent and the user. We first create a Multi-domain Multi-modal Dialogue (MDMMD) dataset having conversations involving both text and images belonging to the three different domains, such as restaurants, electronics, and furniture. We implement a Graph Convolutional Network (GCN) based framework that generates appropriate textual responses from the multi-modal inputs. The multi-modal information having both textual and image representation is fed to the decoder and the aspect information for generating aspect guided responses. Quantitative and qualitative analyses show that the proposed methodology outperforms several baselines for the proposed task of aspect-guided response generation.",0.9959514170040484
SKG_DIA_298,https://openalex.org/W2152342063,2014,32,"['https://openalex.org/W62710299', 'https://openalex.org/W311892248', 'https://openalex.org/W1211946649', 'https://openalex.org/W1513468570', 'https://openalex.org/W1524881148', 'https://openalex.org/W1542941925', 'https://openalex.org/W1681299129', 'https://openalex.org/W1746819321', 'https://openalex.org/W1987326241', 'https://openalex.org/W1999874108', 'https://openalex.org/W2001050921', 'https://openalex.org/W2021151961', 'https://openalex.org/W2035934535', 'https://openalex.org/W2037897789', 'https://openalex.org/W2054716580', 'https://openalex.org/W2056894129', 'https://openalex.org/W2084799336', 'https://openalex.org/W2099618002', 'https://openalex.org/W2101445408', 'https://openalex.org/W2104602264', 'https://openalex.org/W2105715011', 'https://openalex.org/W2109038907', 'https://openalex.org/W2113033979', 'https://openalex.org/W2115714256', 'https://openalex.org/W2119015791', 'https://openalex.org/W2120327309', 'https://openalex.org/W2121863487', 'https://openalex.org/W2142831953', 'https://openalex.org/W2153672931', 'https://openalex.org/W2156974606', 'https://openalex.org/W2168490009', 'https://openalex.org/W2169430966', 'https://openalex.org/W2171079152', 'https://openalex.org/W2231198303', 'https://openalex.org/W2246008130', 'https://openalex.org/W2250245054', 'https://openalex.org/W2250681874', 'https://openalex.org/W2312609093', 'https://openalex.org/W2401150877', 'https://openalex.org/W2586680856', 'https://openalex.org/W3158638686', 'https://openalex.org/W4211049957', 'https://openalex.org/W4214717370', 'https://openalex.org/W4285719527']","We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario.Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from.In particular, we compare the Qlearning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate.Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly.We also show that very high gradually decreasing exploration rates are required for convergence.We conclude that multiagent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.",1.0
SKG_DIA_300,https://openalex.org/W1948566616,2015,838,"['https://openalex.org/W179875071', 'https://openalex.org/W196214544', 'https://openalex.org/W225503657', 'https://openalex.org/W1492935830', 'https://openalex.org/W1521413921', 'https://openalex.org/W1534317862', 'https://openalex.org/W1552182777', 'https://openalex.org/W1591801644', 'https://openalex.org/W1606347560', 'https://openalex.org/W1810943226', 'https://openalex.org/W1905882502', 'https://openalex.org/W1917215959', 'https://openalex.org/W1947758080', 'https://openalex.org/W1970207841', 'https://openalex.org/W1975244201', 'https://openalex.org/W1980340273', 'https://openalex.org/W1999965501', 'https://openalex.org/W2004637830', 'https://openalex.org/W2018116724', 'https://openalex.org/W2024632416', 'https://openalex.org/W2045738181', 'https://openalex.org/W2050523636', 'https://openalex.org/W2055537935', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2099542783', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104368104', 'https://openalex.org/W2107878631', 'https://openalex.org/W2108239140', 'https://openalex.org/W2110313598', 'https://openalex.org/W2115221470', 'https://openalex.org/W2117130368', 'https://openalex.org/W2122514299', 'https://openalex.org/W2122585011', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2135363470', 'https://openalex.org/W2136016850', 'https://openalex.org/W2139079654', 'https://openalex.org/W2143612262', 'https://openalex.org/W2150355110', 'https://openalex.org/W2160815625', 'https://openalex.org/W2161181481', 'https://openalex.org/W2171928131', 'https://openalex.org/W2250539671', 'https://openalex.org/W2257626115', 'https://openalex.org/W2474824677', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950527759', 'https://openalex.org/W2951176429', 'https://openalex.org/W4233573184', 'https://openalex.org/W4292763141', 'https://openalex.org/W4303633609']","Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality.Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language.They are also not easily scaled to systems covering multiple domains and languages.This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure.The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates.With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods.Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.",1.0
SKG_DIA_302,https://openalex.org/W2740595206,2017,41,"['https://openalex.org/W1886507901', 'https://openalex.org/W2045738181', 'https://openalex.org/W2056698510', 'https://openalex.org/W2251211118', 'https://openalex.org/W2252105888']","We build a chat bot with iterative content exploration that leads a user through a personalized knowledge acquisition session. The chat bot is designed as an automated customer support or product recommendation agent assisting a user in learning product features, product usability, suitability, troubleshooting and other related tasks. To control the user navigation through content, we extend the notion of a linguistic discourse tree (DT) towards a set of documents with multiple sections covering a topic. For a given paragraph, a DT is built by DT parsers. We then combine DTs for the paragraphs of documents to form what we call extended DT, which is a basis for interactive content exploration facilitated by the chat bot. To provide cohesive answers, we use a measure of rhetoric agreement between a question and an answer by tree kernel learning of their DTs.",1.0
SKG_DIA_303,https://openalex.org/W2759104452,2017,37,"['https://openalex.org/W52170320', 'https://openalex.org/W99537330', 'https://openalex.org/W582134693', 'https://openalex.org/W1595483645', 'https://openalex.org/W1757796397', 'https://openalex.org/W1778387566', 'https://openalex.org/W1904365287', 'https://openalex.org/W1975244201', 'https://openalex.org/W2021151961', 'https://openalex.org/W2035934535', 'https://openalex.org/W2047335008', 'https://openalex.org/W2062175565', 'https://openalex.org/W2095705004', 'https://openalex.org/W2115101920', 'https://openalex.org/W2122250135', 'https://openalex.org/W2126810476', 'https://openalex.org/W2145339207', 'https://openalex.org/W2157797701', 'https://openalex.org/W2164411961', 'https://openalex.org/W2166550727', 'https://openalex.org/W2168359464', 'https://openalex.org/W2175723363', 'https://openalex.org/W2251058040', 'https://openalex.org/W2257979135', 'https://openalex.org/W2280163991', 'https://openalex.org/W2408200822', 'https://openalex.org/W2417401578', 'https://openalex.org/W2438667436', 'https://openalex.org/W2507592741', 'https://openalex.org/W2594726847', 'https://openalex.org/W2740191615', 'https://openalex.org/W2962776342', 'https://openalex.org/W2962996309', 'https://openalex.org/W2963938771', 'https://openalex.org/W2963993502', 'https://openalex.org/W2963993719', 'https://openalex.org/W2964044380', 'https://openalex.org/W2964059111', 'https://openalex.org/W4298857966']","Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy. The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance. We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a pre-defined rule-based policy acts as a “teacher” and guides a data-driven RL system by giving example actions as well as additional rewards. A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher’s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both safetyand efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.",1.0
SKG_DIA_304,https://openalex.org/W2927341038,2019,0,"['https://openalex.org/W1591706642', 'https://openalex.org/W1832693441', 'https://openalex.org/W1992178348', 'https://openalex.org/W2000324202', 'https://openalex.org/W2005139433', 'https://openalex.org/W2113669296', 'https://openalex.org/W2169110538', 'https://openalex.org/W2186654215', 'https://openalex.org/W2250497056', 'https://openalex.org/W2250553926', 'https://openalex.org/W2250883471', 'https://openalex.org/W2251383488', 'https://openalex.org/W2252128283', 'https://openalex.org/W2314271279', 'https://openalex.org/W2470673105', 'https://openalex.org/W2489334776', 'https://openalex.org/W2504061866', 'https://openalex.org/W2512302303', 'https://openalex.org/W2555428947', 'https://openalex.org/W2613843855', 'https://openalex.org/W2623779865', 'https://openalex.org/W2624259507', 'https://openalex.org/W2739681832', 'https://openalex.org/W2739725258', 'https://openalex.org/W2742148605', 'https://openalex.org/W2761423999', 'https://openalex.org/W2766236516', 'https://openalex.org/W2798955519', 'https://openalex.org/W2887657574', 'https://openalex.org/W2890940245', 'https://openalex.org/W2962832505', 'https://openalex.org/W2962883855', 'https://openalex.org/W2962964385', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963261455', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963790827', 'https://openalex.org/W2964262738']","The recent surge of text-based online counseling applications enables us to collect and analyze interactions between counselors and clients. A dataset of those interactions can be used to learn to automatically classify the client utterances into categories that help counselors in diagnosing client status and predicting counseling outcome. With proper anonymization, we collect counselor-client dialogues, define meaningful categories of client utterances with professional counselors, and develop a novel neural network model for classifying the client utterances. The central idea of our model, ConvMFiT, is a pre-trained conversation model which consists of a general language model built from an out-of-domain corpus and two role-specific language models built from unlabeled in-domain dialogues. The classification result shows that ConvMFiT outperforms state-of-the-art comparison models. Further, the attention weights in the learned model confirm that the model finds expected linguistic patterns for each category.",1.0
SKG_DIA_305,https://openalex.org/W3103942726,2020,4,"['https://openalex.org/W1793121960', 'https://openalex.org/W1959608418', 'https://openalex.org/W2101105183', 'https://openalex.org/W2514480375', 'https://openalex.org/W2547875792', 'https://openalex.org/W2746626573', 'https://openalex.org/W2780155557', 'https://openalex.org/W2788403449', 'https://openalex.org/W2798914047', 'https://openalex.org/W2810840719', 'https://openalex.org/W2891612330', 'https://openalex.org/W2903396356', 'https://openalex.org/W2903538854', 'https://openalex.org/W2914442349', 'https://openalex.org/W2915295540', 'https://openalex.org/W2947182319', 'https://openalex.org/W2949413855', 'https://openalex.org/W2950662112', 'https://openalex.org/W2951008357', 'https://openalex.org/W2951725892', 'https://openalex.org/W2953071719', 'https://openalex.org/W2953073956', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962831269', 'https://openalex.org/W2962858109', 'https://openalex.org/W2962879001', 'https://openalex.org/W2962912551', 'https://openalex.org/W2963134326', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964158321', 'https://openalex.org/W2964222296', 'https://openalex.org/W2964332824', 'https://openalex.org/W2970469901', 'https://openalex.org/W2970786335', 'https://openalex.org/W2970866659', 'https://openalex.org/W2970909667', 'https://openalex.org/W2996507500', 'https://openalex.org/W2998201756', 'https://openalex.org/W3034930293', 'https://openalex.org/W3102521862', 'https://openalex.org/W3102564565', 'https://openalex.org/W4288245792', 'https://openalex.org/W4288614963', 'https://openalex.org/W4295720520', 'https://openalex.org/W4297786749', 'https://openalex.org/W4298393544', 'https://openalex.org/W4306716473', 'https://openalex.org/W4385245566']","Response generation for task-oriented dialogues implicitly optimizes two objectives at the same time: task completion and language quality. Conditioned response generation serves as an effective approach to separately and better optimize these two objectives. Such an approach relies on system action annotations which are expensive to obtain. To alleviate the need of action annotations, latent action learning is introduced to map each utterance to a latent representation. However, this approach is prone to over-dependence on the training data, and the generalization capability is thus restricted. To address this issue, we propose to learn natural language actions that represent utterances as a span of words. This explicit action representation promotes generalization via the compositional structure of language. It also enables an explainable generation process. Our proposed unsupervised approach learns a memory component to summarize system utterances into a short span of words. To further promote a compact action representation, we propose an auxiliary task that restores state annotations as the summarized dialogue context using the memory component. Our proposed approach outperforms latent action baselines on MultiWOZ, a benchmark multi-domain dataset.",1.0
SKG_DIA_306,https://openalex.org/W2963068985,2017,299,"['https://openalex.org/W91852349', 'https://openalex.org/W119047706', 'https://openalex.org/W1948566616', 'https://openalex.org/W1975244201', 'https://openalex.org/W2007807439', 'https://openalex.org/W2024632416', 'https://openalex.org/W2026505290', 'https://openalex.org/W2046765929', 'https://openalex.org/W2059157630', 'https://openalex.org/W2062175565', 'https://openalex.org/W2083205357', 'https://openalex.org/W2099118758', 'https://openalex.org/W2108682071', 'https://openalex.org/W2116009284', 'https://openalex.org/W2119717200', 'https://openalex.org/W2128965063', 'https://openalex.org/W2151814822', 'https://openalex.org/W2157331557', 'https://openalex.org/W2204302769', 'https://openalex.org/W2250297846', 'https://openalex.org/W2295953541', 'https://openalex.org/W2412715517', 'https://openalex.org/W2412899141', 'https://openalex.org/W2473329891', 'https://openalex.org/W2473965551', 'https://openalex.org/W2551571666', 'https://openalex.org/W2571927164', 'https://openalex.org/W2807142242', 'https://openalex.org/W2949252816', 'https://openalex.org/W2950483141', 'https://openalex.org/W2962682659', 'https://openalex.org/W2962776342', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963412005', 'https://openalex.org/W2963546833', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963974889', 'https://openalex.org/W2964210218', 'https://openalex.org/W4238430687', 'https://openalex.org/W4295249402', 'https://openalex.org/W4298134534']","Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, Li Deng. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017.",1.0
SKG_DIA_307,https://openalex.org/W2741333084,2017,28,[],"Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.",0.993103448275862
SKG_DIA_308,https://openalex.org/W2964046296,2019,57,"['https://openalex.org/W10957333', 'https://openalex.org/W182831726', 'https://openalex.org/W219040644', 'https://openalex.org/W343636949', 'https://openalex.org/W635530177', 'https://openalex.org/W1518951372', 'https://openalex.org/W1591706642', 'https://openalex.org/W1614298861', 'https://openalex.org/W1841959837', 'https://openalex.org/W1902237438', 'https://openalex.org/W1948566616', 'https://openalex.org/W1993378086', 'https://openalex.org/W2064675550', 'https://openalex.org/W2099471712', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140679639', 'https://openalex.org/W2153579005', 'https://openalex.org/W2168490009', 'https://openalex.org/W2170973209', 'https://openalex.org/W2335122196', 'https://openalex.org/W2542835211', 'https://openalex.org/W2571927164', 'https://openalex.org/W2581637843', 'https://openalex.org/W2584185835', 'https://openalex.org/W2594726847', 'https://openalex.org/W2756487349', 'https://openalex.org/W2798494119', 'https://openalex.org/W2798888952', 'https://openalex.org/W2806936550', 'https://openalex.org/W2889186204', 'https://openalex.org/W2890969459', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2950577311', 'https://openalex.org/W2951883832', 'https://openalex.org/W2952729433', 'https://openalex.org/W2952938873', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963360026', 'https://openalex.org/W2963420272', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963854351', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964268978', 'https://openalex.org/W2964308564', 'https://openalex.org/W3022187094', 'https://openalex.org/W3088653102', 'https://openalex.org/W4285719527', 'https://openalex.org/W4294149591', 'https://openalex.org/W4294170691', 'https://openalex.org/W4295249402', 'https://openalex.org/W4306716473', 'https://openalex.org/W4320013936']","The sequential order of utterances is often meaningful in coherent dialogues, and the order changes of utterances could lead to low-quality and incoherent conversations. We consider the order information as a crucial supervised signal for dialogue learning, which, however, has been neglected by many previous dialogue systems. Therefore, in this paper, we introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the flow of conversation in dialogues. Given a sampled utterance pair triple, the task is to predict whether it is ordered or misordered. Then we propose a sampling-based self-supervised network SSN to perform the prediction with sampled triple references from previous dialogue history. Furthermore, we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and task-oriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets.",1.0
SKG_DIA_309,https://openalex.org/W3035068109,2020,120,"['https://openalex.org/W1840435438', 'https://openalex.org/W2741986794', 'https://openalex.org/W2898875342', 'https://openalex.org/W2913443447', 'https://openalex.org/W2916772188', 'https://openalex.org/W2938704169', 'https://openalex.org/W2946609015', 'https://openalex.org/W2950681488', 'https://openalex.org/W2955315729', 'https://openalex.org/W2962788902', 'https://openalex.org/W2962805889', 'https://openalex.org/W2962974452', 'https://openalex.org/W2962989446', 'https://openalex.org/W2963149412', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963531095', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963846996', 'https://openalex.org/W2968297680', 'https://openalex.org/W2969574947', 'https://openalex.org/W2970453125', 'https://openalex.org/W2970476646', 'https://openalex.org/W2971883198', 'https://openalex.org/W2995404354', 'https://openalex.org/W2996287690', 'https://openalex.org/W2997012196', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035239386', 'https://openalex.org/W4288113479', 'https://openalex.org/W4385245566']","Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases. We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. We demonstrate the efficacy of our approach across several dialogue tasks.",1.0
SKG_DIA_310,https://openalex.org/W2963167310,2016,1047,"['https://openalex.org/W10957333', 'https://openalex.org/W319421170', 'https://openalex.org/W1518951372', 'https://openalex.org/W1519256446', 'https://openalex.org/W1591706642', 'https://openalex.org/W1592751638', 'https://openalex.org/W1604513301', 'https://openalex.org/W1681299129', 'https://openalex.org/W1757796397', 'https://openalex.org/W1847211030', 'https://openalex.org/W1934909785', 'https://openalex.org/W1948566616', 'https://openalex.org/W1970207841', 'https://openalex.org/W1975244201', 'https://openalex.org/W1987326241', 'https://openalex.org/W2004637830', 'https://openalex.org/W2046765929', 'https://openalex.org/W2068687198', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111526438', 'https://openalex.org/W2115101920', 'https://openalex.org/W2117989772', 'https://openalex.org/W2119717200', 'https://openalex.org/W2125308790', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132997613', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140054881', 'https://openalex.org/W2155027007', 'https://openalex.org/W2160458012', 'https://openalex.org/W2163068732', 'https://openalex.org/W2164637623', 'https://openalex.org/W2168490009', 'https://openalex.org/W2176263492', 'https://openalex.org/W2252011326', 'https://openalex.org/W2257979135', 'https://openalex.org/W2296073425', 'https://openalex.org/W2328886022', 'https://openalex.org/W2335122196', 'https://openalex.org/W2340944142', 'https://openalex.org/W2395531022', 'https://openalex.org/W2417401578', 'https://openalex.org/W2949801941', 'https://openalex.org/W2951813108', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963797754', 'https://openalex.org/W2963903950', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964179661', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352131', 'https://openalex.org/W3099293669', 'https://openalex.org/W3121854931', 'https://openalex.org/W4234099752', 'https://openalex.org/W4298857966']","Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes.Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning.In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue.The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity, coherence, and ease of answering (related to forward-looking function).We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation.This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.",1.0
SKG_DIA_311,https://openalex.org/W3106495716,2020,51,"['https://openalex.org/W10050918', 'https://openalex.org/W1522301498', 'https://openalex.org/W1604102475', 'https://openalex.org/W1975244201', 'https://openalex.org/W1986362700', 'https://openalex.org/W1993567041', 'https://openalex.org/W2055537935', 'https://openalex.org/W2116341502', 'https://openalex.org/W2133564696', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250456405', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251058040', 'https://openalex.org/W2251235149', 'https://openalex.org/W2556468274', 'https://openalex.org/W2606974598', 'https://openalex.org/W2798367796', 'https://openalex.org/W2882101086', 'https://openalex.org/W2891732163', 'https://openalex.org/W2892375082', 'https://openalex.org/W2896457183', 'https://openalex.org/W2945475330', 'https://openalex.org/W2946824041', 'https://openalex.org/W2950038834', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962831269', 'https://openalex.org/W2962946486', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963106169', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963858333', 'https://openalex.org/W2963964898', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970404807', 'https://openalex.org/W2971124188', 'https://openalex.org/W2973230427', 'https://openalex.org/W2979400990', 'https://openalex.org/W2988252747', 'https://openalex.org/W2996317432', 'https://openalex.org/W2998228050', 'https://openalex.org/W3008966357', 'https://openalex.org/W3034573951', 'https://openalex.org/W3119649668', 'https://openalex.org/W4288027128', 'https://openalex.org/W4288094254', 'https://openalex.org/W4297733535', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Dialogue state tracking (DST) aims at estimating the current dialogue state given all the preceding conversation. For multi-domain DST, the data sparsity problem is a major obstacle due to increased numbers of state candidates and dialogue lengths. To encode the dialogue context efficiently, we utilize the previous dialogue state (predicted) and the current dialogue utterance as the input for DST. To consider relations among different domain-slots, the schema graph involving prior knowledge is exploited. In this paper, a novel context and schema fusion network is proposed to encode the dialogue context and schema graph by using internal and external attention mechanisms. Experiment results show that our approach can outperform strong baselines, and the previous state-of-the-art method (SOM-DST) can also be improved by our proposed schema graph.",1.0
SKG_DIA_313,https://openalex.org/W2971316210,2019,12,"['https://openalex.org/W10050918', 'https://openalex.org/W1577202350', 'https://openalex.org/W2108598243', 'https://openalex.org/W2250297846', 'https://openalex.org/W2318810549', 'https://openalex.org/W2413533759', 'https://openalex.org/W2419539795', 'https://openalex.org/W2593864460', 'https://openalex.org/W2604698497', 'https://openalex.org/W2611029872', 'https://openalex.org/W2612445135', 'https://openalex.org/W2806600904', 'https://openalex.org/W2896457183', 'https://openalex.org/W2939689074', 'https://openalex.org/W2948110372', 'https://openalex.org/W2949252816', 'https://openalex.org/W2952267213', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963469388', 'https://openalex.org/W2963491014', 'https://openalex.org/W2963662719', 'https://openalex.org/W2963788376', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964210218', 'https://openalex.org/W2972437240', 'https://openalex.org/W4239072543', 'https://openalex.org/W4295249402', 'https://openalex.org/W4297775537', 'https://openalex.org/W4297785815', 'https://openalex.org/W4299585995', 'https://openalex.org/W4385245566']","Matthew Henderson, Ivan Vulić, Iñigo Casanueva, Paweł Budzianowski, Daniela Gerz, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrkšić, Pei-Hao Su. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations. 2019.",0.995475113122172
SKG_DIA_314,https://openalex.org/W3034643843,2020,18,"['https://openalex.org/W1947758080', 'https://openalex.org/W1948566616', 'https://openalex.org/W1970207841', 'https://openalex.org/W2004637830', 'https://openalex.org/W2050523636', 'https://openalex.org/W2097828466', 'https://openalex.org/W2104787899', 'https://openalex.org/W2122514299', 'https://openalex.org/W2139079654', 'https://openalex.org/W2156718681', 'https://openalex.org/W2157812664', 'https://openalex.org/W2176263492', 'https://openalex.org/W2291723583', 'https://openalex.org/W2752047430', 'https://openalex.org/W2808815309', 'https://openalex.org/W2949413855', 'https://openalex.org/W2951176429', 'https://openalex.org/W2953035981', 'https://openalex.org/W2962956378', 'https://openalex.org/W2964077278', 'https://openalex.org/W3100380967', 'https://openalex.org/W4230563027']","Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness.",1.0
SKG_DIA_315,https://openalex.org/W3105480731,2020,11,"['https://openalex.org/W1522301498', 'https://openalex.org/W1785674045', 'https://openalex.org/W2119015791', 'https://openalex.org/W2154455818', 'https://openalex.org/W2250297846', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251355666', 'https://openalex.org/W2293363371', 'https://openalex.org/W2431080869', 'https://openalex.org/W2438667436', 'https://openalex.org/W2507756961', 'https://openalex.org/W2556468274', 'https://openalex.org/W2606974598', 'https://openalex.org/W2798367796', 'https://openalex.org/W2888882903', 'https://openalex.org/W2889448364', 'https://openalex.org/W2912889105', 'https://openalex.org/W2930865789', 'https://openalex.org/W2945475330', 'https://openalex.org/W2952592807', 'https://openalex.org/W2954492830', 'https://openalex.org/W2962831269', 'https://openalex.org/W2963009325', 'https://openalex.org/W2963243930', 'https://openalex.org/W2963283951', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963435192', 'https://openalex.org/W2963641152', 'https://openalex.org/W2963797754', 'https://openalex.org/W2964006684', 'https://openalex.org/W2964046296', 'https://openalex.org/W2964057895', 'https://openalex.org/W2964101860', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970404807', 'https://openalex.org/W2970529793', 'https://openalex.org/W2972777589', 'https://openalex.org/W2973230427', 'https://openalex.org/W2978426779', 'https://openalex.org/W2979400990', 'https://openalex.org/W2997771882', 'https://openalex.org/W2998228050', 'https://openalex.org/W2998432370', 'https://openalex.org/W3008186200', 'https://openalex.org/W3016625483', 'https://openalex.org/W3034573951', 'https://openalex.org/W3034917899', 'https://openalex.org/W3100110884', 'https://openalex.org/W3119649668', 'https://openalex.org/W3148060700', 'https://openalex.org/W4231665360', 'https://openalex.org/W4288094254']","Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical DST problem that is rarely discussed, i.e., learning efficiently with limited labeled data. We present and investigate two self-supervised objectives: preserving latent consistency and modeling conversational behavior. We encourage a DST model to have consistent latent distributions given a perturbed input, making it more robust to an unseen scenario. We also add an auxiliary utterance generation task, modeling a potential correlation between conversational behavior and dialogue states. The experimental results show that our proposed self-supervised signals can improve joint goal accuracy by 8.95% when only 1% labeled data is used on the MultiWOZ dataset. We can achieve an additional 1.76% improvement if some unlabeled data is jointly trained as semi-supervised learning. We analyze and visualize how our proposed self-supervised signals help the DST task and hope to stimulate future data-efficient DST research.",1.0
SKG_DIA_316,https://openalex.org/W2803503442,2018,22,"['https://openalex.org/W175385064', 'https://openalex.org/W1505802906', 'https://openalex.org/W1944193361', 'https://openalex.org/W2131799829', 'https://openalex.org/W2176669720', 'https://openalex.org/W2180571361', 'https://openalex.org/W2525212249', 'https://openalex.org/W2806560294', 'https://openalex.org/W4300925350']","Stephanie M. Lukin, Felix Gervits, Cory J. Hayes, Pooja Moolchandani, Anton Leuski, John G. Rogers III, Carlos Sanchez Amaro, Matthew Marge, Clare R. Voss, David Traum. Proceedings of ACL 2018, System Demonstrations. 2018.",0.990990990990991
SKG_DIA_317,https://openalex.org/W3104123491,2020,78,"['https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W2157331557', 'https://openalex.org/W2176263492', 'https://openalex.org/W2547875792', 'https://openalex.org/W2581377246', 'https://openalex.org/W2586847566', 'https://openalex.org/W2752047430', 'https://openalex.org/W2794557536', 'https://openalex.org/W2807873315', 'https://openalex.org/W2891103209', 'https://openalex.org/W2891501508', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898875342', 'https://openalex.org/W2915295540', 'https://openalex.org/W2950009015', 'https://openalex.org/W2950635152', 'https://openalex.org/W2950902819', 'https://openalex.org/W2951508633', 'https://openalex.org/W2951807227', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963159735', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963789888', 'https://openalex.org/W2963825865', 'https://openalex.org/W2963945575', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964345285', 'https://openalex.org/W2964458951', 'https://openalex.org/W2966404868', 'https://openalex.org/W2969895640', 'https://openalex.org/W2970139579', 'https://openalex.org/W2970287057', 'https://openalex.org/W2970988759', 'https://openalex.org/W2972664115', 'https://openalex.org/W2979478117', 'https://openalex.org/W2986867746', 'https://openalex.org/W2995183464', 'https://openalex.org/W2997300509', 'https://openalex.org/W2997896082', 'https://openalex.org/W2998083599', 'https://openalex.org/W3000779003', 'https://openalex.org/W3006065545', 'https://openalex.org/W4287900772', 'https://openalex.org/W4297798436', 'https://openalex.org/W4385245566']","Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.",1.0
SKG_DIA_318,https://openalex.org/W2929767294,2019,17,"['https://openalex.org/W119047706', 'https://openalex.org/W630532510', 'https://openalex.org/W1518951372', 'https://openalex.org/W1840435438', 'https://openalex.org/W1879966306', 'https://openalex.org/W2151814822', 'https://openalex.org/W2153579005', 'https://openalex.org/W2328886022', 'https://openalex.org/W2525127255', 'https://openalex.org/W2608787653', 'https://openalex.org/W2784400615', 'https://openalex.org/W2787560479', 'https://openalex.org/W2794557536', 'https://openalex.org/W2899231639', 'https://openalex.org/W2899503556', 'https://openalex.org/W2907308297', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963035145', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963527228', 'https://openalex.org/W2963790827', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963918774', 'https://openalex.org/W2964352131', 'https://openalex.org/W3023071679']","Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.",1.0
SKG_DIA_1,https://openalex.org/W2890394457,2018,211,"['https://openalex.org/W1793121960', 'https://openalex.org/W2399060250', 'https://openalex.org/W2586847566', 'https://openalex.org/W2688962481', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963149412', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963520511', 'https://openalex.org/W2964119254', 'https://openalex.org/W2964210218', 'https://openalex.org/W2964352131']","Current dialogue systems fail at being engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and only contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.",1.0
NOVEL_MT_0,https://openalex.org/W3197014887,2021,0,"['https://openalex.org/W1821462560', 'https://openalex.org/W2004915807', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2183341477', 'https://openalex.org/W2594047108', 'https://openalex.org/W2611838487', 'https://openalex.org/W2896457183', 'https://openalex.org/W2918914336', 'https://openalex.org/W2944815030', 'https://openalex.org/W2949973181', 'https://openalex.org/W2962863357', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964212410', 'https://openalex.org/W2964308564', 'https://openalex.org/W2985301125', 'https://openalex.org/W3031432954']",Neural machine translation (NMT) models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against the gold labels. In low-resource scenarios and NMT models tend to perform poorly because the model training quickly converges to a point where the softmax distribution computed using logits approaches the gold label distribution. Although label smoothing is a well-known solution to address this issue and we further propose to divide the logits by a temperature coefficient greater than one and forcing the softmax distribution to be smoother during training. This makes it harder for the model to quickly over-fit. In our experiments on 11 language pairs in the low-resource Asian Language Treebank dataset and we observed significant improvements in translation quality. Our analysis focuses on finding the right balance of label smoothing and softmax tempering which indicates that they are orthogonal methods. Finally and a study of softmax entropies and gradients reveal the impact of our method on the internal behavior of our NMT models.,1.0
NOVEL_MT_2,https://openalex.org/W3196896228,2021,12,"['https://openalex.org/W222053410', 'https://openalex.org/W1503071992', 'https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133012565', 'https://openalex.org/W2136353104', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250969425', 'https://openalex.org/W2397490041', 'https://openalex.org/W2608029998', 'https://openalex.org/W2669742347', 'https://openalex.org/W2767019613', 'https://openalex.org/W2771249222', 'https://openalex.org/W2794365787', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2904829696', 'https://openalex.org/W2922158773', 'https://openalex.org/W2951039930', 'https://openalex.org/W2952446148', 'https://openalex.org/W2952889708', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962882341', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963686995', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964345285', 'https://openalex.org/W2964669873', 'https://openalex.org/W2970240344', 'https://openalex.org/W2970489252', 'https://openalex.org/W2971287409', 'https://openalex.org/W2971347700', 'https://openalex.org/W2971737394', 'https://openalex.org/W2985067290', 'https://openalex.org/W3012405907', 'https://openalex.org/W3033962033', 'https://openalex.org/W3034351728', 'https://openalex.org/W3034999754', 'https://openalex.org/W3035520602', 'https://openalex.org/W3035629723', 'https://openalex.org/W3099925113', 'https://openalex.org/W3101668578', 'https://openalex.org/W3101683892', 'https://openalex.org/W3103733040', 'https://openalex.org/W3105218667', 'https://openalex.org/W3111706314', 'https://openalex.org/W3113225429', 'https://openalex.org/W3120168417', 'https://openalex.org/W3120749277', 'https://openalex.org/W3120964679', 'https://openalex.org/W3147710239', 'https://openalex.org/W3168420886', 'https://openalex.org/W3173680274', 'https://openalex.org/W3175424132', 'https://openalex.org/W3175870450', 'https://openalex.org/W4385245566', 'https://openalex.org/W4404781009']","Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue characteristics of chat, such as dialogue coherence and speaker personality, are neglected. In this paper, we propose to promote the chat translation by introducing the modeling of dialogue characteristics into the NCT model. To this end, we design four auxiliary tasks including monolingual response generation, cross-lingual response generation, next utterance discrimination, and speaker identification. Together with the main chat translation task, we optimize the enhanced NCT model through the training objectives of all these tasks. By this means, the NCT model can be enhanced by capturing the inherent dialogue characteristics, thus generating more coherent and speaker-relevant translations. Comprehensive experiments on four language directions (English<->German and English<->Chinese) verify the effectiveness and superiority of the proposed approach.",1.0
NOVEL_MT_4,https://openalex.org/W3197322108,2021,0,"['https://openalex.org/W222053410', 'https://openalex.org/W2126725946', 'https://openalex.org/W2143017621', 'https://openalex.org/W2156985047', 'https://openalex.org/W2251994258', 'https://openalex.org/W2572474373', 'https://openalex.org/W2885950361', 'https://openalex.org/W2905266130', 'https://openalex.org/W2916835973', 'https://openalex.org/W2945808722', 'https://openalex.org/W2953173959', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963062785', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963877297', 'https://openalex.org/W2964247056', 'https://openalex.org/W2970641574', 'https://openalex.org/W2984051011', 'https://openalex.org/W2986791765', 'https://openalex.org/W3098341425']",Product reviews provide valuable feedback of the customers and however and they are available today only in English on most of the e-commerce platforms. The nature of reviews provided by customers in any multilingual country poses unique challenges for machine translation such as code-mixing and ungrammatical sentences and presence of colloquial terms and lack of e-commerce parallel corpus etc. Given that 44% of Indian population speaks and operates in Hindi language and we address the above challenges by presenting an English–to–Hindi neural machine translation (NMT) system to translate the product reviews available on e-commerce websites by creating an in-domain parallel corpora and handling various types of noise in reviews via two data augmentation techniques and viz. (i). a novel phrase augmentation technique (PhrRep) where the syntactic noun phrases in sentences are replaced by the other noun phrases carrying different meanings but in similar context; and (ii). a novel attention guided noise augmentation (AttnNoise) technique to make our NMT model robust towards various noise. Evaluation shows that using the proposed augmentation techniques we achieve a 6.67 BLEU score improvement over the baseline model. In order to show that our proposed approach is not language-specific and we also perform experiments for two other language pairs and viz. En-Fr (MTNT18 corpus) and En-De (IWSLT17) that yield the improvements of 2.55 and 0.91 BLEU points and respectively and over the baselines.,0.994535519125683
NOVEL_MT_5,https://openalex.org/W3161838540,2021,13,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2157331557', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250969425', 'https://openalex.org/W2251180427', 'https://openalex.org/W2467834614', 'https://openalex.org/W2539350388', 'https://openalex.org/W2550821151', 'https://openalex.org/W2557436004', 'https://openalex.org/W2595715041', 'https://openalex.org/W2758334418', 'https://openalex.org/W2915573484', 'https://openalex.org/W2933138175', 'https://openalex.org/W2933966104', 'https://openalex.org/W2950513705', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970328625', 'https://openalex.org/W2972680990', 'https://openalex.org/W2972972637', 'https://openalex.org/W2978824654', 'https://openalex.org/W2982026991', 'https://openalex.org/W2985976073', 'https://openalex.org/W3034987021', 'https://openalex.org/W3045734701', 'https://openalex.org/W3087197566', 'https://openalex.org/W3093871477', 'https://openalex.org/W4385245566']","One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state of the art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, human evaluation confirms that the translations are of the same quality.",1.0
NOVEL_MT_9,https://openalex.org/W3016145922,2020,4,"['https://openalex.org/W2101210369', 'https://openalex.org/W2111316763', 'https://openalex.org/W2124807415', 'https://openalex.org/W2145094598', 'https://openalex.org/W2163568299', 'https://openalex.org/W2561274697', 'https://openalex.org/W2932618389', 'https://openalex.org/W2944815030', 'https://openalex.org/W2950428495', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964121744', 'https://openalex.org/W2995746049', 'https://openalex.org/W3011824510', 'https://openalex.org/W3034906024']","Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems.",1.0
NOVEL_MT_10,https://openalex.org/W3197336992,2021,13,"['https://openalex.org/W1484551447', 'https://openalex.org/W1522301498', 'https://openalex.org/W1542523037', 'https://openalex.org/W1968355947', 'https://openalex.org/W1992208280', 'https://openalex.org/W2089559088', 'https://openalex.org/W2101105183', 'https://openalex.org/W2296319761', 'https://openalex.org/W2531499355', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2564590721', 'https://openalex.org/W2691152923', 'https://openalex.org/W2804815760', 'https://openalex.org/W2887920589', 'https://openalex.org/W2896457183', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2952638691', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963322627', 'https://openalex.org/W2963326510', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963818033', 'https://openalex.org/W2964085268', 'https://openalex.org/W2970789589', 'https://openalex.org/W2991497298', 'https://openalex.org/W3035019713', 'https://openalex.org/W3035390927', 'https://openalex.org/W3092189037', 'https://openalex.org/W3092425680', 'https://openalex.org/W3102576821', 'https://openalex.org/W3127518054', 'https://openalex.org/W3128413221', 'https://openalex.org/W3188960136', 'https://openalex.org/W4229812034', 'https://openalex.org/W4230471307', 'https://openalex.org/W4294908407', 'https://openalex.org/W4385245566']","Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly across language pairs. In this paper, we propose a new learning objective for MNMT based on distributionally robust optimization, which minimizes the worst-case expected loss over the set of language pairs. We further show how to practically optimize this objective for large translation corpora using an iterated best response scheme, which is both effective and incurs negligible additional computational cost compared to standard empirical risk minimization. We perform extensive experiments on three sets of languages from two datasets and show that our method consistently outperforms strong baseline methods in terms of average and per-language performance under both many-to-one and one-to-many translation settings.",1.0
NOVEL_MT_11,https://openalex.org/W3174659183,2021,17,"['https://openalex.org/W62973420', 'https://openalex.org/W203948990', 'https://openalex.org/W222053410', 'https://openalex.org/W1489525520', 'https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2608029998', 'https://openalex.org/W2739918945', 'https://openalex.org/W2767019613', 'https://openalex.org/W2798636921', 'https://openalex.org/W2799051177', 'https://openalex.org/W2806412155', 'https://openalex.org/W2806532810', 'https://openalex.org/W2842186253', 'https://openalex.org/W2888159079', 'https://openalex.org/W2889436406', 'https://openalex.org/W2891534142', 'https://openalex.org/W2898936689', 'https://openalex.org/W2902582221', 'https://openalex.org/W2922158773', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962788148', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962834107', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964308564', 'https://openalex.org/W2966924523', 'https://openalex.org/W2971347700', 'https://openalex.org/W2983108239', 'https://openalex.org/W3029849870', 'https://openalex.org/W3033962033', 'https://openalex.org/W3035629723', 'https://openalex.org/W3046531489', 'https://openalex.org/W3097996341', 'https://openalex.org/W3104564752', 'https://openalex.org/W3105214104', 'https://openalex.org/W3110300144', 'https://openalex.org/W3118485656', 'https://openalex.org/W3155609600', 'https://openalex.org/W4287373797', 'https://openalex.org/W4287637331', 'https://openalex.org/W4385245566']","Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T. Martins, Graham Neubig. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",0.9915966386554622
NOVEL_MT_15,https://openalex.org/W3175212568,2021,58,"['https://openalex.org/W46679369', 'https://openalex.org/W1522301498', 'https://openalex.org/W1562624811', 'https://openalex.org/W1976084081', 'https://openalex.org/W1998503232', 'https://openalex.org/W2153579005', 'https://openalex.org/W2158131535', 'https://openalex.org/W2465663554', 'https://openalex.org/W2531207078', 'https://openalex.org/W2793477525', 'https://openalex.org/W2889518897', 'https://openalex.org/W2896457183', 'https://openalex.org/W2896691342', 'https://openalex.org/W2912521296', 'https://openalex.org/W2921317107', 'https://openalex.org/W2949973181', 'https://openalex.org/W2955388956', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088785', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963887123', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964236337', 'https://openalex.org/W2964473287', 'https://openalex.org/W2978943549', 'https://openalex.org/W2998353611', 'https://openalex.org/W3001816066', 'https://openalex.org/W3035207248', 'https://openalex.org/W3066373881', 'https://openalex.org/W4289440907', 'https://openalex.org/W4294170691', 'https://openalex.org/W4385245566']","Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_16,https://openalex.org/W3168264548,2021,20,"['https://openalex.org/W658020064', 'https://openalex.org/W1614298861', 'https://openalex.org/W1840435438', 'https://openalex.org/W1956340063', 'https://openalex.org/W1975742580', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2143668817', 'https://openalex.org/W2154652894', 'https://openalex.org/W2164984707', 'https://openalex.org/W2313299119', 'https://openalex.org/W2564590796', 'https://openalex.org/W2756675635', 'https://openalex.org/W2760738985', 'https://openalex.org/W2915756181', 'https://openalex.org/W2936695845', 'https://openalex.org/W2945405384', 'https://openalex.org/W2947771965', 'https://openalex.org/W2952215760', 'https://openalex.org/W2953287808', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963909453', 'https://openalex.org/W2970597249', 'https://openalex.org/W2970641574', 'https://openalex.org/W2970785793', 'https://openalex.org/W2970986500', 'https://openalex.org/W2970986510', 'https://openalex.org/W2971094522', 'https://openalex.org/W2996403597', 'https://openalex.org/W3035252911', 'https://openalex.org/W3035390927', 'https://openalex.org/W3084095723', 'https://openalex.org/W3100806282', 'https://openalex.org/W3103901889', 'https://openalex.org/W3104033643', 'https://openalex.org/W3119107795', 'https://openalex.org/W3119878165', 'https://openalex.org/W3119881489', 'https://openalex.org/W3120791402', 'https://openalex.org/W3172806051', 'https://openalex.org/W3205309080', 'https://openalex.org/W4287642955', 'https://openalex.org/W4300223101']","Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the source with the machine translated sentence, thus avoiding the need for both reference translations and labelled training data. The metric builds on state-of-the-art embedding-based approaches – namely BERTScore and Word Mover's Distance – by incorporating a notion of sentence semantic similarity. By doing so, it achieves better correlation with human scores on different datasets. We show that it outperforms these and other metrics in the standard monolingual setting (MT-reference translation), a well as in the source-MT bilingual setting, where it performs on par with glass-box approaches to quality estimation that rely on MT model information.",0.9921259842519684
NOVEL_MT_17,https://openalex.org/W3197257519,2021,1,"['https://openalex.org/W19526395', 'https://openalex.org/W21337280', 'https://openalex.org/W22168010', 'https://openalex.org/W77450199', 'https://openalex.org/W222053410', 'https://openalex.org/W2013025416', 'https://openalex.org/W2064557848', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105410942', 'https://openalex.org/W2138517257', 'https://openalex.org/W2153653739', 'https://openalex.org/W2161943765', 'https://openalex.org/W2177801600', 'https://openalex.org/W2250872883', 'https://openalex.org/W2305312316', 'https://openalex.org/W2561792472', 'https://openalex.org/W2789910672', 'https://openalex.org/W2894218541', 'https://openalex.org/W2903158431', 'https://openalex.org/W2953072129', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963505445', 'https://openalex.org/W2963877297', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970641574', 'https://openalex.org/W3082714809', 'https://openalex.org/W3099107321']","Interactive-predictive translation is a collaborative iterative process, where human translators produce translations with the help of machine translation (MT) systems interactively. Various sampling techniques in active learning (AL) exist to update the neural MT (NMT) model in the interactive-predictive scenario. In this paper, we explore term based (named entity count (NEC)) and quality based (quality estimation (QE), sentence similarity (Sim)) sampling techniques – which are used to find the ideal candidates from the incoming data – for human supervision and MT model’s weight updation. We carried out experiments with three language pairs, viz. German-English, Spanish-English and Hindi-English. Our proposed sampling technique yields 1.82, 0.77 and 0.81 BLEU points improvements for German-English, Spanish-English and Hindi-English, respectively, over random sampling based baseline. It also improves the present state-of-the-art by 0.35 and 0.12 BLEU points for German-English and Spanish-English, respectively. Human editing effort in terms of number-of-words-changed also improves by 5 and 4 points for German-English and Spanish-English, respectively, compared to the state-of-the-art.",1.0
NOVEL_MT_19,https://openalex.org/W3203186688,2021,0,"['https://openalex.org/W2101105183', 'https://openalex.org/W2126807068', 'https://openalex.org/W2594047108', 'https://openalex.org/W2962982474', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499433', 'https://openalex.org/W2964007535', 'https://openalex.org/W2970290486', 'https://openalex.org/W3037928693', 'https://openalex.org/W3082928416', 'https://openalex.org/W3093871477', 'https://openalex.org/W3105038888']","This paper describes Charles University submission for Multilingual Low-Resource Translation for Indo-European Languages shared task at WMT21. We competed in translation from Catalan into Romanian, Italian and Occitan. Our systems are based on shared multilingual model. We show that using joint model for multiple similar language pairs improves upon translation quality in each pair. We also demonstrate that chararacter-level bilingual models are competitive for very similar language pairs (Catalan-Occitan) but less so for more distant pairs. We also describe our experiments with multi-task learning, where aside from a textual translation, the models are also trained to perform grapheme-to-phoneme conversion.",0.9950248756218906
NOVEL_MT_21,https://openalex.org/W3167968824,2021,2,"['https://openalex.org/W57814663', 'https://openalex.org/W222053410', 'https://openalex.org/W606759049', 'https://openalex.org/W1494198834', 'https://openalex.org/W1821462560', 'https://openalex.org/W1905522558', 'https://openalex.org/W2014552602', 'https://openalex.org/W2018869373', 'https://openalex.org/W2059637352', 'https://openalex.org/W2077947937', 'https://openalex.org/W2087388117', 'https://openalex.org/W2103123519', 'https://openalex.org/W2121495627', 'https://openalex.org/W2135708429', 'https://openalex.org/W2149327368', 'https://openalex.org/W2183341477', 'https://openalex.org/W2186089609', 'https://openalex.org/W2407080277', 'https://openalex.org/W2493041683', 'https://openalex.org/W2571713365', 'https://openalex.org/W2582956876', 'https://openalex.org/W2768123736', 'https://openalex.org/W2792296443', 'https://openalex.org/W2802023636', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2949328740', 'https://openalex.org/W2950617329', 'https://openalex.org/W2952079278', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963779652', 'https://openalex.org/W2964089206', 'https://openalex.org/W2964161387', 'https://openalex.org/W2972448360', 'https://openalex.org/W2973048981', 'https://openalex.org/W2977183928', 'https://openalex.org/W2982129078', 'https://openalex.org/W2982727400', 'https://openalex.org/W3007142233', 'https://openalex.org/W3015633994', 'https://openalex.org/W3015698636', 'https://openalex.org/W3015703505', 'https://openalex.org/W3015889230', 'https://openalex.org/W3016137827', 'https://openalex.org/W3034625919', 'https://openalex.org/W3037217258', 'https://openalex.org/W3037465386', 'https://openalex.org/W3037542581', 'https://openalex.org/W3037625699', 'https://openalex.org/W3042657922', 'https://openalex.org/W3082845185', 'https://openalex.org/W3084827186', 'https://openalex.org/W3092085609', 'https://openalex.org/W3097301532', 'https://openalex.org/W3101648800', 'https://openalex.org/W3105752438', 'https://openalex.org/W3202218022']","Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms. Focusing on three language directions (English-German/Italian/Spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations. Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that: i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other.",0.988095238095238
NOVEL_MT_23,https://openalex.org/W3200345787,2021,1,"['https://openalex.org/W1522301498', 'https://openalex.org/W2113104171', 'https://openalex.org/W2587694128', 'https://openalex.org/W2892192320', 'https://openalex.org/W2902081112', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963925437', 'https://openalex.org/W2970295111', 'https://openalex.org/W2996287690', 'https://openalex.org/W3102816807', 'https://openalex.org/W3119961061', 'https://openalex.org/W3120459995', 'https://openalex.org/W3121018626', 'https://openalex.org/W3121049435', 'https://openalex.org/W3144827309', 'https://openalex.org/W3173417753', 'https://openalex.org/W3204406378']","This paper describes NiuTrans neural machine translation systems of the WMT 2021 news translation tasks. We made submissions to 9 language directions, including English$\leftrightarrow$$\{$Chinese, Japanese, Russian, Icelandic$\}$ and English$\rightarrow$Hausa tasks. Our primary systems are built on several effective variants of Transformer, e.g., Transformer-DLCL, ODE-Transformer. We also utilize back-translation, knowledge distillation, post-ensemble, and iterative fine-tuning techniques to enhance the model performance further.",1.0
NOVEL_MT_24,https://openalex.org/W3176711365,2021,70,"['https://openalex.org/W1522301498', 'https://openalex.org/W1682403713', 'https://openalex.org/W1821462560', 'https://openalex.org/W2767204723', 'https://openalex.org/W2767434619', 'https://openalex.org/W2807535859', 'https://openalex.org/W2901389167', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2945700568', 'https://openalex.org/W2962743139', 'https://openalex.org/W2962864421', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963430933', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963759780', 'https://openalex.org/W2963779652', 'https://openalex.org/W2964012862', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970925270', 'https://openalex.org/W2972448360', 'https://openalex.org/W2991018369', 'https://openalex.org/W2995343314', 'https://openalex.org/W2997436923', 'https://openalex.org/W3007142233', 'https://openalex.org/W3015633994', 'https://openalex.org/W3015703505', 'https://openalex.org/W3017454464', 'https://openalex.org/W3034672970', 'https://openalex.org/W3035464238', 'https://openalex.org/W3035490255', 'https://openalex.org/W3037542581', 'https://openalex.org/W3085046840', 'https://openalex.org/W3092424727', 'https://openalex.org/W3096490862', 'https://openalex.org/W3101498587', 'https://openalex.org/W3112092703', 'https://openalex.org/W3118578889', 'https://openalex.org/W3162037819', 'https://openalex.org/W3173767661', 'https://openalex.org/W4294103325', 'https://openalex.org/W4297730150', 'https://openalex.org/W6784050962']","Yun Tang, Juan Pino, Xian Li, Changhan Wang, Dmitriy Genzel. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_25,https://openalex.org/W3214532454,2021,29,"['https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123442489', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2613904329', 'https://openalex.org/W2752047430', 'https://openalex.org/W2767206889', 'https://openalex.org/W2789543585', 'https://openalex.org/W2794365787', 'https://openalex.org/W2890501761', 'https://openalex.org/W2892213699', 'https://openalex.org/W2898972706', 'https://openalex.org/W2920538220', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946375144', 'https://openalex.org/W2949644922', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962915948', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964026424', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970101155', 'https://openalex.org/W2970832665', 'https://openalex.org/W2971167892', 'https://openalex.org/W2976965654', 'https://openalex.org/W2981648103', 'https://openalex.org/W2987253016', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990372437', 'https://openalex.org/W2990389671', 'https://openalex.org/W2995999067', 'https://openalex.org/W2996843693', 'https://openalex.org/W2996987694', 'https://openalex.org/W2997284037', 'https://openalex.org/W3000840023', 'https://openalex.org/W3015162217', 'https://openalex.org/W3034892578', 'https://openalex.org/W3035289598', 'https://openalex.org/W3035416964', 'https://openalex.org/W3035490389', 'https://openalex.org/W3039805635', 'https://openalex.org/W3101095987', 'https://openalex.org/W3113715281', 'https://openalex.org/W3114869109', 'https://openalex.org/W3127901106', 'https://openalex.org/W3168817639', 'https://openalex.org/W3174255604', 'https://openalex.org/W4385245566']","Non-autoregressive neural machine translation, which decomposes the dependence on previous target tokens from the inputs of the decoder, has achieved impressive inference speedup but at the cost of inferior accuracy. Previous works employ iterative decoding to improve the translation by applying multiple refinement iterations. However, a serious drawback is that these approaches expose the serious weakness in recognizing the erroneous translation pieces. In this paper, we propose an architecture named RewriteNAT to explicitly learn to rewrite the erroneous translation pieces. Specifically, RewriteNAT utilizes a locator module to locate the erroneous ones, which are then revised into the correct ones by a revisor module. Towards keeping the consistency of data distribution with iterative decoding, an iterative training strategy is employed to further improve the capacity of rewriting. Extensive experiments conducted on several widely-used benchmarks show that RewriteNAT can achieve better performance while significantly reducing decoding time, compared with previous iterative decoding strategies. In particular, RewriteNAT can obtain competitive results with autoregressive translation on WMT14 En-De, En-Fr and WMT16 Ro-En translation benchmarks.",1.0
NOVEL_MT_26,https://openalex.org/W3209783775,2021,15,"['https://openalex.org/W630532510', 'https://openalex.org/W2251743902', 'https://openalex.org/W2891924676', 'https://openalex.org/W2905933322', 'https://openalex.org/W2919290281', 'https://openalex.org/W2949303037', 'https://openalex.org/W2963250244', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970109976', 'https://openalex.org/W3035019713', 'https://openalex.org/W3035464238', 'https://openalex.org/W3090350559', 'https://openalex.org/W3093871477', 'https://openalex.org/W3096966601', 'https://openalex.org/W3102816807', 'https://openalex.org/W3103169714', 'https://openalex.org/W3105005398', 'https://openalex.org/W3105038888', 'https://openalex.org/W3105425516', 'https://openalex.org/W3107826490', 'https://openalex.org/W3127719526', 'https://openalex.org/W3132170818', 'https://openalex.org/W3154456300', 'https://openalex.org/W3173190788', 'https://openalex.org/W3173333551', 'https://openalex.org/W3175301726', 'https://openalex.org/W3175374354']","This report describes Microsoft's machine translation systems for the WMT21 shared task on large-scale multilingual machine translation. We participated in all three evaluation tracks including Large Track and two Small Tracks where the former one is unconstrained and the latter two are fully constrained. Our model submissions to the shared task were initialized with DeltaLM\footnote{\url{https://aka.ms/deltalm}}, a generic pre-trained multilingual encoder-decoder model, and fine-tuned correspondingly with the vast collected parallel data and allowed data sources according to track settings, together with applying progressive learning and iterative back-translation approaches to further improve the performance. Our final submissions ranked first on three tracks in terms of the automatic evaluation metric.",1.0
NOVEL_MT_29,https://openalex.org/W3174937206,2021,0,"['https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153653739', 'https://openalex.org/W2492716757', 'https://openalex.org/W2537667581', 'https://openalex.org/W2576482813', 'https://openalex.org/W2903012348', 'https://openalex.org/W2951770285', 'https://openalex.org/W2962823775', 'https://openalex.org/W2963188492', 'https://openalex.org/W2963225662', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963583256', 'https://openalex.org/W2963691697', 'https://openalex.org/W2963877622', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964029788', 'https://openalex.org/W2964121744', 'https://openalex.org/W2978264350', 'https://openalex.org/W3035669818', 'https://openalex.org/W3082303676', 'https://openalex.org/W3113780189', 'https://openalex.org/W3152995402']","Placeholder translation systems enable the users to specify how a specific phrase is translated in the output sentence. The system is trained to output special placeholder tokens, and the user-specified term is injected into the output through the context-free replacement of the placeholder token. However, this approach could result in ungrammatical sentences because it is often the case that the specified term needs to be inflected according to the context of the output, which is unknown before the translation. To address this problem, we propose a novel method of placeholder translation that can inflect specified terms according to the grammatical construction of the output sentence. We extend the sequence-to-sequence architecture with a character-level decoder that takes the lemma of a user-specified term and the words generated from the word-level decoder to output the correct inflected form of the lemma. We evaluate our approach with a Japanese-to-English translation task in the scientific writing domain, and show that our model can incorporate specified terms in the correct form more successfully than other comparable models.",1.0
NOVEL_MT_31,https://openalex.org/W3188760190,2021,1,"['https://openalex.org/W2463507112', 'https://openalex.org/W2889326796', 'https://openalex.org/W2902503418', 'https://openalex.org/W2904829696', 'https://openalex.org/W2950913377', 'https://openalex.org/W2952649152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963456134', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970947975', 'https://openalex.org/W3010476030', 'https://openalex.org/W3034772996', 'https://openalex.org/W3035072529', 'https://openalex.org/W3035214886', 'https://openalex.org/W3035618017', 'https://openalex.org/W3066373881', 'https://openalex.org/W3101668578', 'https://openalex.org/W3120459995', 'https://openalex.org/W3174172622', 'https://openalex.org/W3176120057', 'https://openalex.org/W3204406378', 'https://openalex.org/W3214236736']","This paper introduces WeChat AI's participation in WMT 2021 shared news translation task on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and English-&gt;German. Our systems are based on the Transformer (Vaswani et al., 2017) with several novel and effective variants. In our experiments, we employ data filtering, large-scale synthetic data generation (i.e., back-translation, knowledge distillation, forward-translation, iterative in-domain knowledge transfer), advanced finetuning approaches, and boosted Self-BLEU based model ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3 case-sensitive BLEU scores on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and English-&gt;German, respectively. The BLEU scores of English-&gt;Chinese, English-&gt;Japanese and Japanese-&gt;English are the highest among all submissions, and that of English-&gt;German is the highest among all constrained submissions.",1.0
NOVEL_MT_35,https://openalex.org/W3199756813,2021,2,"['https://openalex.org/W2144211451', 'https://openalex.org/W2251159668', 'https://openalex.org/W2538358357', 'https://openalex.org/W2804387108', 'https://openalex.org/W2963403868', 'https://openalex.org/W2989156240', 'https://openalex.org/W3082303676', 'https://openalex.org/W3152995402', 'https://openalex.org/W3153567180', 'https://openalex.org/W3174888972']","The majority of language domains require prudent use of terminology to ensure clarity and adequacy of information conveyed. While the correct use of terminology for some languages and domains can be achieved by adapting general-purpose MT systems on large volumes of in-domain parallel data, such quantities of domain-specific data are seldom available for less-resourced languages and niche domains. Furthermore, as exemplified by COVID-19 recently, no domain-specific parallel data is readily available for emerging domains. However, the gravity of this recent calamity created a high demand for reliable translation of critical information regarding pandemic and infection prevention. This work is part of WMT2021 Shared Task: Machine Translation using Terminologies, where we describe Tilde MT systems that are capable of dynamic terminology integration at the time of translation. Our systems achieve up to 94% COVID-19 term use accuracy on the test set of the EN-FR language pair without having access to any form of in-domain information during system training. We conclude our work with a broader discussion considering the Shared Task itself and terminology translation in MT.",1.0
NOVEL_MT_37,https://openalex.org/W3173680274,2021,23,"['https://openalex.org/W222053410', 'https://openalex.org/W1503071992', 'https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W1959608418', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133012565', 'https://openalex.org/W2136353104', 'https://openalex.org/W2149327368', 'https://openalex.org/W2188365844', 'https://openalex.org/W2250969425', 'https://openalex.org/W2397490041', 'https://openalex.org/W2559703163', 'https://openalex.org/W2608029998', 'https://openalex.org/W2669742347', 'https://openalex.org/W2767019613', 'https://openalex.org/W2771249222', 'https://openalex.org/W2794365787', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2904829696', 'https://openalex.org/W2922158773', 'https://openalex.org/W2950886580', 'https://openalex.org/W2951039930', 'https://openalex.org/W2952446148', 'https://openalex.org/W2952889708', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962882341', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963686995', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963714898', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964345285', 'https://openalex.org/W2964669873', 'https://openalex.org/W2970240344', 'https://openalex.org/W2971287409', 'https://openalex.org/W2971347700', 'https://openalex.org/W2985067290', 'https://openalex.org/W3012405907', 'https://openalex.org/W3034351728', 'https://openalex.org/W3034999754', 'https://openalex.org/W3035511085', 'https://openalex.org/W3035629723', 'https://openalex.org/W3097240187', 'https://openalex.org/W3099925113', 'https://openalex.org/W3101668578', 'https://openalex.org/W3101683892', 'https://openalex.org/W3111706314', 'https://openalex.org/W3113225429', 'https://openalex.org/W3118273013', 'https://openalex.org/W3119625789', 'https://openalex.org/W3120168417', 'https://openalex.org/W3120749277', 'https://openalex.org/W3120964679', 'https://openalex.org/W3147710239', 'https://openalex.org/W3168420886', 'https://openalex.org/W4385245566', 'https://openalex.org/W4404781009']","Yunlong Liang, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_39,https://openalex.org/W3170463198,2021,17,"['https://openalex.org/W1631260214', 'https://openalex.org/W1753482797', 'https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2250876691', 'https://openalex.org/W2567571499', 'https://openalex.org/W2579496717', 'https://openalex.org/W2613904329', 'https://openalex.org/W2707890836', 'https://openalex.org/W2744813330', 'https://openalex.org/W2760452458', 'https://openalex.org/W2764043458', 'https://openalex.org/W2886342729', 'https://openalex.org/W2892244498', 'https://openalex.org/W2933138175', 'https://openalex.org/W2945383715', 'https://openalex.org/W2946379889', 'https://openalex.org/W2946794439', 'https://openalex.org/W2947187520', 'https://openalex.org/W2950760213', 'https://openalex.org/W2954647460', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962796826', 'https://openalex.org/W2962863357', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963287528', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963503967', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963643655', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970286654', 'https://openalex.org/W2970925270', 'https://openalex.org/W2996428491', 'https://openalex.org/W2997936605', 'https://openalex.org/W3093818688', 'https://openalex.org/W3098170909', 'https://openalex.org/W3105648266', 'https://openalex.org/W3106144205', 'https://openalex.org/W3116273903', 'https://openalex.org/W3174805614', 'https://openalex.org/W3204406378', 'https://openalex.org/W4385245566']","Domain Adaptation is widely used in practical applications of neural machine translation, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for domain adaptation usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of “divide and conquer” which is based on the importance of neurons or parameters for the translation model. In this method, we first prune the model and only keep the important neurons or parameters, making them responsible for both general-domain and in-domain translation. Then we further train the pruned model supervised by the original whole model with knowledge distillation. Last we expand the model to the original size and fine-tune the added parameters for the in-domain translation. We conducted experiments on different language pairs and domains and the results show that our method can achieve significant improvements compared with several strong baselines.",1.0
NOVEL_MT_40,https://openalex.org/W3169145812,2021,1,"['https://openalex.org/W1522301498', 'https://openalex.org/W1660460062', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2251843845', 'https://openalex.org/W2525778437', 'https://openalex.org/W2540404261', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2757222607', 'https://openalex.org/W2783831488', 'https://openalex.org/W2798742628', 'https://openalex.org/W2886776719', 'https://openalex.org/W2890964657', 'https://openalex.org/W2908336025', 'https://openalex.org/W2912937082', 'https://openalex.org/W2920538220', 'https://openalex.org/W2935811960', 'https://openalex.org/W2949644922', 'https://openalex.org/W2952809536', 'https://openalex.org/W2953345635', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962941628', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963905071', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964047362', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965785213', 'https://openalex.org/W2969762049', 'https://openalex.org/W2985694911', 'https://openalex.org/W4301368689', 'https://openalex.org/W4385245566']","Jian Yang, Shuming Ma, Dongdong Zhang, Juncheng Wan, Zhoujun Li, Ming Zhou. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_42,https://openalex.org/W3154987757,2021,31,"['https://openalex.org/W1895577753', 'https://openalex.org/W1938755728', 'https://openalex.org/W2034031845', 'https://openalex.org/W2040555227', 'https://openalex.org/W2082983083', 'https://openalex.org/W2127863960', 'https://openalex.org/W2194187530', 'https://openalex.org/W2563351168', 'https://openalex.org/W2604838247', 'https://openalex.org/W2606837874', 'https://openalex.org/W2752461400', 'https://openalex.org/W2767899794', 'https://openalex.org/W2785769188', 'https://openalex.org/W2794561587', 'https://openalex.org/W2890623477', 'https://openalex.org/W2896631555', 'https://openalex.org/W2912473624', 'https://openalex.org/W2914094910', 'https://openalex.org/W2916835973', 'https://openalex.org/W2923325523', 'https://openalex.org/W2933138175', 'https://openalex.org/W2951559648', 'https://openalex.org/W2955388956', 'https://openalex.org/W2962782614', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963887123', 'https://openalex.org/W2963919854', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964010678', 'https://openalex.org/W2964047813', 'https://openalex.org/W2964123686', 'https://openalex.org/W2964132420', 'https://openalex.org/W2964247056', 'https://openalex.org/W2964325863', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970558573', 'https://openalex.org/W2970681607', 'https://openalex.org/W2978943549', 'https://openalex.org/W2998353611', 'https://openalex.org/W3001816066', 'https://openalex.org/W3032746405', 'https://openalex.org/W3034771443', 'https://openalex.org/W3035207248', 'https://openalex.org/W3094132514', 'https://openalex.org/W3094194187', 'https://openalex.org/W3094502228', 'https://openalex.org/W3105214104', 'https://openalex.org/W3106274667', 'https://openalex.org/W3115778530', 'https://openalex.org/W3119067070', 'https://openalex.org/W3119786062', 'https://openalex.org/W3120929527', 'https://openalex.org/W3135427360', 'https://openalex.org/W4237752057', 'https://openalex.org/W4289375518', 'https://openalex.org/W4289552783']","Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an ‘open vocabulary.’ This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that models using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German–English task where subword models degrade to 1.9.",1.0
NOVEL_MT_45,https://openalex.org/W4287031370,2021,2,[],In the media industry and the focus of global reporting can shift overnight. There is a compelling need to be able to develop new machine translation systems in a short period of time and in order to more efficiently cover quickly developing stories. As part of the EU project GoURMET and which focusses on low-resource machine translation and our media partners selected a surprise language for which a machine translation system had to be built and evaluated in two months(February and March 2021). The language selected was Pashto and an Indo-Iranian language spoken in Afghanistan and Pakistan and India. In this period we completed the full pipeline of development of a neural machine translation system: data crawling and cleaning and aligning and creating test sets and developing and testing models and and delivering them to the user partners. In this paperwe describe rapid data creation and experiments with transfer learning and pretraining for this low-resource language pair. We find that starting from an existing large model pre-trained on 50languages leads to far better BLEU scores than pretraining on one high-resource language pair with a smaller model. We also present human evaluation of our systems and which indicates that the resulting systems perform better than a freely available commercial system when translating from English into Pashto direction and and similarly when translating from Pashto into English.,0.9956709956709956
NOVEL_MT_47,https://openalex.org/W3173571438,2021,11,"['https://openalex.org/W630532510', 'https://openalex.org/W1558584194', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117621558', 'https://openalex.org/W2250342921', 'https://openalex.org/W2531207078', 'https://openalex.org/W2752630748', 'https://openalex.org/W2898962441', 'https://openalex.org/W2946817437', 'https://openalex.org/W2949973181', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963711067', 'https://openalex.org/W2963887123', 'https://openalex.org/W2963979492', 'https://openalex.org/W2984171820', 'https://openalex.org/W2988249555', 'https://openalex.org/W3034407863', 'https://openalex.org/W3035207248', 'https://openalex.org/W3105718208', 'https://openalex.org/W3106096614', 'https://openalex.org/W3129014787', 'https://openalex.org/W4288025088', 'https://openalex.org/W4385245566']","Jiahuan Li, Yutong Shen, Shujian Huang, Xinyu Dai, Jiajun Chen. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",0.9954337899543378
NOVEL_MT_49,https://openalex.org/W3198736145,2021,2,"['https://openalex.org/W19526395', 'https://openalex.org/W133394232', 'https://openalex.org/W1915251500', 'https://openalex.org/W2041532239', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2149327368', 'https://openalex.org/W2550821151', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970109976', 'https://openalex.org/W3000965575', 'https://openalex.org/W3012541776', 'https://openalex.org/W3036422752', 'https://openalex.org/W3115711567', 'https://openalex.org/W3158270807']",In this paper and we explore different techniques of overcoming the challenges of low-resource in Neural Machine Translation (NMT) and specifically focusing on the case of English-Marathi NMT. NMT systems require a large amount of parallel corpora to obtain good quality translations. We try to mitigate the low-resource problem by augmenting parallel corpora or by using transfer learning. Techniques such as Phrase Table Injection (PTI) and back-translation and mixing of language corpora are used for enhancing the parallel data; whereas pivoting and multilingual embeddings are used to leverage transfer learning. For pivoting and Hindi comes in as assisting language for English-Marathi translation. Compared to baseline transformer model and a significant improvement trend in BLEU score is observed across various techniques. We have done extensive manual and automatic and qualitative evaluation of our systems. Since the trend in Machine Translation (MT) today is post-editing and measuring of Human Effort Reduction (HER) and we have given our preliminary observations on Translation Edit Rate (TER) vs. BLEU score study and where TER is regarded as a measure of HER.,0.9942857142857144
NOVEL_MT_52,https://openalex.org/W3167690611,2021,8,"['https://openalex.org/W1915251500', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2608029998', 'https://openalex.org/W2626967530', 'https://openalex.org/W2767019613', 'https://openalex.org/W2767206889', 'https://openalex.org/W2799051177', 'https://openalex.org/W2806412155', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964110616', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964212410', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970845336', 'https://openalex.org/W2971347700', 'https://openalex.org/W2984500026', 'https://openalex.org/W3042199843', 'https://openalex.org/W3093843097', 'https://openalex.org/W3099770676', 'https://openalex.org/W3101683892', 'https://openalex.org/W3103878009', 'https://openalex.org/W3167690611', 'https://openalex.org/W4385245566']","Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint, our core contribution is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation, by evaluating with BLEU and contrastive tests for context-aware translation.",1.0
NOVEL_MT_53,https://openalex.org/W3186188014,2021,1,"['https://openalex.org/W1828724394', 'https://openalex.org/W1840435438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112184938', 'https://openalex.org/W2126725946', 'https://openalex.org/W2141440284', 'https://openalex.org/W2148708890', 'https://openalex.org/W2229725139', 'https://openalex.org/W2250263931', 'https://openalex.org/W2251765408', 'https://openalex.org/W2595715041', 'https://openalex.org/W2891555348', 'https://openalex.org/W2949531524', 'https://openalex.org/W2950797315', 'https://openalex.org/W2951522086', 'https://openalex.org/W2952037945', 'https://openalex.org/W2952116087', 'https://openalex.org/W2955450582', 'https://openalex.org/W2962795068', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963002901', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963165489', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963366649', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963721344', 'https://openalex.org/W2963846996', 'https://openalex.org/W2964266061', 'https://openalex.org/W2970037872', 'https://openalex.org/W2978094541', 'https://openalex.org/W3035422697', 'https://openalex.org/W3037459865', 'https://openalex.org/W3099859218', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390']","Sosuke Nishikawa, Ryokan Ri, Yoshimasa Tsuruoka. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop. 2021.",1.0
NOVEL_MT_54,https://openalex.org/W3175374354,2021,53,"['https://openalex.org/W2133564696', 'https://openalex.org/W2251743902', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2560647685', 'https://openalex.org/W2764043458', 'https://openalex.org/W2794365787', 'https://openalex.org/W2805003733', 'https://openalex.org/W2891924676', 'https://openalex.org/W2896409484', 'https://openalex.org/W2908336025', 'https://openalex.org/W2919290281', 'https://openalex.org/W2952650870', 'https://openalex.org/W2952809536', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963211188', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963714898', 'https://openalex.org/W2963813662', 'https://openalex.org/W2963983698', 'https://openalex.org/W2970925270', 'https://openalex.org/W2990844796', 'https://openalex.org/W2998103904', 'https://openalex.org/W3023986361', 'https://openalex.org/W3035144493', 'https://openalex.org/W3035180000', 'https://openalex.org/W3035464238', 'https://openalex.org/W3089072946', 'https://openalex.org/W3092327118', 'https://openalex.org/W3093871477', 'https://openalex.org/W3096966601', 'https://openalex.org/W3105281812', 'https://openalex.org/W3105409430', 'https://openalex.org/W3106106701', 'https://openalex.org/W3106321930', 'https://openalex.org/W3107826490', 'https://openalex.org/W3127719526', 'https://openalex.org/W4287597359', 'https://openalex.org/W4385245566']","Zehui Lin, Liwei Wu, Mingxuan Wang, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_60,https://openalex.org/W4225166672,2022,7,[],"In this paper, we present our submission to Shared Metrics Task: RoBLEURT (Robustly Optimizing the training of BLEURT). After investigating the recent advances of trainable metrics, we conclude several aspects of vital importance to obtain a well-performed metric model by: 1) jointly leveraging the advantages of source-included model and reference-only model, 2) continuously pre-training the model with massive synthetic data pairs, and 3) fine-tuning the model with data denoising strategy. Experimental results show that our model reaching state-of-the-art correlations with the WMT2020 human annotations upon 8 out of 10 to-English language pairs.",0.9565217391304348
NOVEL_MT_61,https://openalex.org/W3188239957,2021,3,"['https://openalex.org/W46679369', 'https://openalex.org/W630532510', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2165698076', 'https://openalex.org/W2250342921', 'https://openalex.org/W2395579298', 'https://openalex.org/W2548799116', 'https://openalex.org/W2550821151', 'https://openalex.org/W2887920589', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963633299', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970009562', 'https://openalex.org/W2970279348', 'https://openalex.org/W2996403597', 'https://openalex.org/W3082928416', 'https://openalex.org/W3098824823', 'https://openalex.org/W3104652516', 'https://openalex.org/W3105214104', 'https://openalex.org/W3107826490', 'https://openalex.org/W3120199834', 'https://openalex.org/W3120929527', 'https://openalex.org/W3147742655', 'https://openalex.org/W3152788712', 'https://openalex.org/W3156371424', 'https://openalex.org/W3170996386', 'https://openalex.org/W3173162544']","In this work, we investigate methods for the challenging task of translating between low-resource language pairs that exhibit some level of similarity. In particular, we consider the utility of transfer learning for translating between several Indo-European low-resource languages from the Germanic and Romance language families. In particular, we build two main classes of transfer-based systems to study how relatedness can benefit the translation performance. The primary system fine-tunes a model pre-trained on a related language pair and the contrastive system fine-tunes one pre-trained on an unrelated language pair. Our experiments show that although relatedness is not necessary for transfer learning to work, it does benefit model performance.",1.0
NOVEL_MT_62,https://openalex.org/W3197365178,2021,9,"['https://openalex.org/W222053410', 'https://openalex.org/W582134693', 'https://openalex.org/W586722081', 'https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108677974', 'https://openalex.org/W2119717200', 'https://openalex.org/W2120354757', 'https://openalex.org/W2124659975', 'https://openalex.org/W2164411961', 'https://openalex.org/W2251654371', 'https://openalex.org/W2251743902', 'https://openalex.org/W2257408573', 'https://openalex.org/W2550821151', 'https://openalex.org/W2595715041', 'https://openalex.org/W2600383743', 'https://openalex.org/W2727300753', 'https://openalex.org/W2750588180', 'https://openalex.org/W2802153702', 'https://openalex.org/W2892244498', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2951065878', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963675284', 'https://openalex.org/W2963696295', 'https://openalex.org/W2964030506', 'https://openalex.org/W2964059111', 'https://openalex.org/W2964068917', 'https://openalex.org/W2964090065', 'https://openalex.org/W2964113870', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970854433', 'https://openalex.org/W2971302374', 'https://openalex.org/W2983490653', 'https://openalex.org/W3035019713', 'https://openalex.org/W3035128703', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035608860', 'https://openalex.org/W3084095723', 'https://openalex.org/W3092189037', 'https://openalex.org/W3101235629', 'https://openalex.org/W3114806211', 'https://openalex.org/W3119866316', 'https://openalex.org/W3123791752', 'https://openalex.org/W3128459263', 'https://openalex.org/W3146885639', 'https://openalex.org/W3173378169', 'https://openalex.org/W3212573671', 'https://openalex.org/W4287636493', 'https://openalex.org/W4297782088', 'https://openalex.org/W4385245566']","Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model’s uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods.",1.0
NOVEL_MT_63,https://openalex.org/W3176994339,2021,6,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133512280', 'https://openalex.org/W2184135559', 'https://openalex.org/W2550821151', 'https://openalex.org/W2582446770', 'https://openalex.org/W2595715041', 'https://openalex.org/W2608029998', 'https://openalex.org/W2767019613', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2891534142', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2970529093', 'https://openalex.org/W2971278086', 'https://openalex.org/W2971347700', 'https://openalex.org/W2980462515', 'https://openalex.org/W2996264288', 'https://openalex.org/W3001434439', 'https://openalex.org/W3034715004', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035520602', 'https://openalex.org/W3042199843', 'https://openalex.org/W3101683892', 'https://openalex.org/W3102507836', 'https://openalex.org/W3107826490', 'https://openalex.org/W3118485656', 'https://openalex.org/W4287637331']","Linqing Chen, Junhui Li, Zhengxian Gong, Boxing Chen, Weihua Luo, Min Zhang, Guodong Zhou. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_64,https://openalex.org/W3207182695,2021,6,"['https://openalex.org/W22168010', 'https://openalex.org/W117861810', 'https://openalex.org/W1522301498', 'https://openalex.org/W1601432161', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101456909', 'https://openalex.org/W2148708890', 'https://openalex.org/W2257408573', 'https://openalex.org/W2483215953', 'https://openalex.org/W2512924740', 'https://openalex.org/W2561274697', 'https://openalex.org/W2567571499', 'https://openalex.org/W2595715041', 'https://openalex.org/W2744813330', 'https://openalex.org/W2792115266', 'https://openalex.org/W2883872108', 'https://openalex.org/W2889326796', 'https://openalex.org/W2891525068', 'https://openalex.org/W2903193068', 'https://openalex.org/W2921633540', 'https://openalex.org/W2933138175', 'https://openalex.org/W2945700568', 'https://openalex.org/W2950018712', 'https://openalex.org/W2950888501', 'https://openalex.org/W2952328691', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963027654', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963457723', 'https://openalex.org/W2963526187', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970311224', 'https://openalex.org/W2972568911', 'https://openalex.org/W2972680990', 'https://openalex.org/W2972866455', 'https://openalex.org/W2972972637', 'https://openalex.org/W3021299983', 'https://openalex.org/W3034458735', 'https://openalex.org/W3035070478', 'https://openalex.org/W3037697022', 'https://openalex.org/W3037831233', 'https://openalex.org/W3041866211', 'https://openalex.org/W3082714809', 'https://openalex.org/W3093000437', 'https://openalex.org/W3093818688', 'https://openalex.org/W3096713445', 'https://openalex.org/W3101004475', 'https://openalex.org/W3119159844', 'https://openalex.org/W3119161189', 'https://openalex.org/W3120819287', 'https://openalex.org/W3133764719', 'https://openalex.org/W3195725782', 'https://openalex.org/W3202218022', 'https://openalex.org/W3204130541', 'https://openalex.org/W4248517003', 'https://openalex.org/W4252004167', 'https://openalex.org/W4312663109', 'https://openalex.org/W4385245566']","Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. Our GFST approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We evaluate GFST on translation from English into five languages, finding that it improves gender accuracy without damaging generic quality. We also show the viability of GFST on several experimental settings, including re-training from scratch, fine-tuning, controlling the gender balance of the data, forward translation, and back-translation.",0.9932885906040269
NOVEL_MT_66,https://openalex.org/W3173602377,2021,16,"['https://openalex.org/W1531245479', 'https://openalex.org/W1718008561', 'https://openalex.org/W1916559533', 'https://openalex.org/W1986345088', 'https://openalex.org/W2046384065', 'https://openalex.org/W2100271871', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2251171258', 'https://openalex.org/W2257919548', 'https://openalex.org/W2397106859', 'https://openalex.org/W2473895758', 'https://openalex.org/W2511277317', 'https://openalex.org/W2525778437', 'https://openalex.org/W2561792472', 'https://openalex.org/W2613904329', 'https://openalex.org/W2789910672', 'https://openalex.org/W2804439688', 'https://openalex.org/W2896457183', 'https://openalex.org/W2936627440', 'https://openalex.org/W2945735543', 'https://openalex.org/W2949830548', 'https://openalex.org/W2951770285', 'https://openalex.org/W2955347425', 'https://openalex.org/W2962714778', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963583256', 'https://openalex.org/W2964029788', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964998528', 'https://openalex.org/W2967806182', 'https://openalex.org/W2971046749', 'https://openalex.org/W2997088366', 'https://openalex.org/W3035540807', 'https://openalex.org/W3116160706', 'https://openalex.org/W3164747806', 'https://openalex.org/W3211848854', 'https://openalex.org/W4241645538', 'https://openalex.org/W4385245566']","Huayang Li, Lemao Liu, Guoping Huang, Shuming Shi. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",0.9929078014184396
NOVEL_MT_67,https://openalex.org/W3198763390,2021,0,"['https://openalex.org/W22168010', 'https://openalex.org/W342285082', 'https://openalex.org/W630532510', 'https://openalex.org/W1626233182', 'https://openalex.org/W2006969979', 'https://openalex.org/W2048176942', 'https://openalex.org/W2057069782', 'https://openalex.org/W2079656678', 'https://openalex.org/W2121415745', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126725946', 'https://openalex.org/W2138247936', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2316904474', 'https://openalex.org/W2561995736', 'https://openalex.org/W2594021297', 'https://openalex.org/W2741602058', 'https://openalex.org/W2788353357', 'https://openalex.org/W2888740011', 'https://openalex.org/W2891896107', 'https://openalex.org/W2950577311', 'https://openalex.org/W2950797315', 'https://openalex.org/W2962844668', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963472233', 'https://openalex.org/W2964266061', 'https://openalex.org/W3012989741', 'https://openalex.org/W3022891705', 'https://openalex.org/W3029889931', 'https://openalex.org/W3176344506']",Aimed at generating a seed lexicon for use in downstream natural language tasks and unsupervised methods for bilingual lexicon induction have received much attention in the academic literature recently. While interesting and fully unsupervised settings are unrealistic; small amounts of bilingual data are usually available due to the existence of massively multilingual parallel corpora and or linguists can create small amounts of parallel data. In this work and we demonstrate an effective bootstrapping approach for semi-supervised bilingual lexicon induction that capitalizes upon the complementary strengths of two disparate methods for inducing bilingual lexicons. Whereas statistical methods are highly effective at inducing correct translation pairs for words frequently occurring in a parallel corpus and monolingual embedding spaces have the advantage of having been trained on large amounts of data and and therefore may induce accurate translations for words absent from the small corpus. By combining these relative strengths and our method achieves state-of-the-art results on 3 of 4 language pairs in the challenging VecMap test set using minimal amounts of parallel data and without the need for a translation dictionary. We release our implementation at www.blind-review.code.,1.0
NOVEL_MT_71,https://openalex.org/W3202547134,2021,4,"['https://openalex.org/W2395259056', 'https://openalex.org/W2493916176', 'https://openalex.org/W2798908575', 'https://openalex.org/W2914120296', 'https://openalex.org/W2944815030', 'https://openalex.org/W2962784628', 'https://openalex.org/W3014634151', 'https://openalex.org/W3027825632', 'https://openalex.org/W3088631993', 'https://openalex.org/W3107826490', 'https://openalex.org/W3120378683']","This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2021 Unsupervised Machine Translation task for German--Lower Sorbian (DE--DSB): a high-resource language to a low-resource one. Our system uses a transformer encoder-decoder architecture in which we make three changes to the standard training procedure. First, our training focuses on two languages at a time, contrasting with a wealth of research on multilingual systems. Second, we introduce a novel method for initializing the vocabulary of an unseen language, achieving improvements of 3.2 BLEU for DE$\rightarrow$DSB and 4.0 BLEU for DSB$\rightarrow$DE. Lastly, we experiment with the order in which offline and online back-translation are used to train an unsupervised system, finding that using online back-translation first works better for DE$\rightarrow$DSB by 2.76 BLEU. Our submissions ranked first (tied with another team) for DSB$\rightarrow$DE and third for DE$\rightarrow$DSB.",0.9836065573770492
NOVEL_MT_74,https://openalex.org/W3162228117,2021,1,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2025768430', 'https://openalex.org/W2124807415', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2567571499', 'https://openalex.org/W2604763608', 'https://openalex.org/W2744813330', 'https://openalex.org/W2753160622', 'https://openalex.org/W2944815030', 'https://openalex.org/W2950940239', 'https://openalex.org/W2951980657', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970286654', 'https://openalex.org/W2970505118', 'https://openalex.org/W2970678056', 'https://openalex.org/W2971031524', 'https://openalex.org/W2997197207']","Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baseline models.",1.0
NOVEL_MT_77,https://openalex.org/W3199143471,2021,4,"['https://openalex.org/W1810943226', 'https://openalex.org/W2124807415', 'https://openalex.org/W2251743902', 'https://openalex.org/W2593543827', 'https://openalex.org/W2798664956', 'https://openalex.org/W2889326796', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2952614664', 'https://openalex.org/W2952918253', 'https://openalex.org/W2962853934', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963403868', 'https://openalex.org/W3035464238', 'https://openalex.org/W3169369929']","This paper illustrates our approach to the shared task on large-scale multilingual machine translation in the sixth conference on machine translation (WMT-21). This work aims to build a single multilingual translation system with a hypothesis that a universal cross-language representation leads to better multilingual translation performance. We extend the exploration of different back-translation methods from bilingual translation to multilingual translation. Better performance is obtained by the constrained sampling method, which is different from the finding of the bilingual translation. Besides, we also explore the effect of vocabularies and the amount of synthetic data. Surprisingly, the smaller size of vocabularies perform better, and the extensive monolingual English data offers a modest improvement. We submitted to both the small tasks and achieved the second place.",1.0
NOVEL_MT_78,https://openalex.org/W3175809709,2021,27,"['https://openalex.org/W1494198834', 'https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W1970987322', 'https://openalex.org/W2113106066', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136530135', 'https://openalex.org/W2327501763', 'https://openalex.org/W2466918907', 'https://openalex.org/W2763421725', 'https://openalex.org/W2785350307', 'https://openalex.org/W2787998955', 'https://openalex.org/W2892009249', 'https://openalex.org/W2928941594', 'https://openalex.org/W2936774411', 'https://openalex.org/W2945700568', 'https://openalex.org/W2949328740', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2972448360', 'https://openalex.org/W2972818416', 'https://openalex.org/W2975712713', 'https://openalex.org/W2997436923', 'https://openalex.org/W3034571331', 'https://openalex.org/W3036601975', 'https://openalex.org/W3037217258', 'https://openalex.org/W3037465386', 'https://openalex.org/W3092424727', 'https://openalex.org/W3096403968', 'https://openalex.org/W3097787369', 'https://openalex.org/W3099782249', 'https://openalex.org/W3105669983', 'https://openalex.org/W3113908264', 'https://openalex.org/W3172198372', 'https://openalex.org/W3177215302', 'https://openalex.org/W3184309109', 'https://openalex.org/W4301239768', 'https://openalex.org/W4385245566']","Chengqi Zhao, Mingxuan Wang, Qianqian Dong, Rong Ye, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. 2021.",0.9876543209876544
NOVEL_MT_81,https://openalex.org/W3214581470,2021,24,"['https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2016508180', 'https://openalex.org/W2077797402', 'https://openalex.org/W2101105183', 'https://openalex.org/W2156985047', 'https://openalex.org/W2162245945', 'https://openalex.org/W2163403121', 'https://openalex.org/W2295894802', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2555745756', 'https://openalex.org/W2613904329', 'https://openalex.org/W2767206889', 'https://openalex.org/W2787384361', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798931235', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890964657', 'https://openalex.org/W2908336025', 'https://openalex.org/W2912237282', 'https://openalex.org/W2914120296', 'https://openalex.org/W2949973181', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970045405', 'https://openalex.org/W2970311224', 'https://openalex.org/W2970731908', 'https://openalex.org/W2971257618', 'https://openalex.org/W2989027560', 'https://openalex.org/W2990555152', 'https://openalex.org/W2995304149', 'https://openalex.org/W3034938700', 'https://openalex.org/W3034998639', 'https://openalex.org/W3091540052', 'https://openalex.org/W3092327118', 'https://openalex.org/W3102220759', 'https://openalex.org/W3105912780', 'https://openalex.org/W3106321930', 'https://openalex.org/W3107826490', 'https://openalex.org/W3113715281', 'https://openalex.org/W3113838476', 'https://openalex.org/W3119653668', 'https://openalex.org/W3120749277', 'https://openalex.org/W3127901106', 'https://openalex.org/W3173367141', 'https://openalex.org/W3174255604', 'https://openalex.org/W3174805484', 'https://openalex.org/W3186748023', 'https://openalex.org/W3186883165', 'https://openalex.org/W3202201199', 'https://openalex.org/W4233420827', 'https://openalex.org/W4385245566']","We present a simple and effective pretraining strategy – bidirectional training (BiT) for neural machine translation. Specifically, we bidirectionally update the model parameters at the early stage and then tune the model normally. To achieve bidirectional updating, we simply reconstruct the training samples from ""src→tgt"" to ""src+tgt→tgt+src"" without any complicated model modifications. Notably, our approach does not increase any parameters or training steps, requiring the parallel data merely. Experimental results show that BiT pushes the SOTA neural machine translation performance across 15 translation tasks on 8 language pairs (data sizes range from 160K to 38M) significantly higher. Encouragingly, our proposed model can complement existing data manipulation strategies, i.e. back translation, data distillation, and data diversification. Extensive analyses show that our approach functions as a novel bilingual code-switcher, obtaining better bilingual alignment.",1.0
NOVEL_MT_82,https://openalex.org/W3200875022,2021,3,"['https://openalex.org/W201288405', 'https://openalex.org/W1513168562', 'https://openalex.org/W1522301498', 'https://openalex.org/W1901714926', 'https://openalex.org/W2016856586', 'https://openalex.org/W2144600658', 'https://openalex.org/W2250332179', 'https://openalex.org/W2257408573', 'https://openalex.org/W2757041753', 'https://openalex.org/W2767206889', 'https://openalex.org/W2772302643', 'https://openalex.org/W2787088044', 'https://openalex.org/W2792115266', 'https://openalex.org/W2805490244', 'https://openalex.org/W2896509746', 'https://openalex.org/W2946375144', 'https://openalex.org/W2951770285', 'https://openalex.org/W2962680795', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963225662', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963521307', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963547384', 'https://openalex.org/W2963583256', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963965928', 'https://openalex.org/W2964029788', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970832665', 'https://openalex.org/W2971167892', 'https://openalex.org/W2973226110', 'https://openalex.org/W2988975212', 'https://openalex.org/W2995999067', 'https://openalex.org/W3015766957', 'https://openalex.org/W3029286378', 'https://openalex.org/W3035540807', 'https://openalex.org/W3092701783', 'https://openalex.org/W3113715281', 'https://openalex.org/W3127901106', 'https://openalex.org/W3153567180', 'https://openalex.org/W3156246620', 'https://openalex.org/W4294619240', 'https://openalex.org/W4385245566']","Current approaches to incorporating terminology constraints in machine translation (MT) typically assume that the constraint terms are provided in their correct morphological forms. This limits their application to real-world scenarios where constraint terms are provided as lemmas. In this paper, we introduce a modular framework for incorporating lemma constraints in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate lemma constraints more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs.",1.0
NOVEL_MT_83,https://openalex.org/W3198479318,2021,1,"['https://openalex.org/W22168010', 'https://openalex.org/W87617161', 'https://openalex.org/W131881164', 'https://openalex.org/W854541894', 'https://openalex.org/W1507213344', 'https://openalex.org/W1514971736', 'https://openalex.org/W1531245479', 'https://openalex.org/W2006969979', 'https://openalex.org/W2046384065', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100271871', 'https://openalex.org/W2101181502', 'https://openalex.org/W2112815206', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134670378', 'https://openalex.org/W2148354767', 'https://openalex.org/W2161874394', 'https://openalex.org/W2165170939', 'https://openalex.org/W2165188277', 'https://openalex.org/W2344202309', 'https://openalex.org/W2561792472', 'https://openalex.org/W2782227846', 'https://openalex.org/W2800233718', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2979249747', 'https://openalex.org/W3034673518', 'https://openalex.org/W3081828086', 'https://openalex.org/W3116160706', 'https://openalex.org/W3124197686']",The quality of the translations generated by Machine Translation (MT) systems has highly improved through the years and but we are still far away to obtain fully automatic high-quality translations. To generate them and translators make use of Computer-Assisted Translation (CAT) tools and among which we find the Interactive-Predictive Machine Translation (IPMT) systems. In this paper and we use bandit feedback as the main and only information needed to generate new predictions that correct the previous translations. The application of bandit feedback reduces significantly the number of words that the translator need to type in an IPMT session. In conclusion and the use of this technique saves useful time and effort to translators and its performance improves with the future advances in MT and so we recommend its application in the actuals IPMT systems.,1.0
NOVEL_MT_86,https://openalex.org/W3156665996,2021,60,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W2154652894', 'https://openalex.org/W2572474373', 'https://openalex.org/W2742113707', 'https://openalex.org/W2842511635', 'https://openalex.org/W2891555348', 'https://openalex.org/W2914120296', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945260553', 'https://openalex.org/W2952468927', 'https://openalex.org/W2960374072', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963877297', 'https://openalex.org/W2964114970', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970752815', 'https://openalex.org/W2970854433', 'https://openalex.org/W2971207485', 'https://openalex.org/W2971274815', 'https://openalex.org/W2971863715', 'https://openalex.org/W2973088264', 'https://openalex.org/W2982399380', 'https://openalex.org/W2989539713', 'https://openalex.org/W2995015695', 'https://openalex.org/W2995118574', 'https://openalex.org/W2995230342', 'https://openalex.org/W2996264288', 'https://openalex.org/W2996822578', 'https://openalex.org/W2998653236', 'https://openalex.org/W3001434439', 'https://openalex.org/W3007759824', 'https://openalex.org/W3013840636', 'https://openalex.org/W3034469191', 'https://openalex.org/W3034715004', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035497479', 'https://openalex.org/W3042711927', 'https://openalex.org/W3045462440', 'https://openalex.org/W3051275803', 'https://openalex.org/W3082274269', 'https://openalex.org/W3092973241', 'https://openalex.org/W3093517588', 'https://openalex.org/W3097879195', 'https://openalex.org/W3105813095', 'https://openalex.org/W3106445907', 'https://openalex.org/W3107826490', 'https://openalex.org/W3118106810', 'https://openalex.org/W3119175506', 'https://openalex.org/W3168481568', 'https://openalex.org/W3169425228', 'https://openalex.org/W3169483174', 'https://openalex.org/W3171975879', 'https://openalex.org/W3175746962', 'https://openalex.org/W3175898847', 'https://openalex.org/W3177035927', 'https://openalex.org/W3186903869', 'https://openalex.org/W4288284086', 'https://openalex.org/W4297808394', 'https://openalex.org/W4300427991', 'https://openalex.org/W4301187301', 'https://openalex.org/W4385245566']","Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang, Saksham Singhal, Xian-Ling Mao, Heyan Huang, Xia Song, Furu Wei. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",0.9933774834437086
NOVEL_MT_88,https://openalex.org/W3176366152,2021,33,"['https://openalex.org/W22168010', 'https://openalex.org/W2194775991', 'https://openalex.org/W2212703438', 'https://openalex.org/W2515741950', 'https://openalex.org/W2550821151', 'https://openalex.org/W2552124255', 'https://openalex.org/W2555745756', 'https://openalex.org/W2763856713', 'https://openalex.org/W2767204723', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888456631', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921280978', 'https://openalex.org/W2952153923', 'https://openalex.org/W2962776659', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964091381', 'https://openalex.org/W2970223278', 'https://openalex.org/W2970316683', 'https://openalex.org/W2970925270', 'https://openalex.org/W2971152344', 'https://openalex.org/W2997518171', 'https://openalex.org/W3000965575', 'https://openalex.org/W3005389111', 'https://openalex.org/W3005962597', 'https://openalex.org/W3017454464', 'https://openalex.org/W3030741555', 'https://openalex.org/W3035464238', 'https://openalex.org/W3093871477', 'https://openalex.org/W3096489356', 'https://openalex.org/W3112593586', 'https://openalex.org/W3119866316', 'https://openalex.org/W4287636493', 'https://openalex.org/W4297730150', 'https://openalex.org/W4385245566']","Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. We show that this can be easily alleviated by removing residual connections in an encoder layer. With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions. The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation. Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage. By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations.",1.0
NOVEL_MT_89,https://openalex.org/W3175863856,2021,67,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2593864460', 'https://openalex.org/W2788330850', 'https://openalex.org/W2794365787', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963149635', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963829526', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970678329', 'https://openalex.org/W2995154514', 'https://openalex.org/W3098593077', 'https://openalex.org/W3102839769', 'https://openalex.org/W3126425262', 'https://openalex.org/W4287649493', 'https://openalex.org/W4288087322', 'https://openalex.org/W4299585995', 'https://openalex.org/W4385245566']","Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing Chen, Weihua Luo, Jiajun Chen. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",1.0
NOVEL_MT_91,https://openalex.org/W3212104907,2021,2,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116341502', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2528305545', 'https://openalex.org/W2595715041', 'https://openalex.org/W2788550262', 'https://openalex.org/W2889326796', 'https://openalex.org/W2899286282', 'https://openalex.org/W2900014366', 'https://openalex.org/W2912624765', 'https://openalex.org/W2944898795', 'https://openalex.org/W2949193663', 'https://openalex.org/W2952032096', 'https://openalex.org/W2962945603', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963899988', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971160629', 'https://openalex.org/W2996854111', 'https://openalex.org/W2997763445', 'https://openalex.org/W3017454464', 'https://openalex.org/W3034835156', 'https://openalex.org/W3035464238', 'https://openalex.org/W3046368065', 'https://openalex.org/W3093871477', 'https://openalex.org/W3101642036', 'https://openalex.org/W3102264439', 'https://openalex.org/W4287694131', 'https://openalex.org/W4287874506', 'https://openalex.org/W4288025992', 'https://openalex.org/W4288601872', 'https://openalex.org/W4289494028', 'https://openalex.org/W4385245566']","Schema translation is the task of automatically translating headers of tabular data from one language to another. High-quality schema translation plays an important role in cross-lingual table searching, understanding and analysis. Despite its importance, schema translation is not well studied in the community, and state-of-the-art neural machine translation models cannot work well on this task because of two intrinsic differences between plain text and tabular data: morphological difference and context difference. To facilitate the research study, we construct the first parallel dataset for schema translation, which consists of 3,158 tables with 11,979 headers written in 6 different languages, including English, Chinese, French, German, Spanish, and Japanese. Also, we propose the first schema translation model called CAST, which is a header-to-header neural machine translation model augmented with schema context. Specifically, we model a target header and its context as a directed graph to represent their entity types and relations. Then CAST encodes the graph with a relational-aware transformer and uses another transformer to decode the header in the target language. Experiments on our dataset demonstrate that CAST significantly outperforms state-of-the-art neural machine translation models. Our dataset will be released at https://github.com/microsoft/ContextualSP.",0.993006993006993
NOVEL_MT_92,https://openalex.org/W3094519420,2021,16,"['https://openalex.org/W1787224781', 'https://openalex.org/W2099224638', 'https://openalex.org/W2336525064', 'https://openalex.org/W2594633041', 'https://openalex.org/W2741040846', 'https://openalex.org/W2741838462', 'https://openalex.org/W2756888147', 'https://openalex.org/W2759173152', 'https://openalex.org/W2799051177', 'https://openalex.org/W2811277447', 'https://openalex.org/W2833393231', 'https://openalex.org/W2888539709', 'https://openalex.org/W2897507397', 'https://openalex.org/W2902614977', 'https://openalex.org/W2909737760', 'https://openalex.org/W2946468379', 'https://openalex.org/W2946794439', 'https://openalex.org/W2948635472', 'https://openalex.org/W2953483091', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962807820', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963400886', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963503967', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963813662', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964174820', 'https://openalex.org/W2964190861', 'https://openalex.org/W2964204621', 'https://openalex.org/W2964298349', 'https://openalex.org/W2971134989', 'https://openalex.org/W2971154170', 'https://openalex.org/W2973136764', 'https://openalex.org/W2996309822', 'https://openalex.org/W3034553593', 'https://openalex.org/W3035214886']","In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.",1.0
NOVEL_MT_93,https://openalex.org/W3156064004,2021,29,"['https://openalex.org/W1522301498', 'https://openalex.org/W2550821151', 'https://openalex.org/W2604763608', 'https://openalex.org/W2610245951', 'https://openalex.org/W2798931235', 'https://openalex.org/W2888541716', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2944815030', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962830144', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963813679', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970854433', 'https://openalex.org/W2982399380', 'https://openalex.org/W2989276524', 'https://openalex.org/W2996854111', 'https://openalex.org/W2997763445', 'https://openalex.org/W2998653236', 'https://openalex.org/W3001434439', 'https://openalex.org/W3006381853', 'https://openalex.org/W3017454464', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035579820', 'https://openalex.org/W3036839309', 'https://openalex.org/W3092327118', 'https://openalex.org/W3093517588', 'https://openalex.org/W3093818688', 'https://openalex.org/W3093871477', 'https://openalex.org/W3094540663', 'https://openalex.org/W3099771192', 'https://openalex.org/W3105378761', 'https://openalex.org/W3106321930', 'https://openalex.org/W3107826490', 'https://openalex.org/W3116484298', 'https://openalex.org/W3118942129', 'https://openalex.org/W3119175506', 'https://openalex.org/W3169483174', 'https://openalex.org/W3176366152', 'https://openalex.org/W4287874506', 'https://openalex.org/W4385245566']","Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder (MPE), or improving the performance on supervised machine translation with BERT. However, it is under-explored that whether the MPE can help to facilitate the cross-lingual transferability of NMT model. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with parallel dataset of only one language pair and an off-the-shelf MPE, then it is directly tested on zero-shot language pairs. We propose SixT, a simple yet effective model for this task. SixT leverages the MPE with a two-stage training schedule and gets further improvement with a position disentangled encoder and a capacity-enhanced decoder. Using this method, SixT significantly outperforms mBART, a pretrained multilingual encoder-decoder model explicitly designed for NMT, with an average improvement of 7.1 BLEU on zero-shot any-to-English test sets across 14 source languages. Furthermore, with much less training computation cost and training data, our model achieves better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines.",1.0
NOVEL_MT_95,https://openalex.org/W3111696764,2020,3,"['https://openalex.org/W222053410', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2184135559', 'https://openalex.org/W2194775991', 'https://openalex.org/W2525778437', 'https://openalex.org/W2600702321', 'https://openalex.org/W2608029998', 'https://openalex.org/W2612881151', 'https://openalex.org/W2789541106', 'https://openalex.org/W2795138957', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2888159079', 'https://openalex.org/W2889769646', 'https://openalex.org/W2891534142', 'https://openalex.org/W2923779212', 'https://openalex.org/W2931198394', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949335953', 'https://openalex.org/W2951563833', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962788148', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964302946', 'https://openalex.org/W2964401366', 'https://openalex.org/W2970290486', 'https://openalex.org/W2970529093', 'https://openalex.org/W2971278086', 'https://openalex.org/W2971347700', 'https://openalex.org/W2980462515', 'https://openalex.org/W2989066524', 'https://openalex.org/W3034351728', 'https://openalex.org/W3035520602', 'https://openalex.org/W3035629723', 'https://openalex.org/W3035643691', 'https://openalex.org/W3037932933', 'https://openalex.org/W3101683892', 'https://openalex.org/W3102507836']","Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods only consider a few number of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English--French, Chinese-English, WMT English--German and Opensubtitle English--Russian, demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena.",1.0
NOVEL_MT_96,https://openalex.org/W3171088343,2021,7,"['https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2515741950', 'https://openalex.org/W2529194139', 'https://openalex.org/W2613904329', 'https://openalex.org/W2767206889', 'https://openalex.org/W2888196092', 'https://openalex.org/W2892205701', 'https://openalex.org/W2892213699', 'https://openalex.org/W2908336025', 'https://openalex.org/W2914924671', 'https://openalex.org/W2921311659', 'https://openalex.org/W2946417913', 'https://openalex.org/W2946794439', 'https://openalex.org/W2948947170', 'https://openalex.org/W2951977278', 'https://openalex.org/W2952356761', 'https://openalex.org/W2952638691', 'https://openalex.org/W2952682849', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962776659', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962915948', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963430224', 'https://openalex.org/W2963536265', 'https://openalex.org/W2964147026', 'https://openalex.org/W2964204621', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970810442', 'https://openalex.org/W2970820321', 'https://openalex.org/W2970862333', 'https://openalex.org/W2971154170', 'https://openalex.org/W2976965654', 'https://openalex.org/W2983902802', 'https://openalex.org/W2997244573', 'https://openalex.org/W3035747971', 'https://openalex.org/W3104235057', 'https://openalex.org/W3105990194', 'https://openalex.org/W3125507956', 'https://openalex.org/W3135335819', 'https://openalex.org/W4301785137', 'https://openalex.org/W4385245566']","Hongfei Xu, Josef van Genabith, Qiuhui Liu, Deyi Xiong. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_99,https://openalex.org/W3199917453,2021,14,"['https://openalex.org/W165283731', 'https://openalex.org/W365755509', 'https://openalex.org/W2006969979', 'https://openalex.org/W2095755718', 'https://openalex.org/W2108460050', 'https://openalex.org/W2148708890', 'https://openalex.org/W2157817599', 'https://openalex.org/W2162245945', 'https://openalex.org/W2187252394', 'https://openalex.org/W2251530174', 'https://openalex.org/W2251542837', 'https://openalex.org/W2529548870', 'https://openalex.org/W2554098328', 'https://openalex.org/W2767114424', 'https://openalex.org/W2951456627', 'https://openalex.org/W2952992734', 'https://openalex.org/W2956791634', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963950336', 'https://openalex.org/W2966045039', 'https://openalex.org/W2970074184', 'https://openalex.org/W2970084653', 'https://openalex.org/W2989027560', 'https://openalex.org/W3105422997', 'https://openalex.org/W3134644026', 'https://openalex.org/W4385245566']","Simultaneous translation is vastly different from full-sentence translation, in the sense that it starts translation before the source sentence ends, with only a few words delay. However, due to the lack of large-scale, high-quality simultaneous translation datasets, most such systems are still trained on conventional full-sentence bitexts. This is far from ideal for the simultaneous scenario due to the abundance of unnecessary long-distance reorderings in those bitexts. We propose a novel method that rewrites the target side of existing full-sentence corpora into simultaneous-style translation. Experiments on Zh→En and Ja→En simultaneous translation show substantial improvements (up to +2.7 BLEU) with the addition of these generated pseudo-references.",1.0
NOVEL_MT_101,https://openalex.org/W3212217304,2021,3,"['https://openalex.org/W28136092', 'https://openalex.org/W1986345088', 'https://openalex.org/W2060008829', 'https://openalex.org/W2079735306', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109553965', 'https://openalex.org/W2147227066', 'https://openalex.org/W2149327368', 'https://openalex.org/W2149791639', 'https://openalex.org/W2250679855', 'https://openalex.org/W2251431759', 'https://openalex.org/W2622075087', 'https://openalex.org/W2884808449', 'https://openalex.org/W2894218541', 'https://openalex.org/W2902463012', 'https://openalex.org/W2902608666', 'https://openalex.org/W2902943690', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2942274440', 'https://openalex.org/W2953072129', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963816901', 'https://openalex.org/W2963824830', 'https://openalex.org/W2970156971', 'https://openalex.org/W2971120958', 'https://openalex.org/W2977978669', 'https://openalex.org/W3034673518', 'https://openalex.org/W3037879901', 'https://openalex.org/W3097330605', 'https://openalex.org/W3119881489', 'https://openalex.org/W3120951354', 'https://openalex.org/W3165234151', 'https://openalex.org/W3174693399', 'https://openalex.org/W3184138828', 'https://openalex.org/W3205309080', 'https://openalex.org/W3213919616', 'https://openalex.org/W4211166112', 'https://openalex.org/W4230872509']","Compared to fully manual translation, post-editing (PE) machine translation (MT) output can save time and reduce errors. Automatic word-level quality estimation (QE) aims to predict the correctness of words in MT output and holds great promise to aid PE by flagging problematic output. Quality of QE is crucial, as incorrect QE might lead to translators missing errors or wasting time on already correct MT output. Achieving accurate automatic word-level QE is very hard, and it is currently not known (i) at what quality threshold QE is actually beginning to be useful for human PE, and (ii), how to best present word-level QE information to translators. In particular, should word-level QE visualization indicate uncertainty of the QE model or not? In this paper, we address both research questions with real and simulated word-level QE, visualizations, and user studies, where time, subjective ratings, and quality of the final translations are assessed. Results show that current word-level QE models are not yet good enough to support PE. Instead, quality levels of > 80% F1 are required. For helpful quality levels, a visualization reflecting the uncertainty of the QE model is preferred. Our analysis further shows that speed gains achieved through QE are not merely a result of blindly trusting the QE system, but that the quality of the final translations also improves. The threshold results from the paper establish a quality goal for future word-level QE research.",1.0
NOVEL_MT_104,https://openalex.org/W3213774132,2021,4,"['https://openalex.org/W52724552', 'https://openalex.org/W331019419', 'https://openalex.org/W650948056', 'https://openalex.org/W1522301498', 'https://openalex.org/W1675450783', 'https://openalex.org/W2011450768', 'https://openalex.org/W2014902591', 'https://openalex.org/W2059569661', 'https://openalex.org/W2099120987', 'https://openalex.org/W2108287887', 'https://openalex.org/W2115834228', 'https://openalex.org/W2136140395', 'https://openalex.org/W2144012961', 'https://openalex.org/W2146111747', 'https://openalex.org/W2147880316', 'https://openalex.org/W2148708890', 'https://openalex.org/W2152691628', 'https://openalex.org/W2159457224', 'https://openalex.org/W2161044106', 'https://openalex.org/W2250389961', 'https://openalex.org/W2250525911', 'https://openalex.org/W2251994480', 'https://openalex.org/W2252024663', 'https://openalex.org/W2336840621', 'https://openalex.org/W2340160601', 'https://openalex.org/W2463809169', 'https://openalex.org/W2511598956', 'https://openalex.org/W2739967986', 'https://openalex.org/W2758640791', 'https://openalex.org/W2796150856', 'https://openalex.org/W2888456631', 'https://openalex.org/W2889894161', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908510526', 'https://openalex.org/W2911300548', 'https://openalex.org/W2944852028', 'https://openalex.org/W2946269785', 'https://openalex.org/W2949061542', 'https://openalex.org/W2952638691', 'https://openalex.org/W2962836003', 'https://openalex.org/W2962945654', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963460546', 'https://openalex.org/W2963888891', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964303773', 'https://openalex.org/W2970469088', 'https://openalex.org/W2971133142', 'https://openalex.org/W3034438872', 'https://openalex.org/W3035380217', 'https://openalex.org/W3105421296', 'https://openalex.org/W4302455086', 'https://openalex.org/W4322588812', 'https://openalex.org/W4385245566']","Opinion Role Labeling (ORL), aiming to identify the key roles of opinion, has received increasing interest. Unlike most of the previous works focusing on the English language, in this paper, we present the first work of Chinese ORL. We construct a Chinese dataset by manually translating and projecting annotations from a standard English MPQA dataset. Then, we investigate the effectiveness of cross-lingual transfer methods, including model transfer and corpus translation. We exploit multilingual BERT with Contextual Parameter Generator and Adapter methods to examine the potentials of unsupervised cross-lingual learning and our experiments and analyses for both bilingual and multilingual transfers establish a foundation for the future research of this task.",0.9925925925925926
NOVEL_MT_105,https://openalex.org/W3197148518,2021,3,"['https://openalex.org/W630532510', 'https://openalex.org/W1587998903', 'https://openalex.org/W2118434577', 'https://openalex.org/W2472403012', 'https://openalex.org/W2933138175', 'https://openalex.org/W2945735543', 'https://openalex.org/W2951770285', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963583256', 'https://openalex.org/W2963725234', 'https://openalex.org/W2963877622', 'https://openalex.org/W2964029788', 'https://openalex.org/W2971167892', 'https://openalex.org/W3035540807', 'https://openalex.org/W3082303676', 'https://openalex.org/W3082675885']",The paper presents experiments in neural machine translation with lexical constraints into a morphologically rich language. In particular and we introduce a method and based on constrained decoding and which handles the inflected forms of lexical entries and does not require any modification to the training data or model architecture. To evaluate its effectiveness and we carry out experiments in two different scenarios: general and domain-specific. We compare our method with baseline translation and i.e. translation without lexical constraints and in terms of translation speed and translation quality. To evaluate how well the method handles the constraints and we propose new evaluation metrics which take into account the presence and placement and duplication and inflectional correctness of lexical terms in the output sentence.,1.0
NOVEL_MT_107,https://openalex.org/W3171376244,2021,7,"['https://openalex.org/W1649645444', 'https://openalex.org/W2133564696', 'https://openalex.org/W2606032440', 'https://openalex.org/W2608029998', 'https://openalex.org/W2626778328', 'https://openalex.org/W2740711318', 'https://openalex.org/W2767019613', 'https://openalex.org/W2787560479', 'https://openalex.org/W2799051177', 'https://openalex.org/W2885185669', 'https://openalex.org/W2889326796', 'https://openalex.org/W2891534142', 'https://openalex.org/W2892036039', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2922709902', 'https://openalex.org/W2927746189', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945260553', 'https://openalex.org/W2951514583', 'https://openalex.org/W2951560313', 'https://openalex.org/W2952444318', 'https://openalex.org/W2952468927', 'https://openalex.org/W2952516684', 'https://openalex.org/W2956301977', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963842551', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970529093', 'https://openalex.org/W2970597249', 'https://openalex.org/W2971274815', 'https://openalex.org/W2971347700', 'https://openalex.org/W2971495253', 'https://openalex.org/W2971760773', 'https://openalex.org/W2983108239', 'https://openalex.org/W2985808369', 'https://openalex.org/W2994928925', 'https://openalex.org/W2996264288', 'https://openalex.org/W3006381853', 'https://openalex.org/W3034715004', 'https://openalex.org/W3035629723', 'https://openalex.org/W3036939249', 'https://openalex.org/W3042199843', 'https://openalex.org/W3046531489', 'https://openalex.org/W3093559770', 'https://openalex.org/W3098903812', 'https://openalex.org/W3118485656', 'https://openalex.org/W3120545354', 'https://openalex.org/W4287637331', 'https://openalex.org/W4288562606', 'https://openalex.org/W4292779060', 'https://openalex.org/W4295805527', 'https://openalex.org/W4297804809', 'https://openalex.org/W4300428972', 'https://openalex.org/W4302343710', 'https://openalex.org/W4385245566']","We propose a new architecture for adapting a sentence-level\nsequence-to-sequence transformer by incorporating multiple pretrained document\ncontext signals and assess the impact on translation performance of (1)\ndifferent pretraining approaches for generating these signals, (2) the quantity\nof parallel data for which document context is available, and (3) conditioning\non source, target, or source and target contexts. Experiments on the NIST\nChinese-English, and IWSLT and WMT English-German tasks support four general\nconclusions: that using pretrained context representations markedly improves\nsample efficiency, that adequate parallel data resources are crucial for\nlearning to use document context, that jointly conditioning on multiple context\nrepresentations outperforms any single representation, and that source context\nis more valuable for translation performance than target side context. Our best\nmulti-context model consistently outperforms the best existing context-aware\ntransformers.\n",1.0
NOVEL_MT_108,https://openalex.org/W3172698324,2021,54,"['https://openalex.org/W197865394', 'https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W2147800946', 'https://openalex.org/W2911300548', 'https://openalex.org/W2936774411', 'https://openalex.org/W2945700568', 'https://openalex.org/W2963211188', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964303773', 'https://openalex.org/W2970925270', 'https://openalex.org/W2971840980', 'https://openalex.org/W2979826702', 'https://openalex.org/W2982399380', 'https://openalex.org/W3017650141', 'https://openalex.org/W3034999214', 'https://openalex.org/W3036601975', 'https://openalex.org/W3037932933', 'https://openalex.org/W3046368065', 'https://openalex.org/W3049591882', 'https://openalex.org/W3097338456', 'https://openalex.org/W3098824823', 'https://openalex.org/W3099782249', 'https://openalex.org/W3099793224', 'https://openalex.org/W3100370880', 'https://openalex.org/W3101498587', 'https://openalex.org/W3107826490', 'https://openalex.org/W3110524561', 'https://openalex.org/W3112092703', 'https://openalex.org/W3118578889', 'https://openalex.org/W3153675281', 'https://openalex.org/W3153805297', 'https://openalex.org/W3173767661', 'https://openalex.org/W3183148055', 'https://openalex.org/W3197845195', 'https://openalex.org/W4226513777', 'https://openalex.org/W4287694131', 'https://openalex.org/W4288817190', 'https://openalex.org/W4322588812', 'https://openalex.org/W4385245566']","Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non-parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.",1.0
NOVEL_MT_109,https://openalex.org/W3168847912,2021,6,"['https://openalex.org/W2888541716', 'https://openalex.org/W2889326796', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963925437', 'https://openalex.org/W2965373594', 'https://openalex.org/W2972324944', 'https://openalex.org/W2973154008', 'https://openalex.org/W2984582583', 'https://openalex.org/W2991265431', 'https://openalex.org/W2996428491', 'https://openalex.org/W3015468748', 'https://openalex.org/W3021293129', 'https://openalex.org/W3035206215', 'https://openalex.org/W3035229828', 'https://openalex.org/W3082274269', 'https://openalex.org/W3097795905', 'https://openalex.org/W3102094970', 'https://openalex.org/W3106061119', 'https://openalex.org/W3117312003', 'https://openalex.org/W3122515622', 'https://openalex.org/W3152698349', 'https://openalex.org/W3155806510']","Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.",1.0
NOVEL_MT_110,https://openalex.org/W4287272567,2021,1,[],"Large web-crawled corpora represent an excellent resource for improving the\nperformance of Neural Machine Translation (NMT) systems across several language\npairs. However, since these corpora are typically extremely noisy, their use is\nfairly limited. Current approaches to dealing with this problem mainly focus on\nfiltering using heuristics or single features such as language model scores or\nbi-lingual similarity. This work presents an alternative approach which learns\nweights for multiple sentence-level features. These feature weights which are\noptimized directly for the task of improving translation performance, are used\nto score and filter sentences in the noisy corpora more effectively. We provide\nresults of applying this technique to building NMT systems using the Paracrawl\ncorpus for Estonian-English and show that it beats strong single feature\nbaselines and hand designed combinations. Additionally, we analyze the\nsensitivity of this method to different types of noise and explore if the\nlearned weights generalize to other language pairs using the Maltese-English\nParacrawl corpus.\n",0.9871794871794872
NOVEL_MT_112,https://openalex.org/W3212892036,2021,8,"['https://openalex.org/W108437174', 'https://openalex.org/W1832693441', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2162245945', 'https://openalex.org/W2586524454', 'https://openalex.org/W2597655663', 'https://openalex.org/W2755806193', 'https://openalex.org/W2794365787', 'https://openalex.org/W2799051177', 'https://openalex.org/W2897507397', 'https://openalex.org/W2912351236', 'https://openalex.org/W2946794439', 'https://openalex.org/W2949193663', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962788148', 'https://openalex.org/W2962822108', 'https://openalex.org/W2962945603', 'https://openalex.org/W2963084773', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963386218', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963829526', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964089206', 'https://openalex.org/W2964174820', 'https://openalex.org/W2964189376', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964302946', 'https://openalex.org/W2970682957', 'https://openalex.org/W3021915462', 'https://openalex.org/W4298170715', 'https://openalex.org/W4385245566']","Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some self-attention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then vitalize their potential by learning syntactic relations and prior knowledge in the text without sacrificing the roles of important heads. Two novel syntax-enhanced attention (SEA) mechanisms: a dependency mask bias and a relative local-phrasal position bias, are introduced to revise self-attention distributions for syntactic enhancement in machine translation. The importance of individual heads is dynamically evaluated during the redundant heads identification, on which we apply SEA to vitalize redundant heads while maintaining the strength of important heads. Experimental results on widely adopted WMT14 and WMT16 English to German and English to Czech language machine translation validate the RHE effectiveness.",1.0
NOVEL_MT_113,https://openalex.org/W3205417735,2021,2,"['https://openalex.org/W2250342921', 'https://openalex.org/W2555745756', 'https://openalex.org/W2763176669', 'https://openalex.org/W2911300548', 'https://openalex.org/W2913659301', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949911645', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964303773', 'https://openalex.org/W2970925270', 'https://openalex.org/W2982399380', 'https://openalex.org/W2996403597', 'https://openalex.org/W3034906024', 'https://openalex.org/W3035390927', 'https://openalex.org/W3046368065', 'https://openalex.org/W3096966601', 'https://openalex.org/W3101498587', 'https://openalex.org/W3103544486', 'https://openalex.org/W3105214104', 'https://openalex.org/W3105421296', 'https://openalex.org/W3107826490', 'https://openalex.org/W3153805297', 'https://openalex.org/W3170253133']","We consider the problem of multilingual unsupervised machine translation, translating to and from languages that only have monolingual data by using auxiliary parallel language pairs. For this problem the standard procedure so far to leverage the monolingual data is back-translation, which is computationally costly and hard to tune. In this paper we propose instead to use denoising adapters, adapter layers with a denoising objective, on top of pre-trained mBART-50. In addition to the modularity and flexibility of such an approach we show that the resulting translations are on-par with back-translating as measured by BLEU, and furthermore it allows adding unseen languages incrementally.",1.0
NOVEL_MT_115,https://openalex.org/W3213005305,2021,12,"['https://openalex.org/W1522301498', 'https://openalex.org/W1828163288', 'https://openalex.org/W2101105183', 'https://openalex.org/W2148708890', 'https://openalex.org/W2257408573', 'https://openalex.org/W2419292002', 'https://openalex.org/W2529548870', 'https://openalex.org/W2566475580', 'https://openalex.org/W2887005286', 'https://openalex.org/W2949847915', 'https://openalex.org/W2951456627', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963833497', 'https://openalex.org/W2963909453', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964140243', 'https://openalex.org/W2970971581', 'https://openalex.org/W3035348852', 'https://openalex.org/W3082274269', 'https://openalex.org/W3104234009', 'https://openalex.org/W3153994542', 'https://openalex.org/W4295312788', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394657458']","We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by reinforcement learning. Here we formulate simultaneous translation as a structural sequence-to-sequence learning problem. A latent variable is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the model to explicitly balance translation quality and latency. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.",1.0
NOVEL_MT_116,https://openalex.org/W3088631993,2021,4,"['https://openalex.org/W1753482797', 'https://openalex.org/W1916559533', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121745180', 'https://openalex.org/W2130942839', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2572474373', 'https://openalex.org/W2610245951', 'https://openalex.org/W2798643159', 'https://openalex.org/W2887920589', 'https://openalex.org/W2913659301', 'https://openalex.org/W2919290281', 'https://openalex.org/W2944815030', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963877297', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964073484', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964707311', 'https://openalex.org/W2971031524', 'https://openalex.org/W2982399380', 'https://openalex.org/W2995460523', 'https://openalex.org/W3015566765', 'https://openalex.org/W3034906024', 'https://openalex.org/W3035144493', 'https://openalex.org/W3035257795', 'https://openalex.org/W3101672304', 'https://openalex.org/W3103544486', 'https://openalex.org/W3107826490']","Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform a large collection of supervised WMT submissions for various language pairs as well as match the performance of the current state-of-the-art supervised model for Nepali-English. We conduct a series of ablation studies to establish the robustness of our model under different degrees of data quality, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional unsupervised models.",1.0
NOVEL_MT_117,https://openalex.org/W3200388885,2021,36,"['https://openalex.org/W1522301498', 'https://openalex.org/W2006969979', 'https://openalex.org/W2127141656', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146685010', 'https://openalex.org/W2148708890', 'https://openalex.org/W2156985047', 'https://openalex.org/W2741986820', 'https://openalex.org/W2767206889', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946200149', 'https://openalex.org/W2948197522', 'https://openalex.org/W2952356761', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963499882', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970730223', 'https://openalex.org/W2970832665', 'https://openalex.org/W2981648103', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990372437', 'https://openalex.org/W2990389671', 'https://openalex.org/W2995999067', 'https://openalex.org/W2996843693', 'https://openalex.org/W2998135987', 'https://openalex.org/W3005347330', 'https://openalex.org/W3015162217', 'https://openalex.org/W3026874504', 'https://openalex.org/W3034892578', 'https://openalex.org/W3035289598', 'https://openalex.org/W3035416964', 'https://openalex.org/W3039805635', 'https://openalex.org/W3092062690', 'https://openalex.org/W3100753857', 'https://openalex.org/W3101095987', 'https://openalex.org/W3106104873', 'https://openalex.org/W3114869109', 'https://openalex.org/W3125507956', 'https://openalex.org/W3135335819', 'https://openalex.org/W3156927220', 'https://openalex.org/W3175164646', 'https://openalex.org/W3175665465', 'https://openalex.org/W3177172118', 'https://openalex.org/W4285719527', 'https://openalex.org/W4287372954', 'https://openalex.org/W4385245566']","Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and (ii) translation with aligned decoder inputs, guiding the decoder to focus on simplified one-to-one translation. To alleviate the alignment estimation problem, we further propose a novel alignment decomposition method. Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 En↔De and WMT16 Ro→En. Furthermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art connectionist temporal classification based models on WMT14 En↔De. We also observe that AligNART effectively addresses the token repetition problem even without sequence-level knowledge distillation.",0.995475113122172
NOVEL_MT_118,https://openalex.org/W3213993043,2021,28,"['https://openalex.org/W1828163288', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2419292002', 'https://openalex.org/W2525778437', 'https://openalex.org/W2529548870', 'https://openalex.org/W2794365787', 'https://openalex.org/W2913718171', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2945700568', 'https://openalex.org/W2951456627', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970084653', 'https://openalex.org/W2975711469', 'https://openalex.org/W2982413405', 'https://openalex.org/W2995428172', 'https://openalex.org/W3016010032', 'https://openalex.org/W3034586846', 'https://openalex.org/W3037217258', 'https://openalex.org/W3037465386', 'https://openalex.org/W3037793211', 'https://openalex.org/W3092424727', 'https://openalex.org/W3096888553', 'https://openalex.org/W3101727459', 'https://openalex.org/W3115075512', 'https://openalex.org/W3118578889', 'https://openalex.org/W3162000275', 'https://openalex.org/W4288088457', 'https://openalex.org/W4385245566']","This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online automatic speech recognition (ASR) model, RNN-T, but remove the strong monotonic constraint, which is critical for the translation task to consider reordering. To make CAAT work, we introduce a novel latency loss whose expectation can be optimized by a forward-backward algorithm. We implement CAAT with Transformer while the general CAAT architecture can also be implemented with other attention-based encoder-decoder frameworks. Experiments on both speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks show that CAAT achieves significantly better latency-quality trade-offs compared to the state-of-the-art simultaneous translation approaches.",1.0
NOVEL_MT_120,https://openalex.org/W3201295503,2021,52,"['https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2250539671', 'https://openalex.org/W2257408573', 'https://openalex.org/W2395935897', 'https://openalex.org/W2493916176', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2889326796', 'https://openalex.org/W2905933322', 'https://openalex.org/W2908336025', 'https://openalex.org/W2912521296', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946232455', 'https://openalex.org/W2948947170', 'https://openalex.org/W2948981900', 'https://openalex.org/W2950577311', 'https://openalex.org/W2952650870', 'https://openalex.org/W2952809536', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964121744', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970597249', 'https://openalex.org/W2986154550', 'https://openalex.org/W2986562961', 'https://openalex.org/W2989276524', 'https://openalex.org/W2994928925', 'https://openalex.org/W2997763445', 'https://openalex.org/W3006381853', 'https://openalex.org/W3008110149', 'https://openalex.org/W3034857244', 'https://openalex.org/W3035390927', 'https://openalex.org/W3036120435', 'https://openalex.org/W3036939249', 'https://openalex.org/W3041866211', 'https://openalex.org/W3088592174', 'https://openalex.org/W3101860695', 'https://openalex.org/W3103182178', 'https://openalex.org/W3106433641', 'https://openalex.org/W3109959371', 'https://openalex.org/W3115011379', 'https://openalex.org/W3119175506', 'https://openalex.org/W3135176278', 'https://openalex.org/W3172096628', 'https://openalex.org/W3212888739', 'https://openalex.org/W4288333985', 'https://openalex.org/W4292779060', 'https://openalex.org/W4385245566']","The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pre-trained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of a dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En→De and 38.61 for De→En on the IWSLT’14 dataset, and 31.26 for En→De and 34.94 for De→En on the WMT’14 dataset, which exceeds all published numbers.",0.994475138121547
NOVEL_MT_121,https://openalex.org/W3168577192,2021,22,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2624871570', 'https://openalex.org/W2767206889', 'https://openalex.org/W2790235966', 'https://openalex.org/W2799124508', 'https://openalex.org/W2912351236', 'https://openalex.org/W2922349260', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946375144', 'https://openalex.org/W2950976310', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962915948', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963430933', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963677766', 'https://openalex.org/W2963759780', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165804', 'https://openalex.org/W2964204621', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970550739', 'https://openalex.org/W2970820321', 'https://openalex.org/W2971167892', 'https://openalex.org/W2976965654', 'https://openalex.org/W2988975212', 'https://openalex.org/W2996987694', 'https://openalex.org/W3035812575', 'https://openalex.org/W3045688849', 'https://openalex.org/W3102226577', 'https://openalex.org/W3105848458', 'https://openalex.org/W3113715281', 'https://openalex.org/W3113838476', 'https://openalex.org/W3127901106', 'https://openalex.org/W4294103325', 'https://openalex.org/W4385245566']","Yongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu, Xing Wang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_122,https://openalex.org/W3196292088,2021,7,"['https://openalex.org/W211509693', 'https://openalex.org/W222053410', 'https://openalex.org/W630532510', 'https://openalex.org/W1753482797', 'https://openalex.org/W1915251500', 'https://openalex.org/W2121879602', 'https://openalex.org/W2122270629', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2252154714', 'https://openalex.org/W2467834614', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2566564022', 'https://openalex.org/W2746762542', 'https://openalex.org/W2757920198', 'https://openalex.org/W2786253471', 'https://openalex.org/W2794365787', 'https://openalex.org/W2805790316', 'https://openalex.org/W2886095922', 'https://openalex.org/W2888541716', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890007195', 'https://openalex.org/W2902614977', 'https://openalex.org/W2919290281', 'https://openalex.org/W2928941594', 'https://openalex.org/W2944815030', 'https://openalex.org/W2949973181', 'https://openalex.org/W2952468927', 'https://openalex.org/W2954218767', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963281280', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963877297', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970152385', 'https://openalex.org/W2970259631', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970397283', 'https://openalex.org/W2970731908', 'https://openalex.org/W2970900338', 'https://openalex.org/W2971073020', 'https://openalex.org/W3017454464', 'https://openalex.org/W3035013535', 'https://openalex.org/W3035464238', 'https://openalex.org/W3100806282', 'https://openalex.org/W3104652516', 'https://openalex.org/W3105038888', 'https://openalex.org/W3106541240', 'https://openalex.org/W3117640374', 'https://openalex.org/W3118449360', 'https://openalex.org/W3120628641', 'https://openalex.org/W3175955584', 'https://openalex.org/W4285719527', 'https://openalex.org/W4287597717', 'https://openalex.org/W4298137069', 'https://openalex.org/W4299574851', 'https://openalex.org/W4385245566']","Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT—a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don’t filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: Hindi,Gujarati,Tamil-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.",0.9933774834437086
NOVEL_MT_124,https://openalex.org/W3176011954,2021,5,"['https://openalex.org/W582134693', 'https://openalex.org/W1902237438', 'https://openalex.org/W2021618504', 'https://openalex.org/W2070493638', 'https://openalex.org/W2107031757', 'https://openalex.org/W2141440284', 'https://openalex.org/W2146574666', 'https://openalex.org/W2156985047', 'https://openalex.org/W2183341477', 'https://openalex.org/W2295598076', 'https://openalex.org/W2595715041', 'https://openalex.org/W2626967530', 'https://openalex.org/W2913659301', 'https://openalex.org/W2947046281', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963238274', 'https://openalex.org/W2963532001', 'https://openalex.org/W2971120622', 'https://openalex.org/W3017068741', 'https://openalex.org/W3084095723', 'https://openalex.org/W3102476541', 'https://openalex.org/W3103915675', 'https://openalex.org/W3119107795', 'https://openalex.org/W4235132546', 'https://openalex.org/W4285719527', 'https://openalex.org/W4300870773', 'https://openalex.org/W4381032693']","Shiyue Zhang, Benjamin Frey, Mohit Bansal. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. 2021.",0.9952153110047848
NOVEL_MT_125,https://openalex.org/W3198078159,2021,19,"['https://openalex.org/W2080373976', 'https://openalex.org/W2101105183', 'https://openalex.org/W2143177362', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157614928', 'https://openalex.org/W2212703438', 'https://openalex.org/W2251743902', 'https://openalex.org/W2467834614', 'https://openalex.org/W2550821151', 'https://openalex.org/W2561274697', 'https://openalex.org/W2594978815', 'https://openalex.org/W2756566411', 'https://openalex.org/W2758950307', 'https://openalex.org/W2767434619', 'https://openalex.org/W2886776719', 'https://openalex.org/W2888519496', 'https://openalex.org/W2889326796', 'https://openalex.org/W2902128069', 'https://openalex.org/W2905927205', 'https://openalex.org/W2909737760', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946068894', 'https://openalex.org/W2953173959', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962743139', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963443335', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963919854', 'https://openalex.org/W2963962369', 'https://openalex.org/W2969999977', 'https://openalex.org/W2970015022', 'https://openalex.org/W2988249555', 'https://openalex.org/W3035214886', 'https://openalex.org/W3041866211', 'https://openalex.org/W3089659770', 'https://openalex.org/W3098341425', 'https://openalex.org/W3100268441', 'https://openalex.org/W3102315351', 'https://openalex.org/W3104652516', 'https://openalex.org/W3105718208', 'https://openalex.org/W3114856720', 'https://openalex.org/W3153391445', 'https://openalex.org/W3162734203', 'https://openalex.org/W3172669006', 'https://openalex.org/W3173506780', 'https://openalex.org/W4297801368', 'https://openalex.org/W4298393544', 'https://openalex.org/W4301230920', 'https://openalex.org/W4385245566']","In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs that contain infrequent words, thus making it closer to the true data distribution of parallel sentences. In this paper, we propose to follow a completely different approach and present a multi-task DA approach in which we generate new sentence pairs with transformations, such as reversing the order of the target sentence, which produce unfluent target sentences. During training, these augmented sentences are used as auxiliary tasks in a multi-task framework with the aim of providing new contexts where the target prefix is not informative enough to predict the next word. This strengthens the encoder and forces the decoder to pay more attention to the source representations of the encoder. Experiments carried out on six low-resource translation tasks show consistent improvements over the baseline and over DA methods aiming at extending the support of the empirical data distribution. The systems trained with our approach rely more on the source tokens, are more robust against domain shift and suffer less hallucinations.",0.9951690821256038
NOVEL_MT_127,https://openalex.org/W3200829979,2021,1,"['https://openalex.org/W2114013702', 'https://openalex.org/W2171074980', 'https://openalex.org/W2933138175', 'https://openalex.org/W2953072129', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970295111', 'https://openalex.org/W2971167892', 'https://openalex.org/W3034579764', 'https://openalex.org/W3093871477', 'https://openalex.org/W3118917012', 'https://openalex.org/W3120791402', 'https://openalex.org/W3120951354', 'https://openalex.org/W3120955312', 'https://openalex.org/W3176009285', 'https://openalex.org/W3200924470']","This paper presents the JHU-Microsoft joint submission for WMT 2021 quality estimation shared task. We only participate in Task 2 (post-editing effort estimation) of the shared task, focusing on the target-side word-level quality estimation. The techniques we experimented with include Levenshtein Transformer training and data augmentation with a combination of forward, backward, round-trip translation, and pseudo post-editing of the MT output. We demonstrate the competitiveness of our system compared to the widely adopted OpenKiwi-XLM baseline. Our system is also the top-ranking system on the MT MCC metric for the English-German language pair.",1.0
NOVEL_MT_128,https://openalex.org/W3176009285,2021,13,"['https://openalex.org/W1531245479', 'https://openalex.org/W1986345088', 'https://openalex.org/W2032191478', 'https://openalex.org/W2034224543', 'https://openalex.org/W2101105183', 'https://openalex.org/W2143221616', 'https://openalex.org/W2149327368', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165170939', 'https://openalex.org/W2251171258', 'https://openalex.org/W2518844522', 'https://openalex.org/W2571989689', 'https://openalex.org/W2795559220', 'https://openalex.org/W2799493995', 'https://openalex.org/W2914120296', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963979492', 'https://openalex.org/W2970045405', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970871182', 'https://openalex.org/W2971046749', 'https://openalex.org/W2971120958', 'https://openalex.org/W3011411500', 'https://openalex.org/W3034673518', 'https://openalex.org/W3035390927', 'https://openalex.org/W3082505376', 'https://openalex.org/W3097330605', 'https://openalex.org/W3118878571', 'https://openalex.org/W3119881489', 'https://openalex.org/W3120104809', 'https://openalex.org/W3120459072', 'https://openalex.org/W3120951354', 'https://openalex.org/W3121071870', 'https://openalex.org/W3186483169', 'https://openalex.org/W3205309080', 'https://openalex.org/W4385245566']","Dongjun Lee, Junhyeong Ahn, Heesoo Park, Jaemin Jo. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. 2021.",0.9953051643192488
NOVEL_MT_130,https://openalex.org/W3212906336,2021,1,"['https://openalex.org/W1965555277', 'https://openalex.org/W2466633577', 'https://openalex.org/W2466918907', 'https://openalex.org/W2582956876', 'https://openalex.org/W2593011301', 'https://openalex.org/W2605131327', 'https://openalex.org/W2620998106', 'https://openalex.org/W2753738274', 'https://openalex.org/W2901607128', 'https://openalex.org/W2924334974', 'https://openalex.org/W2945700568', 'https://openalex.org/W2951635603', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963979492', 'https://openalex.org/W2971345947', 'https://openalex.org/W2972448360', 'https://openalex.org/W3034625919', 'https://openalex.org/W3115711339', 'https://openalex.org/W3116524089', 'https://openalex.org/W3118578889']","A currently popular research area in end-to-end speech translation is the use of knowledge distillation from a machine translation (MT) task to improve the speech translation (ST) task. However, such scenario obviously only allows one way transfer, which is limited by the performance of the teacher model. Therefore, We hypothesis that the knowledge distillation-based approaches are sub-optimal. In this paper, we propose an alternative–a trainable mutual-learning scenario, where the MT and the ST models are collaboratively trained and are considered as peers, rather than teacher/student. This allows us to improve the performance of end-to-end ST more effectively than with a teacher-student paradigm. As a side benefit, performance of the MT model also improves. Experimental results show that in our mutual-learning scenario, models can effectively utilise the auxiliary information from peer models and achieve compelling results on Must-C dataset.",1.0
NOVEL_MT_132,https://openalex.org/W3186667189,2021,2,"['https://openalex.org/W211509693', 'https://openalex.org/W222053410', 'https://openalex.org/W1915251500', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2143017621', 'https://openalex.org/W2144600658', 'https://openalex.org/W2153579005', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555428947', 'https://openalex.org/W2739967986', 'https://openalex.org/W2741602058', 'https://openalex.org/W2767982226', 'https://openalex.org/W2886095922', 'https://openalex.org/W2889326796', 'https://openalex.org/W2904817833', 'https://openalex.org/W2949303037', 'https://openalex.org/W2949911645', 'https://openalex.org/W2950485982', 'https://openalex.org/W2951165112', 'https://openalex.org/W2962735107', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963532104', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964013027', 'https://openalex.org/W2970049541', 'https://openalex.org/W2970494057', 'https://openalex.org/W2970686691', 'https://openalex.org/W2971120622', 'https://openalex.org/W2973088264', 'https://openalex.org/W2986806214', 'https://openalex.org/W3000562670', 'https://openalex.org/W3014432386', 'https://openalex.org/W3015504467', 'https://openalex.org/W3018564137', 'https://openalex.org/W3035390927', 'https://openalex.org/W3046368065', 'https://openalex.org/W3081679571', 'https://openalex.org/W3103544486', 'https://openalex.org/W3103621467', 'https://openalex.org/W3105425516', 'https://openalex.org/W3107826490', 'https://openalex.org/W3137065374', 'https://openalex.org/W3137881600', 'https://openalex.org/W3152788712', 'https://openalex.org/W3154516802', 'https://openalex.org/W3179401712', 'https://openalex.org/W3197966603']","For most language combinations, parallel data is either scarce or simply unavailable. To address this, unsupervised machine translation (UMT) exploits large amounts of monolingual data by using synthetic data generation techniques such as back-translation and noising, while self-supervised NMT (SSNMT) identifies parallel sentences in smaller comparable data and trains on them. To date, the inclusion of UMT data generation techniques in SSNMT has not been investigated. We show that including UMT techniques into SSNMT significantly outperforms SSNMT and UMT on all tested language pairs, with improvements of up to +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT, respectively, on Afrikaans to English. We further show that the combination of multilingual denoising autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 (English to Swahili).",1.0
NOVEL_MT_133,https://openalex.org/W3155003847,2021,2,"['https://openalex.org/W179314280', 'https://openalex.org/W2141179513', 'https://openalex.org/W2509973494', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531867140', 'https://openalex.org/W2757521750', 'https://openalex.org/W2766182427', 'https://openalex.org/W2786226395', 'https://openalex.org/W2808423603', 'https://openalex.org/W2883210849', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964274713', 'https://openalex.org/W2980282514', 'https://openalex.org/W3017311573', 'https://openalex.org/W3035390927', 'https://openalex.org/W3082928416', 'https://openalex.org/W3137065374']","The explosion of user-generated content (UGC)--e.g. social media posts, comments, and reviews--has motivated the development of NLP applications tailored to these types of informal texts. Prevalent among these applications have been sentiment analysis and machine translation (MT). Grounded in the observation that UGC features highly idiomatic, sentiment-charged language, we propose a decoder-side approach that incorporates automatic sentiment scoring into the MT candidate selection process. We train separate English and Spanish sentiment classifiers, then, using n-best candidates generated by a baseline MT model with beam search, select the candidate that minimizes the absolute difference between the sentiment score of the source sentence and that of the translation, and perform a human evaluation to assess the produced translations. Unlike previous work, we select this minimally divergent translation by considering the sentiment scores of the source sentence and translation on a continuous interval, rather than using e.g. binary classification, allowing for more fine-grained selection of translation candidates. The results of human evaluations show that, in comparison to the open-source MT baseline model on top of which our sentiment-based pipeline is built, our pipeline produces more accurate translations of colloquial, sentiment-heavy source texts.",1.0
NOVEL_MT_137,https://openalex.org/W3211384195,2021,6,"['https://openalex.org/W1522301498', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121457870', 'https://openalex.org/W2144576765', 'https://openalex.org/W2148708890', 'https://openalex.org/W2176263492', 'https://openalex.org/W2251955814', 'https://openalex.org/W2529548870', 'https://openalex.org/W2605141709', 'https://openalex.org/W2890698823', 'https://openalex.org/W2933138175', 'https://openalex.org/W2951456627', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963248296', 'https://openalex.org/W2964078338', 'https://openalex.org/W2964272710', 'https://openalex.org/W2970074184', 'https://openalex.org/W2975711469', 'https://openalex.org/W2995428172', 'https://openalex.org/W3041866211', 'https://openalex.org/W3091991378', 'https://openalex.org/W3153994542', 'https://openalex.org/W3203146303', 'https://openalex.org/W3204406378', 'https://openalex.org/W4294619417']","In simultaneous machine translation, finding an agent with the optimal action sequence of reads and writes that maintain a high level of translation quality while minimizing the average lag in producing target tokens remains an extremely challenging problem. We propose a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences. These oracle sequences can then be used to train a supervised model for action generation at inference time. Our approach provides an alternative to current heuristic methods in simultaneous translation by introducing a new training objective, which is easier to train than previous attempts at training the agent using reinforcement learning techniques for this task. Our experimental results show that our novel training method for action generation produces much higher quality translations while minimizing the average lag in simultaneous translation.",1.0
NOVEL_MT_139,https://openalex.org/W3176913643,2021,55,"['https://openalex.org/W222053410', 'https://openalex.org/W832270446', 'https://openalex.org/W1522301498', 'https://openalex.org/W1625582487', 'https://openalex.org/W1915251500', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2126712675', 'https://openalex.org/W2127589108', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145790651', 'https://openalex.org/W2169054943', 'https://openalex.org/W2171590421', 'https://openalex.org/W2182922969', 'https://openalex.org/W2252272516', 'https://openalex.org/W2546938941', 'https://openalex.org/W2606974598', 'https://openalex.org/W2788330850', 'https://openalex.org/W2805394970', 'https://openalex.org/W2885421725', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890397703', 'https://openalex.org/W2891713103', 'https://openalex.org/W2896457183', 'https://openalex.org/W2905342727', 'https://openalex.org/W2951166594', 'https://openalex.org/W2951352467', 'https://openalex.org/W2951434086', 'https://openalex.org/W2952317054', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963018920', 'https://openalex.org/W2963056065', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963371754', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963469388', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963521540', 'https://openalex.org/W2963792777', 'https://openalex.org/W2963829526', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964308564', 'https://openalex.org/W2976223659', 'https://openalex.org/W2990391280', 'https://openalex.org/W2995154514', 'https://openalex.org/W2995746049', 'https://openalex.org/W2998702515', 'https://openalex.org/W3007672467', 'https://openalex.org/W3027879771', 'https://openalex.org/W3034640977', 'https://openalex.org/W3035531963', 'https://openalex.org/W3037854022', 'https://openalex.org/W3098341425', 'https://openalex.org/W3099700870', 'https://openalex.org/W3105721709', 'https://openalex.org/W3126425262', 'https://openalex.org/W3164747806', 'https://openalex.org/W3174160883', 'https://openalex.org/W3174481817', 'https://openalex.org/W3202105828', 'https://openalex.org/W4245202262', 'https://openalex.org/W4285719527', 'https://openalex.org/W4287649493', 'https://openalex.org/W4288087322', 'https://openalex.org/W4292779060', 'https://openalex.org/W4385245566']","Deng Cai, Yan Wang, Huayang Li, Wai Lam, Lemao Liu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_140,https://openalex.org/W3162711154,2021,3,"['https://openalex.org/W8550301', 'https://openalex.org/W630532510', 'https://openalex.org/W1682403713', 'https://openalex.org/W1959503543', 'https://openalex.org/W1975879668', 'https://openalex.org/W2026445738', 'https://openalex.org/W2077302143', 'https://openalex.org/W2130942839', 'https://openalex.org/W2157331557', 'https://openalex.org/W2419539795', 'https://openalex.org/W2556468274', 'https://openalex.org/W2566898803', 'https://openalex.org/W2594021297', 'https://openalex.org/W2739967986', 'https://openalex.org/W2791169651', 'https://openalex.org/W2803609229', 'https://openalex.org/W2911227954', 'https://openalex.org/W2917128112', 'https://openalex.org/W2933138175', 'https://openalex.org/W2953183526', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963288406', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963691697', 'https://openalex.org/W2964067969', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965373594', 'https://openalex.org/W2966087730', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970706333', 'https://openalex.org/W2971160427', 'https://openalex.org/W2986562961', 'https://openalex.org/W2989276524', 'https://openalex.org/W3018647120', 'https://openalex.org/W3023207088', 'https://openalex.org/W3034238904', 'https://openalex.org/W3035390927', 'https://openalex.org/W3047398590', 'https://openalex.org/W3082619646', 'https://openalex.org/W3101516616', 'https://openalex.org/W3155312918', 'https://openalex.org/W3155578004', 'https://openalex.org/W3169244955']","The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.",0.995850622406639
NOVEL_MT_141,https://openalex.org/W3136071852,2021,2,"['https://openalex.org/W630532510', 'https://openalex.org/W1828724394', 'https://openalex.org/W2025768430', 'https://openalex.org/W2250342921', 'https://openalex.org/W2462831000', 'https://openalex.org/W2493916176', 'https://openalex.org/W2890007195', 'https://openalex.org/W2913659301', 'https://openalex.org/W2948902769', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964266061', 'https://openalex.org/W2971031524', 'https://openalex.org/W3005441132', 'https://openalex.org/W3015504467', 'https://openalex.org/W3099178230', 'https://openalex.org/W3101286153', 'https://openalex.org/W3101498587', 'https://openalex.org/W3104723404']","Successful methods for unsupervised neural machine translation (UNMT) employ crosslingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical- and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to a UNMT baseline.",1.0
NOVEL_MT_144,https://openalex.org/W3175665465,2021,102,"['https://openalex.org/W648786980', 'https://openalex.org/W1522301498', 'https://openalex.org/W1647671624', 'https://openalex.org/W2127141656', 'https://openalex.org/W2296073425', 'https://openalex.org/W2342204193', 'https://openalex.org/W2767206889', 'https://openalex.org/W2892213699', 'https://openalex.org/W2907945666', 'https://openalex.org/W2946375144', 'https://openalex.org/W2952649152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962915948', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963536265', 'https://openalex.org/W2964121744', 'https://openalex.org/W2966746916', 'https://openalex.org/W2970832665', 'https://openalex.org/W2971167892', 'https://openalex.org/W2981648103', 'https://openalex.org/W2988536374', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990004017', 'https://openalex.org/W2990372437', 'https://openalex.org/W2990389671', 'https://openalex.org/W2996843693', 'https://openalex.org/W2996987694', 'https://openalex.org/W3000840023', 'https://openalex.org/W3004979489', 'https://openalex.org/W3015162217', 'https://openalex.org/W3030343076', 'https://openalex.org/W3034892578', 'https://openalex.org/W3035289598', 'https://openalex.org/W3035416964', 'https://openalex.org/W3039805635', 'https://openalex.org/W3100753857', 'https://openalex.org/W3101012756', 'https://openalex.org/W3114869109', 'https://openalex.org/W3129170862', 'https://openalex.org/W3167880383', 'https://openalex.org/W3175164646', 'https://openalex.org/W4385245566']","Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_148,https://openalex.org/W3176344849,2021,7,"['https://openalex.org/W77450199', 'https://openalex.org/W1541981365', 'https://openalex.org/W2098777910', 'https://openalex.org/W2105410942', 'https://openalex.org/W2117278770', 'https://openalex.org/W2161943765', 'https://openalex.org/W2172268343', 'https://openalex.org/W2317700292', 'https://openalex.org/W2402118343', 'https://openalex.org/W2467669267', 'https://openalex.org/W2744813330', 'https://openalex.org/W2888519496', 'https://openalex.org/W2912024054', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962735107', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963443335', 'https://openalex.org/W2963505445', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963897095', 'https://openalex.org/W2979249747', 'https://openalex.org/W2984227554', 'https://openalex.org/W3013840636', 'https://openalex.org/W3014114301', 'https://openalex.org/W3034640977', 'https://openalex.org/W3035579820', 'https://openalex.org/W3098341425', 'https://openalex.org/W3204406378']","Neural machine translation (NMT) is sensitive to domain shift. In this paper, we address this problem in an active learning setting where we can spend a given budget on translating in-domain data, and gradually fine-tune a pre-trained out-of-domain NMT model on the newly translated data. Existing active learning methods for NMT usually select sentences based on uncertainty scores, but these methods require costly translation of full sentences even when only one or two key phrases within the sentence are informative. To address this limitation, we re-examine previous work from the phrase-based machine translation (PBMT) era that selected not full sentences, but rather individual phrases. However, while incorporating these phrases into PBMT systems was relatively simple, it is less trivial for NMT systems, which need to be trained on full sequences to capture larger structural properties of sentences unique to the new domain. To overcome these hurdles, we propose to select both full sentences and individual phrases from unlabelled data in the new domain for routing to human translators. In a German-English translation task, our active learning approach achieves consistent improvements over uncertainty-based sentence selection methods, improving up to 1.2 BLEU score over strong active learning baselines.",1.0
NOVEL_MT_150,https://openalex.org/W3176120057,2021,35,"['https://openalex.org/W222053410', 'https://openalex.org/W1821462560', 'https://openalex.org/W2130942839', 'https://openalex.org/W2587694128', 'https://openalex.org/W2741838462', 'https://openalex.org/W2767206889', 'https://openalex.org/W2904829696', 'https://openalex.org/W2905933322', 'https://openalex.org/W2912521296', 'https://openalex.org/W2923622379', 'https://openalex.org/W2924902521', 'https://openalex.org/W2933138175', 'https://openalex.org/W2944869267', 'https://openalex.org/W2952650870', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963350559', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970454332', 'https://openalex.org/W2970849705', 'https://openalex.org/W2978544343', 'https://openalex.org/W2979303251', 'https://openalex.org/W2991786320', 'https://openalex.org/W3021293129', 'https://openalex.org/W3034368386', 'https://openalex.org/W3034938700', 'https://openalex.org/W3035317912', 'https://openalex.org/W3092173171', 'https://openalex.org/W3098576111', 'https://openalex.org/W3101668578', 'https://openalex.org/W3104123491', 'https://openalex.org/W3105966348', 'https://openalex.org/W3120459995', 'https://openalex.org/W3173417753', 'https://openalex.org/W4287651588', 'https://openalex.org/W4288256350', 'https://openalex.org/W4298159529', 'https://openalex.org/W4385245566']","Fusheng Wang, Jianhao Yan, Fandong Meng, Jie Zhou. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_153,https://openalex.org/W3200079248,2021,3,"['https://openalex.org/W22168010', 'https://openalex.org/W68733909', 'https://openalex.org/W342285082', 'https://openalex.org/W1249771036', 'https://openalex.org/W1522301498', 'https://openalex.org/W1574126082', 'https://openalex.org/W1636405317', 'https://openalex.org/W1726342440', 'https://openalex.org/W1889081078', 'https://openalex.org/W2016630033', 'https://openalex.org/W2047295649', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114609248', 'https://openalex.org/W2114879013', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143954309', 'https://openalex.org/W2148708890', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157331557', 'https://openalex.org/W2194775991', 'https://openalex.org/W2250313959', 'https://openalex.org/W2250681176', 'https://openalex.org/W2251139694', 'https://openalex.org/W2509282593', 'https://openalex.org/W2512924740', 'https://openalex.org/W2538197161', 'https://openalex.org/W2572474373', 'https://openalex.org/W2574640638', 'https://openalex.org/W2595715041', 'https://openalex.org/W2752630748', 'https://openalex.org/W2757521750', 'https://openalex.org/W2782590789', 'https://openalex.org/W2884476676', 'https://openalex.org/W2886095922', 'https://openalex.org/W2886198413', 'https://openalex.org/W2886641317', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890007195', 'https://openalex.org/W2914096745', 'https://openalex.org/W2914120296', 'https://openalex.org/W2922197727', 'https://openalex.org/W2944815030', 'https://openalex.org/W2949405462', 'https://openalex.org/W2949911645', 'https://openalex.org/W2950485982', 'https://openalex.org/W2952468927', 'https://openalex.org/W2952768586', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962735107', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963048642', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963360627', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963667932', 'https://openalex.org/W2963743213', 'https://openalex.org/W2963877297', 'https://openalex.org/W2964073484', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970971581', 'https://openalex.org/W2977458338', 'https://openalex.org/W2981994675', 'https://openalex.org/W2988451549', 'https://openalex.org/W2992478697', 'https://openalex.org/W2995460523', 'https://openalex.org/W3011608779', 'https://openalex.org/W3015354748', 'https://openalex.org/W3015504467', 'https://openalex.org/W3016672431', 'https://openalex.org/W3034773362', 'https://openalex.org/W3036839309', 'https://openalex.org/W3037109418', 'https://openalex.org/W3038734866', 'https://openalex.org/W3082017874', 'https://openalex.org/W3089109144', 'https://openalex.org/W3089515120', 'https://openalex.org/W3092752437', 'https://openalex.org/W3097619042', 'https://openalex.org/W3105378761', 'https://openalex.org/W3107826490', 'https://openalex.org/W3119378114', 'https://openalex.org/W3122659018', 'https://openalex.org/W3135099046', 'https://openalex.org/W3135321472', 'https://openalex.org/W3137065374', 'https://openalex.org/W3155103776', 'https://openalex.org/W3156892778', 'https://openalex.org/W3167047789', 'https://openalex.org/W3182683290', 'https://openalex.org/W4287813958', 'https://openalex.org/W4288025890', 'https://openalex.org/W4288284086', 'https://openalex.org/W4295312788', 'https://openalex.org/W4297801177', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4301187301', 'https://openalex.org/W4324016632', 'https://openalex.org/W4385245566']","We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our wikily translation models to unsupervised image captioning, and cross-lingual dependency parser transfer. In image captioning, we train a multi-tasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a wikily translation of the English captioning data. Our captioning results on Arabic are slightly better than that of its supervised model. In dependency parsing, we translate a large amount of monolingual text, and use it as an artificial training data in an annotation projection framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers.",1.0
NOVEL_MT_154,https://openalex.org/W3168212167,2021,35,"['https://openalex.org/W1522301498', 'https://openalex.org/W1524333225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2148708890', 'https://openalex.org/W2149327368', 'https://openalex.org/W2183341477', 'https://openalex.org/W2407080277', 'https://openalex.org/W2595715041', 'https://openalex.org/W2741049976', 'https://openalex.org/W2766219058', 'https://openalex.org/W2767206889', 'https://openalex.org/W2914120296', 'https://openalex.org/W2936774411', 'https://openalex.org/W2936969148', 'https://openalex.org/W2945700568', 'https://openalex.org/W2949328740', 'https://openalex.org/W2952443824', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963001247', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964212550', 'https://openalex.org/W2964213257', 'https://openalex.org/W2972448360', 'https://openalex.org/W2972818416', 'https://openalex.org/W2987395887', 'https://openalex.org/W2988975212', 'https://openalex.org/W2991786320', 'https://openalex.org/W2995999067', 'https://openalex.org/W2997436923', 'https://openalex.org/W2998386507', 'https://openalex.org/W3000840023', 'https://openalex.org/W3007142233', 'https://openalex.org/W3008549139', 'https://openalex.org/W3015633994', 'https://openalex.org/W3025165719', 'https://openalex.org/W3034363136', 'https://openalex.org/W3034571331', 'https://openalex.org/W3037217258', 'https://openalex.org/W3043665049', 'https://openalex.org/W3054645415', 'https://openalex.org/W3092424727', 'https://openalex.org/W3095901788', 'https://openalex.org/W3096700052', 'https://openalex.org/W3097777922', 'https://openalex.org/W3099942180', 'https://openalex.org/W3102811925', 'https://openalex.org/W3105962700', 'https://openalex.org/W3113715281', 'https://openalex.org/W3118578889', 'https://openalex.org/W3127901106', 'https://openalex.org/W3162919436', 'https://openalex.org/W4300558631', 'https://openalex.org/W4385245566']","Hirofumi Inaguma, Tatsuya Kawahara, Shinji Watanabe. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_155,https://openalex.org/W3160224178,2021,2,"['https://openalex.org/W630532510', 'https://openalex.org/W1828163288', 'https://openalex.org/W1993666847', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133459682', 'https://openalex.org/W2136657878', 'https://openalex.org/W2251912507', 'https://openalex.org/W2395935897', 'https://openalex.org/W2511950321', 'https://openalex.org/W2525778437', 'https://openalex.org/W2566623769', 'https://openalex.org/W2608146174', 'https://openalex.org/W2626778328', 'https://openalex.org/W2790319220', 'https://openalex.org/W2885950361', 'https://openalex.org/W2909737760', 'https://openalex.org/W2918914336', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950304420', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963919854', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964345285', 'https://openalex.org/W3082928416', 'https://openalex.org/W3092854299', 'https://openalex.org/W3099617520', 'https://openalex.org/W3105214104', 'https://openalex.org/W3118026775']","Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search -- the de facto standard inference algorithm in NMT -- and Eikema &amp; Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift.",1.0
NOVEL_MT_156,https://openalex.org/W3199713193,2021,3,"['https://openalex.org/W222053410', 'https://openalex.org/W1494198834', 'https://openalex.org/W1522301498', 'https://openalex.org/W1556750318', 'https://openalex.org/W1623072288', 'https://openalex.org/W1905522558', 'https://openalex.org/W1965555277', 'https://openalex.org/W1987869189', 'https://openalex.org/W2127141656', 'https://openalex.org/W2133943399', 'https://openalex.org/W2135708429', 'https://openalex.org/W2138477841', 'https://openalex.org/W2141766660', 'https://openalex.org/W2164777277', 'https://openalex.org/W2183341477', 'https://openalex.org/W2250821182', 'https://openalex.org/W2251309269', 'https://openalex.org/W2605131327', 'https://openalex.org/W2759932073', 'https://openalex.org/W2803728898', 'https://openalex.org/W2805402788', 'https://openalex.org/W2806429264', 'https://openalex.org/W2823653893', 'https://openalex.org/W2908743445', 'https://openalex.org/W2909170176', 'https://openalex.org/W2936774411', 'https://openalex.org/W2949328740', 'https://openalex.org/W2951562155', 'https://openalex.org/W2951770285', 'https://openalex.org/W2962714778', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964029788', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964172053', 'https://openalex.org/W2972448360', 'https://openalex.org/W2978099976', 'https://openalex.org/W2979198629', 'https://openalex.org/W2988008229', 'https://openalex.org/W2998135987', 'https://openalex.org/W3005972982', 'https://openalex.org/W3015698636', 'https://openalex.org/W3015889230', 'https://openalex.org/W3032322556', 'https://openalex.org/W3034625919', 'https://openalex.org/W3035169087', 'https://openalex.org/W3037465386', 'https://openalex.org/W3037542581', 'https://openalex.org/W3037625699', 'https://openalex.org/W3038962328', 'https://openalex.org/W3092085609', 'https://openalex.org/W3092283011', 'https://openalex.org/W3095918555', 'https://openalex.org/W3101648800', 'https://openalex.org/W3114894085', 'https://openalex.org/W3116524089', 'https://openalex.org/W3153567180', 'https://openalex.org/W4285719527', 'https://openalex.org/W4385245566']","Automatic translation systems are known to struggle with rare words. Among these, named entities (NEs) and domain-specific terms are crucial, since errors in their translation can lead to severe meaning distortions. Despite their importance, previous speech translation (ST) studies have neglected them, also due to the dearth of publicly available resources tailored to their specific evaluation. To fill this gap, we i) present the first systematic analysis of the behavior of state-of-the-art ST systems in translating NEs and terminology, and ii) release NEuRoparl-ST, a novel benchmark built from European Parliament speeches annotated with NEs and terminology. Our experiments on the three language directions covered by our benchmark (en→es/fr/it) show that ST systems correctly translate 75–80% of terms and 65–70% of NEs, with very low performance (37–40%) on person names.",0.9941520467836256
NOVEL_MT_161,https://openalex.org/W3177285425,2021,4,"['https://openalex.org/W20811212', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108325777', 'https://openalex.org/W2123301721', 'https://openalex.org/W2133459682', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251251208', 'https://openalex.org/W2801219566', 'https://openalex.org/W2903193068', 'https://openalex.org/W2936695845', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970791445', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971094522', 'https://openalex.org/W2996403597', 'https://openalex.org/W3034938700', 'https://openalex.org/W3035252911', 'https://openalex.org/W3035408261', 'https://openalex.org/W3103450644', 'https://openalex.org/W3174805484']","Runzhe Zhan, Xuebo Liu, Derek F. Wong, Lidia S. Chao. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",1.0
NOVEL_MT_163,https://openalex.org/W3161752359,2020,2,"['https://openalex.org/W1810943226', 'https://openalex.org/W1816313093', 'https://openalex.org/W1924770834', 'https://openalex.org/W2100271871', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113651538', 'https://openalex.org/W2117086786', 'https://openalex.org/W2125074935', 'https://openalex.org/W2155027007', 'https://openalex.org/W2413794162', 'https://openalex.org/W2472373455', 'https://openalex.org/W2532807140', 'https://openalex.org/W2606974598', 'https://openalex.org/W2608787653', 'https://openalex.org/W2621975677', 'https://openalex.org/W2752047430', 'https://openalex.org/W2757980860', 'https://openalex.org/W2883757330', 'https://openalex.org/W2890238976', 'https://openalex.org/W2898972706', 'https://openalex.org/W2902128219', 'https://openalex.org/W2933138175', 'https://openalex.org/W2948399755', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963824830', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964308564']","Encoder-decoder has been widely used in neural machine translation (NMT). A few methods have been proposed to improve it with multiple passes of decoding. However, their full potential is limited by a lack of appropriate termination policies. To address this issue, we present a novel architecture, Rewriter-Evaluator. It consists of a rewriter and an evaluator. Translating a source sentence involves multiple passes. At every pass, the rewriter produces a new translation to improve the past translation and the evaluator estimates the translation quality to decide whether to terminate the rewriting process. We also propose prioritized gradient descent (PGD) that facilitates training the rewriter and the evaluator jointly. Though incurring multiple passes of decoding, Rewriter-Evaluator with the proposed PGD method can be trained with a similar time to that of training encoder-decoder models. We apply the proposed architecture to improve the general NMT models (e.g., Transformer). We conduct extensive experiments on two translation tasks, Chinese-English and English-German, and show that the proposed architecture notably improves the performances of NMT models and significantly outperforms previous baselines.",1.0
NOVEL_MT_165,https://openalex.org/W3198216919,2021,1,"['https://openalex.org/W2251766657', 'https://openalex.org/W2251955814', 'https://openalex.org/W2529548870', 'https://openalex.org/W2593543827', 'https://openalex.org/W2611838487', 'https://openalex.org/W2799051177', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970074184', 'https://openalex.org/W2970084653', 'https://openalex.org/W2971347700', 'https://openalex.org/W3035520602', 'https://openalex.org/W3104234009', 'https://openalex.org/W3120734549']",In a real-time simultaneous translation setting and neural machine translation (NMT) models start generating target language tokens from incomplete source language sentences and making them harder to translate and leading to poor translation quality. Previous research has shown that document-level NMT and comprising of sentence and context encoders and a decoder and leverages context from neighboring sentences and helps improve translation quality. In simultaneous translation settings and the context from previous sentences should be even more critical. To this end and in this paper and we propose wait-k simultaneous document-level NMT where we keep the context encoder as it is and replace the source sentence encoder and target language decoder with their wait-k equivalents. We experiment with low and high resource settings using the ALT and OpenSubtitles2018 corpora and where we observe minor improvements in translation quality. We then perform an analysis of the translations obtained using our models by focusing on sentences that should benefit from the context where we found out that the model does and in fact and benefit from context but is unable to effectively leverage it and especially in a low-resource setting. This shows that there is a need for further innovation in the way useful context is identified and leveraged.,0.9943502824858758
NOVEL_MT_167,https://openalex.org/W3167880383,2021,28,"['https://openalex.org/W648786980', 'https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W1991133427', 'https://openalex.org/W2101105183', 'https://openalex.org/W2138615112', 'https://openalex.org/W2147880316', 'https://openalex.org/W2552110825', 'https://openalex.org/W2595715041', 'https://openalex.org/W2767206889', 'https://openalex.org/W2789543585', 'https://openalex.org/W2906111771', 'https://openalex.org/W2907945666', 'https://openalex.org/W2933138175', 'https://openalex.org/W2948197522', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963571341', 'https://openalex.org/W2964026424', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970832665', 'https://openalex.org/W2981648103', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990004017', 'https://openalex.org/W2990389671', 'https://openalex.org/W2996843693', 'https://openalex.org/W3054488230', 'https://openalex.org/W3106104873', 'https://openalex.org/W3175665465', 'https://openalex.org/W4385245566']","Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, Jiajun Chen. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_168,https://openalex.org/W3198891452,2021,7,"['https://openalex.org/W597389032', 'https://openalex.org/W630532510', 'https://openalex.org/W962302690', 'https://openalex.org/W2101105183', 'https://openalex.org/W2154238592', 'https://openalex.org/W2250617563', 'https://openalex.org/W2252166243', 'https://openalex.org/W2525857792', 'https://openalex.org/W2563845258', 'https://openalex.org/W2579496717', 'https://openalex.org/W2607892599', 'https://openalex.org/W2741104967', 'https://openalex.org/W2756566411', 'https://openalex.org/W2804191098', 'https://openalex.org/W2805394970', 'https://openalex.org/W2886095922', 'https://openalex.org/W2889933256', 'https://openalex.org/W2898700502', 'https://openalex.org/W2919290281', 'https://openalex.org/W2936695845', 'https://openalex.org/W2949611393', 'https://openalex.org/W2950940239', 'https://openalex.org/W2951451051', 'https://openalex.org/W2962708992', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963897095', 'https://openalex.org/W2965780468', 'https://openalex.org/W2970956481', 'https://openalex.org/W2971307358', 'https://openalex.org/W2971965968', 'https://openalex.org/W2979826702', 'https://openalex.org/W2982399380', 'https://openalex.org/W2982756474', 'https://openalex.org/W2990971812', 'https://openalex.org/W2998431971', 'https://openalex.org/W2998453866', 'https://openalex.org/W3001434439', 'https://openalex.org/W3027495063', 'https://openalex.org/W3033187248', 'https://openalex.org/W3034238904', 'https://openalex.org/W3034589622', 'https://openalex.org/W3034807061', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035379020', 'https://openalex.org/W3046368065', 'https://openalex.org/W3088304121', 'https://openalex.org/W3098593077', 'https://openalex.org/W3098824823', 'https://openalex.org/W3100727892', 'https://openalex.org/W3103169714', 'https://openalex.org/W3105214104', 'https://openalex.org/W3107826490', 'https://openalex.org/W3107858036', 'https://openalex.org/W3122890974', 'https://openalex.org/W3146044506', 'https://openalex.org/W3172669006', 'https://openalex.org/W3172813388', 'https://openalex.org/W3175204760', 'https://openalex.org/W3186081172', 'https://openalex.org/W4287069804', 'https://openalex.org/W4287585148', 'https://openalex.org/W4287694131', 'https://openalex.org/W4301764775', 'https://openalex.org/W4322614701', 'https://openalex.org/W4385245566']","Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-language-family models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore, COMET) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms bilingual fine-tuning on poetic data.",0.992248062015504
NOVEL_MT_172,https://openalex.org/W3175963743,2021,13,"['https://openalex.org/W10548402', 'https://openalex.org/W1522301498', 'https://openalex.org/W2547875792', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2923014074', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945260553', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963799213', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964266061', 'https://openalex.org/W2970597249', 'https://openalex.org/W2971031524', 'https://openalex.org/W2971274815', 'https://openalex.org/W2982399380', 'https://openalex.org/W2994928925', 'https://openalex.org/W2996854111', 'https://openalex.org/W2997763445', 'https://openalex.org/W3006381853', 'https://openalex.org/W3034999214', 'https://openalex.org/W3036601975', 'https://openalex.org/W3082274269', 'https://openalex.org/W3092327118', 'https://openalex.org/W3093345276', 'https://openalex.org/W3099782249', 'https://openalex.org/W3100311862', 'https://openalex.org/W3105912780', 'https://openalex.org/W3106321930', 'https://openalex.org/W3107826490', 'https://openalex.org/W3135367836', 'https://openalex.org/W3166396011', 'https://openalex.org/W4287874506', 'https://openalex.org/W4298393544']","Shuo Ren, Long Zhou, Shujie Liu, Furu Wei, Ming Zhou, Shuai Ma. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",0.9948717948717948
NOVEL_MT_174,https://openalex.org/W3198959024,2021,8,['https://openalex.org/W2933138175'],"Transformer-based models have gained increasing popularity achieving\nstate-of-the-art performance in many research fields including speech\ntranslation. However, Transformer's quadratic complexity with respect to the\ninput sequence length prevents its adoption as is with audio signals, which are\ntypically represented by long sequences. Current solutions resort to an initial\nsub-optimal compression based on a fixed sampling of raw audio features.\nTherefore, potentially useful linguistic information is not accessible to\nhigher-level layers in the architecture. To solve this issue, we propose\nSpeechformer, an architecture that, thanks to reduced memory usage in the\nattention layers, avoids the initial lossy compression and aggregates\ninformation only at a higher level according to more informed linguistic\ncriteria. Experiments on three language pairs (en-&gt;de/es/nl) show the efficacy\nof our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and\nof up to 4.0 BLEU in a low resource scenario.\n",0.9925925925925926
NOVEL_MT_177,https://openalex.org/W3167860749,2021,22,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W2026810221', 'https://openalex.org/W2250375035', 'https://openalex.org/W2250537251', 'https://openalex.org/W2250864392', 'https://openalex.org/W2252123671', 'https://openalex.org/W2550821151', 'https://openalex.org/W2606900862', 'https://openalex.org/W2619249607', 'https://openalex.org/W2760664922', 'https://openalex.org/W2804491852', 'https://openalex.org/W2836780890', 'https://openalex.org/W2888912028', 'https://openalex.org/W2916721475', 'https://openalex.org/W2918919089', 'https://openalex.org/W2951391755', 'https://openalex.org/W2962947230', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964116568', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964164798', 'https://openalex.org/W2964233404', 'https://openalex.org/W2966469393', 'https://openalex.org/W2970796523', 'https://openalex.org/W2977458338', 'https://openalex.org/W2979826702', 'https://openalex.org/W2982399380', 'https://openalex.org/W2983104185', 'https://openalex.org/W2983845924', 'https://openalex.org/W2986009340', 'https://openalex.org/W2996428491', 'https://openalex.org/W3001434439', 'https://openalex.org/W3029413185', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035169973', 'https://openalex.org/W3035586188', 'https://openalex.org/W3037095066', 'https://openalex.org/W3046368065', 'https://openalex.org/W3082928416', 'https://openalex.org/W3093157554', 'https://openalex.org/W3098739824', 'https://openalex.org/W3098824823', 'https://openalex.org/W3099405210', 'https://openalex.org/W3101221466', 'https://openalex.org/W3103028291', 'https://openalex.org/W3104890489', 'https://openalex.org/W3107826490', 'https://openalex.org/W3120355725', 'https://openalex.org/W3154511441', 'https://openalex.org/W3173677700', 'https://openalex.org/W4287694131', 'https://openalex.org/W4293430466', 'https://openalex.org/W4294990258', 'https://openalex.org/W4385245566']","Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation: we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https://github.com/SapienzaNLP/sgl.",0.9938650306748468
NOVEL_MT_179,https://openalex.org/W3174160883,2021,37,"['https://openalex.org/W832270446', 'https://openalex.org/W1522301498', 'https://openalex.org/W1563858398', 'https://openalex.org/W1569117599', 'https://openalex.org/W1901440691', 'https://openalex.org/W1968554861', 'https://openalex.org/W2025183033', 'https://openalex.org/W2040852548', 'https://openalex.org/W2101105183', 'https://openalex.org/W2126712675', 'https://openalex.org/W2131969445', 'https://openalex.org/W2148708890', 'https://openalex.org/W2250649184', 'https://openalex.org/W2251469009', 'https://openalex.org/W2251743902', 'https://openalex.org/W2522770641', 'https://openalex.org/W2618463334', 'https://openalex.org/W2757592053', 'https://openalex.org/W2888519496', 'https://openalex.org/W2888808532', 'https://openalex.org/W2897831761', 'https://openalex.org/W2905342727', 'https://openalex.org/W2952317054', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962834107', 'https://openalex.org/W2962982474', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963829526', 'https://openalex.org/W2963841178', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2969999977', 'https://openalex.org/W2978101627', 'https://openalex.org/W2998508940', 'https://openalex.org/W3031968991', 'https://openalex.org/W3035531963', 'https://openalex.org/W3098341425', 'https://openalex.org/W3105038888', 'https://openalex.org/W3105813095', 'https://openalex.org/W3126425262', 'https://openalex.org/W3155609600', 'https://openalex.org/W3164673114', 'https://openalex.org/W3164747806', 'https://openalex.org/W3175810841', 'https://openalex.org/W3176913643', 'https://openalex.org/W3197274356', 'https://openalex.org/W3202105828', 'https://openalex.org/W4245202262', 'https://openalex.org/W4287373797', 'https://openalex.org/W4287649493', 'https://openalex.org/W4385245566']","Qiuxiang He, Guoping Huang, Qu Cui, Li Li, Lemao Liu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_180,https://openalex.org/W3169680668,2021,1,"['https://openalex.org/W2101105183', 'https://openalex.org/W2560647685', 'https://openalex.org/W2767019613', 'https://openalex.org/W2808508619', 'https://openalex.org/W2891534142', 'https://openalex.org/W2914120296', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963693355', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2971347700', 'https://openalex.org/W2987998469', 'https://openalex.org/W2996822578', 'https://openalex.org/W3034351728', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035629723', 'https://openalex.org/W3102507836', 'https://openalex.org/W3107826490', 'https://openalex.org/W4385245566']","Pengcheng Yang, Pei Zhang, Boxing Chen, Jun Xie, Weihua Luo. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_181,https://openalex.org/W3205514791,2021,4,"['https://openalex.org/W1498238796', 'https://openalex.org/W2111798208', 'https://openalex.org/W2148708890', 'https://openalex.org/W2156985047', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251994258', 'https://openalex.org/W2419292002', 'https://openalex.org/W2529548870', 'https://openalex.org/W2572474373', 'https://openalex.org/W2741049976', 'https://openalex.org/W2758491225', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890698823', 'https://openalex.org/W2933138175', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2963001247', 'https://openalex.org/W2963126845', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963697731', 'https://openalex.org/W2964078338', 'https://openalex.org/W2964247056', 'https://openalex.org/W2970074184', 'https://openalex.org/W2971167892', 'https://openalex.org/W2995404354', 'https://openalex.org/W2995428172', 'https://openalex.org/W2996403597', 'https://openalex.org/W3005389111', 'https://openalex.org/W3034982693', 'https://openalex.org/W3035348852', 'https://openalex.org/W3035463087', 'https://openalex.org/W3037231493', 'https://openalex.org/W3037793211', 'https://openalex.org/W3045875328', 'https://openalex.org/W3093501075', 'https://openalex.org/W3104234009', 'https://openalex.org/W3105626768', 'https://openalex.org/W3105962700', 'https://openalex.org/W3113677452', 'https://openalex.org/W3121114617', 'https://openalex.org/W3174892288']","Recent work in simultaneous machine translation is often trained with conventional full sentence translation corpora, leading to either excessive latency or necessity to anticipate as-yet-unarrived words, when dealing with a language pair whose word orders significantly differ. This is unlike human simultaneous interpreters who produce largely monotonic translations at the expense of the grammaticality of a sentence being translated. In this paper, we thus propose an algorithm to reorder and refine the target side of a full sentence translation corpus, so that the words/phrases between the source and target sentences are aligned largely monotonically, using word alignment and non-autoregressive neural machine translation. We then train a widely used wait-k simultaneous translation model on this reordered-and-refined corpus. The proposed approach improves BLEU scores and resulting translations exhibit enhanced monotonicity with source sentences.",1.0
NOVEL_MT_182,https://openalex.org/W3199464215,2021,19,"['https://openalex.org/W1593114658', 'https://openalex.org/W1980264541', 'https://openalex.org/W2148708890', 'https://openalex.org/W2150884987', 'https://openalex.org/W2251955814', 'https://openalex.org/W2419292002', 'https://openalex.org/W2529548870', 'https://openalex.org/W2605141709', 'https://openalex.org/W2809290718', 'https://openalex.org/W2886751231', 'https://openalex.org/W2890698823', 'https://openalex.org/W2905927205', 'https://openalex.org/W2915573484', 'https://openalex.org/W2933138175', 'https://openalex.org/W2941193543', 'https://openalex.org/W2950513705', 'https://openalex.org/W2951456627', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952339051', 'https://openalex.org/W2952992734', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962710014', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962822108', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963684875', 'https://openalex.org/W2964078338', 'https://openalex.org/W2970074184', 'https://openalex.org/W2970717545', 'https://openalex.org/W2975711469', 'https://openalex.org/W2995428172', 'https://openalex.org/W3025832015', 'https://openalex.org/W3034296505', 'https://openalex.org/W3035348852', 'https://openalex.org/W3037186962', 'https://openalex.org/W3037231493', 'https://openalex.org/W3037337508', 'https://openalex.org/W3037542581', 'https://openalex.org/W3037698816', 'https://openalex.org/W3045631589', 'https://openalex.org/W3046024489', 'https://openalex.org/W3091991378', 'https://openalex.org/W3093501075', 'https://openalex.org/W3105422997', 'https://openalex.org/W3105626768', 'https://openalex.org/W3169408111', 'https://openalex.org/W3174892288', 'https://openalex.org/W4293718192', 'https://openalex.org/W4385245566']","Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods usually need to train multiple SiMT models for different latency levels, resulting in large computational costs. In this paper, we propose a universal SiMT model with Mixture-of-Experts Wait-k Policy to achieve the best translation quality under arbitrary latency with only one trained model. Specifically, our method employs multi-head attention to accomplish the mixture of experts where each head is treated as a wait-k expert with its own waiting words number, and given a test latency and source inputs, the weights of the experts are accordingly adjusted to produce the best translation. Experiments on three datasets show that our method outperforms all the strong baselines under different latency, including the state-of-the-art adaptive policy.",1.0
NOVEL_MT_183,https://openalex.org/W3213137415,2021,5,"['https://openalex.org/W565596797', 'https://openalex.org/W1508977358', 'https://openalex.org/W1632114991', 'https://openalex.org/W1909180194', 'https://openalex.org/W2035913210', 'https://openalex.org/W2038248725', 'https://openalex.org/W2093647425', 'https://openalex.org/W2096204319', 'https://openalex.org/W2115289978', 'https://openalex.org/W2121745180', 'https://openalex.org/W2124656564', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136346830', 'https://openalex.org/W2143391265', 'https://openalex.org/W2160133551', 'https://openalex.org/W2167393476', 'https://openalex.org/W2251088156', 'https://openalex.org/W2493916176', 'https://openalex.org/W2546938941', 'https://openalex.org/W2572474373', 'https://openalex.org/W2594047108', 'https://openalex.org/W2737638662', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2753628379', 'https://openalex.org/W2768763386', 'https://openalex.org/W2798304389', 'https://openalex.org/W2890007195', 'https://openalex.org/W2890176538', 'https://openalex.org/W2899486018', 'https://openalex.org/W2914120296', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945059185', 'https://openalex.org/W2950428495', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962811598', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963084773', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963467085', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963643701', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964015378', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970247882', 'https://openalex.org/W2970626985', 'https://openalex.org/W2972974661', 'https://openalex.org/W2982399380', 'https://openalex.org/W2995460523', 'https://openalex.org/W2996428491', 'https://openalex.org/W2998230451', 'https://openalex.org/W3011411500', 'https://openalex.org/W3011573365', 'https://openalex.org/W3034906024', 'https://openalex.org/W3034999214', 'https://openalex.org/W3082017874', 'https://openalex.org/W3092468804', 'https://openalex.org/W3103544486', 'https://openalex.org/W3105601320', 'https://openalex.org/W3105940451', 'https://openalex.org/W3128933491', 'https://openalex.org/W3175666528', 'https://openalex.org/W4287824654', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Machine translation usually relies on parallel corpora to provide parallel signals for training. The advent of unsupervised machine translation has brought machine translation away from this reliance, though performance still lags behind traditional supervised machine translation. In unsupervised machine translation, the model seeks symmetric language similarities as a source of weak parallel signal to achieve translation. Chomsky's Universal Grammar theory postulates that grammar is an innate form of knowledge to humans and is governed by universal principles and constraints. Therefore, in this paper, we seek to leverage such shared grammar clues to provide more explicit language parallel signals to enhance the training of unsupervised machine translation models. Through experiments on multiple typical language pairs, we demonstrate the effectiveness of our proposed approaches.",1.0
NOVEL_MT_185,https://openalex.org/W3176957840,2021,22,"['https://openalex.org/W23077562', 'https://openalex.org/W179314280', 'https://openalex.org/W1522301498', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108862644', 'https://openalex.org/W2117278770', 'https://openalex.org/W2121127625', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2176263492', 'https://openalex.org/W2251071050', 'https://openalex.org/W2607987856', 'https://openalex.org/W2786054661', 'https://openalex.org/W2786656291', 'https://openalex.org/W2885250264', 'https://openalex.org/W2896457183', 'https://openalex.org/W2899427634', 'https://openalex.org/W2903012348', 'https://openalex.org/W2903193068', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963163972', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963833497', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964190861', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970694516', 'https://openalex.org/W2983040767', 'https://openalex.org/W3018305985', 'https://openalex.org/W3034775979', 'https://openalex.org/W3035390927', 'https://openalex.org/W3089025530', 'https://openalex.org/W4385245566']","Ann Lee, Michael Auli, Marc'Aurelio Ranzato. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_186,https://openalex.org/W3174556939,2021,20,"['https://openalex.org/W1902237438', 'https://openalex.org/W2051840895', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2525778437', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2903268415', 'https://openalex.org/W2906152891', 'https://openalex.org/W2912351236', 'https://openalex.org/W2934842096', 'https://openalex.org/W2946794439', 'https://openalex.org/W2950768109', 'https://openalex.org/W2954730351', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962834107', 'https://openalex.org/W2962851944', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963798744', 'https://openalex.org/W2964159778', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970045405', 'https://openalex.org/W2970726176', 'https://openalex.org/W2972498556', 'https://openalex.org/W2972918955', 'https://openalex.org/W2977162702', 'https://openalex.org/W2979200397', 'https://openalex.org/W2996061341', 'https://openalex.org/W2998655810', 'https://openalex.org/W3027886415', 'https://openalex.org/W3035281110', 'https://openalex.org/W3037212200', 'https://openalex.org/W3088181395', 'https://openalex.org/W3094519420', 'https://openalex.org/W3101609372', 'https://openalex.org/W3103729510', 'https://openalex.org/W3106185885', 'https://openalex.org/W3173506780', 'https://openalex.org/W3211259717', 'https://openalex.org/W4288103164']","Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, Mu Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_187,https://openalex.org/W3175663471,2021,37,"['https://openalex.org/W1673923490', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2546938941', 'https://openalex.org/W2767899794', 'https://openalex.org/W2811010710', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2922293812', 'https://openalex.org/W2949128310', 'https://openalex.org/W2962732637', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963823140', 'https://openalex.org/W2963969878', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964153729', 'https://openalex.org/W2964159778', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970295111', 'https://openalex.org/W3034397670', 'https://openalex.org/W3035219095', 'https://openalex.org/W3084992427', 'https://openalex.org/W3099729825', 'https://openalex.org/W3176909378', 'https://openalex.org/W4385245566']","Xinze Zhang, Junzhe Zhang, Zhenhua Chen, Kun He. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_188,https://openalex.org/W4200635539,2025,6,[],664,0.9953488372093025
NOVEL_MT_189,https://openalex.org/W3172475066,2021,7,"['https://openalex.org/W1647671624', 'https://openalex.org/W1995470235', 'https://openalex.org/W2067608147', 'https://openalex.org/W2098447295', 'https://openalex.org/W2251265976', 'https://openalex.org/W2251939518', 'https://openalex.org/W2395826315', 'https://openalex.org/W2512924740', 'https://openalex.org/W2785674851', 'https://openalex.org/W2794070657', 'https://openalex.org/W2896457183', 'https://openalex.org/W2907461267', 'https://openalex.org/W2940683898', 'https://openalex.org/W2955450582', 'https://openalex.org/W2963126845', 'https://openalex.org/W2963341956', 'https://openalex.org/W2970295111', 'https://openalex.org/W2978094541', 'https://openalex.org/W2995006653', 'https://openalex.org/W3004061212', 'https://openalex.org/W3035013535']","Hoang-Quoc Nguyen-Son, Tran Thao, Seira Hidano, Ishita Gupta, Shinsaku Kiyomoto. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_190,https://openalex.org/W3211399536,2021,3,"['https://openalex.org/W1988843054', 'https://openalex.org/W2114296159', 'https://openalex.org/W2250293945', 'https://openalex.org/W2251263615', 'https://openalex.org/W2251841106', 'https://openalex.org/W2252241921', 'https://openalex.org/W2253444830', 'https://openalex.org/W2539350388', 'https://openalex.org/W2802105481', 'https://openalex.org/W2952328691', 'https://openalex.org/W2953494151', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963631950', 'https://openalex.org/W2963803533', 'https://openalex.org/W2969896603', 'https://openalex.org/W2973088264', 'https://openalex.org/W2973192523', 'https://openalex.org/W2987395887', 'https://openalex.org/W2991497298', 'https://openalex.org/W3024803553', 'https://openalex.org/W3049568452', 'https://openalex.org/W3112486745', 'https://openalex.org/W3120763539', 'https://openalex.org/W3134374554', 'https://openalex.org/W3154152568', 'https://openalex.org/W3166707765', 'https://openalex.org/W4287778568', 'https://openalex.org/W4288287305', 'https://openalex.org/W4297663312']","Sentiment analysis systems have been shown to exhibit sensitivity to protected attributes. Round-trip translation, on the other hand, has been shown to normalize text. We explore the impact of round-trip translation on the demographic parity of sentiment classifiers and show how round-trip translation consistently improves classification fairness at test time (reducing up to 47% of between-group gaps). We also explore the idea of retraining sentiment classifiers on round-trip-translated data.",1.0
NOVEL_MT_192,https://openalex.org/W3210390771,2021,15,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1906341845', 'https://openalex.org/W1975879668', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123442489', 'https://openalex.org/W2148334774', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250590325', 'https://openalex.org/W2251654371', 'https://openalex.org/W2419539795', 'https://openalex.org/W2496235729', 'https://openalex.org/W2525778437', 'https://openalex.org/W2905927205', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949303037', 'https://openalex.org/W2953830716', 'https://openalex.org/W2955388956', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964336292', 'https://openalex.org/W2973088264', 'https://openalex.org/W2978927999', 'https://openalex.org/W2978943549', 'https://openalex.org/W3008897815', 'https://openalex.org/W3100806282', 'https://openalex.org/W3104453603', 'https://openalex.org/W3105425516', 'https://openalex.org/W3106445907', 'https://openalex.org/W3107826490', 'https://openalex.org/W3152788712', 'https://openalex.org/W4285706658', 'https://openalex.org/W4298857067', 'https://openalex.org/W4385245566']","We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations: the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation. We release our dataset at: https://github.com/VinAIResearch/PhoMT",0.9948717948717948
NOVEL_MT_193,https://openalex.org/W3205818463,2021,5,"['https://openalex.org/W22168010', 'https://openalex.org/W620750443', 'https://openalex.org/W1990611844', 'https://openalex.org/W2032175749', 'https://openalex.org/W2104401056', 'https://openalex.org/W2152263452', 'https://openalex.org/W2251542837', 'https://openalex.org/W2337887686', 'https://openalex.org/W2419292002', 'https://openalex.org/W2461231802', 'https://openalex.org/W2554098328', 'https://openalex.org/W2595715041', 'https://openalex.org/W2766723428', 'https://openalex.org/W2933138175', 'https://openalex.org/W2951456627', 'https://openalex.org/W2970295111', 'https://openalex.org/W3037109418', 'https://openalex.org/W3100608856', 'https://openalex.org/W3153994542', 'https://openalex.org/W3166908602', 'https://openalex.org/W3202197379', 'https://openalex.org/W4241645538']","Most existing simultaneous machine translation (SiMT) systems are trained and evaluated on offline translation corpora. We argue that SiMT systems should be trained and tested on real interpretation data. To illustrate this argument, we propose an interpretation test set and conduct a realistic evaluation of SiMT trained on offline translations. Our results, on our test set along with 3 existing smaller scale language pairs, highlight the difference of up-to 13.83 BLEU score when SiMT models are evaluated on translation vs interpretation data. In the absence of interpretation training data, we propose a translation-to-interpretation (T2I) style transfer method which allows converting existing offline translations into interpretation-style data, leading to up-to 2.8 BLEU improvement. However, the evaluation gap remains notable, calling for constructing large-scale interpretation corpora better suited for evaluating and developing SiMT systems.",1.0
NOVEL_MT_198,https://openalex.org/W3175283066,2021,13,"['https://openalex.org/W222053410', 'https://openalex.org/W2035846950', 'https://openalex.org/W2064675550', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2508824506', 'https://openalex.org/W2613904329', 'https://openalex.org/W2885588803', 'https://openalex.org/W2908336025', 'https://openalex.org/W2921311659', 'https://openalex.org/W2949745489', 'https://openalex.org/W2951563833', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963597829', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970550739', 'https://openalex.org/W2984274107', 'https://openalex.org/W2995971510', 'https://openalex.org/W3034687015', 'https://openalex.org/W3035548285', 'https://openalex.org/W3035747971', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, Meng Zhang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_202,https://openalex.org/W3175092301,2021,12,"['https://openalex.org/W1587141723', 'https://openalex.org/W1753482797', 'https://openalex.org/W1821462560', 'https://openalex.org/W2016589492', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2162245945', 'https://openalex.org/W2294370754', 'https://openalex.org/W2460550122', 'https://openalex.org/W2466062786', 'https://openalex.org/W2487501366', 'https://openalex.org/W2525778437', 'https://openalex.org/W2532807140', 'https://openalex.org/W2580192806', 'https://openalex.org/W2601324753', 'https://openalex.org/W2613904329', 'https://openalex.org/W2738371943', 'https://openalex.org/W2752047430', 'https://openalex.org/W2769298630', 'https://openalex.org/W2783831488', 'https://openalex.org/W2794365787', 'https://openalex.org/W2886776719', 'https://openalex.org/W2888442053', 'https://openalex.org/W2890220768', 'https://openalex.org/W2898972706', 'https://openalex.org/W2933138175', 'https://openalex.org/W2935811960', 'https://openalex.org/W2952103439', 'https://openalex.org/W2952356761', 'https://openalex.org/W2954647460', 'https://openalex.org/W2962780935', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963905071', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964268978', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964345285', 'https://openalex.org/W2964352247', 'https://openalex.org/W2965785213', 'https://openalex.org/W2970520743', 'https://openalex.org/W2970849705', 'https://openalex.org/W2979303251', 'https://openalex.org/W2990372437', 'https://openalex.org/W2998655810', 'https://openalex.org/W2999635570', 'https://openalex.org/W3034629641', 'https://openalex.org/W3100439847', 'https://openalex.org/W3174892288', 'https://openalex.org/W4385245566']","Yang Feng, Shuhao Gu, Dengji Guo, Zhengxin Yang, Chenze Shao. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_205,https://openalex.org/W3214353211,2021,7,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2156985047', 'https://openalex.org/W2531207078', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2897507397', 'https://openalex.org/W2903193068', 'https://openalex.org/W2912095972', 'https://openalex.org/W2912351236', 'https://openalex.org/W2913109738', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921280978', 'https://openalex.org/W2946794439', 'https://openalex.org/W2950858167', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963831310', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970045405', 'https://openalex.org/W2970290486', 'https://openalex.org/W2970777192', 'https://openalex.org/W2971152344', 'https://openalex.org/W2972342261', 'https://openalex.org/W2998135987', 'https://openalex.org/W3013713299', 'https://openalex.org/W3017454464', 'https://openalex.org/W3035464238', 'https://openalex.org/W3090350559', 'https://openalex.org/W3096489356', 'https://openalex.org/W3104565655', 'https://openalex.org/W3105038888', 'https://openalex.org/W3106504817', 'https://openalex.org/W3112593586', 'https://openalex.org/W3119000810', 'https://openalex.org/W3119866316', 'https://openalex.org/W3127719526', 'https://openalex.org/W3155609600', 'https://openalex.org/W3155915431', 'https://openalex.org/W3175260681', 'https://openalex.org/W3175374354', 'https://openalex.org/W4287373797', 'https://openalex.org/W4287636493', 'https://openalex.org/W4300963525', 'https://openalex.org/W4385245566']","Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target language. However, recent work has highlighted several flaws of these models in zero-shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results. In this paper, we investigate the benefits of an explicit alignment to language labels in Transformer-based MNMT models in the zero-shot context, by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label. We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs.",1.0
NOVEL_MT_207,https://openalex.org/W3185517710,2021,5,"['https://openalex.org/W1902237438', 'https://openalex.org/W2039133703', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133564696', 'https://openalex.org/W2509282593', 'https://openalex.org/W2525778437', 'https://openalex.org/W2619947201', 'https://openalex.org/W2737766105', 'https://openalex.org/W2891534142', 'https://openalex.org/W2903343986', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963524571', 'https://openalex.org/W2963909453', 'https://openalex.org/W2964308564', 'https://openalex.org/W2989322838', 'https://openalex.org/W3036286589', 'https://openalex.org/W3082061474', 'https://openalex.org/W4385245566']","Weiqi Gu, Haiyue Song, Chenhui Chu, Sadao Kurohashi. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop. 2021.",1.0
NOVEL_MT_208,https://openalex.org/W3176382501,2021,66,"['https://openalex.org/W53604701', 'https://openalex.org/W1821462560', 'https://openalex.org/W2113106066', 'https://openalex.org/W2127141656', 'https://openalex.org/W2134546430', 'https://openalex.org/W2466918907', 'https://openalex.org/W2595715041', 'https://openalex.org/W2605131327', 'https://openalex.org/W2766219058', 'https://openalex.org/W2785350307', 'https://openalex.org/W2936774411', 'https://openalex.org/W2936969148', 'https://openalex.org/W2945700568', 'https://openalex.org/W2949328740', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962780374', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963779652', 'https://openalex.org/W2964067969', 'https://openalex.org/W2964089206', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964302946', 'https://openalex.org/W2972389417', 'https://openalex.org/W2972448360', 'https://openalex.org/W2982129078', 'https://openalex.org/W2996854111', 'https://openalex.org/W2997436923', 'https://openalex.org/W3006988520', 'https://openalex.org/W3007142233', 'https://openalex.org/W3034571331', 'https://openalex.org/W3037217258', 'https://openalex.org/W3096490862', 'https://openalex.org/W3097301532', 'https://openalex.org/W3097777922', 'https://openalex.org/W3105825505', 'https://openalex.org/W3113676066', 'https://openalex.org/W3173417753', 'https://openalex.org/W3176113130', 'https://openalex.org/W4287629556', 'https://openalex.org/W4287874506', 'https://openalex.org/W4300558631', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394649814']","Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",0.995260663507109
NOVEL_MT_209,https://openalex.org/W3198097706,2021,0,"['https://openalex.org/W222053410', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104852516', 'https://openalex.org/W2164777277', 'https://openalex.org/W2250342921', 'https://openalex.org/W2550821151', 'https://openalex.org/W2805910418', 'https://openalex.org/W2885950361', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2970015022', 'https://openalex.org/W2985165968', 'https://openalex.org/W2990400634', 'https://openalex.org/W3034474651', 'https://openalex.org/W3035013535', 'https://openalex.org/W3035408261', 'https://openalex.org/W3196290596']",We revisit the topic of translation direction in the data used for training neural machine translation systems and focusing on a real-world scenario with known translation direction and imbalances in translation direction: the Canadian Hansard. According to automatic metrics and we observe that using parallel data that was produced in the “matching” translation direction (Authentic source and translationese target) improves translation quality. In cases of data imbalance in terms of translation direction and we find that tagging of translation direction can close the performance gap. We perform a human evaluation that differs slightly from the automatic metrics and but nevertheless confirms that for this French-English dataset that is known to contain high-quality translations and authentic or tagged mixed source improves over translationese source for training.,0.9928057553956836
NOVEL_MT_210,https://openalex.org/W3167056186,2021,18,"['https://openalex.org/W368409423', 'https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2097333193', 'https://openalex.org/W2121863487', 'https://openalex.org/W2124807415', 'https://openalex.org/W2146574666', 'https://openalex.org/W2159246181', 'https://openalex.org/W2554616628', 'https://openalex.org/W2560647685', 'https://openalex.org/W2567571499', 'https://openalex.org/W2595715041', 'https://openalex.org/W2622263826', 'https://openalex.org/W2737492962', 'https://openalex.org/W2886342729', 'https://openalex.org/W2895723011', 'https://openalex.org/W2933138175', 'https://openalex.org/W2945383715', 'https://openalex.org/W2947187520', 'https://openalex.org/W2948734064', 'https://openalex.org/W2950760213', 'https://openalex.org/W2954929116', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963588172', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964121744', 'https://openalex.org/W2992693294', 'https://openalex.org/W3034640977', 'https://openalex.org/W3089472875', 'https://openalex.org/W3103800629', 'https://openalex.org/W3204406378', 'https://openalex.org/W4214717370', 'https://openalex.org/W4301163820', 'https://openalex.org/W4385245566']","Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the general domain is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings.",1.0
NOVEL_MT_213,https://openalex.org/W3173506858,2021,12,"['https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2184135559', 'https://openalex.org/W2194775991', 'https://openalex.org/W2542860122', 'https://openalex.org/W2606974598', 'https://openalex.org/W2669742347', 'https://openalex.org/W2803258077', 'https://openalex.org/W2890244613', 'https://openalex.org/W2905537158', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963593215', 'https://openalex.org/W2963823140', 'https://openalex.org/W2963881719', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970235285', 'https://openalex.org/W2991090520', 'https://openalex.org/W3030986950', 'https://openalex.org/W3035169087', 'https://openalex.org/W3035636774', 'https://openalex.org/W3035691816', 'https://openalex.org/W3101668578', 'https://openalex.org/W4385245566']","Tong Zhang, Long Zhang, Wei Ye, Bo Li, Jinan Sun, Xiaoyu Zhu, Wen Zhao, Shikun Zhang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",0.9948186528497408
NOVEL_MT_214,https://openalex.org/W3214236736,2021,9,"['https://openalex.org/W222053410', 'https://openalex.org/W648786980', 'https://openalex.org/W1569296262', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2176263492', 'https://openalex.org/W2606974598', 'https://openalex.org/W2786887383', 'https://openalex.org/W2911666613', 'https://openalex.org/W2935811960', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945260553', 'https://openalex.org/W2950913377', 'https://openalex.org/W2952468927', 'https://openalex.org/W2952649152', 'https://openalex.org/W2962701888', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963033554', 'https://openalex.org/W2963084599', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963620441', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964345285', 'https://openalex.org/W2971274815', 'https://openalex.org/W2996264288', 'https://openalex.org/W2999635570', 'https://openalex.org/W3034715004', 'https://openalex.org/W3035023562', 'https://openalex.org/W3035359363', 'https://openalex.org/W3036120435', 'https://openalex.org/W3100439847', 'https://openalex.org/W3106118243', 'https://openalex.org/W3120459995', 'https://openalex.org/W3127622310', 'https://openalex.org/W3127986334', 'https://openalex.org/W3133644679', 'https://openalex.org/W3174172622', 'https://openalex.org/W4287651588', 'https://openalex.org/W4287692509', 'https://openalex.org/W4288333914', 'https://openalex.org/W4385245566']","Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation. Its core motivation is to simulate the inference scene during training by replacing ground-truth tokens with predicted tokens, thus bridging the gap between training and inference. However, vanilla scheduled sampling is merely based on training steps and equally treats all decoding steps. Namely, it simulates an inference scene with uniform error rates, which disobeys the real inference scene, where larger decoding steps usually have higher error rates due to error accumulations. To alleviate the above discrepancy, we propose scheduled sampling methods based on decoding steps, increasing the selection chance of predicted tokens with the growth of decoding steps. Consequently, we can more realistically simulate the inference scene during training, thus better bridging the gap between training and inference. Moreover, we investigate scheduled sampling based on both training steps and decoding steps for further improvements. Experimentally, our approaches significantly outperform the Transformer baseline and vanilla scheduled sampling on three large-scale WMT tasks. Additionally, our approaches also generalize well to the text summarization task on two popular benchmarks.",1.0
NOVEL_MT_215,https://openalex.org/W3176614736,2021,34,"['https://openalex.org/W46679369', 'https://openalex.org/W630532510', 'https://openalex.org/W1969754958', 'https://openalex.org/W2031292349', 'https://openalex.org/W2034585809', 'https://openalex.org/W2038116248', 'https://openalex.org/W2061272101', 'https://openalex.org/W2094655846', 'https://openalex.org/W2098355803', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107745473', 'https://openalex.org/W2149778059', 'https://openalex.org/W2153433699', 'https://openalex.org/W2153579005', 'https://openalex.org/W2419539795', 'https://openalex.org/W2507756961', 'https://openalex.org/W2585024066', 'https://openalex.org/W2747481658', 'https://openalex.org/W2777647957', 'https://openalex.org/W2798348125', 'https://openalex.org/W2896068220', 'https://openalex.org/W2896457183', 'https://openalex.org/W2936695845', 'https://openalex.org/W2952638691', 'https://openalex.org/W2962832505', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963456134', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963877297', 'https://openalex.org/W2963945964', 'https://openalex.org/W2963952470', 'https://openalex.org/W2964268978', 'https://openalex.org/W2966263504', 'https://openalex.org/W2972702443', 'https://openalex.org/W2973082572', 'https://openalex.org/W2989143494', 'https://openalex.org/W2996403597', 'https://openalex.org/W3017454464', 'https://openalex.org/W3034284720', 'https://openalex.org/W3034430025', 'https://openalex.org/W3035464238', 'https://openalex.org/W3088665616', 'https://openalex.org/W3105813095', 'https://openalex.org/W3107826490', 'https://openalex.org/W3153869297', 'https://openalex.org/W3156476125', 'https://openalex.org/W3171500670', 'https://openalex.org/W3171877493', 'https://openalex.org/W4247535068', 'https://openalex.org/W4294170691', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4299838440', 'https://openalex.org/W4301187301', 'https://openalex.org/W4385245566']","Ishan Tarunesh, Syamantak Kumar, Preethi Jyothi. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",0.9941520467836256
NOVEL_MT_218,https://openalex.org/W3206604436,2021,4,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2169200297', 'https://openalex.org/W2560647685', 'https://openalex.org/W2567571499', 'https://openalex.org/W2744813330', 'https://openalex.org/W2760452458', 'https://openalex.org/W2885950361', 'https://openalex.org/W2916548775', 'https://openalex.org/W2945383715', 'https://openalex.org/W2947187520', 'https://openalex.org/W2947461406', 'https://openalex.org/W2950635152', 'https://openalex.org/W2950760213', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962724315', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970279348', 'https://openalex.org/W3089067043', 'https://openalex.org/W3093818688', 'https://openalex.org/W3120168417', 'https://openalex.org/W3120929527', 'https://openalex.org/W3204406378', 'https://openalex.org/W4295883599', 'https://openalex.org/W4385245566', 'https://openalex.org/W4404781009']","Building neural machine translation systems to perform well on a specific target domain is a well-studied problem. Optimizing system performance for multiple, diverse target domains however remains a challenge. We study this problem in an adaptation setting where the goal is to preserve the existing system quality while incorporating data for domains that were not the focus of the original translation system. We find that we can improve over the performance trade-off offered by Elastic Weight Consolidation with a relatively simple data mixing strategy. At comparable performance on the new domains, catastrophic forgetting is mitigated significantly on strong WMT baselines. Combining both approaches improves the Pareto frontier on this task.",1.0
NOVEL_MT_219,https://openalex.org/W3167515372,2021,1,"['https://openalex.org/W1988012253', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105673178', 'https://openalex.org/W2107695330', 'https://openalex.org/W2123301721', 'https://openalex.org/W2170716095', 'https://openalex.org/W2467834614', 'https://openalex.org/W2573665256', 'https://openalex.org/W2741900445', 'https://openalex.org/W2786991584', 'https://openalex.org/W2790319220', 'https://openalex.org/W2885950361', 'https://openalex.org/W2902918014', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963919854', 'https://openalex.org/W2964022663', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964212410', 'https://openalex.org/W2970858854', 'https://openalex.org/W2972780142', 'https://openalex.org/W2973088264', 'https://openalex.org/W3022579377', 'https://openalex.org/W3088421316', 'https://openalex.org/W3098396250', 'https://openalex.org/W3119872155', 'https://openalex.org/W3171241877']","While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.",0.9950738916256158
NOVEL_MT_221,https://openalex.org/W3200218906,2021,12,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1989285098', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251654371', 'https://openalex.org/W2419539795', 'https://openalex.org/W2538358357', 'https://openalex.org/W2595715041', 'https://openalex.org/W2760656271', 'https://openalex.org/W2847467100', 'https://openalex.org/W2887920589', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2944815030', 'https://openalex.org/W2949303037', 'https://openalex.org/W2952468927', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971120622', 'https://openalex.org/W2976330679', 'https://openalex.org/W2977458338', 'https://openalex.org/W2982399380', 'https://openalex.org/W2989539713', 'https://openalex.org/W3013122861', 'https://openalex.org/W3032816972', 'https://openalex.org/W3034469191', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035579820', 'https://openalex.org/W3036839309', 'https://openalex.org/W3037332907', 'https://openalex.org/W3037838322', 'https://openalex.org/W3090350559', 'https://openalex.org/W3093517588', 'https://openalex.org/W3098998028', 'https://openalex.org/W3105378761', 'https://openalex.org/W3107826490', 'https://openalex.org/W3169483174', 'https://openalex.org/W3176466730', 'https://openalex.org/W3197536862', 'https://openalex.org/W4287028630', 'https://openalex.org/W4287826678', 'https://openalex.org/W4288323699', 'https://openalex.org/W4385245566']","Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.",0.9948186528497408
NOVEL_MT_224,https://openalex.org/W3174481817,2021,27,"['https://openalex.org/W222053410', 'https://openalex.org/W1560797130', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2134800885', 'https://openalex.org/W2512924740', 'https://openalex.org/W2557436004', 'https://openalex.org/W2561274697', 'https://openalex.org/W2609701267', 'https://openalex.org/W2790319220', 'https://openalex.org/W2794365787', 'https://openalex.org/W2888808532', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908336025', 'https://openalex.org/W2922349260', 'https://openalex.org/W2923622379', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949938546', 'https://openalex.org/W2952103439', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963476860', 'https://openalex.org/W2963516811', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963736842', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970311224', 'https://openalex.org/W2971302374', 'https://openalex.org/W2976223659', 'https://openalex.org/W2995746049', 'https://openalex.org/W2995999067', 'https://openalex.org/W3023986361', 'https://openalex.org/W3034201598', 'https://openalex.org/W3034474651', 'https://openalex.org/W3035072529', 'https://openalex.org/W3089635645', 'https://openalex.org/W3091002423', 'https://openalex.org/W3091877132', 'https://openalex.org/W3092501442', 'https://openalex.org/W3098903812', 'https://openalex.org/W3099417250', 'https://openalex.org/W3102233600', 'https://openalex.org/W3102315351', 'https://openalex.org/W3105848458', 'https://openalex.org/W3107826490', 'https://openalex.org/W3119014050', 'https://openalex.org/W3121049435', 'https://openalex.org/W3164673114', 'https://openalex.org/W3164747806', 'https://openalex.org/W3176913643', 'https://openalex.org/W4292779060', 'https://openalex.org/W4297801368', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi, Michael Lyu, Irwin King. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_227,https://openalex.org/W3168900788,2021,37,"['https://openalex.org/W1522301498', 'https://openalex.org/W1544201619', 'https://openalex.org/W1745334888', 'https://openalex.org/W1861492603', 'https://openalex.org/W1895577753', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2185175083', 'https://openalex.org/W2405756170', 'https://openalex.org/W2509282593', 'https://openalex.org/W2512381898', 'https://openalex.org/W2513263213', 'https://openalex.org/W2553522418', 'https://openalex.org/W2568262903', 'https://openalex.org/W2581101319', 'https://openalex.org/W2593341061', 'https://openalex.org/W2888070626', 'https://openalex.org/W2896234464', 'https://openalex.org/W2903343986', 'https://openalex.org/W2950207430', 'https://openalex.org/W2950886580', 'https://openalex.org/W2962845008', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963360627', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963407669', 'https://openalex.org/W2963966654', 'https://openalex.org/W2963988211', 'https://openalex.org/W2964024144', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964192290', 'https://openalex.org/W2964308564', 'https://openalex.org/W2995558462', 'https://openalex.org/W2996854111', 'https://openalex.org/W3010232603', 'https://openalex.org/W3034773362', 'https://openalex.org/W3034871396', 'https://openalex.org/W4287874506', 'https://openalex.org/W4296000566', 'https://openalex.org/W4296979096', 'https://openalex.org/W4297780100', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566']","There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence - image for training and tuples of source sentence - image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the ""imagined representation"" to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.",1.0
NOVEL_MT_228,https://openalex.org/W3198258591,2021,0,"['https://openalex.org/W46877293', 'https://openalex.org/W1544426622', 'https://openalex.org/W1905522558', 'https://openalex.org/W1965511524', 'https://openalex.org/W2117278770', 'https://openalex.org/W2119850747', 'https://openalex.org/W2138859735', 'https://openalex.org/W2147262247', 'https://openalex.org/W2151834591', 'https://openalex.org/W2296073425', 'https://openalex.org/W2550821151', 'https://openalex.org/W2567571499', 'https://openalex.org/W2578927531', 'https://openalex.org/W2604763608', 'https://openalex.org/W2898846200', 'https://openalex.org/W2919188216', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946379889', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963088995', 'https://openalex.org/W2970925270', 'https://openalex.org/W2971120622', 'https://openalex.org/W2985301125', 'https://openalex.org/W3019421297', 'https://openalex.org/W3204406378']",Low-resource Multilingual Neural Machine Translation (MNMT) is typically tasked with improving the translation performance on one or more language pairs with the aid of high-resource language pairs. In this paper and we propose two simple search based curricula – orderings of the multilingual training data – which help improve translation performance in conjunction with existing techniques such as fine-tuning. Additionally and we attempt to learn a curriculum for MNMT from scratch jointly with the training of the translation system using contextual multi-arm bandits. We show on the FLORES low-resource translation dataset that these learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system.,0.993006993006993
NOVEL_MT_230,https://openalex.org/W3089025530,2021,33,"['https://openalex.org/W179314280', 'https://openalex.org/W225641137', 'https://openalex.org/W1522301498', 'https://openalex.org/W1984635093', 'https://openalex.org/W2133564696', 'https://openalex.org/W2152790380', 'https://openalex.org/W2161914416', 'https://openalex.org/W2176263492', 'https://openalex.org/W2471679455', 'https://openalex.org/W2597891111', 'https://openalex.org/W2613904329', 'https://openalex.org/W2758310181', 'https://openalex.org/W2804946931', 'https://openalex.org/W2888442053', 'https://openalex.org/W2896457183', 'https://openalex.org/W2913659301', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2950735465', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963962369', 'https://openalex.org/W2964110616', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W2965850471', 'https://openalex.org/W2970295111', 'https://openalex.org/W2980282514', 'https://openalex.org/W2986562961', 'https://openalex.org/W2988388418', 'https://openalex.org/W2988975212', 'https://openalex.org/W2994928925', 'https://openalex.org/W2995338136', 'https://openalex.org/W3006381853', 'https://openalex.org/W3018305985', 'https://openalex.org/W3027871355', 'https://openalex.org/W3034775979', 'https://openalex.org/W3093345276', 'https://openalex.org/W3100311862', 'https://openalex.org/W3118026775', 'https://openalex.org/W4288288444', 'https://openalex.org/W4288601832', 'https://openalex.org/W4289104499', 'https://openalex.org/W4301230920', 'https://openalex.org/W4385245566']","The discrepancy between maximum likelihood estimation (MLE) and task measures\nsuch as BLEU score has been studied before for autoregressive neural machine\ntranslation (NMT) and resulted in alternative training algorithms (Ranzato et\nal., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However,\nMLE training remains the de facto approach for autoregressive NMT because of\nits computational efficiency and stability. Despite this mismatch between the\ntraining objective and task measure, we notice that the samples drawn from an\nMLE-based trained NMT support the desired distribution -- there are samples\nwith much higher BLEU score comparing to the beam decoding output. To benefit\nfrom this observation, we train an energy-based model to mimic the behavior of\nthe task measure (i.e., the energy-based model assigns lower energy to samples\nwith higher BLEU score), which is resulted in a re-ranking algorithm based on\nthe samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal\nenergy models (over target sentence) and joint energy models (over both source\nand target sentences). Our EBR with the joint energy model consistently\nimproves the performance of the Transformer-based NMT: +4 BLEU points on\nIWSLT'14 German-English, +3.0 BELU points on Sinhala-English, +1.2 BLEU on\nWMT'16 English-German tasks.\n",0.9941520467836256
NOVEL_MT_234,https://openalex.org/W3212864953,2021,3,"['https://openalex.org/W1654441844', 'https://openalex.org/W1753482797', 'https://openalex.org/W2106411961', 'https://openalex.org/W2131062877', 'https://openalex.org/W2251251208', 'https://openalex.org/W2294699749', 'https://openalex.org/W2605277756', 'https://openalex.org/W2917452219', 'https://openalex.org/W2949676527', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963672599', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W3101017384', 'https://openalex.org/W3120929527', 'https://openalex.org/W3127861905', 'https://openalex.org/W3159892921', 'https://openalex.org/W3167348220', 'https://openalex.org/W3168165554', 'https://openalex.org/W3175955584', 'https://openalex.org/W3176765170', 'https://openalex.org/W3199925742', 'https://openalex.org/W3204921014', 'https://openalex.org/W3208474812', 'https://openalex.org/W3214173179']","Human evaluation has always been expensive while researchers struggle to trust the automatic metrics. To address this, we propose to customise traditional metrics by taking advantages of the pre-trained language models (PLMs) and the limited available human labelled scores. We first re-introduce the hLEPOR metric factors, followed by the Python version we developed (ported) which achieved the automatic tuning of the weighting parameters in hLEPOR metric. Then we present the customised hLEPOR (cushLEPOR) which uses Optuna hyper-parameter optimisation framework to fine-tune hLEPOR weighting parameters towards better agreement to pre-trained language models (using LaBSE) regarding the exact MT language pairs that cushLEPOR is deployed to. We also optimise cushLEPOR towards professional human evaluation data based on MQM and pSQM framework on English-German and Chinese-English language pairs. The experimental investigations show cushLEPOR boosts hLEPOR performances towards better agreements to PLMs like LaBSE with much lower cost, and better agreements to human evaluations including MQM and pSQM scores, and yields much better performances than BLEU (data available at \url{https://github.com/poethan/cushLEPOR}). Official results show that our submissions win three language pairs including \textbf{English-German} and \textbf{Chinese-English} on \textit{News} domain via cushLEPOR(LM) and \textbf{English-Russian} on \textit{TED} domain via hLEPOR.",0.9960474308300395
NOVEL_MT_235,https://openalex.org/W3163353020,2021,1,"['https://openalex.org/W630532510', 'https://openalex.org/W1915251500', 'https://openalex.org/W1990061958', 'https://openalex.org/W2537667581', 'https://openalex.org/W2542860122', 'https://openalex.org/W2561274697', 'https://openalex.org/W2759932073', 'https://openalex.org/W2933138175', 'https://openalex.org/W2944815030', 'https://openalex.org/W2950940239', 'https://openalex.org/W2951770285', 'https://openalex.org/W2962714778', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963583256', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964029788', 'https://openalex.org/W2970059135', 'https://openalex.org/W2971167892', 'https://openalex.org/W2998135987', 'https://openalex.org/W3011411500', 'https://openalex.org/W3014114301', 'https://openalex.org/W3034640977', 'https://openalex.org/W3035393249', 'https://openalex.org/W3035540807', 'https://openalex.org/W3082274269', 'https://openalex.org/W3082303676', 'https://openalex.org/W3105718208', 'https://openalex.org/W3120929527']","Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output. However, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni- and bi-grams (&gt;98%). In this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer n-gram and highly specialized terms. Inspired by the recent success of masked span prediction models, we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs.",1.0
NOVEL_MT_236,https://openalex.org/W3205203171,2021,3,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1682403713', 'https://openalex.org/W2250342921', 'https://openalex.org/W2567571499', 'https://openalex.org/W2760452458', 'https://openalex.org/W2887920589', 'https://openalex.org/W2891555348', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2953109491', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963211188', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964303773', 'https://openalex.org/W2970925270', 'https://openalex.org/W2971134989', 'https://openalex.org/W2982399380', 'https://openalex.org/W2986791765', 'https://openalex.org/W2995015695', 'https://openalex.org/W2996159613', 'https://openalex.org/W2996844526', 'https://openalex.org/W3013840636', 'https://openalex.org/W3034112745', 'https://openalex.org/W3034640977', 'https://openalex.org/W3034771276', 'https://openalex.org/W3035016936', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035464238', 'https://openalex.org/W3045462440', 'https://openalex.org/W3093871477', 'https://openalex.org/W3096966601', 'https://openalex.org/W3101498587', 'https://openalex.org/W3107826490', 'https://openalex.org/W3119866316', 'https://openalex.org/W3130032352', 'https://openalex.org/W3153675281', 'https://openalex.org/W3153805297', 'https://openalex.org/W3166567197', 'https://openalex.org/W3175955584', 'https://openalex.org/W3204406378', 'https://openalex.org/W3214715529']","Adapter layers are lightweight, learnable units inserted between transformer layers. Recent work explores using such layers for neural machine translation (NMT), to adapt pre-trained models to new domains or language pairs, training only a small set of parameters for each new setting (language pair or domain). In this work we study the compositionality of language and domain adapters in the context of Machine Translation. We aim to study, 1) parameter-efficient adaptation to multiple domains and languages simultaneously (full-resource scenario) and 2) cross-lingual transfer in domains where parallel data is unavailable for certain language pairs (partial-resource scenario). We find that in the partial resource scenario a naive combination of domain-specific and language-specific adapters often results in `catastrophic forgetting' of the missing languages. We study other ways to combine the adapters to alleviate this issue and maximize cross-lingual transfer. With our best adapter combinations, we obtain improvements of 3-4 BLEU on average for source languages that do not have in-domain data. For target languages without in-domain data, we achieve a similar improvement by combining adapters with back-translation. Supplementary material is available at https://tinyurl.com/r66stbxj",0.9947643979057592
NOVEL_MT_238,https://openalex.org/W3211978535,2021,17,"['https://openalex.org/W222053410', 'https://openalex.org/W255975419', 'https://openalex.org/W1497274008', 'https://openalex.org/W1527575280', 'https://openalex.org/W1537121970', 'https://openalex.org/W1583743863', 'https://openalex.org/W1901714926', 'https://openalex.org/W2067815623', 'https://openalex.org/W2115030595', 'https://openalex.org/W2131969445', 'https://openalex.org/W2136353104', 'https://openalex.org/W2148708890', 'https://openalex.org/W2184135559', 'https://openalex.org/W2582446770', 'https://openalex.org/W2595715041', 'https://openalex.org/W2608029998', 'https://openalex.org/W2623559126', 'https://openalex.org/W2767019613', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2891534142', 'https://openalex.org/W2905927205', 'https://openalex.org/W2952446148', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964289193', 'https://openalex.org/W2971278086', 'https://openalex.org/W2971347700', 'https://openalex.org/W2980462515', 'https://openalex.org/W3005680577', 'https://openalex.org/W3034978746', 'https://openalex.org/W3035520602', 'https://openalex.org/W3042199843', 'https://openalex.org/W3101683892', 'https://openalex.org/W3102507836', 'https://openalex.org/W3201792308', 'https://openalex.org/W3203909556', 'https://openalex.org/W4285719527', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Recently a number of approaches have been proposed to improve translation performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply ""one translation per discourse"" in NMT, and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document, which tells the positions where the source word appears. Then we encourage the translation of those words within a link to be consistent in two ways. On the one hand, when encoding sentences within a document we properly share context information of those words. On the other hand, we propose an auxiliary loss function to better constrain that their translation should be consistent. Experimental results on Chinese↔English and English→French translation tasks show that our approach not only achieves state-of-the-art performance in BLEU scores, but also greatly improves lexical consistency in translation.",1.0
NOVEL_MT_243,https://openalex.org/W3175424132,2021,16,"['https://openalex.org/W16967297', 'https://openalex.org/W21337280', 'https://openalex.org/W255975419', 'https://openalex.org/W932413789', 'https://openalex.org/W1411230545', 'https://openalex.org/W1902237438', 'https://openalex.org/W2095705004', 'https://openalex.org/W2097732278', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111305191', 'https://openalex.org/W2116492146', 'https://openalex.org/W2127836646', 'https://openalex.org/W2130942839', 'https://openalex.org/W2136353104', 'https://openalex.org/W2250969425', 'https://openalex.org/W2467834614', 'https://openalex.org/W2539350388', 'https://openalex.org/W2757592053', 'https://openalex.org/W2808508619', 'https://openalex.org/W2892244498', 'https://openalex.org/W2892357930', 'https://openalex.org/W2946379889', 'https://openalex.org/W2951902588', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963842551', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964086597', 'https://openalex.org/W2970286654', 'https://openalex.org/W2990804422', 'https://openalex.org/W3034201598', 'https://openalex.org/W3034238904', 'https://openalex.org/W3101155369', 'https://openalex.org/W3115909586', 'https://openalex.org/W4385245566']","Huan Lin, Liang Yao, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua Luo, Degen Huang, Jinsong Su. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_244,https://openalex.org/W4285112771,2022,4,"['https://openalex.org/W222053410', 'https://openalex.org/W1556207603', 'https://openalex.org/W2080373976', 'https://openalex.org/W2101105183', 'https://openalex.org/W2143331230', 'https://openalex.org/W2148708890', 'https://openalex.org/W2401082558', 'https://openalex.org/W2550821151', 'https://openalex.org/W2594047108', 'https://openalex.org/W2768091364', 'https://openalex.org/W2798569372', 'https://openalex.org/W2889404673', 'https://openalex.org/W2896556401', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946028745', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963807318', 'https://openalex.org/W2970045405', 'https://openalex.org/W2971351900', 'https://openalex.org/W2973651602', 'https://openalex.org/W2986267869', 'https://openalex.org/W2988309730', 'https://openalex.org/W3104881680', 'https://openalex.org/W3176607063', 'https://openalex.org/W3185537577', 'https://openalex.org/W6631190155', 'https://openalex.org/W6732150968', 'https://openalex.org/W6739901393', 'https://openalex.org/W6761349998']","It has been reported that grammatical information is useful for machine translation (MT) tasks. However, the annotation of grammatical information incurs significant human costs. Furthermore, it is not trivial to adapt grammatical information to MT because grammatical annotation usually employs tokenization standards that might not capture the relation between two languages and consequently, subword tokenization such as byte-pair-encoding is used to alleviate out-of-vocabulary problems; however, this might not be compatible with those annotations. In this work, we introduce two methods to incorporate grammatical information without supervising annotation explicitly: first, the latent phrase structure is induced in an unsupervised fashion from an attention mechanism; and second, the induced latent phrase structures in the encoder and decoder are synchronized so that they are compatible with each other using constraints during training. We demonstrate that our approach performs better in two tasks: translation and word alignment, without extra resources. We found that the induced phrase structures enhance the precision of alignments through the synchronization constraint after exact phrase and alignment structure analysis.",1.0
NOVEL_MT_245,https://openalex.org/W3176846012,2021,25,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107695330', 'https://openalex.org/W2158600149', 'https://openalex.org/W2466175319', 'https://openalex.org/W2561715562', 'https://openalex.org/W2563574619', 'https://openalex.org/W2692059227', 'https://openalex.org/W2741040846', 'https://openalex.org/W2767899794', 'https://openalex.org/W2773956126', 'https://openalex.org/W2794365787', 'https://openalex.org/W2811010710', 'https://openalex.org/W2887970879', 'https://openalex.org/W2889326796', 'https://openalex.org/W2892205701', 'https://openalex.org/W2933138175', 'https://openalex.org/W2939413764', 'https://openalex.org/W2946794439', 'https://openalex.org/W2951227497', 'https://openalex.org/W2953151445', 'https://openalex.org/W2962777840', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962968135', 'https://openalex.org/W2963005248', 'https://openalex.org/W2963084773', 'https://openalex.org/W2963267799', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963518342', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963823140', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964247056', 'https://openalex.org/W2970731908', 'https://openalex.org/W2970820321', 'https://openalex.org/W2971079754', 'https://openalex.org/W2971154170', 'https://openalex.org/W2971473121', 'https://openalex.org/W2982616641', 'https://openalex.org/W2986378306', 'https://openalex.org/W2986791765', 'https://openalex.org/W2990379018', 'https://openalex.org/W2994928925', 'https://openalex.org/W2996132992', 'https://openalex.org/W2996346899', 'https://openalex.org/W2999565069', 'https://openalex.org/W3006381853', 'https://openalex.org/W3035072529', 'https://openalex.org/W3035331128', 'https://openalex.org/W3037109418', 'https://openalex.org/W3045885009', 'https://openalex.org/W3066373881', 'https://openalex.org/W3082274269', 'https://openalex.org/W3105725479', 'https://openalex.org/W3152530299', 'https://openalex.org/W4288089799', 'https://openalex.org/W4288094673', 'https://openalex.org/W4288331674', 'https://openalex.org/W4288376504', 'https://openalex.org/W4301259831', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385570274']","Yafu Li, Yongjing Yin, Yulong Chen, Yue Zhang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_246,https://openalex.org/W3217576620,2021,1,"['https://openalex.org/W2251743902', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2587694128', 'https://openalex.org/W2885250264', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963532001', 'https://openalex.org/W3105038888', 'https://openalex.org/W3105214104', 'https://openalex.org/W3107826490', 'https://openalex.org/W3120929527']","This paper describes our submission to the constrained track of WMT21 shared news translation task. We focus on the three relatively low resource language pairs Bengali to and from Hindi, English to and from Hausa, and Xhosa to and from Zulu. To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective employing both parallel and monolingual data. In addition, we augment the data using back translation. We also train a bilingual model incorporating back translation and knowledge distillation then combine the two models using sequence-to-sequence mapping. We see around 70% relative gain in BLEU point for English to and from Hausa, and around 25% relative improvements for both Bengali to and from Hindi, and Xhosa to and from Zulu compared to bilingual baselines.",1.0
NOVEL_MT_247,https://openalex.org/W3167716537,2021,1,"['https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2157331557', 'https://openalex.org/W2493916176', 'https://openalex.org/W2890007195', 'https://openalex.org/W2944815030', 'https://openalex.org/W2948384082', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970049541', 'https://openalex.org/W2994885767', 'https://openalex.org/W3015504467', 'https://openalex.org/W3022847728', 'https://openalex.org/W3035547806', 'https://openalex.org/W3082017874', 'https://openalex.org/W3092115657', 'https://openalex.org/W3101286153', 'https://openalex.org/W3102425047', 'https://openalex.org/W3105005398', 'https://openalex.org/W3107826490', 'https://openalex.org/W3117473549', 'https://openalex.org/W3118013546', 'https://openalex.org/W3136071852']","Recent advances in Unsupervised Neural Machine Translation (UNMT) have minimized the gap between supervised and unsupervised machine translation performance for closely related language pairs. However, the situation is very different for distant language pairs. Lack of lexical overlap and low syntactic similarities such as between English and Indo-Aryan languages leads to poor translation quality in existing UNMT systems. In this paper, we show that initializing the embedding layer of UNMT models with cross-lingual embeddings shows significant improvements in BLEU score over existing approaches with embeddings randomly initialized. Further, static embeddings (freezing the embedding layer weights) lead to better gains compared to updating the embedding layer weights during training (non-static). We experimented using Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT approaches for three distant language pairs. The proposed cross-lingual embedding initialization yields BLEU score improvement of as much as ten times over the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our analysis shows the importance of cross-lingual embedding, comparisons between approaches, and the scope of improvements in these systems.",0.9951219512195122
NOVEL_MT_250,https://openalex.org/W3176395632,2021,55,"['https://openalex.org/W1554663460', 'https://openalex.org/W2111406701', 'https://openalex.org/W2120972216', 'https://openalex.org/W2123301721', 'https://openalex.org/W2124136621', 'https://openalex.org/W2194775991', 'https://openalex.org/W2247931231', 'https://openalex.org/W2471094925', 'https://openalex.org/W2509282593', 'https://openalex.org/W2512381898', 'https://openalex.org/W2513263213', 'https://openalex.org/W2593341061', 'https://openalex.org/W2766151966', 'https://openalex.org/W2773827481', 'https://openalex.org/W2788330850', 'https://openalex.org/W2799615702', 'https://openalex.org/W2885421725', 'https://openalex.org/W2887678105', 'https://openalex.org/W2888070626', 'https://openalex.org/W2889545026', 'https://openalex.org/W2889903020', 'https://openalex.org/W2896234464', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902031175', 'https://openalex.org/W2903343986', 'https://openalex.org/W2908336025', 'https://openalex.org/W2933138175', 'https://openalex.org/W2950207430', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962929176', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963360627', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963407669', 'https://openalex.org/W2963898017', 'https://openalex.org/W2963909453', 'https://openalex.org/W2970283086', 'https://openalex.org/W2986752547', 'https://openalex.org/W2989322838', 'https://openalex.org/W2994928925', 'https://openalex.org/W2995154514', 'https://openalex.org/W2995558462', 'https://openalex.org/W3006381853', 'https://openalex.org/W3007672467', 'https://openalex.org/W3027879771', 'https://openalex.org/W3034773362', 'https://openalex.org/W3034871396', 'https://openalex.org/W3098357269', 'https://openalex.org/W3100801259', 'https://openalex.org/W3102475290', 'https://openalex.org/W3104234009', 'https://openalex.org/W4288087322', 'https://openalex.org/W4299590935', 'https://openalex.org/W4385245566', 'https://openalex.org/W4388297464']","Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",0.9957805907172996
NOVEL_MT_252,https://openalex.org/W3166829167,2021,10,"['https://openalex.org/W1522301498', 'https://openalex.org/W2067815623', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126209950', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131494463', 'https://openalex.org/W2133564696', 'https://openalex.org/W2176263492', 'https://openalex.org/W2184135559', 'https://openalex.org/W2521709538', 'https://openalex.org/W2595715041', 'https://openalex.org/W2606531491', 'https://openalex.org/W2608029998', 'https://openalex.org/W2613904329', 'https://openalex.org/W2669742347', 'https://openalex.org/W2750557179', 'https://openalex.org/W2752047430', 'https://openalex.org/W2759173152', 'https://openalex.org/W2767019613', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2891534142', 'https://openalex.org/W2903728819', 'https://openalex.org/W2951008357', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962739703', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962882341', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963344337', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963652649', 'https://openalex.org/W2963693355', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964091467', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964267515', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970529093', 'https://openalex.org/W2971278086', 'https://openalex.org/W2983108239', 'https://openalex.org/W3033962033', 'https://openalex.org/W3034351728', 'https://openalex.org/W3035520602', 'https://openalex.org/W3093404841', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","Long Zhang, Tong Zhang, Haibo Zhang, Baosong Yang, Wei Ye, Shikun Zhang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_253,https://openalex.org/W3165946561,2021,8,[],"Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counter-evidence to these claims.",1.0
NOVEL_MT_254,https://openalex.org/W3173767661,2021,98,"['https://openalex.org/W2593011301', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936848022', 'https://openalex.org/W2945260553', 'https://openalex.org/W2949328740', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963460174', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963779652', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964303773', 'https://openalex.org/W2966715458', 'https://openalex.org/W2969876226', 'https://openalex.org/W2970120757', 'https://openalex.org/W2970597249', 'https://openalex.org/W2970608575', 'https://openalex.org/W2970854433', 'https://openalex.org/W2971274815', 'https://openalex.org/W2972448360', 'https://openalex.org/W2973049979', 'https://openalex.org/W2981851019', 'https://openalex.org/W2983040767', 'https://openalex.org/W2989195139', 'https://openalex.org/W2997436923', 'https://openalex.org/W3007142233', 'https://openalex.org/W3008125272', 'https://openalex.org/W3015633994', 'https://openalex.org/W3015698636', 'https://openalex.org/W3016181583', 'https://openalex.org/W3023622314', 'https://openalex.org/W3034571331', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035202887', 'https://openalex.org/W3035390927', 'https://openalex.org/W3036601975', 'https://openalex.org/W3037057938', 'https://openalex.org/W3037465386', 'https://openalex.org/W3037542581', 'https://openalex.org/W3046368065', 'https://openalex.org/W3054645415', 'https://openalex.org/W3082274269', 'https://openalex.org/W3092424727', 'https://openalex.org/W3099782249', 'https://openalex.org/W3101498587', 'https://openalex.org/W3102342027', 'https://openalex.org/W3107826490', 'https://openalex.org/W3118578889', 'https://openalex.org/W3153805297', 'https://openalex.org/W3162037819', 'https://openalex.org/W3176711365', 'https://openalex.org/W3198429080', 'https://openalex.org/W4287694131', 'https://openalex.org/W4287824654', 'https://openalex.org/W4288026527', 'https://openalex.org/W4288089799']","Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_255,https://openalex.org/W3176690085,2021,181,"['https://openalex.org/W1647671624', 'https://openalex.org/W2108646579', 'https://openalex.org/W2251294039', 'https://openalex.org/W2251648804', 'https://openalex.org/W2252024663', 'https://openalex.org/W2465978385', 'https://openalex.org/W2562607067', 'https://openalex.org/W2604205681', 'https://openalex.org/W2757541972', 'https://openalex.org/W2891300059', 'https://openalex.org/W2900401454', 'https://openalex.org/W2946015932', 'https://openalex.org/W2949660355', 'https://openalex.org/W2950488390', 'https://openalex.org/W2950601686', 'https://openalex.org/W2963264961', 'https://openalex.org/W2963274454', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963556938', 'https://openalex.org/W2965510113', 'https://openalex.org/W2971014768', 'https://openalex.org/W2985056549', 'https://openalex.org/W2997464781', 'https://openalex.org/W2998446468', 'https://openalex.org/W3034884160', 'https://openalex.org/W3034990686', 'https://openalex.org/W3035407080', 'https://openalex.org/W3082274269', 'https://openalex.org/W3100451998', 'https://openalex.org/W3105083537', 'https://openalex.org/W3106340866', 'https://openalex.org/W3120304799', 'https://openalex.org/W3177239031', 'https://openalex.org/W4211186029', 'https://openalex.org/W4288089799', 'https://openalex.org/W4299026567', 'https://openalex.org/W4385245566']","Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, Wai Lam. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",1.0
NOVEL_MT_256,https://openalex.org/W3200798267,2021,6,"['https://openalex.org/W1522301498', 'https://openalex.org/W2095705004', 'https://openalex.org/W2127141656', 'https://openalex.org/W2148708890', 'https://openalex.org/W2183341477', 'https://openalex.org/W2933138175', 'https://openalex.org/W2948798935', 'https://openalex.org/W2950541952', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970731908', 'https://openalex.org/W2970947975', 'https://openalex.org/W2985165968', 'https://openalex.org/W3120459995', 'https://openalex.org/W3121018626', 'https://openalex.org/W3175212568', 'https://openalex.org/W3175665465']","This paper describes the Volctrans' submission to the WMT21 news translation shared task for German-&gt;English translation. We build a parallel (i.e., non-autoregressive) translation system using the Glancing Transformer, which enables fast and accurate parallel decoding in contrast to the currently prevailing autoregressive models. To the best of our knowledge, this is the first parallel translation system that can be scaled to such a practical scenario like WMT competition. More importantly, our parallel translation system achieves the best BLEU score (35.0) on German-&gt;English translation task, outperforming all strong autoregressive counterparts.",0.9927007299270072
NOVEL_MT_257,https://openalex.org/W3173315356,2021,20,"['https://openalex.org/W2251294039', 'https://openalex.org/W2251648804', 'https://openalex.org/W2604205681', 'https://openalex.org/W2604668619', 'https://openalex.org/W2741252866', 'https://openalex.org/W2890240222', 'https://openalex.org/W2896457183', 'https://openalex.org/W2896786335', 'https://openalex.org/W2897908126', 'https://openalex.org/W2901440799', 'https://openalex.org/W2946015932', 'https://openalex.org/W2949660355', 'https://openalex.org/W2950404230', 'https://openalex.org/W2950488390', 'https://openalex.org/W2962741379', 'https://openalex.org/W2962843214', 'https://openalex.org/W2963168371', 'https://openalex.org/W2963264961', 'https://openalex.org/W2963274454', 'https://openalex.org/W2963337756', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2965510113', 'https://openalex.org/W2970971581', 'https://openalex.org/W2979826702', 'https://openalex.org/W2996035354', 'https://openalex.org/W2998446468', 'https://openalex.org/W3014547480', 'https://openalex.org/W3034990686', 'https://openalex.org/W3092043977', 'https://openalex.org/W3098824823', 'https://openalex.org/W3100785471', 'https://openalex.org/W3120304799', 'https://openalex.org/W3122060391', 'https://openalex.org/W3177239031', 'https://openalex.org/W3198311766', 'https://openalex.org/W4287824654', 'https://openalex.org/W4295312788', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Existing works for aspect-based sentiment analysis (ABSA) have adopted a\nunified approach, which allows the interactive relations among subtasks.\nHowever, we observe that these methods tend to predict polarities based on the\nliteral meaning of aspect and opinion terms and mainly consider relations\nimplicitly among subtasks at the word level. In addition, identifying multiple\naspect-opinion pairs with their polarities is much more challenging. Therefore,\na comprehensive understanding of contextual information w.r.t. the aspect and\nopinion are further required in ABSA. In this paper, we propose Deep\nContextualized Relation-Aware Network (DCRAN), which allows interactive\nrelations among subtasks with deep contextual information based on two modules\n(i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies).\nEspecially, we design novel self-supervised strategies for ABSA, which have\nstrengths in dealing with multiple aspects. Experimental results show that\nDCRAN significantly outperforms previous state-of-the-art methods by large\nmargins on three widely used benchmarks.\n",1.0
NOVEL_MT_258,https://openalex.org/W3168577192,2021,22,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2624871570', 'https://openalex.org/W2767206889', 'https://openalex.org/W2790235966', 'https://openalex.org/W2799124508', 'https://openalex.org/W2912351236', 'https://openalex.org/W2922349260', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946375144', 'https://openalex.org/W2950976310', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962915948', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963430933', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963677766', 'https://openalex.org/W2963759780', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165804', 'https://openalex.org/W2964204621', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970550739', 'https://openalex.org/W2970820321', 'https://openalex.org/W2971167892', 'https://openalex.org/W2976965654', 'https://openalex.org/W2988975212', 'https://openalex.org/W2996987694', 'https://openalex.org/W3035812575', 'https://openalex.org/W3045688849', 'https://openalex.org/W3102226577', 'https://openalex.org/W3105848458', 'https://openalex.org/W3113715281', 'https://openalex.org/W3113838476', 'https://openalex.org/W3127901106', 'https://openalex.org/W4294103325', 'https://openalex.org/W4385245566']","Yongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu, Xing Wang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",1.0
NOVEL_MT_259,https://openalex.org/W3198270094,2021,2,"['https://openalex.org/W630532510', 'https://openalex.org/W1905522558', 'https://openalex.org/W1915251500', 'https://openalex.org/W2117278770', 'https://openalex.org/W2124807415', 'https://openalex.org/W2147262247', 'https://openalex.org/W2148274347', 'https://openalex.org/W2403717966', 'https://openalex.org/W2740743644', 'https://openalex.org/W2757592053', 'https://openalex.org/W2760452458', 'https://openalex.org/W2810533336', 'https://openalex.org/W2892202790', 'https://openalex.org/W2902689991', 'https://openalex.org/W2933138175', 'https://openalex.org/W2950940239', 'https://openalex.org/W2963024368', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963281280', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963777311', 'https://openalex.org/W2963826681', 'https://openalex.org/W2963897095', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970409072', 'https://openalex.org/W2971277088', 'https://openalex.org/W2978017171', 'https://openalex.org/W3004726085', 'https://openalex.org/W3015580305', 'https://openalex.org/W3032746405', 'https://openalex.org/W3034112745', 'https://openalex.org/W3034640977', 'https://openalex.org/W3034978746', 'https://openalex.org/W3039149172', 'https://openalex.org/W3094515085', 'https://openalex.org/W3095131375', 'https://openalex.org/W3095594087', 'https://openalex.org/W3133713241']","This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner. Once the in-domain data is detected by the classifier, the NMT model is then adapted to the new domain by jointly learning translation and domain discrimination tasks. We evaluate our cross-lingual data selection method on NMT across five diverse domains in three language pairs, as well as a real-world scenario of translation for COVID-19. The results show that our proposed method outperforms other selection baselines up to +1.5 BLEU score.",1.0
NOVEL_MT_261,https://openalex.org/W3214153756,2021,5,"['https://openalex.org/W66597527', 'https://openalex.org/W582134693', 'https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2095705004', 'https://openalex.org/W2100271871', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141440284', 'https://openalex.org/W2149327368', 'https://openalex.org/W2164984707', 'https://openalex.org/W2186839874', 'https://openalex.org/W2525778437', 'https://openalex.org/W2595715041', 'https://openalex.org/W2669742347', 'https://openalex.org/W2757980860', 'https://openalex.org/W2791510479', 'https://openalex.org/W2838081464', 'https://openalex.org/W2896457183', 'https://openalex.org/W2903445197', 'https://openalex.org/W2914120296', 'https://openalex.org/W2923014074', 'https://openalex.org/W2925618549', 'https://openalex.org/W2936695845', 'https://openalex.org/W2952357537', 'https://openalex.org/W2962678612', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963824830', 'https://openalex.org/W2964059111', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970156971', 'https://openalex.org/W2971051558', 'https://openalex.org/W2979826702', 'https://openalex.org/W2988975212', 'https://openalex.org/W2996403597', 'https://openalex.org/W3035390927', 'https://openalex.org/W3084095723', 'https://openalex.org/W3098824823', 'https://openalex.org/W3099925113', 'https://openalex.org/W3103840770', 'https://openalex.org/W3115049126', 'https://openalex.org/W3118692969', 'https://openalex.org/W3120060094', 'https://openalex.org/W3120791402', 'https://openalex.org/W3120951354', 'https://openalex.org/W3153832840', 'https://openalex.org/W3163874900', 'https://openalex.org/W3174751200', 'https://openalex.org/W3175810841', 'https://openalex.org/W3177376359', 'https://openalex.org/W3210120707', 'https://openalex.org/W4287642955', 'https://openalex.org/W4385245566']","Yuanhang Zheng, Zhixing Tan, Meng Zhang, Mieradilijiang Maimaiti, Huanbo Luan, Maosong Sun, Qun Liu, Yang Liu. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",1.0
NOVEL_MT_262,https://openalex.org/W3211481627,2021,2,"['https://openalex.org/W1832693441', 'https://openalex.org/W1880262756', 'https://openalex.org/W2006258004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115161902', 'https://openalex.org/W2117045850', 'https://openalex.org/W2124807415', 'https://openalex.org/W2525778437', 'https://openalex.org/W2573728411', 'https://openalex.org/W2595715041', 'https://openalex.org/W2905555874', 'https://openalex.org/W2912521296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963593215', 'https://openalex.org/W2964199361', 'https://openalex.org/W2969696767', 'https://openalex.org/W3035548285', 'https://openalex.org/W3035629723', 'https://openalex.org/W3045464143', 'https://openalex.org/W3211259717', 'https://openalex.org/W4231510805', 'https://openalex.org/W4385245566']","Neural Machine Translation (NMT) has shown a strong ability to utilize local context to disambiguate the meaning of words. However, it remains a challenge for NMT to leverage broader context information like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve translation performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English -> German and English -> French translation tasks.",1.0
NOVEL_MT_264,https://openalex.org/W3200074555,2021,1,"['https://openalex.org/W222053410', 'https://openalex.org/W630532510', 'https://openalex.org/W1682403713', 'https://openalex.org/W2101105183', 'https://openalex.org/W2257408573', 'https://openalex.org/W2399033357', 'https://openalex.org/W2525778437', 'https://openalex.org/W2593864460', 'https://openalex.org/W2757592053', 'https://openalex.org/W2759173152', 'https://openalex.org/W2788330850', 'https://openalex.org/W2891713103', 'https://openalex.org/W2952317054', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962796826', 'https://openalex.org/W2963149635', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963829526', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970756316', 'https://openalex.org/W2970925270', 'https://openalex.org/W2971296908', 'https://openalex.org/W3034640977', 'https://openalex.org/W3035531963', 'https://openalex.org/W3105816068', 'https://openalex.org/W3126425262', 'https://openalex.org/W3130523865', 'https://openalex.org/W3139958517', 'https://openalex.org/W3174702398', 'https://openalex.org/W3174805614', 'https://openalex.org/W3175374354', 'https://openalex.org/W3200396895']","How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online. Experiments on domain adaptation and multi-domain machine translation datasets show that even without expensive retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over the best existing online adaptation methods. The code and trained models are released at https://github.com/jiangqn/KSTER.",1.0
NOVEL_MT_265,https://openalex.org/W3185106120,2021,6,"['https://openalex.org/W2250539671', 'https://openalex.org/W2483215953', 'https://openalex.org/W2530828552', 'https://openalex.org/W2769358515', 'https://openalex.org/W2912457762', 'https://openalex.org/W2952328691', 'https://openalex.org/W2962772361', 'https://openalex.org/W2963457723', 'https://openalex.org/W2963526187', 'https://openalex.org/W3034458735', 'https://openalex.org/W3034515982', 'https://openalex.org/W3034987021', 'https://openalex.org/W3181414820']","Human gender bias is reflected in language and text production. Because state-of-the-art machine translation (MT) systems are trained on large corpora of text, mostly generated by humans, gender bias can also be found in MT. For instance when occupations are translated from a language like English, which mostly uses gender neutral words, to a language like German, which mostly uses a feminine and a masculine version for an occupation, a decision must be made by the MT System. Recent research showed that MT systems are biased towards stereotypical translation of occupations. In 2019 the first, and so far only, challenge set, explicitly designed to measure the extent of gender bias in MT systems has been published. In this set measurement of gender bias is solely based on the translation of occupations. In this paper we present an extension of this challenge set, called WiBeMT, with gender-biased adjectives and adds sentences with gender-biased verbs. The resulting challenge set consists of over 70, 000 sentences and has been translated with three commercial MT systems: DeepL Translator, Microsoft Translator, and Google Translate. Results show a gender bias for all three MT systems. This gender bias is to a great extent significantly influenced by adjectives and to a lesser extent by verbs.",0.9956709956709956
NOVEL_MT_266,https://openalex.org/W3176997878,2021,18,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2176263492', 'https://openalex.org/W2253795368', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548228487', 'https://openalex.org/W2798471433', 'https://openalex.org/W2798935874', 'https://openalex.org/W2883616626', 'https://openalex.org/W2891534142', 'https://openalex.org/W2896457183', 'https://openalex.org/W2903810591', 'https://openalex.org/W2904683980', 'https://openalex.org/W2922709902', 'https://openalex.org/W2928720664', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936695845', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963123301', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963679377', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964190861', 'https://openalex.org/W2970024416', 'https://openalex.org/W2970785793', 'https://openalex.org/W2986562961', 'https://openalex.org/W2994928925', 'https://openalex.org/W2996403597', 'https://openalex.org/W2996854111', 'https://openalex.org/W3006381853', 'https://openalex.org/W3034857244', 'https://openalex.org/W3104273515', 'https://openalex.org/W4287874506', 'https://openalex.org/W4385245566']","Inigo Jauregi Unanue, Jacob Parnell, Massimo Piccardi. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",0.992
NOVEL_MT_267,https://openalex.org/W3177195196,2021,3,"['https://openalex.org/W630532510', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2251743902', 'https://openalex.org/W2549139847', 'https://openalex.org/W2550821151', 'https://openalex.org/W2613904329', 'https://openalex.org/W2807535859', 'https://openalex.org/W2809456172', 'https://openalex.org/W2888456631', 'https://openalex.org/W2891924676', 'https://openalex.org/W2905933322', 'https://openalex.org/W2912095972', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921311659', 'https://openalex.org/W2949911645', 'https://openalex.org/W2952614664', 'https://openalex.org/W2952650870', 'https://openalex.org/W2953190730', 'https://openalex.org/W2962807144', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499433', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963983698', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964073484', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970316683', 'https://openalex.org/W2970925270', 'https://openalex.org/W2970925677', 'https://openalex.org/W2985301125', 'https://openalex.org/W3017454464', 'https://openalex.org/W3023986361', 'https://openalex.org/W3034719878', 'https://openalex.org/W3034906024', 'https://openalex.org/W3035019713', 'https://openalex.org/W3035144493', 'https://openalex.org/W3035464238', 'https://openalex.org/W3035747971', 'https://openalex.org/W3099065620', 'https://openalex.org/W3101668578', 'https://openalex.org/W3105038888', 'https://openalex.org/W3106106701', 'https://openalex.org/W3106321930', 'https://openalex.org/W3119866316', 'https://openalex.org/W3120929527', 'https://openalex.org/W3127719526', 'https://openalex.org/W4287636493', 'https://openalex.org/W4385245566']","Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",1.0
NOVEL_MT_268,https://openalex.org/W3196727201,2021,0,"['https://openalex.org/W222053410', 'https://openalex.org/W1797158486', 'https://openalex.org/W1902237438', 'https://openalex.org/W1975879668', 'https://openalex.org/W2020854665', 'https://openalex.org/W2119717200', 'https://openalex.org/W2251550922', 'https://openalex.org/W2274912527', 'https://openalex.org/W2296073425', 'https://openalex.org/W2467834614', 'https://openalex.org/W2576401592', 'https://openalex.org/W2803633004', 'https://openalex.org/W2888442053', 'https://openalex.org/W2896457183', 'https://openalex.org/W2940514631', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962955856', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963881255', 'https://openalex.org/W2964352247', 'https://openalex.org/W2970155008', 'https://openalex.org/W2980481399', 'https://openalex.org/W2986791765', 'https://openalex.org/W2998081612', 'https://openalex.org/W3034978379', 'https://openalex.org/W3082104416', 'https://openalex.org/W3101705677', 'https://openalex.org/W3124243366']",Machine Translation (MT) systems often fail to preserve different stylistic and pragmatic properties of the source text (e.g. sentiment and emotion and gender traits and etc.) to the target and especially in a low-resource scenario. Such loss can affect the performance of any downstream Natural Language Processing (NLP) task and such as sentiment analysis and that heavily relies on the output of the MT systems. The susceptibility to sentiment polarity loss becomes even more severe when an MT system is employed for translating a source content that lacks a legitimate language structure (e.g. review text). Therefore and we must find ways to minimize the undesirable effects of sentiment loss in translation without compromising with the adequacy. In our current work and we present a deep re-inforcement learning (RL) framework in conjunction with the curriculum learning (as per difficulties of the reward) to fine-tune the parameters of a pre-trained neural MT system so that the generated translation successfully encodes the underlying sentiment of the source without compromising the adequacy unlike previous methods. We evaluate our proposed method on the English–Hindi (product domain) and French–English (restaurant domain) review datasets and and found that our method brings a significant improvement over several baselines in the machine translation and and sentiment classification tasks.,1.0
NOVEL_MT_269,https://openalex.org/W3157748223,2021,6,[],"Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.",1.0
NOVEL_MT_270,https://openalex.org/W3170483380,2021,0,"['https://openalex.org/W22168010', 'https://openalex.org/W68733909', 'https://openalex.org/W266752158', 'https://openalex.org/W1588612820', 'https://openalex.org/W1977427797', 'https://openalex.org/W2047295649', 'https://openalex.org/W2101105183', 'https://openalex.org/W2765961751', 'https://openalex.org/W2951476960', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963293463', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2982399380', 'https://openalex.org/W3018870313', 'https://openalex.org/W3034368185', 'https://openalex.org/W3087204350', 'https://openalex.org/W3087503240', 'https://openalex.org/W3107826490']","High-performing machine translation (MT) systems can help overcome language barriers while making it possible for everyone to communicate and use language technologies in the language of their choice. However, such systems require large amounts of parallel sentences for training, and translators can be difficult to find and expensive. Here, we present a data collection strategy for MT which, in contrast, is cheap and simple, as it does not require bilingual speakers. Based on the insight that humans pay specific attention to movements, we use graphics interchange formats (GIFs) as a pivot to collect parallel sentences from monolingual annotators. We use our strategy to collect data in Hindi, Tamil and English. As a baseline, we also collect data using images as a pivot. We perform an intrinsic evaluation by manually evaluating a subset of the sentence pairs and an extrinsic evaluation by finetuning mBART on the collected data. We find that sentences collected via GIFs are indeed of higher quality.",0.9942857142857144
NOVEL_MT_271,https://openalex.org/W3173657420,2021,18,"['https://openalex.org/W44695385', 'https://openalex.org/W90695545', 'https://openalex.org/W808583520', 'https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2148708890', 'https://openalex.org/W2162429782', 'https://openalex.org/W2251955814', 'https://openalex.org/W2512593609', 'https://openalex.org/W2582446770', 'https://openalex.org/W2595715041', 'https://openalex.org/W2606531491', 'https://openalex.org/W2747874407', 'https://openalex.org/W2759173152', 'https://openalex.org/W2767019613', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2902582221', 'https://openalex.org/W2945700568', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963779652', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964291396', 'https://openalex.org/W2973048981', 'https://openalex.org/W2983108239', 'https://openalex.org/W3015583403', 'https://openalex.org/W3017643757', 'https://openalex.org/W3033962033', 'https://openalex.org/W3034351728', 'https://openalex.org/W3034571331', 'https://openalex.org/W3035520602', 'https://openalex.org/W3037793211', 'https://openalex.org/W3046024489', 'https://openalex.org/W3046531489', 'https://openalex.org/W3087091162', 'https://openalex.org/W3094903517', 'https://openalex.org/W3101096852', 'https://openalex.org/W3101683892', 'https://openalex.org/W3103878009', 'https://openalex.org/W3105669983', 'https://openalex.org/W3114997930', 'https://openalex.org/W3118485656', 'https://openalex.org/W3119399851', 'https://openalex.org/W3162000275', 'https://openalex.org/W3175809709', 'https://openalex.org/W4287637331', 'https://openalex.org/W4385245566']","Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (E2E) speech translation (ST) is still under-studied. We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency. We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model. Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST. Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.18-2.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation.",0.9923664122137404
NOVEL_MT_274,https://openalex.org/W3201371171,2021,9,"['https://openalex.org/W580497036', 'https://openalex.org/W1553669113', 'https://openalex.org/W1631260214', 'https://openalex.org/W1779279021', 'https://openalex.org/W2006832571', 'https://openalex.org/W2015099947', 'https://openalex.org/W2038253946', 'https://openalex.org/W2044916741', 'https://openalex.org/W2064675550', 'https://openalex.org/W2123082355', 'https://openalex.org/W2143426320', 'https://openalex.org/W2148365102', 'https://openalex.org/W2148959489', 'https://openalex.org/W2232536453', 'https://openalex.org/W2250753706', 'https://openalex.org/W2252160048', 'https://openalex.org/W2472237015', 'https://openalex.org/W2563351168', 'https://openalex.org/W2579146096', 'https://openalex.org/W2610003462', 'https://openalex.org/W2727767747', 'https://openalex.org/W2741373390', 'https://openalex.org/W2803503091', 'https://openalex.org/W2897142347', 'https://openalex.org/W2922158773', 'https://openalex.org/W2963004259', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963352326', 'https://openalex.org/W2970038984', 'https://openalex.org/W2971043182', 'https://openalex.org/W2985165968', 'https://openalex.org/W2990400634', 'https://openalex.org/W3015810608', 'https://openalex.org/W3034562926', 'https://openalex.org/W3034716087', 'https://openalex.org/W3035013535', 'https://openalex.org/W3037278729', 'https://openalex.org/W3104976898', 'https://openalex.org/W3115744841', 'https://openalex.org/W3141884538', 'https://openalex.org/W3196290596', 'https://openalex.org/W3212926742', 'https://openalex.org/W3214117304', 'https://openalex.org/W4285719527']","Daria Pylypenko, Kwabena Amponsah-Kaakyire, Koel Dutta Chowdhury, Josef van Genabith, Cristina España-Bonet. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",1.0
NOVEL_MT_275,https://openalex.org/W3199890931,2021,16,"['https://openalex.org/W1522301498', 'https://openalex.org/W2133564696', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2563351168', 'https://openalex.org/W2594047108', 'https://openalex.org/W2624871570', 'https://openalex.org/W2807535859', 'https://openalex.org/W2888456631', 'https://openalex.org/W2889263348', 'https://openalex.org/W2891076394', 'https://openalex.org/W2891924676', 'https://openalex.org/W2896457183', 'https://openalex.org/W2933138175', 'https://openalex.org/W2940545298', 'https://openalex.org/W2949911645', 'https://openalex.org/W2952153923', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962717763', 'https://openalex.org/W2962807144', 'https://openalex.org/W2962982474', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963665552', 'https://openalex.org/W2963983698', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2997359900', 'https://openalex.org/W3017454464', 'https://openalex.org/W3035019713', 'https://openalex.org/W3035464238', 'https://openalex.org/W3046368065', 'https://openalex.org/W3092189037', 'https://openalex.org/W3104652292', 'https://openalex.org/W3105038888', 'https://openalex.org/W3106539628', 'https://openalex.org/W3116484298', 'https://openalex.org/W3128413221', 'https://openalex.org/W3134228182', 'https://openalex.org/W3176366152', 'https://openalex.org/W4287694131', 'https://openalex.org/W4385245566']","Multilingual Neural Machine Translation (NMT) enables one model to serve all translation directions, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations – commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our method also works well when the small amount of direct data is not available.",1.0
NOVEL_MT_277,https://openalex.org/W3214250531,2021,16,"['https://openalex.org/W2148708890', 'https://openalex.org/W2467834614', 'https://openalex.org/W2512924740', 'https://openalex.org/W2550821151', 'https://openalex.org/W2757920198', 'https://openalex.org/W2760656271', 'https://openalex.org/W2807895655', 'https://openalex.org/W2946417913', 'https://openalex.org/W2954218767', 'https://openalex.org/W2962731009', 'https://openalex.org/W2963027654', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963351145', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963697731', 'https://openalex.org/W2963979492', 'https://openalex.org/W2970015022', 'https://openalex.org/W2982026991', 'https://openalex.org/W2986670783', 'https://openalex.org/W3087197566', 'https://openalex.org/W3092144570', 'https://openalex.org/W3092651934', 'https://openalex.org/W3100727892', 'https://openalex.org/W3130032352', 'https://openalex.org/W3134111219', 'https://openalex.org/W3167841296', 'https://openalex.org/W3211941349', 'https://openalex.org/W4385245566']","Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users' trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks: continuous values must be binned into discrete categories, which is unnatural for certain applications; interference between multiple tags is poorly understood. We address these problems by introducing vector-valued interventions which allow for fine-grained control over multiple attributes simultaneously via a weighted linear combination of the corresponding vectors. For some attributes, our approach even allows for fine-tuning a model trained without annotations to support such interventions. In experiments with three attributes (length, politeness and monotonicity) and two language pairs (English to German and Japanese) our models achieve better control over a wider range of tasks compared to tagging, and translation quality does not degrade when no control is requested. Finally, we demonstrate how to enable control in an already trained model after a relatively cheap fine-tuning stage.",1.0
NOVEL_MT_280,https://openalex.org/W4225470987,2022,10,"['https://openalex.org/W222053410', 'https://openalex.org/W582134693', 'https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W1915251500', 'https://openalex.org/W1995875735', 'https://openalex.org/W2053154970', 'https://openalex.org/W2079656678', 'https://openalex.org/W2133564696', 'https://openalex.org/W2164777277', 'https://openalex.org/W2597891111', 'https://openalex.org/W2613904329', 'https://openalex.org/W2888779557', 'https://openalex.org/W2904829696', 'https://openalex.org/W2905266130', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946068894', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963174344', 'https://openalex.org/W2963351448', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963807318', 'https://openalex.org/W2986562961', 'https://openalex.org/W2995460523', 'https://openalex.org/W3006381853', 'https://openalex.org/W3099417250', 'https://openalex.org/W3101155369', 'https://openalex.org/W3102138045', 'https://openalex.org/W3104273515', 'https://openalex.org/W3106185885', 'https://openalex.org/W3173844397', 'https://openalex.org/W3174172622', 'https://openalex.org/W3175995572', 'https://openalex.org/W3214236736', 'https://openalex.org/W4385245566']","Songming Zhang, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jian Liu, Jie Zhou. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",0.9325842696629212
NOVEL_MT_281,https://openalex.org/W3199575985,2021,3,"['https://openalex.org/W312806309', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2149327368', 'https://openalex.org/W2251816503', 'https://openalex.org/W2512924740', 'https://openalex.org/W2933138175', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962678612', 'https://openalex.org/W2963816901', 'https://openalex.org/W2964212550', 'https://openalex.org/W2970461932', 'https://openalex.org/W2970871182', 'https://openalex.org/W2973088264', 'https://openalex.org/W3006530332', 'https://openalex.org/W3118917012', 'https://openalex.org/W3120104809', 'https://openalex.org/W3120896619', 'https://openalex.org/W3121041981', 'https://openalex.org/W3121071870']",,1.0
NOVEL_MT_282,https://openalex.org/W3199258042,2021,123,"['https://openalex.org/W2126725946', 'https://openalex.org/W2294774419', 'https://openalex.org/W2561995736', 'https://openalex.org/W2741602058', 'https://openalex.org/W2752630748', 'https://openalex.org/W2887920589', 'https://openalex.org/W2933138175', 'https://openalex.org/W2945383715', 'https://openalex.org/W2946794439', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963633299', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964303773', 'https://openalex.org/W2970925270', 'https://openalex.org/W2972534020', 'https://openalex.org/W2990555152', 'https://openalex.org/W3001434439', 'https://openalex.org/W3034469191', 'https://openalex.org/W3034955736', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035254119', 'https://openalex.org/W3035390927', 'https://openalex.org/W3093886841', 'https://openalex.org/W3094502228', 'https://openalex.org/W3096841893', 'https://openalex.org/W3096966601', 'https://openalex.org/W3098903812', 'https://openalex.org/W3103182178', 'https://openalex.org/W3104652516', 'https://openalex.org/W3107826490', 'https://openalex.org/W3116273903', 'https://openalex.org/W3134307371', 'https://openalex.org/W3152956381', 'https://openalex.org/W3174770825', 'https://openalex.org/W3174784402', 'https://openalex.org/W4205991051', 'https://openalex.org/W4287597359', 'https://openalex.org/W4292779060', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.",0.9943502824858758
NOVEL_MT_283,https://openalex.org/W3185652831,2021,3,"['https://openalex.org/W1522301498', 'https://openalex.org/W2012672587', 'https://openalex.org/W2581377246', 'https://openalex.org/W2896457183', 'https://openalex.org/W2923014074', 'https://openalex.org/W2936695845', 'https://openalex.org/W2937343562', 'https://openalex.org/W2946609015', 'https://openalex.org/W2963083752', 'https://openalex.org/W2963159690', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963677766', 'https://openalex.org/W2965373594', 'https://openalex.org/W2996403597', 'https://openalex.org/W3000295996', 'https://openalex.org/W3006647218', 'https://openalex.org/W3007685714', 'https://openalex.org/W3035682985', 'https://openalex.org/W3093419064', 'https://openalex.org/W3099319035', 'https://openalex.org/W3102631365', 'https://openalex.org/W3113454798', 'https://openalex.org/W3167136668', 'https://openalex.org/W3186134690', 'https://openalex.org/W4297798436', 'https://openalex.org/W4298393544', 'https://openalex.org/W4300996741']","This paper introduces our systems for all three subtasks of SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning. To help our model better represent and understand abstract concepts in natural language, we well-design many simple and effective approaches adapted to the backbone model (RoBERTa). Specifically, we formalize the subtasks into the multiple-choice question answering format and add special tokens to abstract concepts, then, the final prediction of QA is considered as the result of subtasks. Additionally, we employ many finetuning tricks to improve the performance. Experimental results show that our approach gains significant performance compared with the baseline systems. Our system achieves eighth rank (87.51%) and tenth rank (89.64%) on the official blind test set of subtask 1 and subtask 2 respectively.",0.9961685823754788
NOVEL_MT_284,https://openalex.org/W3173190788,2021,137,"['https://openalex.org/W1522301498', 'https://openalex.org/W2251743902', 'https://openalex.org/W2550821151', 'https://openalex.org/W2593543827', 'https://openalex.org/W2610245951', 'https://openalex.org/W2760535273', 'https://openalex.org/W2770394828', 'https://openalex.org/W2905933322', 'https://openalex.org/W2914120296', 'https://openalex.org/W2919290281', 'https://openalex.org/W2944815030', 'https://openalex.org/W2952153923', 'https://openalex.org/W2952200364', 'https://openalex.org/W2952468927', 'https://openalex.org/W2952614664', 'https://openalex.org/W2952650870', 'https://openalex.org/W2953190730', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962830144', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964073484', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970925270', 'https://openalex.org/W2973088264', 'https://openalex.org/W2985204668', 'https://openalex.org/W2997518171', 'https://openalex.org/W2998388430', 'https://openalex.org/W3005680577', 'https://openalex.org/W3017454464', 'https://openalex.org/W3023986361', 'https://openalex.org/W3026732421', 'https://openalex.org/W3033879023', 'https://openalex.org/W3034781633', 'https://openalex.org/W3034978746', 'https://openalex.org/W3035144493', 'https://openalex.org/W3035464238', 'https://openalex.org/W3035524453', 'https://openalex.org/W3036839309', 'https://openalex.org/W3092327118', 'https://openalex.org/W3093871477', 'https://openalex.org/W3105378761', 'https://openalex.org/W3106321930', 'https://openalex.org/W3107826490', 'https://openalex.org/W3108655343', 'https://openalex.org/W3115295967', 'https://openalex.org/W4249573750', 'https://openalex.org/W4287774231', 'https://openalex.org/W4298153606', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","Xiao Pan, Mingxuan Wang, Liwei Wu, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_286,https://openalex.org/W3168236275,2021,2,"['https://openalex.org/W214028658', 'https://openalex.org/W273093436', 'https://openalex.org/W1972648128', 'https://openalex.org/W2065165780', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116599427', 'https://openalex.org/W2117717100', 'https://openalex.org/W2184135559', 'https://openalex.org/W2493916176', 'https://openalex.org/W2572474373', 'https://openalex.org/W2756566411', 'https://openalex.org/W2757281913', 'https://openalex.org/W2806962830', 'https://openalex.org/W2904683980', 'https://openalex.org/W2949303037', 'https://openalex.org/W2950940239', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962912551', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963500732', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964144280', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970791445', 'https://openalex.org/W2977458338', 'https://openalex.org/W2994689640', 'https://openalex.org/W2995197202', 'https://openalex.org/W2996403597', 'https://openalex.org/W3015504467', 'https://openalex.org/W3035252911', 'https://openalex.org/W3035408261', 'https://openalex.org/W3037109418', 'https://openalex.org/W3100806282', 'https://openalex.org/W3101286153', 'https://openalex.org/W3101672304', 'https://openalex.org/W3101860695', 'https://openalex.org/W3103849166', 'https://openalex.org/W3105425516', 'https://openalex.org/W3106076062', 'https://openalex.org/W3145181028', 'https://openalex.org/W3152788712', 'https://openalex.org/W3166514234', 'https://openalex.org/W3213234281', 'https://openalex.org/W4230872509']","State-of-the-art machine translation (MT) systems are typically trained to generate the ""standard"" target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. Such varieties are often low-resource, and hence do not benefit from contemporary NLP solutions, MT included. We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to, but different from, the standard target language, using no parallel (source--variety) data. This also includes adaptation of MT systems to low-resource typologically-related target languages. We experiment with adapting an English--Russian MT system to generate Ukrainian and Belarusian, an English--Norwegian Bokmål system to generate Nynorsk, and an English--Arabic system to generate four Arabic dialects, obtaining significant improvements over competitive baselines.",1.0
NOVEL_MT_287,https://openalex.org/W3198379503,2021,0,"['https://openalex.org/W19526395', 'https://openalex.org/W222053410', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153579005', 'https://openalex.org/W2157331557', 'https://openalex.org/W2890007195', 'https://openalex.org/W2932618389', 'https://openalex.org/W2944815030', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964091381', 'https://openalex.org/W2964247056', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970049541', 'https://openalex.org/W3107826490']",In this paper and we identify an interesting kind of error in the output of Unsupervised Neural Machine Translation (UNMT) systems like Undreamt1. We refer to this error type as Scrambled Translation problem. We observe that UNMT models which use word shuffle noise (as in case of Undreamt) can generate correct words and but fail to stitch them together to form phrases. As a result and words of the translated sentence look scrambled and resulting in decreased BLEU. We hypothesise that the reason behind scrambled translation problem is ’shuffling noise’ which is introduced in every input sentence as a denoising strategy. To test our hypothesis and we experiment by retraining UNMT models with a simple retraining strategy. We stop the training of the Denoising UNMT model after a pre-decided number of iterations and resume the training for the remaining iterations- which number is also pre-decided- using original sentence as input without adding any noise. Our proposed solution achieves significant performance improvement UNMT models that train conventionally. We demonstrate these performance gains on four language pairs and viz. and English-French and English-German and English-Spanish and Hindi-Punjabi. Our qualitative and quantitative analysis shows that the retraining strategy helps achieve better alignment as observed by attention heatmap and better phrasal translation and leading to statistically significant improvement in BLEU scores.,0.9827586206896552
NOVEL_MT_289,https://openalex.org/W3161391716,2021,1,"['https://openalex.org/W2101105183', 'https://openalex.org/W2184135559', 'https://openalex.org/W2608029998', 'https://openalex.org/W2794365787', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2922158773', 'https://openalex.org/W2933138175', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2977458338', 'https://openalex.org/W2983108239', 'https://openalex.org/W2995722251', 'https://openalex.org/W3022822895', 'https://openalex.org/W3033962033', 'https://openalex.org/W3046531489', 'https://openalex.org/W3093404841', 'https://openalex.org/W3105214104', 'https://openalex.org/W3116490761']","Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context -- context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify the usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that conditioning on a longer context has a diminishing effect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method increases context usage and that this reflects on the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.",1.0
NOVEL_MT_290,https://openalex.org/W3185537577,2021,2,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116957398', 'https://openalex.org/W2127863960', 'https://openalex.org/W2138974169', 'https://openalex.org/W2152263452', 'https://openalex.org/W2169200297', 'https://openalex.org/W2183341477', 'https://openalex.org/W2525778437', 'https://openalex.org/W2576482813', 'https://openalex.org/W2888539709', 'https://openalex.org/W2897507397', 'https://openalex.org/W2912351236', 'https://openalex.org/W2946794439', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970045405', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970550739', 'https://openalex.org/W2986267869', 'https://openalex.org/W3035589854', 'https://openalex.org/W3037109418', 'https://openalex.org/W3086917037', 'https://openalex.org/W3183995373', 'https://openalex.org/W4285112771', 'https://openalex.org/W4385245566']","Hiroyuki Deguchi, Akihiro Tamura, Takashi Ninomiya. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop. 2021.",1.0
NOVEL_MT_291,https://openalex.org/W3171766884,2021,4,"['https://openalex.org/W1522301498', 'https://openalex.org/W1810943226', 'https://openalex.org/W1909320841', 'https://openalex.org/W1924770834', 'https://openalex.org/W1959608418', 'https://openalex.org/W2064675550', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145094598', 'https://openalex.org/W2257408573', 'https://openalex.org/W2402144811', 'https://openalex.org/W2512924740', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2613904329', 'https://openalex.org/W2752047430', 'https://openalex.org/W2807753879', 'https://openalex.org/W2885023989', 'https://openalex.org/W2914120296', 'https://openalex.org/W2953384591', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963714898', 'https://openalex.org/W2964054038', 'https://openalex.org/W2964076537', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970287057', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970646865', 'https://openalex.org/W2970854433', 'https://openalex.org/W2995015695', 'https://openalex.org/W2995230342', 'https://openalex.org/W2995304149', 'https://openalex.org/W2997574889', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","Translation quality can be improved by global information from the required target sentence because the decoder can understand both past and future information. However, the model needs additional cost to produce and consider such global information. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from semantic space for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible semantic from the semantic space. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub.",1.0
NOVEL_MT_295,https://openalex.org/W3173844397,2021,24,"['https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W1915251500', 'https://openalex.org/W2117278770', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2534200568', 'https://openalex.org/W2888779557', 'https://openalex.org/W2903728819', 'https://openalex.org/W2904829696', 'https://openalex.org/W2905266130', 'https://openalex.org/W2909737760', 'https://openalex.org/W2933138175', 'https://openalex.org/W2946068894', 'https://openalex.org/W2946379889', 'https://openalex.org/W2952474700', 'https://openalex.org/W2952479981', 'https://openalex.org/W2953173959', 'https://openalex.org/W2956130159', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962926939', 'https://openalex.org/W2963174344', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963281280', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963442512', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963652649', 'https://openalex.org/W2963699608', 'https://openalex.org/W2963876389', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964302946', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970849705', 'https://openalex.org/W2971302374', 'https://openalex.org/W2986562961', 'https://openalex.org/W2988249555', 'https://openalex.org/W2994928925', 'https://openalex.org/W2996766022', 'https://openalex.org/W2998655810', 'https://openalex.org/W3034368185', 'https://openalex.org/W3035214886', 'https://openalex.org/W3101668578', 'https://openalex.org/W3104273515', 'https://openalex.org/W3105718208', 'https://openalex.org/W3106185885', 'https://openalex.org/W4287718112', 'https://openalex.org/W4385245566']","Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, Jie Zhou. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",1.0
NOVEL_MT_298,https://openalex.org/W3212217304,2021,3,"['https://openalex.org/W28136092', 'https://openalex.org/W1986345088', 'https://openalex.org/W2060008829', 'https://openalex.org/W2079735306', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109553965', 'https://openalex.org/W2147227066', 'https://openalex.org/W2149327368', 'https://openalex.org/W2149791639', 'https://openalex.org/W2250679855', 'https://openalex.org/W2251431759', 'https://openalex.org/W2622075087', 'https://openalex.org/W2884808449', 'https://openalex.org/W2894218541', 'https://openalex.org/W2902463012', 'https://openalex.org/W2902608666', 'https://openalex.org/W2902943690', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2942274440', 'https://openalex.org/W2953072129', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963816901', 'https://openalex.org/W2963824830', 'https://openalex.org/W2970156971', 'https://openalex.org/W2971120958', 'https://openalex.org/W2977978669', 'https://openalex.org/W3034673518', 'https://openalex.org/W3037879901', 'https://openalex.org/W3097330605', 'https://openalex.org/W3119881489', 'https://openalex.org/W3120951354', 'https://openalex.org/W3165234151', 'https://openalex.org/W3174693399', 'https://openalex.org/W3184138828', 'https://openalex.org/W3205309080', 'https://openalex.org/W3213919616', 'https://openalex.org/W4211166112', 'https://openalex.org/W4230872509']","Compared to fully manual translation, post-editing (PE) machine translation (MT) output can save time and reduce errors. Automatic word-level quality estimation (QE) aims to predict the correctness of words in MT output and holds great promise to aid PE by flagging problematic output. Quality of QE is crucial, as incorrect QE might lead to translators missing errors or wasting time on already correct MT output. Achieving accurate automatic word-level QE is very hard, and it is currently not known (i) at what quality threshold QE is actually beginning to be useful for human PE, and (ii), how to best present word-level QE information to translators. In particular, should word-level QE visualization indicate uncertainty of the QE model or not? In this paper, we address both research questions with real and simulated word-level QE, visualizations, and user studies, where time, subjective ratings, and quality of the final translations are assessed. Results show that current word-level QE models are not yet good enough to support PE. Instead, quality levels of > 80% F1 are required. For helpful quality levels, a visualization reflecting the uncertainty of the QE model is preferred. Our analysis further shows that speed gains achieved through QE are not merely a result of blindly trusting the QE system, but that the quality of the final translations also improves. The threshold results from the paper establish a quality goal for future word-level QE research.",1.0
NOVEL_MT_300,https://openalex.org/W3200765429,2021,5,"['https://openalex.org/W22168010', 'https://openalex.org/W222053410', 'https://openalex.org/W630532510', 'https://openalex.org/W1625582487', 'https://openalex.org/W2101105183', 'https://openalex.org/W2419539795', 'https://openalex.org/W2493916176', 'https://openalex.org/W2802153702', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964308564', 'https://openalex.org/W2977458338', 'https://openalex.org/W3034238904', 'https://openalex.org/W3034640977', 'https://openalex.org/W3034771276', 'https://openalex.org/W3035390927', 'https://openalex.org/W3093818688', 'https://openalex.org/W3120929527']","Many works proposed methods to improve the performance of Neural Machine Translation (NMT) models in a domain/multi-domain adaptation scenario. However, an understanding of how NMT baselines represent text domain information internally is still lacking. Here we analyze the sentence representations learned by NMT Transformers and show that these explicitly include the information on text domains, even after only seeing the input sentences without domains labels. Furthermore, we show that this internal information is enough to cluster sentences by their underlying domains without supervision. We show that NMT models produce clusters better aligned to the actual domains compared to pre-trained language models (LMs). Notably, when computed on document-level, NMT cluster-to-domain correspondence nears 100%. We use these findings together with an approach to NMT domain adaptation using automatically extracted domains. Whereas previous work relied on external LMs for text clustering, we propose re-using the NMT model as a source of unsupervised clusters. We perform an extensive experimental study comparing two approaches across two data scenarios, three language pairs, and both sentence-level and document-level clustering, showing equal or significantly superior performance compared to LMs.",1.0
NOVEL_MT_301,https://openalex.org/W3198995825,2021,1,"['https://openalex.org/W630532510', 'https://openalex.org/W1758203568', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2154652894', 'https://openalex.org/W2539350388', 'https://openalex.org/W2567639685', 'https://openalex.org/W2617566453', 'https://openalex.org/W2888161220', 'https://openalex.org/W2888173624', 'https://openalex.org/W2914442349', 'https://openalex.org/W2952250961', 'https://openalex.org/W2962917899', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963366196', 'https://openalex.org/W2963631950', 'https://openalex.org/W2963667126', 'https://openalex.org/W2963796896', 'https://openalex.org/W2964008635', 'https://openalex.org/W2964235839', 'https://openalex.org/W2970562804', 'https://openalex.org/W2970669113', 'https://openalex.org/W2978670439', 'https://openalex.org/W2979826702', 'https://openalex.org/W2995197202', 'https://openalex.org/W2995465878', 'https://openalex.org/W3004665584', 'https://openalex.org/W3034303964', 'https://openalex.org/W3034319502', 'https://openalex.org/W3048384720', 'https://openalex.org/W3098824823', 'https://openalex.org/W3100727892', 'https://openalex.org/W3115753579', 'https://openalex.org/W3172669006', 'https://openalex.org/W4205635927']","Documents as short as a single sentence may inadvertently reveal sensitive information about their authors, including e.g. their gender or ethnicity. Style transfer is an effective way of transforming texts in order to remove any information that enables author profiling. However, for a number of current state-of-the-art approaches the improved privacy is accompanied by an undesirable drop in the down-stream utility of the transformed data. In this paper, we propose a simple, zero-shot way to effectively lower the risk of author profiling through multilingual back-translation using off-the-shelf translation models. We compare our models with five representative text style transfer models on three datasets across different domains. Results from both an automatic and a human evaluation show that our approach achieves the best overall performance while requiring no training data. We are able to lower the adversarial prediction of gender and race by up to 22% while retaining 95% of the original utility on downstream tasks.",1.0
NOVEL_MT_303,https://openalex.org/W3201244947,2021,36,"['https://openalex.org/W1522301498', 'https://openalex.org/W1529731474', 'https://openalex.org/W2129842875', 'https://openalex.org/W2152380671', 'https://openalex.org/W2167187514', 'https://openalex.org/W2250225327', 'https://openalex.org/W2251199578', 'https://openalex.org/W2251913848', 'https://openalex.org/W2295434202', 'https://openalex.org/W2563163303', 'https://openalex.org/W2759211898', 'https://openalex.org/W2785611959', 'https://openalex.org/W2804778516', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963777632', 'https://openalex.org/W2964022985', 'https://openalex.org/W2964121744', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970476646', 'https://openalex.org/W2971136144', 'https://openalex.org/W2971204996', 'https://openalex.org/W2979826702', 'https://openalex.org/W2982399380', 'https://openalex.org/W3005680577', 'https://openalex.org/W3011411500', 'https://openalex.org/W3015465981', 'https://openalex.org/W3021533447', 'https://openalex.org/W3033176962', 'https://openalex.org/W3034978746', 'https://openalex.org/W3034999214', 'https://openalex.org/W3082274269', 'https://openalex.org/W3091432621', 'https://openalex.org/W3092383198', 'https://openalex.org/W3093871960', 'https://openalex.org/W3098903812', 'https://openalex.org/W3099004917', 'https://openalex.org/W3103433205', 'https://openalex.org/W3119164154', 'https://openalex.org/W3119438769', 'https://openalex.org/W3121525843', 'https://openalex.org/W3135367836', 'https://openalex.org/W3137214022', 'https://openalex.org/W3139080614', 'https://openalex.org/W3166396011', 'https://openalex.org/W3173151551', 'https://openalex.org/W3174770825', 'https://openalex.org/W3198659451', 'https://openalex.org/W4288089799', 'https://openalex.org/W4292779060', 'https://openalex.org/W4297795751']","We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.",1.0
NOVEL_MT_304,https://openalex.org/W3169001897,2021,34,"['https://openalex.org/W148133648', 'https://openalex.org/W211509693', 'https://openalex.org/W1753482797', 'https://openalex.org/W2006969979', 'https://openalex.org/W2038698865', 'https://openalex.org/W2122413656', 'https://openalex.org/W2133444727', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143891888', 'https://openalex.org/W2148708890', 'https://openalex.org/W2163605009', 'https://openalex.org/W2296701362', 'https://openalex.org/W2486285194', 'https://openalex.org/W2546938941', 'https://openalex.org/W2756566411', 'https://openalex.org/W2888519496', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896457183', 'https://openalex.org/W2899663614', 'https://openalex.org/W2914120296', 'https://openalex.org/W2919290281', 'https://openalex.org/W2946068894', 'https://openalex.org/W2946442465', 'https://openalex.org/W2951476960', 'https://openalex.org/W2952993422', 'https://openalex.org/W2953173959', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963823140', 'https://openalex.org/W2963962369', 'https://openalex.org/W2964132420', 'https://openalex.org/W2964247056', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971008823', 'https://openalex.org/W2979636403', 'https://openalex.org/W2995435108', 'https://openalex.org/W3005680577', 'https://openalex.org/W3035207248', 'https://openalex.org/W3035577668', 'https://openalex.org/W3082274269', 'https://openalex.org/W3098341425', 'https://openalex.org/W4205512866', 'https://openalex.org/W4288089799', 'https://openalex.org/W4299567010', 'https://openalex.org/W4301230920', 'https://openalex.org/W4385245566']","We propose a data augmentation method for neural machine translation. It works by interpreting language models and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both context and alignment into account to maintain the symmetry between source and target sequences. Experiments on IWSLT'15 English → Vietnamese, WMT'17 English → German, WMT'18 English → Turkish, and WMT'19 robust English → French show that the method can improve the performance of translation, backtranslation and translation robustness.",1.0
NOVEL_MT_305,https://openalex.org/W3177085913,2021,67,"['https://openalex.org/W1480583224', 'https://openalex.org/W1573040851', 'https://openalex.org/W1924770834', 'https://openalex.org/W1985867508', 'https://openalex.org/W2077395415', 'https://openalex.org/W2085662862', 'https://openalex.org/W2250539671', 'https://openalex.org/W2316138215', 'https://openalex.org/W2465534249', 'https://openalex.org/W2470413457', 'https://openalex.org/W2526449353', 'https://openalex.org/W2556605533', 'https://openalex.org/W2584561145', 'https://openalex.org/W2618843390', 'https://openalex.org/W2619383789', 'https://openalex.org/W2740550900', 'https://openalex.org/W2778940641', 'https://openalex.org/W2787581402', 'https://openalex.org/W2798610091', 'https://openalex.org/W2889056793', 'https://openalex.org/W2899197626', 'https://openalex.org/W2903795704', 'https://openalex.org/W2931433835', 'https://openalex.org/W2962931510', 'https://openalex.org/W2963032608', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963686995', 'https://openalex.org/W2963702064', 'https://openalex.org/W2964051877', 'https://openalex.org/W2982645239', 'https://openalex.org/W3012721484', 'https://openalex.org/W3022508977', 'https://openalex.org/W3034266838', 'https://openalex.org/W4385245566']","Jiajia Tang, Kang Li, Xuanyu Jin, Andrzej Cichocki, Qibin Zhao, Wanzeng Kong. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",0.9950738916256158
NOVEL_MT_306,https://openalex.org/W3198182251,2024,8,"['https://openalex.org/W46679369', 'https://openalex.org/W1702700969', 'https://openalex.org/W1872443190', 'https://openalex.org/W2097998348', 'https://openalex.org/W2101105183', 'https://openalex.org/W2129749171', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250342921', 'https://openalex.org/W2478454054', 'https://openalex.org/W2760656271', 'https://openalex.org/W2775233965', 'https://openalex.org/W2884381860', 'https://openalex.org/W2903193068', 'https://openalex.org/W2909389168', 'https://openalex.org/W2916548775', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2978943549', 'https://openalex.org/W2981540061', 'https://openalex.org/W3016151632', 'https://openalex.org/W3103182178', 'https://openalex.org/W3113488190', 'https://openalex.org/W3133702157']","The Transformer model is the state-of-the-art in Machine Translation. However, in general, neural translation models often under perform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on low-resource language pairs. In this study, hyperparameter optimization of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an in-domain 88k public admin corpus were used for evaluation. A Transformer optimized model demonstrated a BLEU score improvement of 7.8 points when compared with a baseline RNN model. Improvements were observed across a range of metrics, including TER, indicating a substantially reduced post editing effort for Transformer optimized models with 16k BPE subword models. Bench-marked against Google Translate, our translation engines demonstrated significant improvements. The question of whether or not Transformers can be used effectively in a low-resource setting of English-Irish translation has been addressed. Is féidir linn - yes we can.",0.990990990990991
SKG_MT_1,https://openalex.org/W2514445058,2016,57,"['https://openalex.org/W6908809', 'https://openalex.org/W1948566616', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113021982', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131462252', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153579005', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2161002933', 'https://openalex.org/W2167665328', 'https://openalex.org/W2239731672', 'https://openalex.org/W2250225488', 'https://openalex.org/W2250644439', 'https://openalex.org/W2251079237', 'https://openalex.org/W2251690405', 'https://openalex.org/W2252136820', 'https://openalex.org/W2437005631', 'https://openalex.org/W2595715041', 'https://openalex.org/W4241645538', 'https://openalex.org/W4294170691']","In this paper, with the help of knowledge base, we build and formulate a semantic space to connect the source and target languages, and apply it to the sequence-to-sequence framework to propose a Knowledge-Based Semantic Embedding (KBSE) method.In our KB-SE method, the source sentence is firstly mapped into a knowledge based semantic space, and the target sentence is generated using a recurrent neural network with the internal meaning preserved.Experiments are conducted on two translation tasks, the electric business data and movie data, and the results show that our proposed method can achieve outstanding performance, compared with both the traditional SMT methods and the existing encoder-decoder models.",1.0
SKG_MT_2,https://openalex.org/W2339995566,2016,102,"['https://openalex.org/W1591801644', 'https://openalex.org/W1753482797', 'https://openalex.org/W1855892484', 'https://openalex.org/W1860935423', 'https://openalex.org/W1902237438', 'https://openalex.org/W1938755728', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101609803', 'https://openalex.org/W2109886035', 'https://openalex.org/W2169724380', 'https://openalex.org/W2220350356', 'https://openalex.org/W2250342921', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251012068', 'https://openalex.org/W2259472270', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950752421', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964325005', 'https://openalex.org/W3204406378']","Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.",1.0
SKG_MT_5,https://openalex.org/W3034425996,2020,40,"['https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W1902237438', 'https://openalex.org/W1909320841', 'https://openalex.org/W1959608418', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2161914416', 'https://openalex.org/W2242818861', 'https://openalex.org/W2547875792', 'https://openalex.org/W2661761953', 'https://openalex.org/W2767206889', 'https://openalex.org/W2785442381', 'https://openalex.org/W2789543585', 'https://openalex.org/W2892213699', 'https://openalex.org/W2935612721', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962791537', 'https://openalex.org/W2962897886', 'https://openalex.org/W2962915948', 'https://openalex.org/W2963141266', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963773505', 'https://openalex.org/W2964026424', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970832665', 'https://openalex.org/W2981648103', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990372437', 'https://openalex.org/W2990389671', 'https://openalex.org/W2995999067', 'https://openalex.org/W3000538780', 'https://openalex.org/W3015162217', 'https://openalex.org/W3019417368', 'https://openalex.org/W3034730263', 'https://openalex.org/W3034892578', 'https://openalex.org/W3100753857', 'https://openalex.org/W3103344181', 'https://openalex.org/W4385245566']","We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.",0.9938650306748468
SKG_MT_7,https://openalex.org/W2130551284,2012,22,"['https://openalex.org/W110357785', 'https://openalex.org/W634032344', 'https://openalex.org/W1508977358', 'https://openalex.org/W1743517014', 'https://openalex.org/W2010835028', 'https://openalex.org/W2095755718', 'https://openalex.org/W2108460050', 'https://openalex.org/W2112900913', 'https://openalex.org/W2118536060', 'https://openalex.org/W2118585731', 'https://openalex.org/W2139885235', 'https://openalex.org/W2144900797', 'https://openalex.org/W2146685010', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158065314', 'https://openalex.org/W2158388102', 'https://openalex.org/W2162245945', 'https://openalex.org/W2165666205', 'https://openalex.org/W2242975712', 'https://openalex.org/W2401082558', 'https://openalex.org/W2407892396', 'https://openalex.org/W2626190081', 'https://openalex.org/W2988119488']","Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase-based SMT system.",1.0
SKG_MT_8,https://openalex.org/W3034216012,2020,40,"['https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2767206889', 'https://openalex.org/W2789543585', 'https://openalex.org/W2890501761', 'https://openalex.org/W2920538220', 'https://openalex.org/W2946375144', 'https://openalex.org/W2948197522', 'https://openalex.org/W2949644922', 'https://openalex.org/W2952356761', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962915948', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964026424', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964307104', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970832665', 'https://openalex.org/W2971167892', 'https://openalex.org/W2976965654', 'https://openalex.org/W2981648103', 'https://openalex.org/W2988536374', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990004017', 'https://openalex.org/W2990372437', 'https://openalex.org/W2990389671', 'https://openalex.org/W2996843693', 'https://openalex.org/W2996987694', 'https://openalex.org/W3106104873', 'https://openalex.org/W3175164646', 'https://openalex.org/W4298580827', 'https://openalex.org/W4385245566']","Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.",1.0
SKG_MT_14,https://openalex.org/W2251409393,2013,11,"['https://openalex.org/W222053410', 'https://openalex.org/W1570013475', 'https://openalex.org/W1915022094', 'https://openalex.org/W2004316800', 'https://openalex.org/W2006969979', 'https://openalex.org/W2015350341', 'https://openalex.org/W2017802499', 'https://openalex.org/W2038698865', 'https://openalex.org/W2045767629', 'https://openalex.org/W2067773864', 'https://openalex.org/W2115979064', 'https://openalex.org/W2123641849', 'https://openalex.org/W2151197196', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2170464899', 'https://openalex.org/W2172138510', 'https://openalex.org/W3104029765', 'https://openalex.org/W3211848854']","Current word alignment models for statistical machine translation do not address morphology beyond merely splitting words. We present a two-level alignment model that distinguishes between words and morphemes, in which we embed an IBM Model 1 inside an HMM based word alignment model. The model jointly induces word and morpheme alignments using an EM algorithm. We evaluated our model on Turkish-English parallel data. We obtained significant improvement of BLEU scores over IBM Model 4. Our results indicate that utilizing information from morphology improves the quality of word alignments.",1.0
SKG_MT_15,https://openalex.org/W2151395225,2010,8,"['https://openalex.org/W1586798152', 'https://openalex.org/W1920788708', 'https://openalex.org/W1961208191', 'https://openalex.org/W2113788796', 'https://openalex.org/W2122495161', 'https://openalex.org/W2125825154', 'https://openalex.org/W2280403519']","Factored Statistical Machine Translation ex-tends the Phrase Based SMT model by al-lowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1",1.0
SKG_MT_17,https://openalex.org/W2123034775,2010,41,"['https://openalex.org/W1544548656', 'https://openalex.org/W1574901103', 'https://openalex.org/W1594517255']","Hindi and Punjabi are closely related languages with lots of similarities in syntax and vocabulary Both Punjabi and Hindi languages have originated from Sanskrit which is one of the oldest language. In terms of speakers, Hindi is third most widely spoken language and Punjabi is twelfth most widely spoken language. Punjabi language is mostly used in the Northern India and in some areas of Pakistan as well as in UK, Canada and USA. Hindi is the national language of India and is spoken and used by the people all over the country. In the present research, Basic Hindi to Punjabi machine translation system using direct translation approach has been developed. The results of this translation system are surprisingly good. The system includes lexicon based translation, transliteration and continuously improving the system through machine learning module. It also takes care of basic word sense disambiguation.",0.8958333333333334
SKG_MT_18,https://openalex.org/W2251823395,2015,65,"['https://openalex.org/W12836875', 'https://openalex.org/W1499506821', 'https://openalex.org/W1535015163', 'https://openalex.org/W1551202288', 'https://openalex.org/W1600311170', 'https://openalex.org/W1708005048', 'https://openalex.org/W1829822087', 'https://openalex.org/W1986543644', 'https://openalex.org/W2006969979', 'https://openalex.org/W2007291639', 'https://openalex.org/W2020278455', 'https://openalex.org/W2038721957', 'https://openalex.org/W2051840895', 'https://openalex.org/W2091671846', 'https://openalex.org/W2096765155', 'https://openalex.org/W2097927681', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102258316', 'https://openalex.org/W2102258849', 'https://openalex.org/W2113641473', 'https://openalex.org/W2119314391', 'https://openalex.org/W2121465811', 'https://openalex.org/W2124204950', 'https://openalex.org/W2138203921', 'https://openalex.org/W2140343992', 'https://openalex.org/W2140584963', 'https://openalex.org/W2147389891', 'https://openalex.org/W2149837184', 'https://openalex.org/W2151170651', 'https://openalex.org/W2163274265', 'https://openalex.org/W2165666205', 'https://openalex.org/W2167393476', 'https://openalex.org/W2172678396', 'https://openalex.org/W2219047839', 'https://openalex.org/W2250375035', 'https://openalex.org/W2251071803', 'https://openalex.org/W2252123671', 'https://openalex.org/W2296308987', 'https://openalex.org/W2436001372', 'https://openalex.org/W2738031524', 'https://openalex.org/W2788296320', 'https://openalex.org/W2950186769', 'https://openalex.org/W2963720566', 'https://openalex.org/W4255710934']","We present a parser for Abstract Meaning Representation (AMR).We treat Englishto-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT).To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling.We introduce an AMR-specific language model and add data and features drawn from semantic resources.Our resulting AMR parser significantly improves upon state-of-the-art results.",1.0
SKG_MT_19,https://openalex.org/W2971257618,2019,13,"['https://openalex.org/W28136092', 'https://openalex.org/W201231365', 'https://openalex.org/W1483849869', 'https://openalex.org/W1522301498', 'https://openalex.org/W1631260214', 'https://openalex.org/W1753482797', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118972857', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2147880316', 'https://openalex.org/W2148708890', 'https://openalex.org/W2159755860', 'https://openalex.org/W2284660317', 'https://openalex.org/W2525778437', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2617566453', 'https://openalex.org/W2767989436', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902081112', 'https://openalex.org/W2902503418', 'https://openalex.org/W2903193068', 'https://openalex.org/W2903490366', 'https://openalex.org/W2908336025', 'https://openalex.org/W2940744433', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963329925', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963366196', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963631950', 'https://openalex.org/W2964008635', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W4323654151', 'https://openalex.org/W4385245566']","This paper describes the University of Sydney’s submission of the WMT 2019 shared news translation task. We participated in the Finnish->English direction and got the best BLEU(33.0) score among all the participants. Our system is based on the self-attentional Transformer networks, into which we integrated the most recent effective strategies from academic research (e.g., BPE, back translation, multi-features data selection, data augmentation, greedy model ensemble, reranking, ConMBR system combination, and postprocessing). Furthermore, we propose a novel augmentation method Cycle Translation and a data mixture strategy Big/Small parallel construction to entirely exploit the synthetic corpus. Extensive experiments show that adding the above techniques can make continuous improvements of the BLEU scores, and the best result outperforms the baseline (Transformer ensemble model trained with the original parallel corpus) by approximately 5.3 BLEU score, achieving the state-of-the-art performance.",1.0
SKG_MT_20,https://openalex.org/W2982616332,2019,9,"['https://openalex.org/W1591706642', 'https://openalex.org/W1753482797', 'https://openalex.org/W1948566616', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2154652894', 'https://openalex.org/W2250539671', 'https://openalex.org/W2257408573', 'https://openalex.org/W2739046565', 'https://openalex.org/W2759088880', 'https://openalex.org/W2760656271', 'https://openalex.org/W2786660442', 'https://openalex.org/W2805493160', 'https://openalex.org/W2893600504', 'https://openalex.org/W2903193068', 'https://openalex.org/W2938704169', 'https://openalex.org/W2962922345', 'https://openalex.org/W2962943802', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963091658', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963592583', 'https://openalex.org/W2963929190', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970752831', 'https://openalex.org/W2977587102', 'https://openalex.org/W4385245566']","This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language.",1.0
SKG_MT_21,https://openalex.org/W2251843378,2013,56,"['https://openalex.org/W176283576', 'https://openalex.org/W331019419', 'https://openalex.org/W1508977358', 'https://openalex.org/W1510052640', 'https://openalex.org/W1551202288', 'https://openalex.org/W1632114991', 'https://openalex.org/W1865928303', 'https://openalex.org/W1955251501', 'https://openalex.org/W1969974515', 'https://openalex.org/W1986543644', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008961349', 'https://openalex.org/W2027979924', 'https://openalex.org/W2038698865', 'https://openalex.org/W2044804339', 'https://openalex.org/W2096291962', 'https://openalex.org/W2097333193', 'https://openalex.org/W2099224638', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105103433', 'https://openalex.org/W2108460050', 'https://openalex.org/W2108740414', 'https://openalex.org/W2114887620', 'https://openalex.org/W2116316001', 'https://openalex.org/W2118692229', 'https://openalex.org/W2119168550', 'https://openalex.org/W2128634885', 'https://openalex.org/W2130551284', 'https://openalex.org/W2139907445', 'https://openalex.org/W2143995218', 'https://openalex.org/W2144900797', 'https://openalex.org/W2144995019', 'https://openalex.org/W2146685010', 'https://openalex.org/W2147880316', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158065314', 'https://openalex.org/W2159107349', 'https://openalex.org/W2159358338', 'https://openalex.org/W2161795601', 'https://openalex.org/W2162245945', 'https://openalex.org/W2175969983', 'https://openalex.org/W2242975712', 'https://openalex.org/W2569308312', 'https://openalex.org/W3203149905']","We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from di erent families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.",1.0
SKG_MT_22,https://openalex.org/W2148429252,2013,20,"['https://openalex.org/W60645813', 'https://openalex.org/W1975809876', 'https://openalex.org/W2006969979', 'https://openalex.org/W2013029404', 'https://openalex.org/W2035363410', 'https://openalex.org/W2036089660', 'https://openalex.org/W2056260421', 'https://openalex.org/W2065240770', 'https://openalex.org/W2075665712', 'https://openalex.org/W2080370442', 'https://openalex.org/W2097897435', 'https://openalex.org/W2102394389', 'https://openalex.org/W2105051853', 'https://openalex.org/W2129251351', 'https://openalex.org/W2135029798', 'https://openalex.org/W2135955143', 'https://openalex.org/W2136542423', 'https://openalex.org/W2140575992', 'https://openalex.org/W2147308966', 'https://openalex.org/W2151429169', 'https://openalex.org/W2162355876', 'https://openalex.org/W2163199865', 'https://openalex.org/W2166959257', 'https://openalex.org/W2170120409', 'https://openalex.org/W2252057914', 'https://openalex.org/W2282146481', 'https://openalex.org/W3143596294']","Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issue. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization. Experiments conducted on a real CQA data show that our proposed approach is promising. 1",1.0
SKG_MT_24,https://openalex.org/W2250660307,2015,47,"['https://openalex.org/W30655990', 'https://openalex.org/W36903255', 'https://openalex.org/W179875071', 'https://openalex.org/W932413789', 'https://openalex.org/W1575384945', 'https://openalex.org/W1665214252', 'https://openalex.org/W1753482797', 'https://openalex.org/W2072128103', 'https://openalex.org/W2083545877', 'https://openalex.org/W2091812280', 'https://openalex.org/W2100664567', 'https://openalex.org/W2118434577', 'https://openalex.org/W2120861206', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131462252', 'https://openalex.org/W2133564696', 'https://openalex.org/W2138204974', 'https://openalex.org/W2163605009', 'https://openalex.org/W2171421863', 'https://openalex.org/W2171928131', 'https://openalex.org/W2184045248', 'https://openalex.org/W2250379827', 'https://openalex.org/W2250612599', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251682575', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998704965', 'https://openalex.org/W4231109964', 'https://openalex.org/W4285719527']","Neural language models (NLMs) have been able to improve machine translation (MT) thanks to their ability to generalize well to long contexts.Despite recent successes of deep neural networks in speech and vision, the general practice in MT is to incorporate NLMs with only one or two hidden layers and there have not been clear results on whether having more layers helps.In this paper, we demonstrate that deep NLMs with three or four layers outperform those with fewer layers in terms of both the perplexity and the translation quality.We combine various techniques to successfully train deep NLMs that jointly condition on both the source and target contexts.When reranking nbest lists of a strong web-forum baseline, our deep models yield an average boost of 0.5 TER / 0.5 BLEU points compared to using a shallow NLM.Additionally, we adapt our models to a new sms-chat domain and obtain a similar gain of 1.0 TER / 0.5 BLEU points. 1",1.0
SKG_MT_25,https://openalex.org/W2759074532,2017,1,"['https://openalex.org/W168564468', 'https://openalex.org/W630532510', 'https://openalex.org/W1631260214', 'https://openalex.org/W1680392829', 'https://openalex.org/W1803332767', 'https://openalex.org/W1905522558', 'https://openalex.org/W2082058602', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101234009', 'https://openalex.org/W2117278770', 'https://openalex.org/W2123318312', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131744502', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153579005', 'https://openalex.org/W2156985047', 'https://openalex.org/W2161192091', 'https://openalex.org/W2250851379', 'https://openalex.org/W2251590347', 'https://openalex.org/W2291367281', 'https://openalex.org/W2356613612', 'https://openalex.org/W2408503330', 'https://openalex.org/W2511717148', 'https://openalex.org/W2512924740', 'https://openalex.org/W2515631395', 'https://openalex.org/W2595715041', 'https://openalex.org/W2758685863', 'https://openalex.org/W4294170691']",We present in this paper the participation of the University of Hamburg in the Biomedical Translation Task of the Second Conference on Machine Translation (WMT 2017).Our contribution lies in adopting a new direction for performing data selection for Machine Translation via Paragraph Vector and a Feed Forward Neural Network Classifier.Continuous distributed vector representations of the sentences are used as features for the binary classifier.Most approaches in data selection rely on scoring and ranking general domain sentences with respect to their similarity to the in-domain and setting a range of thresholds for selecting a percentage of them for training various MT systems.The novelty of our method consists in developing an automatic threshold detection paradigm for data selection which provides an efficient and simple way for selecting the most similar sentences to the in-domain.Encouraging results are obtained using this approach for seven language pairs and four data sets.,1.0
SKG_MT_28,https://openalex.org/W3088264195,2020,0,"['https://openalex.org/W124343944', 'https://openalex.org/W1172683740', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2133459682', 'https://openalex.org/W2148365102', 'https://openalex.org/W2149327368', 'https://openalex.org/W2163361328', 'https://openalex.org/W2250484373', 'https://openalex.org/W2250821182', 'https://openalex.org/W2291367281', 'https://openalex.org/W2471690557', 'https://openalex.org/W2583923862', 'https://openalex.org/W2760738985', 'https://openalex.org/W2890771450', 'https://openalex.org/W2894218541', 'https://openalex.org/W2953072129', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963366552', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971120958', 'https://openalex.org/W2984354699', 'https://openalex.org/W2987249037', 'https://openalex.org/W3016234399', 'https://openalex.org/W3035408261', 'https://openalex.org/W3035459196', 'https://openalex.org/W3106146701']","We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the recall of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than BLEU. To foster further research, we release a dataset containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data.",0.9902912621359224
SKG_MT_29,https://openalex.org/W2801060378,2018,5,[],"Character-level Neural Machine Translation (NMT) models have recently achieved impressive results on many language pairs. They particularly do well for Indo-European language pairs, where the languages share the same writing system. However, for translating between Chinese and English, the gap between the two different writing systems poses a major challenge due to a lack of systematic correspondence between the individual linguistic units. In this paper, we enable character-level NMT for Chinese, by breaking down Chinese characters to linguistic units similar to that of Indo-European languages using the Wubi encoding scheme. We show promising results from training Wubi-based models on the subword- and character-level with recurrent as well as convolutional models.",0.9295774647887324
SKG_MT_30,https://openalex.org/W2913659301,2019,92,"['https://openalex.org/W170711724', 'https://openalex.org/W630532510', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2163361328', 'https://openalex.org/W2170716095', 'https://openalex.org/W2250653840', 'https://openalex.org/W2252166243', 'https://openalex.org/W2493916176', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2573062194', 'https://openalex.org/W2597891111', 'https://openalex.org/W2611838487', 'https://openalex.org/W2626778328', 'https://openalex.org/W2741602058', 'https://openalex.org/W2760656271', 'https://openalex.org/W2773493195', 'https://openalex.org/W2790319220', 'https://openalex.org/W2794365787', 'https://openalex.org/W2806412155', 'https://openalex.org/W2887920589', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890007195', 'https://openalex.org/W2905749056', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2950359962', 'https://openalex.org/W2950855294', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963174344', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970686691']","For machine translation, a vast majority of language pairs in the world are considered low-resource because they have little parallel data available. Besides the technical challenges of learning with limited supervision, it is difficult to evaluate methods trained on low-resource language pairs because of the lack of freely and publicly available benchmarks. In this work, we introduce the FLoRes evaluation datasets for Nepali-English and Sinhala-English, based on sentences translated from Wikipedia. Compared to English, these are languages with very different morphology and syntax, for which little out-of-domain parallel data is available and for which relatively large amounts of monolingual data are freely available. We describe our process to collect and cross-check the quality of translations, and we report baseline performance using several learning settings: fully supervised, weakly supervised, semi-supervised, and fully unsupervised. Our experiments demonstrate that current state-of-the-art methods perform rather poorly on this benchmark, posing a challenge to the research community working on low-resource MT. Data and code to reproduce our experiments are available at https://github.com/facebookresearch/flores.",0.9951219512195122
SKG_MT_31,https://openalex.org/W2539350388,2017,101,"['https://openalex.org/W2476580', 'https://openalex.org/W9292421', 'https://openalex.org/W22168010', 'https://openalex.org/W629278631', 'https://openalex.org/W1779279021', 'https://openalex.org/W2006832571', 'https://openalex.org/W2025403586', 'https://openalex.org/W2057301382', 'https://openalex.org/W2080133951', 'https://openalex.org/W2095674846', 'https://openalex.org/W2097927681', 'https://openalex.org/W2098447295', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103577799', 'https://openalex.org/W2110302976', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124807415', 'https://openalex.org/W2129734311', 'https://openalex.org/W2133990480', 'https://openalex.org/W2134800885', 'https://openalex.org/W2139875525', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148959489', 'https://openalex.org/W2151040995', 'https://openalex.org/W2184135559', 'https://openalex.org/W2232536453', 'https://openalex.org/W2250310654', 'https://openalex.org/W2250958040', 'https://openalex.org/W2250969425', 'https://openalex.org/W2251180427', 'https://openalex.org/W2251263615', 'https://openalex.org/W2251265976', 'https://openalex.org/W2251409655', 'https://openalex.org/W2252241921', 'https://openalex.org/W2292999442', 'https://openalex.org/W2506231060', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950186769', 'https://openalex.org/W3015810608', 'https://openalex.org/W3099514962', 'https://openalex.org/W3186536529', 'https://openalex.org/W4251136000', 'https://openalex.org/W4285719527', 'https://openalex.org/W4292023222']","Ella Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia Specia, Shuly Wintner. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. 2017.",0.9924812030075189
SKG_MT_34,https://openalex.org/W2970069811,2019,3,"['https://openalex.org/W22168010', 'https://openalex.org/W630532510', 'https://openalex.org/W1830628992', 'https://openalex.org/W2021581601', 'https://openalex.org/W2068297964', 'https://openalex.org/W2101105183', 'https://openalex.org/W2132532452', 'https://openalex.org/W2153252192', 'https://openalex.org/W2250539671', 'https://openalex.org/W2294724295', 'https://openalex.org/W2539671052', 'https://openalex.org/W2610935556', 'https://openalex.org/W2613904329', 'https://openalex.org/W2803620078', 'https://openalex.org/W2921990970', 'https://openalex.org/W2922386288', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962992635', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963617771', 'https://openalex.org/W2964265128', 'https://openalex.org/W4252060112', 'https://openalex.org/W4299606006']","Constantine Lignos, Daniel Cohen, Yen-Chieh Lien, Pratik Mehta, W. Bruce Croft, Scott Miller. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_35,https://openalex.org/W2983981554,2019,61,"['https://openalex.org/W2286365479', 'https://openalex.org/W2525778437', 'https://openalex.org/W2751185861', 'https://openalex.org/W2805493160', 'https://openalex.org/W2866343820', 'https://openalex.org/W2885047103', 'https://openalex.org/W2889326796', 'https://openalex.org/W2906987001', 'https://openalex.org/W2947946877', 'https://openalex.org/W2962906592', 'https://openalex.org/W2963174729', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964343359', 'https://openalex.org/W2972361727', 'https://openalex.org/W2987188351', 'https://openalex.org/W4288108164', 'https://openalex.org/W4288337707', 'https://openalex.org/W4289298591', 'https://openalex.org/W4385245566']","Young Jin Kim, Marcin Junczys-Dowmunt, Hany Hassan, Alham Fikri Aji, Kenneth Heafield, Roman Grundkiewicz, Nikolay Bogoychev. Proceedings of the 3rd Workshop on Neural Generation and Translation. 2019.",0.9937888198757764
SKG_MT_36,https://openalex.org/W3094040156,2020,11,"['https://openalex.org/W49707343', 'https://openalex.org/W1916559533', 'https://openalex.org/W1982710018', 'https://openalex.org/W1983969726', 'https://openalex.org/W2044916741', 'https://openalex.org/W2077779631', 'https://openalex.org/W2086039194', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109776855', 'https://openalex.org/W2110481933', 'https://openalex.org/W2123301721', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148365102', 'https://openalex.org/W2149327368', 'https://openalex.org/W2164628107', 'https://openalex.org/W2171421863', 'https://openalex.org/W2507833193', 'https://openalex.org/W2552839021', 'https://openalex.org/W2727767747', 'https://openalex.org/W2741049976', 'https://openalex.org/W2758895583', 'https://openalex.org/W2798362442', 'https://openalex.org/W2903193068', 'https://openalex.org/W2922158773', 'https://openalex.org/W2928941594', 'https://openalex.org/W2952103439', 'https://openalex.org/W2952481978', 'https://openalex.org/W2962890089', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964308564', 'https://openalex.org/W2967576208', 'https://openalex.org/W2970038984', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971043182', 'https://openalex.org/W2980367889', 'https://openalex.org/W2985165968', 'https://openalex.org/W2996403597', 'https://openalex.org/W3025111163', 'https://openalex.org/W3035013535', 'https://openalex.org/W3103450644', 'https://openalex.org/W3106146701', 'https://openalex.org/W3196290596']","Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by Freitag et al. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.",1.0
SKG_MT_37,https://openalex.org/W3034553593,2020,4,"['https://openalex.org/W1902237438', 'https://openalex.org/W1916559533', 'https://openalex.org/W2095577376', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2194775991', 'https://openalex.org/W2613904329', 'https://openalex.org/W2669742347', 'https://openalex.org/W2768763386', 'https://openalex.org/W2803369080', 'https://openalex.org/W2803437449', 'https://openalex.org/W2886845974', 'https://openalex.org/W2890244613', 'https://openalex.org/W2890964657', 'https://openalex.org/W2952682849', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962811598', 'https://openalex.org/W2962834107', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963598809', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964308564', 'https://openalex.org/W3008781785', 'https://openalex.org/W3211848854', 'https://openalex.org/W4241645538', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394643672', 'https://openalex.org/W4394666973']","Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.",1.0
SKG_MT_38,https://openalex.org/W2741401344,2017,1,"['https://openalex.org/W1736600331', 'https://openalex.org/W1828578481', 'https://openalex.org/W1969974515', 'https://openalex.org/W2087735403', 'https://openalex.org/W2095755718', 'https://openalex.org/W2115081467', 'https://openalex.org/W2119168550', 'https://openalex.org/W2122373020', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133768083', 'https://openalex.org/W2134495021', 'https://openalex.org/W2134800885', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150180307', 'https://openalex.org/W2152423400', 'https://openalex.org/W2153653739', 'https://openalex.org/W2169218969', 'https://openalex.org/W2250319048', 'https://openalex.org/W2250907725', 'https://openalex.org/W2251192971', 'https://openalex.org/W2251832051', 'https://openalex.org/W2437005631', 'https://openalex.org/W2595715041', 'https://openalex.org/W3185893030', 'https://openalex.org/W4241645538', 'https://openalex.org/W4254408171', 'https://openalex.org/W4376522561']","Phrase-based and hierarchical phrase-based (Hiero) translation models differ radically in the way reordering is modeled. Lexicalized reordering models play an important role in phrase-based MT and such models have been added to CKY-based decoders for Hiero. Watanabe et al. (2006) proposed a promising decoding algorithm for Hiero (LR-Hiero) that visits input spans in arbitrary order and produces the translation in left to right (LR) order which leads to far fewer language model calls and leads to a considerable speedup in decoding. We introduce a novel shift-reduce algorithm to LR-Hiero to decode with our lexicalized reordering model (LRM) and show that it improves translation quality for Czech-English, Chinese-English and German-English.",1.0
SKG_MT_41,https://openalex.org/W3045734701,2020,3,"['https://openalex.org/W630532510', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107695330', 'https://openalex.org/W2114912785', 'https://openalex.org/W2123301721', 'https://openalex.org/W2134800885', 'https://openalex.org/W2159755860', 'https://openalex.org/W2222235228', 'https://openalex.org/W2250484373', 'https://openalex.org/W2353655624', 'https://openalex.org/W2467834614', 'https://openalex.org/W2539350388', 'https://openalex.org/W2788570750', 'https://openalex.org/W2794365787', 'https://openalex.org/W2807895655', 'https://openalex.org/W2890969459', 'https://openalex.org/W2949938546', 'https://openalex.org/W2950513705', 'https://openalex.org/W2951883832', 'https://openalex.org/W2962731009', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962907349', 'https://openalex.org/W2963104691', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963950336', 'https://openalex.org/W2964022663', 'https://openalex.org/W2970328625', 'https://openalex.org/W3045622142']","This paper describes the University of Maryland’s submission to the Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Unlike the standard machine translation task, STAPLE requires generating a set of outputs for a given input sequence, aiming to cover the space of translations produced by language learners. We adapt neural machine translation models to this requirement by (a) generating n-best translation hypotheses from a model fine-tuned on learner translations, oversampled to reflect the distribution of learner responses, and (b) filtering hypotheses using a feature-rich binary classifier that directly optimizes a close approximation of the official evaluation metric. Combination of systems that use these two strategies achieves F1 scores of 53.9% and 52.5% on Vietnamese and Portuguese, respectively ranking 2nd and 4th on the leaderboard.",1.0
SKG_MT_42,https://openalex.org/W3034579764,2020,23,"['https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2739309726', 'https://openalex.org/W2767206889', 'https://openalex.org/W2889326796', 'https://openalex.org/W2892213699', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962915948', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970832665', 'https://openalex.org/W2976965654', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990372437', 'https://openalex.org/W2996843693', 'https://openalex.org/W3008618223', 'https://openalex.org/W4289293986', 'https://openalex.org/W4385245566']","Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model’s performance, with the goal of transferring the AR model’s generalization ability while preventing overfitting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model’s performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.",1.0
SKG_MT_43,https://openalex.org/W2928720664,2019,7,"['https://openalex.org/W648786980', 'https://openalex.org/W1902237438', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153579005', 'https://openalex.org/W2155806188', 'https://openalex.org/W2157331557', 'https://openalex.org/W2176263492', 'https://openalex.org/W2250539671', 'https://openalex.org/W2410217169', 'https://openalex.org/W2469296930', 'https://openalex.org/W2493916176', 'https://openalex.org/W2525778437', 'https://openalex.org/W2532807140', 'https://openalex.org/W2552839021', 'https://openalex.org/W2574872930', 'https://openalex.org/W2883616626', 'https://openalex.org/W2950874967', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962896399', 'https://openalex.org/W2963011474', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3098341425']","Regularization of neural machine translation is still a significant problem, especially in low-resource settings. To mollify this problem, we propose regressing word embeddings (ReWE) as a new regularization technique in a system that is jointly trained to predict the next word in the translation (categorical value) and its word embedding (continuous value). Such a joint training allows the proposed system to learn the distributional properties represented by the word embeddings, empirically improving the generalization to unseen sentences. Experiments over three translation datasets have showed a consistent improvement over a strong baseline, ranging between 0.91 and 2.54 BLEU points, and also a marked improvement over a state-of-the-art system.",0.9943502824858758
SKG_MT_44,https://openalex.org/W1595109586,2013,20,"['https://openalex.org/W91928571', 'https://openalex.org/W179314280', 'https://openalex.org/W232191560', 'https://openalex.org/W1551202288', 'https://openalex.org/W1606508130', 'https://openalex.org/W1916559533', 'https://openalex.org/W1971987737', 'https://openalex.org/W2006969979', 'https://openalex.org/W2030760474', 'https://openalex.org/W2070740689', 'https://openalex.org/W2077723394', 'https://openalex.org/W2078058974', 'https://openalex.org/W2096557251', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107370612', 'https://openalex.org/W2111142112', 'https://openalex.org/W2118206599', 'https://openalex.org/W2118960083', 'https://openalex.org/W2119168550', 'https://openalex.org/W2123635983', 'https://openalex.org/W2123825474', 'https://openalex.org/W2126610017', 'https://openalex.org/W2139113820', 'https://openalex.org/W2140343992', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147880316', 'https://openalex.org/W2148675933', 'https://openalex.org/W2148861208', 'https://openalex.org/W2152263452', 'https://openalex.org/W2152311128', 'https://openalex.org/W2153653739', 'https://openalex.org/W2160697141', 'https://openalex.org/W2161792612', 'https://openalex.org/W2163548102', 'https://openalex.org/W2165132531', 'https://openalex.org/W2180952760', 'https://openalex.org/W2621423192']","This paper presents a general, statistical framework for modeling phrase translation via Markov random fields. The model allows for arbituary features extracted from a phrase pair to be incorporated as evidence. The parameters of the model are estimated using a large-scale discriminative training approach that is based on stochastic gradient ascent and an N-best list based expected BLEU as the objective function. The model is easy to be incoporated into a standard phrase-based statistical machine translation system, requiring no code change in the runtime engine. Evaluation is performed on two Europarl translation tasks, German-English and French-English. Results show that incoporating the Markov random field model significantly improves the performance of a state-of-the-art phrase-based machine translation system, leading to a gain of 0.8-1.3 BLEU points. 1",1.0
SKG_MT_48,https://openalex.org/W2180554602,2015,9,"['https://openalex.org/W99956235', 'https://openalex.org/W1631260214', 'https://openalex.org/W1964357740', 'https://openalex.org/W1975133852', 'https://openalex.org/W2008652694', 'https://openalex.org/W2060786818', 'https://openalex.org/W2104511424', 'https://openalex.org/W2110503409', 'https://openalex.org/W2115081467', 'https://openalex.org/W2126747756', 'https://openalex.org/W2133512280', 'https://openalex.org/W2138302120', 'https://openalex.org/W2143426320', 'https://openalex.org/W2149327368', 'https://openalex.org/W2160001241', 'https://openalex.org/W2182468807', 'https://openalex.org/W2251994258', 'https://openalex.org/W2595715041']","We use referential translation machines (RTMs) for predicting translation performance.RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource.We improve our RTM models with the ParFDA instance selection model (Bic ¸ici et al., 2015), with additional features for predicting the translation performance, and with improved learning models.We develop RTM models for each WMT15 QET (QET15) subtask and obtain improvements over QET14 results.RTMs achieve top performance in QET15 ranking 1st in document-and sentence-level prediction tasks and 2nd in word-level prediction task.",1.0
SKG_MT_50,https://openalex.org/W2970084653,2019,28,"['https://openalex.org/W1514535095', 'https://openalex.org/W1895577753', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2176263492', 'https://openalex.org/W2178654303', 'https://openalex.org/W2251766657', 'https://openalex.org/W2251955814', 'https://openalex.org/W2529548870', 'https://openalex.org/W2574872930', 'https://openalex.org/W2757222607', 'https://openalex.org/W2930211575', 'https://openalex.org/W2951456627', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824709', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963382396', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963729263', 'https://openalex.org/W2964078338', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970074184', 'https://openalex.org/W4385245566']","Renjie Zheng, Mingbo Ma, Baigong Zheng, Liang Huang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_51,https://openalex.org/W3034201598,2020,78,"['https://openalex.org/W179875071', 'https://openalex.org/W222053410', 'https://openalex.org/W1567512734', 'https://openalex.org/W1581407678', 'https://openalex.org/W2007347635', 'https://openalex.org/W2097333193', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132984949', 'https://openalex.org/W2134800885', 'https://openalex.org/W2146006411', 'https://openalex.org/W2149801996', 'https://openalex.org/W2151834591', 'https://openalex.org/W2256388387', 'https://openalex.org/W2296073425', 'https://openalex.org/W2554600978', 'https://openalex.org/W2727300753', 'https://openalex.org/W2741838462', 'https://openalex.org/W2794365787', 'https://openalex.org/W2796108585', 'https://openalex.org/W2808064329', 'https://openalex.org/W2887842788', 'https://openalex.org/W2898846200', 'https://openalex.org/W2923622379', 'https://openalex.org/W2946379889', 'https://openalex.org/W2949920209', 'https://openalex.org/W2950517871', 'https://openalex.org/W2951563833', 'https://openalex.org/W2951977278', 'https://openalex.org/W2952474700', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962800562', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963675284', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964030506', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964059111', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971302374', 'https://openalex.org/W2974078243', 'https://openalex.org/W2997287044', 'https://openalex.org/W2998681865', 'https://openalex.org/W3010512657', 'https://openalex.org/W3034938700']","Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",1.0
SKG_MT_53,https://openalex.org/W2757521750,2017,19,"['https://openalex.org/W38126138', 'https://openalex.org/W149643677', 'https://openalex.org/W168564468', 'https://openalex.org/W342285082', 'https://openalex.org/W1542713999', 'https://openalex.org/W1828724394', 'https://openalex.org/W1852412531', 'https://openalex.org/W2054141820', 'https://openalex.org/W2072976288', 'https://openalex.org/W2102982709', 'https://openalex.org/W2118090838', 'https://openalex.org/W2119013622', 'https://openalex.org/W2120143966', 'https://openalex.org/W2124033848', 'https://openalex.org/W2126725946', 'https://openalex.org/W2135399222', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140310134', 'https://openalex.org/W2145685230', 'https://openalex.org/W2148429252', 'https://openalex.org/W2153579005', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158199200', 'https://openalex.org/W2172268343', 'https://openalex.org/W2182894058', 'https://openalex.org/W2229725139', 'https://openalex.org/W2250229103', 'https://openalex.org/W2250385423', 'https://openalex.org/W2250976127', 'https://openalex.org/W2251033195', 'https://openalex.org/W2251765408', 'https://openalex.org/W2252121005', 'https://openalex.org/W2252212383', 'https://openalex.org/W2295584157', 'https://openalex.org/W2296268288', 'https://openalex.org/W2425903306', 'https://openalex.org/W2508069829', 'https://openalex.org/W2518268700', 'https://openalex.org/W2525778437', 'https://openalex.org/W2560014971', 'https://openalex.org/W2952037945', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962795068', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963655167', 'https://openalex.org/W2964224278', 'https://openalex.org/W4241645538', 'https://openalex.org/W4244373129', 'https://openalex.org/W4294170691']","Bilingual Lexicon Induction is the task of learning word translations without bilingual parallel corpora. We model this task as a matrix completion problem, and present an effective and extendable framework for completing the matrix. This method harnesses diverse bilingual and monolingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages.",1.0
SKG_MT_54,https://openalex.org/W3035669818,2020,13,"['https://openalex.org/W1604574761', 'https://openalex.org/W1986569439', 'https://openalex.org/W2016856586', 'https://openalex.org/W2250332179', 'https://openalex.org/W2251728420', 'https://openalex.org/W2401109438', 'https://openalex.org/W2586524454', 'https://openalex.org/W2739868283', 'https://openalex.org/W2757041753', 'https://openalex.org/W2778814079', 'https://openalex.org/W2805490244', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963225662', 'https://openalex.org/W2963547384', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2964053711', 'https://openalex.org/W3082674894', 'https://openalex.org/W3104696873']","This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology.Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation.The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.",1.0
SKG_MT_58,https://openalex.org/W2126784652,2011,10,"['https://openalex.org/W1965660534', 'https://openalex.org/W2028162617', 'https://openalex.org/W2062250042', 'https://openalex.org/W2069931267', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107469355', 'https://openalex.org/W2115526192', 'https://openalex.org/W2121404172', 'https://openalex.org/W2123007266', 'https://openalex.org/W2126270798', 'https://openalex.org/W2131932654', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153029569', 'https://openalex.org/W2161227214', 'https://openalex.org/W2161792612', 'https://openalex.org/W2437005631']","We present a novel approach for extracting a minimal synchronous context-free grammar (SCFG) for Hiero-style statistical machine translation using a non-parametric Bayesian framework. Our approach is designed to extract rules that are licensed by the word alignments and heuristically extracted phrase pairs. Our Bayesian model limits the number of SCFG rules extracted, by sampling from the space of all possible hierarchical rules; additionally our informed prior based on the lexical alignment probabilities biases the grammar to extract high quality rules leading to improved generalization and the automatic identification of commonly re-used rules. We show that our Bayesian model is able to extract minimal set of hierarchical phrase rules without impacting the translation quality as measured by the BLEU score. 1",1.0
SKG_MT_60,https://openalex.org/W1605282883,2019,43,"['https://openalex.org/W4629839', 'https://openalex.org/W54534146', 'https://openalex.org/W1994581546', 'https://openalex.org/W2017802499', 'https://openalex.org/W2053306448', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104812589', 'https://openalex.org/W2105850947', 'https://openalex.org/W2111666304', 'https://openalex.org/W2113788796', 'https://openalex.org/W2115081467', 'https://openalex.org/W2117642127', 'https://openalex.org/W2119168550', 'https://openalex.org/W2123675427', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126241965', 'https://openalex.org/W2133888301', 'https://openalex.org/W2134285054', 'https://openalex.org/W2143134347', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2160538511', 'https://openalex.org/W2162245945', 'https://openalex.org/W2164529863', 'https://openalex.org/W2164817004', 'https://openalex.org/W2169242480', 'https://openalex.org/W2170464899']","We propose a language-independent approach for improving statistical machine translation for morphologically rich languages using a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process. Our model extends the classic phrase-based model by means of (1) word boundary-aware morpheme-level phrase extraction, (2) minimum error-rate training for a morpheme-level translation model using word-level BLEU, and (3) joint scoring with morpheme- and word-level language models. Further improvements are achieved by combining our model with the classic one. The evaluation on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments.",1.0
SKG_MT_61,https://openalex.org/W2123007266,2010,13,"['https://openalex.org/W1916559533', 'https://openalex.org/W1965660534', 'https://openalex.org/W2000394794', 'https://openalex.org/W2006969979', 'https://openalex.org/W2040781285', 'https://openalex.org/W2099534719', 'https://openalex.org/W2100238596', 'https://openalex.org/W2109664771', 'https://openalex.org/W2114434620', 'https://openalex.org/W2114550122', 'https://openalex.org/W2119005844', 'https://openalex.org/W2121404172', 'https://openalex.org/W2124818604', 'https://openalex.org/W2126270798', 'https://openalex.org/W2131932654', 'https://openalex.org/W2132634974', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159506050', 'https://openalex.org/W2160245618', 'https://openalex.org/W2160474663', 'https://openalex.org/W2161729680', 'https://openalex.org/W2161792612', 'https://openalex.org/W2166905217', 'https://openalex.org/W2188472272', 'https://openalex.org/W2437005631']","We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignment model. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteriors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices. 1",1.0
SKG_MT_62,https://openalex.org/W2573119710,2017,110,"['https://openalex.org/W222053410', 'https://openalex.org/W1902237438', 'https://openalex.org/W1916559533', 'https://openalex.org/W1924770834', 'https://openalex.org/W1938755728', 'https://openalex.org/W1996456980', 'https://openalex.org/W2052203456', 'https://openalex.org/W2064675550', 'https://openalex.org/W2080373976', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116792345', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146502635', 'https://openalex.org/W2149327368', 'https://openalex.org/W2168360976', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251682575', 'https://openalex.org/W2260677151', 'https://openalex.org/W2292633562', 'https://openalex.org/W2345764277', 'https://openalex.org/W2400065810', 'https://openalex.org/W2508117065', 'https://openalex.org/W2508907594', 'https://openalex.org/W2512924740', 'https://openalex.org/W2513202451', 'https://openalex.org/W2518037268', 'https://openalex.org/W2917324689', 'https://openalex.org/W2951559648', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963641561', 'https://openalex.org/W2964308564', 'https://openalex.org/W3102349420', 'https://openalex.org/W3113525842', 'https://openalex.org/W3211848854']","We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.",1.0
SKG_MT_64,https://openalex.org/W2889404673,2018,54,"['https://openalex.org/W108437174', 'https://openalex.org/W1508977358', 'https://openalex.org/W1551202288', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2006019948', 'https://openalex.org/W2064675550', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2139621418', 'https://openalex.org/W2165666205', 'https://openalex.org/W2250841445', 'https://openalex.org/W2250861254', 'https://openalex.org/W2563574619', 'https://openalex.org/W2594047108', 'https://openalex.org/W2737638662', 'https://openalex.org/W2739894144', 'https://openalex.org/W2751262944', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963617989', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963888305', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964308564']","Recent advances in Neural Machine Translation (NMT) show that adding syntactic information to NMT systems can improve the quality of their translations. Most existing work utilizes some specific types of linguistically-inspired tree structures, like constituency and dependency parse trees. This is often done via a standard RNN decoder that operates on a linearized target tree structure. However, it is an open question of what specific linguistic formalism, if any, is the best structural representation for NMT. In this paper, we (1) propose an NMT model that can naturally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.",1.0
SKG_MT_65,https://openalex.org/W3100509098,2020,19,"['https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2087388117', 'https://openalex.org/W2101105183', 'https://openalex.org/W2171671120', 'https://openalex.org/W2546938941', 'https://openalex.org/W2613904329', 'https://openalex.org/W2785787385', 'https://openalex.org/W2886095922', 'https://openalex.org/W2888808532', 'https://openalex.org/W2898846200', 'https://openalex.org/W2899181341', 'https://openalex.org/W2908336025', 'https://openalex.org/W2914120296', 'https://openalex.org/W2923622379', 'https://openalex.org/W2950485982', 'https://openalex.org/W2950810106', 'https://openalex.org/W2951065878', 'https://openalex.org/W2952474700', 'https://openalex.org/W2952614664', 'https://openalex.org/W2952702888', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962890089', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963218093', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963423556', 'https://openalex.org/W2963505445', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963633674', 'https://openalex.org/W2963742748', 'https://openalex.org/W2963949210', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964282813', 'https://openalex.org/W2970043232', 'https://openalex.org/W2970520743', 'https://openalex.org/W2988121211', 'https://openalex.org/W3034201598', 'https://openalex.org/W3034938700', 'https://openalex.org/W3084906606', 'https://openalex.org/W3101889167', 'https://openalex.org/W3103169714', 'https://openalex.org/W4298393544', 'https://openalex.org/W4322012790', 'https://openalex.org/W4322800759', 'https://openalex.org/W4385245566']","Active learning is an efficient approach for mitigating data dependency when training neural machine translation (NMT) models. In this paper, we explore new training frameworks by incorporating active learning into various techniques such as transfer learning and iterative back-translation (IBT) under a limited human translation budget. We design a word frequency based acquisition function and combine it with a strong uncertainty based method. The combined method steadily outperforms all other acquisition functions in various scenarios. As far as we know, we are the first to do a large-scale study on actively training Transformer for NMT. Specifically, with a human translation budget of only 20% of the original parallel corpus, we manage to surpass Transformer trained on the entire parallel corpus in three language pairs.",1.0
SKG_MT_66,https://openalex.org/W2758162718,2017,11,"['https://openalex.org/W1614298861', 'https://openalex.org/W1753482797', 'https://openalex.org/W1821462560', 'https://openalex.org/W2009303086', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114508814', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143177362', 'https://openalex.org/W2148708890', 'https://openalex.org/W2156737235', 'https://openalex.org/W2167215970', 'https://openalex.org/W2172166488', 'https://openalex.org/W2194775991', 'https://openalex.org/W2220350356', 'https://openalex.org/W2402144811', 'https://openalex.org/W2525778437', 'https://openalex.org/W2529089661', 'https://openalex.org/W2531207078', 'https://openalex.org/W2540404261', 'https://openalex.org/W2546915671', 'https://openalex.org/W2552839021', 'https://openalex.org/W2587694128', 'https://openalex.org/W2950248853', 'https://openalex.org/W2951900777', 'https://openalex.org/W2953384591', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963000224', 'https://openalex.org/W2963141266', 'https://openalex.org/W2963643655', 'https://openalex.org/W2963674932', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963937700', 'https://openalex.org/W2964228333', 'https://openalex.org/W2964299589', 'https://openalex.org/W2964308564', 'https://openalex.org/W3102516861', 'https://openalex.org/W3196603613', 'https://openalex.org/W4297790889', 'https://openalex.org/W4297813615', 'https://openalex.org/W4298159529', 'https://openalex.org/W4301368689']","Neural Machine Translation (NMT) lays intensive burden on computation and memory cost. It is a challenge to deploy NMT models on the devices with limited computation and memory budgets. This paper presents a four stage pipeline to compress model and speed up the decoding for NMT. Our method first introduces a compact architecture based on convolutional encoder and weight shared embeddings. Then weight pruning is applied to obtain a sparse model. Next, we propose a fast sequence interpolation approach which enables the greedy decoding to achieve performance on par with the beam search. Hence, the time-consuming beam search can be replaced by simple greedy decoding. Finally, vocabulary selection is used to reduce the computation of softmax layer. Our final model achieves 10 times speedup, 17 times parameters reduction, less than 35MB storage size and comparable performance compared to the baseline model.",1.0
SKG_MT_67,https://openalex.org/W2971031401,2019,6,"['https://openalex.org/W1753482797', 'https://openalex.org/W2041532239', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134800885', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2512848817', 'https://openalex.org/W2595715041', 'https://openalex.org/W2778814079', 'https://openalex.org/W2903193068', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970279348', 'https://openalex.org/W3082674894', 'https://openalex.org/W4241645538', 'https://openalex.org/W4385245566']","We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolingual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.",1.0
SKG_MT_68,https://openalex.org/W2970544750,2019,13,"['https://openalex.org/W2133564696', 'https://openalex.org/W2525778437', 'https://openalex.org/W2567571499', 'https://openalex.org/W2608029998', 'https://openalex.org/W2740718109', 'https://openalex.org/W2750588180', 'https://openalex.org/W2757592053', 'https://openalex.org/W2767019613', 'https://openalex.org/W2794365787', 'https://openalex.org/W2799051177', 'https://openalex.org/W2841102378', 'https://openalex.org/W2885950361', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2902537726', 'https://openalex.org/W2922158773', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964050280', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964240726', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964308564', 'https://openalex.org/W4297782088', 'https://openalex.org/W4385245566']","We describe LMU Munich’s machine translation system for English→German translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this model by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.",0.9950248756218906
SKG_MT_70,https://openalex.org/W2963542740,2019,611,"['https://openalex.org/W1522301498', 'https://openalex.org/W1543750907', 'https://openalex.org/W1597944220', 'https://openalex.org/W1815076433', 'https://openalex.org/W1902237438', 'https://openalex.org/W2113104171', 'https://openalex.org/W2128892113', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2302255633', 'https://openalex.org/W2525778437', 'https://openalex.org/W2594990650', 'https://openalex.org/W2767008699', 'https://openalex.org/W2798761464', 'https://openalex.org/W2817535134', 'https://openalex.org/W2888520903', 'https://openalex.org/W2890964657', 'https://openalex.org/W2896060389', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902081112', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962931466', 'https://openalex.org/W2963088785', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963302407', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963418779', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963636855', 'https://openalex.org/W2963755523', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964088127', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W4297747548', 'https://openalex.org/W4300831640', 'https://openalex.org/W4385245566']","Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT’16 English-German and NIST OpenMT’12 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.",1.0
SKG_MT_73,https://openalex.org/W2971148473,2019,28,"['https://openalex.org/W2101105183', 'https://openalex.org/W2144746247', 'https://openalex.org/W2149327368', 'https://openalex.org/W2160001241', 'https://openalex.org/W2251610689', 'https://openalex.org/W2251994258', 'https://openalex.org/W2257408573', 'https://openalex.org/W2260677151', 'https://openalex.org/W2508316494', 'https://openalex.org/W2512924740', 'https://openalex.org/W2757980860', 'https://openalex.org/W2758950307', 'https://openalex.org/W2760656271', 'https://openalex.org/W2838081464', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902463012', 'https://openalex.org/W2903188467', 'https://openalex.org/W2903193068', 'https://openalex.org/W2915756181', 'https://openalex.org/W2916548775', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963824830', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973088264', 'https://openalex.org/W4385245566']","We propose the use of pre-trained embeddings as features of a regression model for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).",1.0
SKG_MT_74,https://openalex.org/W2101451733,2012,45,"['https://openalex.org/W24102868', 'https://openalex.org/W222053410', 'https://openalex.org/W239287372', 'https://openalex.org/W596234542', 'https://openalex.org/W1498269992', 'https://openalex.org/W1586104099', 'https://openalex.org/W1631260214', 'https://openalex.org/W1848260265', 'https://openalex.org/W1880262756', 'https://openalex.org/W1989658336', 'https://openalex.org/W1991522508', 'https://openalex.org/W1991995555', 'https://openalex.org/W1995560154', 'https://openalex.org/W2004447574', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105577415', 'https://openalex.org/W2107695330', 'https://openalex.org/W2107743791', 'https://openalex.org/W2110302976', 'https://openalex.org/W2112900913', 'https://openalex.org/W2115410424', 'https://openalex.org/W2116229791', 'https://openalex.org/W2119168550', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2129574239', 'https://openalex.org/W2132001515', 'https://openalex.org/W2136477195', 'https://openalex.org/W2136925175', 'https://openalex.org/W2138309071', 'https://openalex.org/W2139183784', 'https://openalex.org/W2140133598', 'https://openalex.org/W2140460368', 'https://openalex.org/W2145604837', 'https://openalex.org/W2149971620', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154581043', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165132531', 'https://openalex.org/W2165666205', 'https://openalex.org/W2171119909', 'https://openalex.org/W2356613612', 'https://openalex.org/W2394860946', 'https://openalex.org/W2437005631', 'https://openalex.org/W3201918671']","To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1",1.0
SKG_MT_76,https://openalex.org/W3118463587,2020,11,"['https://openalex.org/W1902237438', 'https://openalex.org/W2006969979', 'https://openalex.org/W2025768430', 'https://openalex.org/W2095705004', 'https://openalex.org/W2097333193', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121745180', 'https://openalex.org/W2130942839', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250342921', 'https://openalex.org/W2250552021', 'https://openalex.org/W2250593294', 'https://openalex.org/W2251602405', 'https://openalex.org/W2251897294', 'https://openalex.org/W2493916176', 'https://openalex.org/W2546938941', 'https://openalex.org/W2741602058', 'https://openalex.org/W2757281913', 'https://openalex.org/W2898785264', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963633299', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964308564', 'https://openalex.org/W3104652516']","Availability of bitext dataset has been a key challenge in the conventional machine translation system which requires surplus amount of parallel data. In this work, we devise an unsupervised neural machine translation (UNMT) system consisting of a transformer based shared encoder and language specific decoders using denoising autoencoder and backtranslation with an additional Manipuri side multiple test reference. We report our work on low resource setting for English (en) - Manipuri (mni) language pair and attain a BLEU score of 3.1 for en-mni and 2.7 for mni-en respectively. Subjective evaluation on translated output gives encouraging findings.",1.0
SKG_MT_77,https://openalex.org/W2153999629,2013,35,"['https://openalex.org/W30655990', 'https://openalex.org/W222053410', 'https://openalex.org/W1498238796', 'https://openalex.org/W1517947178', 'https://openalex.org/W1631260214', 'https://openalex.org/W1736600331', 'https://openalex.org/W2006969979', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111355378', 'https://openalex.org/W2116594867', 'https://openalex.org/W2116792345', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2137841811', 'https://openalex.org/W2144879357', 'https://openalex.org/W2153204578', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157956963', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158733102', 'https://openalex.org/W2401082558', 'https://openalex.org/W3166311385']","N-gram-based models co-exist with their phrase-based counterparts as an alternative SMT framework. Both techniques have pros and cons. While the N-gram-based framework provides a better model that captures both source and target contexts and avoids spurious phrasal segmentation, the ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decoding, in terms of better search performance and superior selection of translation units. In this paper we combine N-grambased modeling with phrase-based decoding, and obtain the benefits of both approaches. Our experiments show that using this combination not only improves the search accuracy of the N-gram model but that it also improves the BLEU scores. Our system outperforms state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems by a significant margin on German, French and Spanish to English translation tasks.",1.0
SKG_MT_78,https://openalex.org/W2156422881,2012,239,"['https://openalex.org/W22627370', 'https://openalex.org/W174630521', 'https://openalex.org/W1512155114', 'https://openalex.org/W1551773846', 'https://openalex.org/W1631260214', 'https://openalex.org/W1638924786', 'https://openalex.org/W1647671624', 'https://openalex.org/W1787338159', 'https://openalex.org/W1952210680', 'https://openalex.org/W2031196180', 'https://openalex.org/W2036484499', 'https://openalex.org/W2038721957', 'https://openalex.org/W2042262613', 'https://openalex.org/W2061910127', 'https://openalex.org/W2078861931', 'https://openalex.org/W2083245912', 'https://openalex.org/W2099092905', 'https://openalex.org/W2103081392', 'https://openalex.org/W2106068492', 'https://openalex.org/W2108373063', 'https://openalex.org/W2108701407', 'https://openalex.org/W2109802560', 'https://openalex.org/W2109881807', 'https://openalex.org/W2110481933', 'https://openalex.org/W2115769265', 'https://openalex.org/W2116316001', 'https://openalex.org/W2118119027', 'https://openalex.org/W2124672726', 'https://openalex.org/W2124807415', 'https://openalex.org/W2143927888', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165646132', 'https://openalex.org/W2251831371']","In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems. 1",1.0
SKG_MT_79,https://openalex.org/W1753482797,2013,1332,"['https://openalex.org/W179875071', 'https://openalex.org/W196214544', 'https://openalex.org/W1889268436', 'https://openalex.org/W1999965501', 'https://openalex.org/W2006969979', 'https://openalex.org/W2038968650', 'https://openalex.org/W2103078213', 'https://openalex.org/W2103305545', 'https://openalex.org/W2117130368', 'https://openalex.org/W2125573226', 'https://openalex.org/W2146502635', 'https://openalex.org/W2148708890', 'https://openalex.org/W2155607551', 'https://openalex.org/W2171928131', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251222643', 'https://openalex.org/W2964106094', 'https://openalex.org/W4285719527']","We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",1.0
SKG_MT_80,https://openalex.org/W2250360382,2013,3,"['https://openalex.org/W71111251', 'https://openalex.org/W96390954', 'https://openalex.org/W111226416', 'https://openalex.org/W165283731', 'https://openalex.org/W192665053', 'https://openalex.org/W1590839732', 'https://openalex.org/W1964707402', 'https://openalex.org/W1972871876', 'https://openalex.org/W1973550659', 'https://openalex.org/W2003321079', 'https://openalex.org/W2038721957', 'https://openalex.org/W2102749417', 'https://openalex.org/W2138247936', 'https://openalex.org/W2139956879', 'https://openalex.org/W2140406733', 'https://openalex.org/W2154558620', 'https://openalex.org/W2163953154', 'https://openalex.org/W2168820179', 'https://openalex.org/W2227801490']","We propose a new method for translation ac-quisition which uses a set of synonyms to ac-quire translations from comparable corpora. The motivation is that, given a certain query term, it is often possible for a user to specify one or more synonyms. Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term’s context vector does not always reliably represent a terms meaning due to the context vector’s sparsity. Our proposed method uses a weighted average of the synonyms ’ context vectors, that is derived by inferring the mean vector of the von Mises-Fisher distribution. We evaluate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet. The experiments show that our proposed method significantly improves translation accuracy when compared to a pre-vious method for smoothing context vectors. 1",1.0
SKG_MT_81,https://openalex.org/W3035629723,2020,61,"['https://openalex.org/W1522301498', 'https://openalex.org/W2095705004', 'https://openalex.org/W2096175520', 'https://openalex.org/W2113104171', 'https://openalex.org/W2184135559', 'https://openalex.org/W2608029998', 'https://openalex.org/W2767019613', 'https://openalex.org/W2799051177', 'https://openalex.org/W2806987872', 'https://openalex.org/W2891534142', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949973181', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2983108239', 'https://openalex.org/W3100623455', 'https://openalex.org/W4385245566']","In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.",0.9937888198757764
SKG_MT_82,https://openalex.org/W2897507397,2018,107,"['https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101454539', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2157331557', 'https://openalex.org/W2186615578', 'https://openalex.org/W2613904329', 'https://openalex.org/W2759173152', 'https://openalex.org/W2778814079', 'https://openalex.org/W2798761464', 'https://openalex.org/W2808154809', 'https://openalex.org/W2888539709', 'https://openalex.org/W2962697716', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963715460', 'https://openalex.org/W2964174820', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3082674894', 'https://openalex.org/W4241645538', 'https://openalex.org/W4298170715', 'https://openalex.org/W4385245566', 'https://openalex.org/W4386506836']","Recent work has shown that the encoder-decoder attention mechanisms in neural machine translation (NMT) are different from the word alignment in statistical machine translation. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that attention mechanisms pay more attention to context tokens when translating ambiguous words. We explore the attention distribution patterns when translating ambiguous nouns. Counterintuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that attention is not the main mechanism used by NMT models to incorporate contextual information for WSD. The experimental results suggest that NMT models learn to encode contextual information necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to “align” source and target tokens and the last few layers learn to extract features from the related but unaligned context tokens.",0.9951690821256038
SKG_MT_83,https://openalex.org/W2889326796,2018,1005,"['https://openalex.org/W211509693', 'https://openalex.org/W1522301498', 'https://openalex.org/W1810943226', 'https://openalex.org/W1902237438', 'https://openalex.org/W1915251500', 'https://openalex.org/W1916559533', 'https://openalex.org/W2025768430', 'https://openalex.org/W2097333193', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109664771', 'https://openalex.org/W2122270629', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136156618', 'https://openalex.org/W2141440284', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2169200297', 'https://openalex.org/W2183341477', 'https://openalex.org/W2540404261', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561274697', 'https://openalex.org/W2566564022', 'https://openalex.org/W2581377246', 'https://openalex.org/W2595715041', 'https://openalex.org/W2597891111', 'https://openalex.org/W2612675303', 'https://openalex.org/W2613904329', 'https://openalex.org/W2733239165', 'https://openalex.org/W2756566411', 'https://openalex.org/W2758310181', 'https://openalex.org/W2767982226', 'https://openalex.org/W2767989436', 'https://openalex.org/W2770173563', 'https://openalex.org/W2775795276', 'https://openalex.org/W2794365787', 'https://openalex.org/W2797913374', 'https://openalex.org/W2798931235', 'https://openalex.org/W2806311723', 'https://openalex.org/W2886095922', 'https://openalex.org/W2887516053', 'https://openalex.org/W2953333557', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963096510', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963708445', 'https://openalex.org/W2963768805', 'https://openalex.org/W2963774520', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3211848854', 'https://openalex.org/W4241645538', 'https://openalex.org/W4293388793', 'https://openalex.org/W4297798436', 'https://openalex.org/W4297801368', 'https://openalex.org/W4298137069', 'https://openalex.org/W4298393544', 'https://openalex.org/W4301368689', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT’14 English-German test set.",1.0
SKG_MT_85,https://openalex.org/W2136135294,2012,8,"['https://openalex.org/W13657349', 'https://openalex.org/W39836547', 'https://openalex.org/W132913264', 'https://openalex.org/W1980219648', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118850276', 'https://openalex.org/W2121404172', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2161943765', 'https://openalex.org/W2437005631', 'https://openalex.org/W3197700997']","In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-to-English machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 1",1.0
SKG_MT_86,https://openalex.org/W2892244498,2018,72,"['https://openalex.org/W298172948', 'https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1860935423', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2133622676', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250876691', 'https://openalex.org/W2251743902', 'https://openalex.org/W2261339088', 'https://openalex.org/W2295479927', 'https://openalex.org/W2511131004', 'https://openalex.org/W2527133236', 'https://openalex.org/W2550821151', 'https://openalex.org/W2567571499', 'https://openalex.org/W2573728411', 'https://openalex.org/W2581863816', 'https://openalex.org/W2609482285', 'https://openalex.org/W2740718109', 'https://openalex.org/W2740743644', 'https://openalex.org/W2744813330', 'https://openalex.org/W2750588180', 'https://openalex.org/W2755989362', 'https://openalex.org/W2756978580', 'https://openalex.org/W2757592053', 'https://openalex.org/W2760452458', 'https://openalex.org/W2801900386', 'https://openalex.org/W2802153702', 'https://openalex.org/W2953029945', 'https://openalex.org/W2953127297', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962897020', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963201387', 'https://openalex.org/W2963355640', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964159778', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W3211259717', 'https://openalex.org/W4297782088', 'https://openalex.org/W4385245566']","With great practical value, the study of Multi-domain Neural Machine Translation (NMT) mainly focuses on using mixed-domain parallel sentences to construct a unified model that allows translation to switch between different domains. Intuitively, words in a sentence are related to its domain to varying degrees, so that they will exert disparate impacts on the multi-domain NMT modeling. Based on this intuition, in this paper, we devote to distinguishing and exploiting word-level domain contexts for multi-domain NMT. To this end, we jointly model NMT with monolingual attention-based domain classification tasks and improve NMT as follows: 1) Based on the sentence representations produced by a domain classifier and an adversarial domain classifier, we generate two gating vectors and use them to construct domain-specific and domain-shared annotations, for later translation predictions via different attention models; 2) We utilize the attention weights derived from target-side domain classifier to adjust the weights of target words in the training objective, enabling domain-related words to have greater impacts during model training. Experimental results on Chinese-English and English-French multi-domain translation tasks demonstrate the effectiveness of the proposed model. Source codes of this paper are available on Github https://github.com/DeepLearnXMU/WDCNMT.",1.0
SKG_MT_87,https://openalex.org/W2154591050,2011,20,"['https://openalex.org/W10704533', 'https://openalex.org/W222053410', 'https://openalex.org/W1590952807', 'https://openalex.org/W1593045043', 'https://openalex.org/W1631260214', 'https://openalex.org/W1895030899', 'https://openalex.org/W1991995555', 'https://openalex.org/W1995560154', 'https://openalex.org/W2006969979', 'https://openalex.org/W2017708149', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106540279', 'https://openalex.org/W2109664771', 'https://openalex.org/W2111566774', 'https://openalex.org/W2118536060', 'https://openalex.org/W2124797537', 'https://openalex.org/W2146418175', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158388102', 'https://openalex.org/W2160380653', 'https://openalex.org/W2166905217', 'https://openalex.org/W2188741930', 'https://openalex.org/W2321698605', 'https://openalex.org/W2437005631', 'https://openalex.org/W3198494294']","In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline. 1",1.0
SKG_MT_88,https://openalex.org/W2963069107,2017,16,"['https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2166905217', 'https://openalex.org/W2250451826', 'https://openalex.org/W2407166119', 'https://openalex.org/W2512597464', 'https://openalex.org/W2523000059', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2563574619', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740087922', 'https://openalex.org/W2941519458', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964308564', 'https://openalex.org/W3103252885', 'https://openalex.org/W4241645538', 'https://openalex.org/W4299674251', 'https://openalex.org/W4392352873']","In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same time-scale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.",1.0
SKG_MT_89,https://openalex.org/W137989762,2012,169,"['https://openalex.org/W147720932', 'https://openalex.org/W181901128', 'https://openalex.org/W1544567521', 'https://openalex.org/W1909398668', 'https://openalex.org/W1957504465', 'https://openalex.org/W2017802499', 'https://openalex.org/W2100976324', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109704865', 'https://openalex.org/W2123301721', 'https://openalex.org/W2127849236', 'https://openalex.org/W2135161317', 'https://openalex.org/W2140343992', 'https://openalex.org/W2147272182', 'https://openalex.org/W2149327368', 'https://openalex.org/W2156985047', 'https://openalex.org/W2163361328', 'https://openalex.org/W2166905217', 'https://openalex.org/W2168576900', 'https://openalex.org/W2294764339', 'https://openalex.org/W3170253630']","Arabic Dialects present many challenges for machine translation, not least of which is the lack of data resources. We use crowdsourcing to cheaply and quickly build Levantine-English and Egyptian-English parallel corpora, consisting of 1.1M words and 380k words, respectively. The dialectal sentences are selected from a large corpus of Arabic web text, and translated using Amazon’s Mechanical Turk. We use this data to build Dialectal Arabic MT systems, and find that small amounts of dialectal data have a dramatic impact on translation quality. When translating Egyptian and Levantine test sets, our Dialectal Arabic MT system performs 6.3 and 7.0 BLEU points higher than a Modern Standard Arabic MT system trained on a 150M-word Arabic-English parallel corpus. 1",1.0
SKG_MT_90,https://openalex.org/W2964093309,2019,41,"['https://openalex.org/W222053410', 'https://openalex.org/W605727707', 'https://openalex.org/W1522301498', 'https://openalex.org/W1677182931', 'https://openalex.org/W1686810756', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2613904329', 'https://openalex.org/W2752047430', 'https://openalex.org/W2787802257', 'https://openalex.org/W2794365787', 'https://openalex.org/W2888520903', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896060389', 'https://openalex.org/W2896457183', 'https://openalex.org/W2906987001', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964050280', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964240726', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394643672']","While very deep neural networks have shown effectiveness for computer vision and text classification applications, how to increase the network depth of the neural machine translation (NMT) models for better translation quality remains a challenging problem. Directly stacking more blocks to the NMT model results in no improvement and even drop in performance. In this work, we propose an effective two-stage approach with three specially designed components to construct deeper NMT models, which result in significant improvements over the strong Transformer baselines on WMT14 English→German and English→French translation tasks.",1.0
SKG_MT_91,https://openalex.org/W200886494,2013,6,"['https://openalex.org/W113900115', 'https://openalex.org/W1554944419', 'https://openalex.org/W1748393397', 'https://openalex.org/W1865009630', 'https://openalex.org/W1892207274', 'https://openalex.org/W1964357740', 'https://openalex.org/W1971220772', 'https://openalex.org/W1975133852', 'https://openalex.org/W2006969979', 'https://openalex.org/W2078861931', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115584598', 'https://openalex.org/W2123318312', 'https://openalex.org/W2158732077', 'https://openalex.org/W2200480317', 'https://openalex.org/W2604377206']","We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for automatically grading student answers. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved. We view question answering as translation from the question to the answer, from the&#13;\nquestion to the reference answer, from the answer to the reference answer, or from the question and the answer to the reference answer. Each view is modeled by an RTM model, giving us a new perspective on the ternary relationship between the question, the answer, and the reference answer. We show that all RTM models contribute and a prediction model based on all four perspectives performs the best. Our prediction model is the $2$nd best system on some tasks according to the official&#13;\nresults of the Student Response Analysis (SRA 2013) challenge.",0.9902912621359224
SKG_MT_92,https://openalex.org/W2160721348,2012,24,"['https://openalex.org/W222053410', 'https://openalex.org/W1506806321', 'https://openalex.org/W1663973292', 'https://openalex.org/W1978259121', 'https://openalex.org/W2006969979', 'https://openalex.org/W2038698865', 'https://openalex.org/W2045238294', 'https://openalex.org/W2049633694', 'https://openalex.org/W2086202918', 'https://openalex.org/W2101752047', 'https://openalex.org/W2102098892', 'https://openalex.org/W2111142112', 'https://openalex.org/W2119168550', 'https://openalex.org/W2119224513', 'https://openalex.org/W2124810114', 'https://openalex.org/W2134409515', 'https://openalex.org/W2137122906', 'https://openalex.org/W2139725124', 'https://openalex.org/W2144600631', 'https://openalex.org/W2150066400', 'https://openalex.org/W2153653739', 'https://openalex.org/W2163561955', 'https://openalex.org/W2169724380', 'https://openalex.org/W2172138510', 'https://openalex.org/W2405762604', 'https://openalex.org/W2437005631', 'https://openalex.org/W2760567570', 'https://openalex.org/W2798766386']","Two decades after their invention, the IBM word-based translation models, widely avail-able in the GIZA++ toolkit, remain the dom-inant approach to word alignment and an in-tegral part of many statistical translation sys-tems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an `0 prior to en-courage sparsity in the word-to-word transla-tion model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Ara-bic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). 1",0.9946524064171124
SKG_MT_93,https://openalex.org/W2163238067,2012,48,"['https://openalex.org/W23077562', 'https://openalex.org/W30655990', 'https://openalex.org/W1580518608', 'https://openalex.org/W1957504465', 'https://openalex.org/W1969974515', 'https://openalex.org/W1983599491', 'https://openalex.org/W2006969979', 'https://openalex.org/W2016856586', 'https://openalex.org/W2032175749', 'https://openalex.org/W2036516910', 'https://openalex.org/W2038698865', 'https://openalex.org/W2060127787', 'https://openalex.org/W2083460949', 'https://openalex.org/W2096253869', 'https://openalex.org/W2098603082', 'https://openalex.org/W2100976324', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105891181', 'https://openalex.org/W2106178373', 'https://openalex.org/W2109664771', 'https://openalex.org/W2113788796', 'https://openalex.org/W2113998052', 'https://openalex.org/W2117642127', 'https://openalex.org/W2117873652', 'https://openalex.org/W2118692229', 'https://openalex.org/W2119168550', 'https://openalex.org/W2121227244', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125768350', 'https://openalex.org/W2126241965', 'https://openalex.org/W2135161317', 'https://openalex.org/W2136094405', 'https://openalex.org/W2136703427', 'https://openalex.org/W2143995218', 'https://openalex.org/W2144600658', 'https://openalex.org/W2147880316', 'https://openalex.org/W2149709850', 'https://openalex.org/W2156816974', 'https://openalex.org/W2163353449', 'https://openalex.org/W2164766438', 'https://openalex.org/W2169599995', 'https://openalex.org/W2171421863', 'https://openalex.org/W2183072638', 'https://openalex.org/W2280403519', 'https://openalex.org/W2951562155']","When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1",1.0
SKG_MT_94,https://openalex.org/W2251631319,2013,9,"['https://openalex.org/W30655990', 'https://openalex.org/W82687334', 'https://openalex.org/W149848483', 'https://openalex.org/W201231365', 'https://openalex.org/W232191560', 'https://openalex.org/W1483849869', 'https://openalex.org/W1904457459', 'https://openalex.org/W1972567251', 'https://openalex.org/W1980813323', 'https://openalex.org/W2018869373', 'https://openalex.org/W2041826927', 'https://openalex.org/W2083305840', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097927681', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104027215', 'https://openalex.org/W2108188641', 'https://openalex.org/W2110660056', 'https://openalex.org/W2111142112', 'https://openalex.org/W2111491614', 'https://openalex.org/W2116064496', 'https://openalex.org/W2122609803', 'https://openalex.org/W2125595887', 'https://openalex.org/W2130959832', 'https://openalex.org/W2140051273', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157512532', 'https://openalex.org/W2159755860', 'https://openalex.org/W2168828735', 'https://openalex.org/W2169724380', 'https://openalex.org/W2172144952', 'https://openalex.org/W2180952760', 'https://openalex.org/W2899294844']","We present Positive Diversity Tuning, a new method for tuning machine translation models specifically for improved performance during system combination. System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other. We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems. The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines. We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores. When these individual systems are used together for system combination, our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems. 1",1.0
SKG_MT_96,https://openalex.org/W2962696859,2018,28,"['https://openalex.org/W11511616', 'https://openalex.org/W22168010', 'https://openalex.org/W1663973292', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2183998037', 'https://openalex.org/W2184135559', 'https://openalex.org/W2252272516', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2561274697', 'https://openalex.org/W2594639291', 'https://openalex.org/W2610245951', 'https://openalex.org/W2740433069', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964308564', 'https://openalex.org/W4232023503', 'https://openalex.org/W4241645538', 'https://openalex.org/W4307459710']","Neural Machine Translation (NMT) performs poor on the low-resource language pair (X,Z), especially when Z is a rare language. By introducing another rich language Y, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y,Z) (may be small) and (X,Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X,Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.",1.0
SKG_MT_97,https://openalex.org/W2150634928,2014,35,"['https://openalex.org/W201231365', 'https://openalex.org/W1483849869', 'https://openalex.org/W1582482241', 'https://openalex.org/W2006969979', 'https://openalex.org/W2087735403', 'https://openalex.org/W2105891181', 'https://openalex.org/W2110660056', 'https://openalex.org/W2116492146', 'https://openalex.org/W2118972857', 'https://openalex.org/W2119856489', 'https://openalex.org/W2138934709', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2171802951', 'https://openalex.org/W2188941525', 'https://openalex.org/W2895810819', 'https://openalex.org/W2918014141']","Different machine translation engines can be remarkably dissimilar not only with respect to their technical paradigm, but also with respect to the translation output they yield.System combination is a method for combining the output of multiple machine translation engines in order to take benefit of the strengths of each of the individual engines.In this work we introduce a novel system combination implementation which is integrated into Jane, RWTH's open source statistical machine translation toolkit.On the most recent Workshop on Statistical Machine Translation system combination shared task, we achieve improvements of up to 0.7 points in BLEU over the best system combination hypotheses which were submitted for the official evaluation.Moreover, we enhance our system combination pipeline with additional n-gram language models and lexical translation models.",0.990990990990991
SKG_MT_98,https://openalex.org/W2986670783,2019,21,"['https://openalex.org/W2101105183', 'https://openalex.org/W2127863960', 'https://openalex.org/W2467834614', 'https://openalex.org/W2550821151', 'https://openalex.org/W2576482813', 'https://openalex.org/W2595715041', 'https://openalex.org/W2757920198', 'https://openalex.org/W2758334418', 'https://openalex.org/W2787026069', 'https://openalex.org/W2807895655', 'https://openalex.org/W2962731009', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963697731', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970015022', 'https://openalex.org/W2988037059']","In the Japanese language different levels of honorific speech are used to convey respect, deference, humility, formality and social distance. In this paper, we present a method for controlling the level of formality of Japanese output in English-to-Japanese neural machine translation (NMT). By using heuristics to identify honorific verb forms, we classify Japanese sentences as being one of three levels of informal, polite, or formal speech in parallel text. The English source side is marked with a feature that identifies the level of honorific speech present in the Japanese target side. We use this parallel text to train an English-Japanese NMT model capable of producing Japanese translations in different honorific speech styles for the same English input sentence.",1.0
SKG_MT_99,https://openalex.org/W2967600676,2020,15,"['https://openalex.org/W211509693', 'https://openalex.org/W222053410', 'https://openalex.org/W2044916741', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2130942839', 'https://openalex.org/W2148365102', 'https://openalex.org/W2149327368', 'https://openalex.org/W2164628107', 'https://openalex.org/W2169200297', 'https://openalex.org/W2250875036', 'https://openalex.org/W2260677151', 'https://openalex.org/W2492256392', 'https://openalex.org/W2727767747', 'https://openalex.org/W2889326796', 'https://openalex.org/W2903193068', 'https://openalex.org/W2922158773', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936887742', 'https://openalex.org/W2950416254', 'https://openalex.org/W2952103439', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963631907', 'https://openalex.org/W2963708445', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970791445', 'https://openalex.org/W2996403597', 'https://openalex.org/W3196290596']","Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency.",1.0
SKG_MT_100,https://openalex.org/W2147262247,2013,112,"['https://openalex.org/W222053410', 'https://openalex.org/W1575384945', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W1965154800', 'https://openalex.org/W1970689298', 'https://openalex.org/W2040711288', 'https://openalex.org/W2072128103', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105402874', 'https://openalex.org/W2107878390', 'https://openalex.org/W2117278770', 'https://openalex.org/W2132339004', 'https://openalex.org/W2143719855', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153508793', 'https://openalex.org/W2171928131', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250379827', 'https://openalex.org/W2251150025', 'https://openalex.org/W2251202280', 'https://openalex.org/W2399188371', 'https://openalex.org/W2406343028', 'https://openalex.org/W3167516426']","Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard n-gram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams. 1",0.994413407821229
SKG_MT_101,https://openalex.org/W2970415062,2019,7,"['https://openalex.org/W22168010', 'https://openalex.org/W1902237438', 'https://openalex.org/W2415204069', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2597655663', 'https://openalex.org/W2613904329', 'https://openalex.org/W2752172973', 'https://openalex.org/W2790235966', 'https://openalex.org/W2798389157', 'https://openalex.org/W2807535859', 'https://openalex.org/W2809456172', 'https://openalex.org/W2891924676', 'https://openalex.org/W2899015110', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962807144', 'https://openalex.org/W2963090765', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963386218', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499433', 'https://openalex.org/W2963672008', 'https://openalex.org/W2963691842', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963758829', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964165804', 'https://openalex.org/W2964265128', 'https://openalex.org/W4300822525', 'https://openalex.org/W4302343710', 'https://openalex.org/W4385245566']","In this paper, we explore a multilingual translation model with a cross-lingually shared layer that can be used as fixed-size sentence representation in different downstream tasks. We systematically study the impact of the size of the shared layer and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that the performance in translation does correlate with trainable downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. On the other hand, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. We hypothesize that the training procedure on the downstream task enables the model to identify the encoded information that is useful for the specific task whereas non-trainable benchmarks can be confused by other types of information also encoded in the representation of a sentence.",1.0
SKG_MT_103,https://openalex.org/W2472237015,2016,29,"['https://openalex.org/W81529571', 'https://openalex.org/W105964407', 'https://openalex.org/W126222424', 'https://openalex.org/W181190445', 'https://openalex.org/W596611847', 'https://openalex.org/W622346006', 'https://openalex.org/W625942373', 'https://openalex.org/W627471356', 'https://openalex.org/W1484777458', 'https://openalex.org/W1582869709', 'https://openalex.org/W1779279021', 'https://openalex.org/W1870286075', 'https://openalex.org/W1984771669', 'https://openalex.org/W2000546550', 'https://openalex.org/W2004156982', 'https://openalex.org/W2006832571', 'https://openalex.org/W2021618504', 'https://openalex.org/W2038253946', 'https://openalex.org/W2043408671', 'https://openalex.org/W2044916741', 'https://openalex.org/W2054125330', 'https://openalex.org/W2056132907', 'https://openalex.org/W2108350019', 'https://openalex.org/W2111568449', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124797537', 'https://openalex.org/W2144746247', 'https://openalex.org/W2148365102', 'https://openalex.org/W2148959489', 'https://openalex.org/W2153635508', 'https://openalex.org/W2160153521', 'https://openalex.org/W2164418233', 'https://openalex.org/W2164628107', 'https://openalex.org/W2164645230', 'https://openalex.org/W2172000360', 'https://openalex.org/W2188102889', 'https://openalex.org/W2232536453', 'https://openalex.org/W2250882152', 'https://openalex.org/W2251265976', 'https://openalex.org/W2464667197', 'https://openalex.org/W2586145851', 'https://openalex.org/W3015810608', 'https://openalex.org/W3100577423', 'https://openalex.org/W3120421331', 'https://openalex.org/W3140453591', 'https://openalex.org/W3141884538', 'https://openalex.org/W3170960752', 'https://openalex.org/W3196290596', 'https://openalex.org/W4285719527']","Raphael Rubino, Ekaterina Lapshinova-Koltunski, Josef van Genabith. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",1.0
SKG_MT_104,https://openalex.org/W2962945603,2017,53,"['https://openalex.org/W6908809', 'https://openalex.org/W1498763386', 'https://openalex.org/W1753482797', 'https://openalex.org/W1815076433', 'https://openalex.org/W1972686387', 'https://openalex.org/W1979145089', 'https://openalex.org/W2006969979', 'https://openalex.org/W2100664567', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2178333206', 'https://openalex.org/W2251428653', 'https://openalex.org/W2407166119', 'https://openalex.org/W2413436069', 'https://openalex.org/W2511277317', 'https://openalex.org/W2523000059', 'https://openalex.org/W2525778437', 'https://openalex.org/W2532807140', 'https://openalex.org/W2539201987', 'https://openalex.org/W2542860122', 'https://openalex.org/W2566564022', 'https://openalex.org/W2566623769', 'https://openalex.org/W2573728411', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962700074', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962780935', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962834107', 'https://openalex.org/W2962944953', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4392352873']","Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets.",1.0
SKG_MT_105,https://openalex.org/W3037312903,2020,29,"['https://openalex.org/W19526395', 'https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W1916559533', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2552124255', 'https://openalex.org/W2595715041', 'https://openalex.org/W2607987856', 'https://openalex.org/W2752630748', 'https://openalex.org/W2754166059', 'https://openalex.org/W2798931235', 'https://openalex.org/W2805430026', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963163972', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963877297', 'https://openalex.org/W2964098600', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3104652516', 'https://openalex.org/W3211848854', 'https://openalex.org/W4298393544', 'https://openalex.org/W4301187301', 'https://openalex.org/W4385245566']","A large percentage of the world's population speaks a language of the Indian subcontinent, comprising languages from both Indo-Aryan (e.g. Hindi, Punjabi, Gujarati, etc.) and Dravidian (e.g. Tamil, Telugu, Malayalam, etc.) families. A universal characteristic of Indian languages is their complex morphology, which, when combined with the general lack of sufficient quantities of high-quality parallel data, can make developing machine translation (MT) systems for these languages difficult. Neural Machine Translation (NMT) is a rapidly advancing MT paradigm and has shown promising results for many language pairs, especially in large training data scenarios. Since the condition of large parallel corpora is not met for Indian-English language pairs, we present our efforts towards building efficient NMT systems between Indian languages (specifically Indo-Aryan languages) and English via efficiently exploiting parallel data from the related languages. We propose a technique called Unified Transliteration and Subword Segmentation to leverage language similarity while exploiting parallel data from related language pairs. We also propose a Multilingual Transfer Learning technique to leverage parallel data from multiple related languages to assist translation for low resource language pair of interest. Our experiments demonstrate an overall average improvement of 5 BLEU points over the standard Transformer-based NMT baselines.",1.0
SKG_MT_106,https://openalex.org/W2924679246,2019,11,"['https://openalex.org/W1522301498', 'https://openalex.org/W1773149199', 'https://openalex.org/W1815076433', 'https://openalex.org/W2046552246', 'https://openalex.org/W2095705004', 'https://openalex.org/W2108598243', 'https://openalex.org/W2133459682', 'https://openalex.org/W2144600658', 'https://openalex.org/W2157331557', 'https://openalex.org/W2185175083', 'https://openalex.org/W2194775991', 'https://openalex.org/W2509282593', 'https://openalex.org/W2512381898', 'https://openalex.org/W2513263213', 'https://openalex.org/W2514713644', 'https://openalex.org/W2553522418', 'https://openalex.org/W2581101319', 'https://openalex.org/W2760425631', 'https://openalex.org/W2889545026', 'https://openalex.org/W2889903020', 'https://openalex.org/W2902031175', 'https://openalex.org/W2903049941', 'https://openalex.org/W2903343986', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963407669', 'https://openalex.org/W2964003477', 'https://openalex.org/W2964192290', 'https://openalex.org/W2964308564', 'https://openalex.org/W3010232603', 'https://openalex.org/W3098507616']","Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.",1.0
SKG_MT_107,https://openalex.org/W2468625829,2016,2,"['https://openalex.org/W932413789', 'https://openalex.org/W1520465330', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095755718', 'https://openalex.org/W2100664567', 'https://openalex.org/W2111142112', 'https://openalex.org/W2120861206', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132001515', 'https://openalex.org/W2144600658', 'https://openalex.org/W2152790380', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159755860', 'https://openalex.org/W2180952760', 'https://openalex.org/W2250732891', 'https://openalex.org/W2250907725', 'https://openalex.org/W2251269336', 'https://openalex.org/W2251682575', 'https://openalex.org/W2595715041', 'https://openalex.org/W2963084471', 'https://openalex.org/W2998704965', 'https://openalex.org/W4241645538', 'https://openalex.org/W4285719527']","The neural network joint model of translation or NNJM (Devlin et al., 2014) combines source and target context to produce a powerful translation feature.However, its softmax layer necessitates a sum over the entire output vocabulary, which results in very slow maximum likelihood (MLE) training.This has led some groups to train using Noise Contrastive Estimation (NCE), which side-steps this sum.We carry out the first direct comparison of MLE and NCE training objectives for the NNJM, showing that NCE is significantly outperformed by MLE on large-scale Arabic-English and Chinese-English translation tasks.We also show that this drop can be avoided by using a recently proposed translation noise distribution.",1.0
SKG_MT_108,https://openalex.org/W2251150371,2015,119,"['https://openalex.org/W99956235', 'https://openalex.org/W2094059537', 'https://openalex.org/W2101234009', 'https://openalex.org/W2103288873', 'https://openalex.org/W2126704587', 'https://openalex.org/W2127293685', 'https://openalex.org/W2144746247', 'https://openalex.org/W2164984707', 'https://openalex.org/W2166545452', 'https://openalex.org/W2169939314', 'https://openalex.org/W2186839874', 'https://openalex.org/W2250612440', 'https://openalex.org/W3098663719', 'https://openalex.org/W3100577423', 'https://openalex.org/W6675354045']","This paper presents QUEST++ , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level.It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g.sentences).QUEST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models.Results on recent datasets show that QUEST++ achieves state-of-the-art performance.",1.0
SKG_MT_109,https://openalex.org/W2250320700,2015,1,"['https://openalex.org/W91928571', 'https://openalex.org/W932413789', 'https://openalex.org/W1464282338', 'https://openalex.org/W1595109586', 'https://openalex.org/W1970689298', 'https://openalex.org/W2096557251', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111355378', 'https://openalex.org/W2111798208', 'https://openalex.org/W2114211285', 'https://openalex.org/W2120861206', 'https://openalex.org/W2123825474', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131462252', 'https://openalex.org/W2132109814', 'https://openalex.org/W2132339004', 'https://openalex.org/W2133280805', 'https://openalex.org/W2143719855', 'https://openalex.org/W2152790380', 'https://openalex.org/W2153204578', 'https://openalex.org/W2157331557', 'https://openalex.org/W2158614781', 'https://openalex.org/W2159755860', 'https://openalex.org/W2164019165', 'https://openalex.org/W2180952760', 'https://openalex.org/W2250445771', 'https://openalex.org/W2250489405', 'https://openalex.org/W2251071050', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251395256', 'https://openalex.org/W2251682575', 'https://openalex.org/W2264374399', 'https://openalex.org/W2406079600', 'https://openalex.org/W3037533623']","Continuous-space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems. A simple, yet effective way to integrate such models in inference is to use them in an N -best rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines.",1.0
SKG_MT_111,https://openalex.org/W2933610451,2019,37,"['https://openalex.org/W342285082', 'https://openalex.org/W1542713999', 'https://openalex.org/W2126725946', 'https://openalex.org/W2153579005', 'https://openalex.org/W2251765408', 'https://openalex.org/W2294774419', 'https://openalex.org/W2493916176', 'https://openalex.org/W2513016193', 'https://openalex.org/W2561995736', 'https://openalex.org/W2577946330', 'https://openalex.org/W2594021297', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2760424551', 'https://openalex.org/W2788353357', 'https://openalex.org/W2798931235', 'https://openalex.org/W2887838996', 'https://openalex.org/W2889894161', 'https://openalex.org/W2952190837', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963472233', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964266061', 'https://openalex.org/W3104723404', 'https://openalex.org/W4293568373', 'https://openalex.org/W4294170691', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390']","Tasnim Mohiuddin, Shafiq Joty. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_112,https://openalex.org/W3034351728,2020,72,"['https://openalex.org/W1522301498', 'https://openalex.org/W1906341845', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2133564696', 'https://openalex.org/W2608029998', 'https://openalex.org/W2767019613', 'https://openalex.org/W2794365787', 'https://openalex.org/W2799051177', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945260553', 'https://openalex.org/W2945719503', 'https://openalex.org/W2952446148', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970529093', 'https://openalex.org/W2971347700', 'https://openalex.org/W3102507836', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model.",1.0
SKG_MT_113,https://openalex.org/W2949973181,2019,153,"['https://openalex.org/W1522301498', 'https://openalex.org/W1915251500', 'https://openalex.org/W2080373976', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141440284', 'https://openalex.org/W2159755860', 'https://openalex.org/W2176263492', 'https://openalex.org/W2183341477', 'https://openalex.org/W2212703438', 'https://openalex.org/W2418388682', 'https://openalex.org/W2546938941', 'https://openalex.org/W2555428947', 'https://openalex.org/W2594229957', 'https://openalex.org/W2595715041', 'https://openalex.org/W2610245951', 'https://openalex.org/W2633911929', 'https://openalex.org/W2740433069', 'https://openalex.org/W2745341416', 'https://openalex.org/W2752630748', 'https://openalex.org/W2756566411', 'https://openalex.org/W2786058094', 'https://openalex.org/W2798931235', 'https://openalex.org/W2886845974', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888541716', 'https://openalex.org/W2890007195', 'https://openalex.org/W2907279971', 'https://openalex.org/W2908336025', 'https://openalex.org/W2949193663', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963631431', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963887123', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964190861', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970152385', 'https://openalex.org/W3041866211', 'https://openalex.org/W3104652516', 'https://openalex.org/W4298393544', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT. In our experiments on German–English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean–English dataset, surpassing previously reported results by 4 BLEU.",0.9921259842519684
SKG_MT_114,https://openalex.org/W3118883478,2020,0,"['https://openalex.org/W1508577659', 'https://openalex.org/W1976549386', 'https://openalex.org/W2101105183', 'https://openalex.org/W2149327368', 'https://openalex.org/W2163108352', 'https://openalex.org/W2250875036', 'https://openalex.org/W2512848817', 'https://openalex.org/W2604763608', 'https://openalex.org/W2772421198', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970109976', 'https://openalex.org/W2997197207', 'https://openalex.org/W2998353611', 'https://openalex.org/W3034906024', 'https://openalex.org/W3035144493', 'https://openalex.org/W3035254119', 'https://openalex.org/W3090350559', 'https://openalex.org/W3201758196']","This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing word segmentation so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.",1.0
SKG_MT_115,https://openalex.org/W3120726683,2020,1,"['https://openalex.org/W2124807415', 'https://openalex.org/W2889326796', 'https://openalex.org/W2891177506', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962735107', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964583233', 'https://openalex.org/W2970206392', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970726011', 'https://openalex.org/W2971345087', 'https://openalex.org/W3106224367', 'https://openalex.org/W3164405410']","This report describes YerevaNN’s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide systems for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with en→ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.",0.9951690821256038
SKG_MT_116,https://openalex.org/W2142669279,2014,6,"['https://openalex.org/W298172948', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W2095755718', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132001515', 'https://openalex.org/W2137387514', 'https://openalex.org/W2146950091', 'https://openalex.org/W2158614781', 'https://openalex.org/W2250238837', 'https://openalex.org/W2250695194', 'https://openalex.org/W2251202280', 'https://openalex.org/W2340221426', 'https://openalex.org/W2356613612', 'https://openalex.org/W2406343028', 'https://openalex.org/W2408655637', 'https://openalex.org/W2892587090', 'https://openalex.org/W2912934387', 'https://openalex.org/W3202006521', 'https://openalex.org/W3203276480']","As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains? This challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous subdomains. However, learning from large heterogeneous corpora is quite different from standard adaptation tasks with clear domain distinctions. In this paper, we show that linear mixture models can reliably improve translation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks.",1.0
SKG_MT_117,https://openalex.org/W2146662964,2011,3,"['https://openalex.org/W1707559977', 'https://openalex.org/W1848260265', 'https://openalex.org/W1973923101', 'https://openalex.org/W2038698865', 'https://openalex.org/W2101105183', 'https://openalex.org/W2126610017', 'https://openalex.org/W2126946601', 'https://openalex.org/W2140343992', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2158364201', 'https://openalex.org/W2160697141', 'https://openalex.org/W2160842254', 'https://openalex.org/W2600716915']","In this paper we present a novel discriminative mixture model for statistical machine translation (SMT). We model the feature space with a log-linear combination of multiple mixture components. Each component contains a large set of features trained in a maximumentropy framework. All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance. This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT. It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task. 1",1.0
SKG_MT_118,https://openalex.org/W3104881680,2020,56,"['https://openalex.org/W222053410', 'https://openalex.org/W1411230545', 'https://openalex.org/W1973923101', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134627713', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2212846646', 'https://openalex.org/W2401082558', 'https://openalex.org/W2622084140', 'https://openalex.org/W2741040846', 'https://openalex.org/W2757154661', 'https://openalex.org/W2912070261', 'https://openalex.org/W2952360713', 'https://openalex.org/W2952682849', 'https://openalex.org/W2962714778', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962834107', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499882', 'https://openalex.org/W2964029788', 'https://openalex.org/W2964159778', 'https://openalex.org/W2964174820', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970045405', 'https://openalex.org/W2971296520', 'https://openalex.org/W2977944219', 'https://openalex.org/W2998135987', 'https://openalex.org/W3035393249', 'https://openalex.org/W3035463087', 'https://openalex.org/W3211259717', 'https://openalex.org/W4241645538', 'https://openalex.org/W4298170715', 'https://openalex.org/W4385245566']","Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.",1.0
SKG_MT_123,https://openalex.org/W2962717763,2017,134,"['https://openalex.org/W2052847729', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105245376', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133564696', 'https://openalex.org/W2158899491', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251150025', 'https://openalex.org/W2251367463', 'https://openalex.org/W2251664617', 'https://openalex.org/W2251743902', 'https://openalex.org/W2294370844', 'https://openalex.org/W2512848817', 'https://openalex.org/W2514342461', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2951184134', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964308564', 'https://openalex.org/W3216234416']","Linguistic resources such as part-ofspeech (POS) tags have been extensively used in statistical machine translation (SMT) frameworks and have yielded better performances.However, usage of such linguistic annotations in neural machine translation (NMT) systems has been left under-explored.In this work, we show that multi-task learning is a successful and a easy approach to introduce an additional knowledge into an end-to-end neural attentional model.By jointly training several natural language processing (NLP) tasks in one system, we are able to leverage common information and improve the performance of the individual task.We analyze the impact of three design decisions in multi-task learning: the tasks used in training, the training schedule, and the degree of parameter sharing across the tasks, which is defined by the network architecture.The experiments are conducted for an German to English translation task.As additional linguistic resources, we exploit POS information and named-entities (NE).Experiments show that the translation quality can be improved by up to 1.5 BLEU points under the low-resource condition.The performance of the POS tagger is also improved using the multi-task learning scheme.",1.0
SKG_MT_125,https://openalex.org/W2946028745,2019,25,"['https://openalex.org/W2060127787', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2183341477', 'https://openalex.org/W2576482813', 'https://openalex.org/W2586559132', 'https://openalex.org/W2604164736', 'https://openalex.org/W2739894144', 'https://openalex.org/W2773728868', 'https://openalex.org/W2798569372', 'https://openalex.org/W2949952998', 'https://openalex.org/W2963084773', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964096343', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564']","Chunpeng Ma, Akihiro Tamura, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_128,https://openalex.org/W2970900338,2019,14,"['https://openalex.org/W630532510', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2587694128', 'https://openalex.org/W2595715041', 'https://openalex.org/W2726119835', 'https://openalex.org/W2794365787', 'https://openalex.org/W2902510077', 'https://openalex.org/W2903490366', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964343359', 'https://openalex.org/W4385245566']","This paper describes the Global Tone Communication Co., Ltd.’s submission of the WMT19 shared news translation task. We participate in six directions: English to (Gujarati, Lithuanian and Finnish) and (Gujarati, Lithuanian and Finnish) to English. Further, we get the best BLEU scores in the directions of English to Gujarati and Lithuanian to English (28.2 and 36.3 respectively) among all the participants. The submitted systems mainly focus on back-translation, knowledge distillation and reranking to build a competitive model for this task. Also, we apply language model to filter monolingual data, back-translated data and parallel data. The techniques we apply for data filtering include filtering by rules, language models. Besides, We conduct several experiments to validate different knowledge distillation techniques and right-to-left (R2L) reranking.",1.0
SKG_MT_129,https://openalex.org/W3120441739,2020,2,"['https://openalex.org/W2060277733', 'https://openalex.org/W2153653739', 'https://openalex.org/W2550821151', 'https://openalex.org/W2919290281', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964308564']","In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these languages. The multilingual shared encoder/decoder has been used for all of them. Additionally, we applied back-translation to take advantage of the monolingual data. Finally, we have applied fine-tuning to improve the in-domain data. Each of these techniques brings improvements over the previous one. In the official evaluation, our system was ranked 1st in the Portuguese-to-Spanish direction, 2nd in the opposite direction, and 3rd in the Catalan-Spanish pair.",0.9952153110047848
SKG_MT_130,https://openalex.org/W2950428495,2019,52,"['https://openalex.org/W342285082', 'https://openalex.org/W1522301498', 'https://openalex.org/W1542713999', 'https://openalex.org/W2099471712', 'https://openalex.org/W2121870595', 'https://openalex.org/W2126725946', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145094598', 'https://openalex.org/W2294774419', 'https://openalex.org/W2295584157', 'https://openalex.org/W2493916176', 'https://openalex.org/W2514458001', 'https://openalex.org/W2546938941', 'https://openalex.org/W2561995736', 'https://openalex.org/W2578451281', 'https://openalex.org/W2578868202', 'https://openalex.org/W2594021297', 'https://openalex.org/W2604241755', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2758137671', 'https://openalex.org/W2768231593', 'https://openalex.org/W2768763386', 'https://openalex.org/W2798304389', 'https://openalex.org/W2798931235', 'https://openalex.org/W2803739890', 'https://openalex.org/W2859207840', 'https://openalex.org/W2890007195', 'https://openalex.org/W2898785264', 'https://openalex.org/W2909066711', 'https://openalex.org/W2914120296', 'https://openalex.org/W2952190837', 'https://openalex.org/W2962811598', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963022746', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963329925', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963804993', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964308564', 'https://openalex.org/W2997574889', 'https://openalex.org/W3121416198', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566']","Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT.",1.0
SKG_MT_131,https://openalex.org/W2175969983,2012,58,"['https://openalex.org/W91928571', 'https://openalex.org/W176283576', 'https://openalex.org/W222053410', 'https://openalex.org/W1598771940', 'https://openalex.org/W1955251501', 'https://openalex.org/W1985514943', 'https://openalex.org/W2008652694', 'https://openalex.org/W2008961349', 'https://openalex.org/W2018156128', 'https://openalex.org/W2044804339', 'https://openalex.org/W2083460949', 'https://openalex.org/W2097606805', 'https://openalex.org/W2099224638', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105644991', 'https://openalex.org/W2108460050', 'https://openalex.org/W2116316001', 'https://openalex.org/W2123301721', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2132891320', 'https://openalex.org/W2136544838', 'https://openalex.org/W2140906828', 'https://openalex.org/W2142623206', 'https://openalex.org/W2143263475', 'https://openalex.org/W2143564602', 'https://openalex.org/W2144600658', 'https://openalex.org/W2144900797', 'https://openalex.org/W2146685010', 'https://openalex.org/W2147196093', 'https://openalex.org/W2153508793', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153927306', 'https://openalex.org/W2158388102', 'https://openalex.org/W2160218441', 'https://openalex.org/W2162245945', 'https://openalex.org/W2164454850', 'https://openalex.org/W2168966090', 'https://openalex.org/W2251100697', 'https://openalex.org/W2296437937', 'https://openalex.org/W2401082558', 'https://openalex.org/W2437005631', 'https://openalex.org/W3203605404', 'https://openalex.org/W3209717902']",This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text. This is done by treating the parser’s derivation tree as a latent variable in a model that is trained to maximize reordering accuracy. We demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree. Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1,1.0
SKG_MT_132,https://openalex.org/W3120549770,2020,3,"['https://openalex.org/W22168010', 'https://openalex.org/W630532510', 'https://openalex.org/W2148708890', 'https://openalex.org/W2149327368', 'https://openalex.org/W2251367463', 'https://openalex.org/W2419539795', 'https://openalex.org/W2512848817', 'https://openalex.org/W2883872108', 'https://openalex.org/W2885185669', 'https://openalex.org/W2885250264', 'https://openalex.org/W2885950361', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888519496', 'https://openalex.org/W2949303037', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963011474', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963633299', 'https://openalex.org/W2969856879', 'https://openalex.org/W2970015022', 'https://openalex.org/W2994475016', 'https://openalex.org/W3035207248', 'https://openalex.org/W3098341425', 'https://openalex.org/W3104652516']","We describe the National Research Council of Canada (NRC) neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Transformer models, built using combinations of BPE-dropout, lexical modifications, and backtranslation.",0.99581589958159
SKG_MT_134,https://openalex.org/W2971031524,2019,37,"['https://openalex.org/W38126138', 'https://openalex.org/W1522301498', 'https://openalex.org/W2006969979', 'https://openalex.org/W2141532438', 'https://openalex.org/W2145094598', 'https://openalex.org/W2156985047', 'https://openalex.org/W2169360026', 'https://openalex.org/W2270070752', 'https://openalex.org/W2462831000', 'https://openalex.org/W2493916176', 'https://openalex.org/W2798931235', 'https://openalex.org/W2890007195', 'https://openalex.org/W2891555348', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898785264', 'https://openalex.org/W2899771611', 'https://openalex.org/W2914120296', 'https://openalex.org/W2944815030', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962735107', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964266061', 'https://openalex.org/W2997574889', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, Shuai Ma. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_136,https://openalex.org/W2539350388,2017,101,"['https://openalex.org/W2476580', 'https://openalex.org/W9292421', 'https://openalex.org/W22168010', 'https://openalex.org/W629278631', 'https://openalex.org/W1779279021', 'https://openalex.org/W2006832571', 'https://openalex.org/W2025403586', 'https://openalex.org/W2057301382', 'https://openalex.org/W2080133951', 'https://openalex.org/W2095674846', 'https://openalex.org/W2097927681', 'https://openalex.org/W2098447295', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103577799', 'https://openalex.org/W2110302976', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124807415', 'https://openalex.org/W2129734311', 'https://openalex.org/W2133990480', 'https://openalex.org/W2134800885', 'https://openalex.org/W2139875525', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148959489', 'https://openalex.org/W2151040995', 'https://openalex.org/W2184135559', 'https://openalex.org/W2232536453', 'https://openalex.org/W2250310654', 'https://openalex.org/W2250958040', 'https://openalex.org/W2250969425', 'https://openalex.org/W2251180427', 'https://openalex.org/W2251263615', 'https://openalex.org/W2251265976', 'https://openalex.org/W2251409655', 'https://openalex.org/W2252241921', 'https://openalex.org/W2292999442', 'https://openalex.org/W2506231060', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950186769', 'https://openalex.org/W3015810608', 'https://openalex.org/W3099514962', 'https://openalex.org/W3186536529', 'https://openalex.org/W4251136000', 'https://openalex.org/W4285719527', 'https://openalex.org/W4292023222']","Ella Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia Specia, Shuly Wintner. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. 2017.",0.9924812030075189
SKG_MT_137,https://openalex.org/W2155501171,2010,90,"['https://openalex.org/W1498763386', 'https://openalex.org/W1940278502', 'https://openalex.org/W1972686387', 'https://openalex.org/W1995291277', 'https://openalex.org/W2038721957', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2128192613', 'https://openalex.org/W2142943074', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250245948', 'https://openalex.org/W2621423192']","We conduct a pilot study for task-oriented evaluation of Multiword Expression (MWE) in Statistical Machine Translation (SMT). We propose two different integration strategies for MWE in SMT, which take advantage of differ-ent degrees of MWE semantic compositional-ity and yield complementary improvements in SMT quality on a large-scale translation task.1 1",0.9947643979057592
SKG_MT_138,https://openalex.org/W2887920589,2018,191,"['https://openalex.org/W1753482797', 'https://openalex.org/W2013438303', 'https://openalex.org/W2081212507', 'https://openalex.org/W2103592412', 'https://openalex.org/W2120349760', 'https://openalex.org/W2133564696', 'https://openalex.org/W2162179097', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2752630748', 'https://openalex.org/W2785847164', 'https://openalex.org/W2798931235', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963011474', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963260927', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963826397', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965980827', 'https://openalex.org/W3167238645', 'https://openalex.org/W4294389941', 'https://openalex.org/W4298393544']","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual ""seed models"", which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of ""similar-language regularization"", where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",1.0
SKG_MT_140,https://openalex.org/W3012989741,2020,18,"['https://openalex.org/W342285082', 'https://openalex.org/W569478347', 'https://openalex.org/W2099471712', 'https://openalex.org/W2126725946', 'https://openalex.org/W2153579005', 'https://openalex.org/W2251765408', 'https://openalex.org/W2294774419', 'https://openalex.org/W2493916176', 'https://openalex.org/W2513016193', 'https://openalex.org/W2559655401', 'https://openalex.org/W2561995736', 'https://openalex.org/W2577946330', 'https://openalex.org/W2593390416', 'https://openalex.org/W2594021297', 'https://openalex.org/W2604272474', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2760424551', 'https://openalex.org/W2788353357', 'https://openalex.org/W2887838996', 'https://openalex.org/W2889191148', 'https://openalex.org/W2889894161', 'https://openalex.org/W2933610451', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963472233', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964266061', 'https://openalex.org/W3038022805', 'https://openalex.org/W3104723404']","Crosslingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from machine translation to transfer learning. Adversarial training has shown impressive success in learning crosslingual embeddings and the associated word translation task without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this article, we investigate adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. We use two types of refinement procedures sequentially after obtaining the trained encoders and mappings from the adversarial training, namely, refinement with Procrustes solution and refinement with symmetric re-weighting. Extensive experimentations with high- and low-resource languages from two different data sets show that our method achieves better performance than existing adversarial and non-adversarial approaches and is also competitive with the supervised system. Along with performing comprehensive ablation studies to understand the contribution of different components of our adversarial model, we also conduct a thorough analysis of the refinement procedures to understand their effects.",1.0
SKG_MT_142,https://openalex.org/W2970480900,2019,47,"['https://openalex.org/W8895266', 'https://openalex.org/W143218990', 'https://openalex.org/W161156596', 'https://openalex.org/W222053410', 'https://openalex.org/W658020064', 'https://openalex.org/W1486649854', 'https://openalex.org/W1832693441', 'https://openalex.org/W1902237438', 'https://openalex.org/W1956103381', 'https://openalex.org/W2018116724', 'https://openalex.org/W2041232209', 'https://openalex.org/W2053341212', 'https://openalex.org/W2105673178', 'https://openalex.org/W2125447031', 'https://openalex.org/W2134414769', 'https://openalex.org/W2159932990', 'https://openalex.org/W2189550228', 'https://openalex.org/W2250539671', 'https://openalex.org/W2329847998', 'https://openalex.org/W2566433528', 'https://openalex.org/W2584009249', 'https://openalex.org/W2604799547', 'https://openalex.org/W2617566453', 'https://openalex.org/W2740094762', 'https://openalex.org/W2760830962', 'https://openalex.org/W2793585215', 'https://openalex.org/W2794557536', 'https://openalex.org/W2798851324', 'https://openalex.org/W2808681756', 'https://openalex.org/W2888173624', 'https://openalex.org/W2891348164', 'https://openalex.org/W2897396766', 'https://openalex.org/W2914442349', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962753250', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962937198', 'https://openalex.org/W2963018534', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963241138', 'https://openalex.org/W2963366196', 'https://openalex.org/W2963373786', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963631950', 'https://openalex.org/W2963667126', 'https://openalex.org/W2963918774', 'https://openalex.org/W2964008635', 'https://openalex.org/W2964201867', 'https://openalex.org/W2964222296', 'https://openalex.org/W2964321064', 'https://openalex.org/W2964351105', 'https://openalex.org/W4289306372', 'https://openalex.org/W4297699332', 'https://openalex.org/W4298386828', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4302780227']","Zhijing Jin, Di Jin, Jonas Mueller, Nicholas Matthews, Enrico Santus. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",0.9937888198757764
SKG_MT_143,https://openalex.org/W2889606145,2018,59,"['https://openalex.org/W623762497', 'https://openalex.org/W642242777', 'https://openalex.org/W648786980', 'https://openalex.org/W2016589492', 'https://openalex.org/W2049872937', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2176263492', 'https://openalex.org/W2296073425', 'https://openalex.org/W2460550122', 'https://openalex.org/W2466062786', 'https://openalex.org/W2546938941', 'https://openalex.org/W2607987856', 'https://openalex.org/W2751527518', 'https://openalex.org/W2785093437', 'https://openalex.org/W2794365787', 'https://openalex.org/W2803417661', 'https://openalex.org/W2803569830', 'https://openalex.org/W2804044248', 'https://openalex.org/W2807964741', 'https://openalex.org/W2888442053', 'https://openalex.org/W2903012348', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963096987', 'https://openalex.org/W2963163972', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963500743', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566']","Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the accuracy drop (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.",0.9947089947089948
SKG_MT_144,https://openalex.org/W2146813944,2011,15,"['https://openalex.org/W1575907248', 'https://openalex.org/W2078861931', 'https://openalex.org/W2088781183', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116317530', 'https://openalex.org/W2123301721', 'https://openalex.org/W2131988669', 'https://openalex.org/W2132961219', 'https://openalex.org/W2147192413', 'https://openalex.org/W2149327368', 'https://openalex.org/W2186177529', 'https://openalex.org/W2402900901', 'https://openalex.org/W2467575451', 'https://openalex.org/W2600062507', 'https://openalex.org/W2785522575', 'https://openalex.org/W2787109023', 'https://openalex.org/W2915395439']","Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 1",0.9876543209876544
SKG_MT_145,https://openalex.org/W3028712640,2020,2,"['https://openalex.org/W1524333225', 'https://openalex.org/W1902237438', 'https://openalex.org/W1970987322', 'https://openalex.org/W2183341477', 'https://openalex.org/W2466918907', 'https://openalex.org/W2507959295', 'https://openalex.org/W2530876040', 'https://openalex.org/W2582956876', 'https://openalex.org/W2605131327', 'https://openalex.org/W2605202026', 'https://openalex.org/W2936969148', 'https://openalex.org/W2949328740', 'https://openalex.org/W2952167535', 'https://openalex.org/W2955541912', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963260927', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963834942', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964104866', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964144280', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964308564', 'https://openalex.org/W3006988520', 'https://openalex.org/W3012492057', 'https://openalex.org/W3015703505']","End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work -- by up to 9 BLEU on our low-resource setting.",1.0
SKG_MT_147,https://openalex.org/W2251339511,2013,9,"['https://openalex.org/W137989762', 'https://openalex.org/W1909398668', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109704865', 'https://openalex.org/W2110481933', 'https://openalex.org/W2127849236', 'https://openalex.org/W2140343992', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2163361328', 'https://openalex.org/W2166905217', 'https://openalex.org/W3166369720']","We present a systematic study of the effect of crowdsourced translations on Machine Translation performance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation. 1",1.0
SKG_MT_151,https://openalex.org/W2744961584,2017,0,"['https://openalex.org/W1562877468', 'https://openalex.org/W1998498767', 'https://openalex.org/W2095705004', 'https://openalex.org/W2110006374', 'https://openalex.org/W2117278770', 'https://openalex.org/W2134800885', 'https://openalex.org/W2147488445', 'https://openalex.org/W2411447566', 'https://openalex.org/W2513592723', 'https://openalex.org/W2574872930', 'https://openalex.org/W2609616693', 'https://openalex.org/W2951270685', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964043796']","We describe the University of Maryland machine translation systems submitted to the WMT17 German-English Bandit Learning Task. The task is to adapt a translation system to a new domain, using only bandit feedback: the system receives a German sentence to translate, produces an English sentence, and only gets a scalar score as feedback. Targeting these two challenges (adaptation and bandit learning), we built a standard neural machine translation system and extended it in two ways: (1) robust reinforcement learning techniques to learn effectively from the bandit feedback, and (2) domain adaptation using data selection from a large corpus of parallel data.",1.0
SKG_MT_152,https://openalex.org/W3103915675,2020,21,"['https://openalex.org/W630532510', 'https://openalex.org/W1563026721', 'https://openalex.org/W1902237438', 'https://openalex.org/W1915251500', 'https://openalex.org/W1985081760', 'https://openalex.org/W1995945562', 'https://openalex.org/W2041532239', 'https://openalex.org/W2101105183', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136156618', 'https://openalex.org/W2141440284', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2183341477', 'https://openalex.org/W2252272516', 'https://openalex.org/W2474829637', 'https://openalex.org/W2550821151', 'https://openalex.org/W2573062194', 'https://openalex.org/W2595715041', 'https://openalex.org/W2611838487', 'https://openalex.org/W2896457183', 'https://openalex.org/W2913659301', 'https://openalex.org/W2947046281', 'https://openalex.org/W2949611393', 'https://openalex.org/W2949973181', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963631907', 'https://openalex.org/W2963939565', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971120622', 'https://openalex.org/W2994928925', 'https://openalex.org/W3006381853', 'https://openalex.org/W3014432386', 'https://openalex.org/W3017068741', 'https://openalex.org/W3030384960', 'https://openalex.org/W3104652516', 'https://openalex.org/W4241595375', 'https://openalex.org/W4241645538', 'https://openalex.org/W4285719527', 'https://openalex.org/W4298393544', 'https://openalex.org/W4310299640', 'https://openalex.org/W4381032693', 'https://openalex.org/W4385245566']","Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.",0.9938650306748468
SKG_MT_154,https://openalex.org/W2920195923,2019,10,"['https://openalex.org/W635530177', 'https://openalex.org/W1625582487', 'https://openalex.org/W1682403713', 'https://openalex.org/W2024932032', 'https://openalex.org/W2095705004', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153653739', 'https://openalex.org/W2183341477', 'https://openalex.org/W2187442798', 'https://openalex.org/W2251798853', 'https://openalex.org/W2276085145', 'https://openalex.org/W2399033357', 'https://openalex.org/W2525778437', 'https://openalex.org/W2604368306', 'https://openalex.org/W2608615602', 'https://openalex.org/W2608870981', 'https://openalex.org/W2618463334', 'https://openalex.org/W2740553716', 'https://openalex.org/W2743229121', 'https://openalex.org/W2757592053', 'https://openalex.org/W2765271678', 'https://openalex.org/W2788448041', 'https://openalex.org/W2795933031', 'https://openalex.org/W2885185669', 'https://openalex.org/W2886198413', 'https://openalex.org/W2891713103', 'https://openalex.org/W2896060389', 'https://openalex.org/W2909737760', 'https://openalex.org/W2928941594', 'https://openalex.org/W2949555952', 'https://openalex.org/W2949734169', 'https://openalex.org/W2950179609', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962712961', 'https://openalex.org/W2963018920', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The semi-parametric nature of our approach opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.",1.0
SKG_MT_155,https://openalex.org/W2764124976,2018,13,"['https://openalex.org/W6908809', 'https://openalex.org/W1591801644', 'https://openalex.org/W2100664567', 'https://openalex.org/W2124807415', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047', 'https://openalex.org/W2172140247', 'https://openalex.org/W2194775991', 'https://openalex.org/W2304113845', 'https://openalex.org/W2311921240', 'https://openalex.org/W2333611780', 'https://openalex.org/W2339995566', 'https://openalex.org/W2410217169', 'https://openalex.org/W2527845440', 'https://openalex.org/W2539201987', 'https://openalex.org/W2594407953', 'https://openalex.org/W2613904329', 'https://openalex.org/W2625092622', 'https://openalex.org/W2626778328', 'https://openalex.org/W2784966376', 'https://openalex.org/W2949888546', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963937700', 'https://openalex.org/W2964308564', 'https://openalex.org/W3101227480', 'https://openalex.org/W3204406378']","We explore two solutions to the problem of mistranslating rare words in neural machine translation. First, we argue that the standard output layer, which computes the inner product of a vector representing the context with all possible output word embeddings, rewards frequent words disproportionately, and we propose to fix the norms of both vectors to a constant value. Second, we integrate a simple lexical module which is jointly trained with the rest of the model. We evaluate our approaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrase-based translation in nearly all settings.",1.0
SKG_MT_156,https://openalex.org/W2112473332,2013,3,"['https://openalex.org/W22168010', 'https://openalex.org/W1603903339', 'https://openalex.org/W1631260214', 'https://openalex.org/W1769106613', 'https://openalex.org/W1823542770', 'https://openalex.org/W1916559533', 'https://openalex.org/W1972567251', 'https://openalex.org/W2093717447', 'https://openalex.org/W2101044864', 'https://openalex.org/W2105891181', 'https://openalex.org/W2108161779', 'https://openalex.org/W2113242816', 'https://openalex.org/W2113900137', 'https://openalex.org/W2122373020', 'https://openalex.org/W2132001515', 'https://openalex.org/W2136646623', 'https://openalex.org/W2138470801', 'https://openalex.org/W2151594415', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165666205', 'https://openalex.org/W2912934387', 'https://openalex.org/W2914369697', 'https://openalex.org/W3105962861', 'https://openalex.org/W3170138813', 'https://openalex.org/W3204209287']","We propose the use of stacking, an ensem-ble learning technique, to the statistical machine translation (SMT) models. A diverse ensem-ble of weak learners is created using the same SMT engine (a hierarchical phrase-based sys-tem) by manipulating the training data and a strong model is created by combining the weak models on-the-fly. Experimental results on two language pairs and three different sizes of train-ing data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model. 1",1.0
SKG_MT_157,https://openalex.org/W1697844855,2010,19,"['https://openalex.org/W1631260214', 'https://openalex.org/W1973923101', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118536060', 'https://openalex.org/W2123126659', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2140133598', 'https://openalex.org/W2140343992', 'https://openalex.org/W2146418175', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2154581043', 'https://openalex.org/W2158065314', 'https://openalex.org/W2437005631']","Hierarchical phrase-based (HPB) translation provides a powerful mechanism to capture both short and long distance phrase reorderings. However, the phrase reorderings lack of contextual information in conventional HPB systems. This paper proposes a contextdependent phrase reordering approach that uses the maximum entropy (MaxEnt) model to help the HPB decoder select appropriate reordering patterns. We classify translation rules into several reordering patterns, and build a MaxEnt model for each pattern based on various contextual features. We integrate the MaxEnt models into the HPB model. Experimental results show that our approach achieves significant improvements over a standard HPB system on large-scale translation tasks. On Chinese-to-English translation, the absolute improvements in BLEU (caseinsensitive) range from 1.2 to 2.1. 1",1.0
SKG_MT_158,https://openalex.org/W2963922633,2018,19,"['https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W1793412061', 'https://openalex.org/W1902237438', 'https://openalex.org/W1969974515', 'https://openalex.org/W1985514943', 'https://openalex.org/W2008961349', 'https://openalex.org/W2036291627', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108460050', 'https://openalex.org/W2118206542', 'https://openalex.org/W2136544838', 'https://openalex.org/W2144900797', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153508793', 'https://openalex.org/W2168184608', 'https://openalex.org/W2175969983', 'https://openalex.org/W2242975712', 'https://openalex.org/W2250307129', 'https://openalex.org/W2250947025', 'https://openalex.org/W2251670534', 'https://openalex.org/W2251843378', 'https://openalex.org/W2251997274', 'https://openalex.org/W2294953265', 'https://openalex.org/W2401082558', 'https://openalex.org/W2576482813', 'https://openalex.org/W2963424553', 'https://openalex.org/W2964121744', 'https://openalex.org/W3202296894', 'https://openalex.org/W4239181501', 'https://openalex.org/W4254408171']","The word order between source and target languages significantly influences the translation quality. Preordering can effectively address this problem. Previous preordering methods require a manual feature design, making language dependent design difficult. In this paper, we propose a preordering method with recursive neural networks that learn features from raw inputs. Experiments show the proposed method is comparable to the state-of-the-art method but without a manual feature design.",1.0
SKG_MT_159,https://openalex.org/W2952614664,2019,73,"['https://openalex.org/W32870568', 'https://openalex.org/W99485931', 'https://openalex.org/W171476473', 'https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W2119727789', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124202134', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2155732278', 'https://openalex.org/W2168939893', 'https://openalex.org/W2251743902', 'https://openalex.org/W2295582178', 'https://openalex.org/W2418388682', 'https://openalex.org/W2493916176', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555428947', 'https://openalex.org/W2567571499', 'https://openalex.org/W2610245951', 'https://openalex.org/W2613904329', 'https://openalex.org/W2616180702', 'https://openalex.org/W2741602058', 'https://openalex.org/W2741917668', 'https://openalex.org/W2752630748', 'https://openalex.org/W2756566411', 'https://openalex.org/W2778814079', 'https://openalex.org/W2798931235', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888456631', 'https://openalex.org/W2889894161', 'https://openalex.org/W2890007195', 'https://openalex.org/W2890731353', 'https://openalex.org/W2891896107', 'https://openalex.org/W2902698002', 'https://openalex.org/W2912095972', 'https://openalex.org/W2913659301', 'https://openalex.org/W2914120296', 'https://openalex.org/W2950577311', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963983698', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964308564', 'https://openalex.org/W3015703505', 'https://openalex.org/W3041866211', 'https://openalex.org/W3082674894', 'https://openalex.org/W3104652516', 'https://openalex.org/W3204406378', 'https://openalex.org/W4288601832', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer.",1.0
SKG_MT_161,https://openalex.org/W3034586846,2020,78,"['https://openalex.org/W1522301498', 'https://openalex.org/W2089629691', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121457870', 'https://openalex.org/W2127141656', 'https://openalex.org/W2419292002', 'https://openalex.org/W2529548870', 'https://openalex.org/W2613904329', 'https://openalex.org/W2896234185', 'https://openalex.org/W2901607128', 'https://openalex.org/W2905933322', 'https://openalex.org/W2936969148', 'https://openalex.org/W2945700568', 'https://openalex.org/W2946200149', 'https://openalex.org/W2949328740', 'https://openalex.org/W2950613790', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952650870', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963414781', 'https://openalex.org/W2963418779', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963834942', 'https://openalex.org/W2964078338', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964243274', 'https://openalex.org/W2964265128', 'https://openalex.org/W2970730223', 'https://openalex.org/W2972448360', 'https://openalex.org/W2995999067', 'https://openalex.org/W4297747548', 'https://openalex.org/W4385245566']","In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.",0.992
SKG_MT_163,https://openalex.org/W3086977883,2020,2,"['https://openalex.org/W11511616', 'https://openalex.org/W22168010', 'https://openalex.org/W630532510', 'https://openalex.org/W1724438581', 'https://openalex.org/W1810943226', 'https://openalex.org/W2157331557', 'https://openalex.org/W2184135559', 'https://openalex.org/W2226734577', 'https://openalex.org/W2300242332', 'https://openalex.org/W2773207997', 'https://openalex.org/W2886014761', 'https://openalex.org/W2915589364', 'https://openalex.org/W2947946877', 'https://openalex.org/W2960643108', 'https://openalex.org/W2963114950', 'https://openalex.org/W2963122961', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963318827', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963576971', 'https://openalex.org/W2963674932', 'https://openalex.org/W2963689957', 'https://openalex.org/W2963697731', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964247056', 'https://openalex.org/W2964264300', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970565456', 'https://openalex.org/W2982041622', 'https://openalex.org/W2994849160', 'https://openalex.org/W3009589555', 'https://openalex.org/W3012638462', 'https://openalex.org/W3022331012', 'https://openalex.org/W3028200550']","The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8$\times$ smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3$\times$ reduction in run-time memory footprints and 3.5$\times$ speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.",1.0
SKG_MT_165,https://openalex.org/W2250597803,2015,49,"['https://openalex.org/W1508977358', 'https://openalex.org/W1581407678', 'https://openalex.org/W2015333112', 'https://openalex.org/W2064675550', 'https://openalex.org/W2078861931', 'https://openalex.org/W2100175927', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103305545', 'https://openalex.org/W2107878631', 'https://openalex.org/W2114480995', 'https://openalex.org/W2115081467', 'https://openalex.org/W2126725946', 'https://openalex.org/W2127713198', 'https://openalex.org/W2131744502', 'https://openalex.org/W2133280805', 'https://openalex.org/W2133459682', 'https://openalex.org/W2153579005', 'https://openalex.org/W2250188786', 'https://openalex.org/W2250421192', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250600942', 'https://openalex.org/W2250672026', 'https://openalex.org/W2250790822', 'https://openalex.org/W2251610689', 'https://openalex.org/W2251939518', 'https://openalex.org/W2270190199', 'https://openalex.org/W2294913621', 'https://openalex.org/W2963355447', 'https://openalex.org/W4294170691']","Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve best results.We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks.For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively.We also show how training data is computed automatically from WMT ranks data.",0.9950738916256158
SKG_MT_166,https://openalex.org/W7126385421,2020,0,[],,1.0
SKG_MT_168,https://openalex.org/W2963599677,2017,53,"['https://openalex.org/W6908809', 'https://openalex.org/W1026270304', 'https://openalex.org/W1771459135', 'https://openalex.org/W1810943226', 'https://openalex.org/W1815076433', 'https://openalex.org/W1902237438', 'https://openalex.org/W2016589492', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110104386', 'https://openalex.org/W2118434577', 'https://openalex.org/W2118536060', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2175740768', 'https://openalex.org/W2194775991', 'https://openalex.org/W2250653840', 'https://openalex.org/W2274287116', 'https://openalex.org/W2525778437', 'https://openalex.org/W2561274697', 'https://openalex.org/W2949335953', 'https://openalex.org/W2950527759', 'https://openalex.org/W2950621961', 'https://openalex.org/W2962965465', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964335273', 'https://openalex.org/W2964350391', 'https://openalex.org/W4241645538', 'https://openalex.org/W4303633609']","Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with its capability in modeling complex functions and capturing complex linguistic structures. However NMT with deep architecture in its encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often makes the optimization much more difficult. To address this problem we propose a novel linear associative units (LAU) to reduce the gradient propagation path inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs uses linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported on results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.",1.0
SKG_MT_169,https://openalex.org/W2250531756,2014,133,"['https://openalex.org/W22627370', 'https://openalex.org/W1581218697', 'https://openalex.org/W2025746854', 'https://openalex.org/W2031196180', 'https://openalex.org/W2042262613', 'https://openalex.org/W2049633694', 'https://openalex.org/W2061235289', 'https://openalex.org/W2081388731', 'https://openalex.org/W2083245912', 'https://openalex.org/W2108373063', 'https://openalex.org/W2109802560', 'https://openalex.org/W2109881807', 'https://openalex.org/W2115769265', 'https://openalex.org/W2116316001', 'https://openalex.org/W2118119027', 'https://openalex.org/W2119954850', 'https://openalex.org/W2124672726', 'https://openalex.org/W2137095957', 'https://openalex.org/W2143927888', 'https://openalex.org/W2156422881', 'https://openalex.org/W2168652246', 'https://openalex.org/W2250415286', 'https://openalex.org/W2251362149', 'https://openalex.org/W2251561240', 'https://openalex.org/W2251831371']","We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.",1.0
SKG_MT_171,https://openalex.org/W2251349730,2015,6,"['https://openalex.org/W22168010', 'https://openalex.org/W1489181569', 'https://openalex.org/W1606076566', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119727789', 'https://openalex.org/W2127797489', 'https://openalex.org/W2146574666', 'https://openalex.org/W2170204377', 'https://openalex.org/W2181989146', 'https://openalex.org/W2250548645', 'https://openalex.org/W2251032226', 'https://openalex.org/W2437005631']","Akiva Miura, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2015.",1.0
SKG_MT_172,https://openalex.org/W2971347700,2019,96,"['https://openalex.org/W1522301498', 'https://openalex.org/W2141971526', 'https://openalex.org/W2159086733', 'https://openalex.org/W2514996388', 'https://openalex.org/W2608029998', 'https://openalex.org/W2767019613', 'https://openalex.org/W2794365787', 'https://openalex.org/W2796108585', 'https://openalex.org/W2799051177', 'https://openalex.org/W2806412155', 'https://openalex.org/W2808508619', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2900013662', 'https://openalex.org/W2952446148', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963816901', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2970038984', 'https://openalex.org/W3100623455', 'https://openalex.org/W3104636952', 'https://openalex.org/W4385245566']","Modern sentence-level NMT systems often produce plausible translations of isolated sentences. However, when put in context, these translations may end up being inconsistent with each other. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. For training, the DocRepair model requires only monolingual document-level data in the target language. It is trained as a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. We show that this approach successfully imitates inconsistencies we aim to fix: using contrastive evaluation, we show large improvements in the translation of several contextual phenomena in an English-Russian translation task, as well as improvements in the BLEU score. We also conduct a human evaluation and show a strong preference of the annotators to corrected translations over the baseline ones. Moreover, we analyze which discourse phenomena are hard to capture using monolingual data only.",1.0
SKG_MT_173,https://openalex.org/W2982399380,2020,1222,"['https://openalex.org/W131533222', 'https://openalex.org/W1544827683', 'https://openalex.org/W1599016936', 'https://openalex.org/W2130158090', 'https://openalex.org/W2251939518', 'https://openalex.org/W2418388682', 'https://openalex.org/W2427527485', 'https://openalex.org/W2462831000', 'https://openalex.org/W2768716007', 'https://openalex.org/W2787560479', 'https://openalex.org/W2799054028', 'https://openalex.org/W2899663614', 'https://openalex.org/W2913443447', 'https://openalex.org/W2914120296', 'https://openalex.org/W2922709902', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945260553', 'https://openalex.org/W2950300355', 'https://openalex.org/W2950577311', 'https://openalex.org/W2950813464', 'https://openalex.org/W2952913664', 'https://openalex.org/W2952942550', 'https://openalex.org/W2962753370', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963846996', 'https://openalex.org/W2964040452', 'https://openalex.org/W2965373594', 'https://openalex.org/W2969740599', 'https://openalex.org/W2975059944', 'https://openalex.org/W2978670439']","We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.",0.9955555555555556
SKG_MT_174,https://openalex.org/W2165188277,2010,24,"['https://openalex.org/W1514971736', 'https://openalex.org/W1982816048', 'https://openalex.org/W2004156982', 'https://openalex.org/W2006969979', 'https://openalex.org/W2018482254', 'https://openalex.org/W2100271871', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101181502', 'https://openalex.org/W2111856253', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126118742', 'https://openalex.org/W2144746247', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148354767', 'https://openalex.org/W3197744324']","[EN] This work deals with the application of confidence measures within an interactive-predictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.",1.0
SKG_MT_177,https://openalex.org/W3120090562,2020,3,"['https://openalex.org/W222053410', 'https://openalex.org/W630532510', 'https://openalex.org/W1934041838', 'https://openalex.org/W2141440284', 'https://openalex.org/W2159755860', 'https://openalex.org/W2161227214', 'https://openalex.org/W2250342921', 'https://openalex.org/W2552124255', 'https://openalex.org/W2756566411', 'https://openalex.org/W2801219566', 'https://openalex.org/W2890007195', 'https://openalex.org/W2949973181', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962717763', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641561', 'https://openalex.org/W2971031401', 'https://openalex.org/W2988739750', 'https://openalex.org/W3107826490']","Statistical machine translation (SMT) which was the dominant paradigm in machine translation (MT) research for nearly three decades has recently been superseded by the end-to-end deep learning approaches to MT. Although deep neural models produce state-of-the-art results in many translation tasks, they are found to under-perform on resource-poor scenarios. Despite some success, none of the present-day benchmarks that have tried to overcome this problem can be regarded as a universal solution to the problem of translation of many low-resource languages. In this work, we investigate the performance of phrase-based SMT (PB-SMT) and neural MT (NMT) on a rarely-tested low-resource language-pair, English-to-Tamil, taking a specialised data domain (software localisation) into consideration. 
In particular, we produce rankings of our MT systems via a social media platform-based human evaluation scheme, and demonstrate our findings in the low-resource domain-specific text translation task.",1.0
SKG_MT_178,https://openalex.org/W2106595396,2013,22,"['https://openalex.org/W839768', 'https://openalex.org/W39836547', 'https://openalex.org/W1498238796', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W1989658336', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115410424', 'https://openalex.org/W2115848042', 'https://openalex.org/W2117278770', 'https://openalex.org/W2117339222', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132001515', 'https://openalex.org/W2136477195', 'https://openalex.org/W2137387514', 'https://openalex.org/W2147984332', 'https://openalex.org/W2157956963', 'https://openalex.org/W2159755860', 'https://openalex.org/W2892587090', 'https://openalex.org/W3203276480', 'https://openalex.org/W3203752787']","Previous research on domain adaptation (DA) for statistical machine translation (SMT) has mainly focused on the translation model (TM) and the language model (LM). To the best of our knowledge, there is no previous work on reordering model (RM) adaptation for phrase-based SMT. In this paper, we demonstrate that mixture model adaptation of a lexical-ized RM can significantly improve SMT per-formance, even when the system already con-tains a domain-adapted TM and LM. We find that, surprisingly, different training corpora can vary widely in their reordering character-istics for particular phrase pairs. Furthermore, particular training corpora may be highly suit-able for training the TM or the LM, but unsuit-able for training the RM, or vice versa, so mix-ture weights for these models should be esti-mated separately. An additional contribution of the paper is to propose two improvements to mixture model adaptation: smoothing the in-domain sample, and weighting instances by document frequency. Applied to mixture RMs in our experiments, these techniques (es-pecially smoothing) yield significant perfor-mance improvements. 1",1.0
SKG_MT_180,https://openalex.org/W2115056464,2013,42,"['https://openalex.org/W39836547', 'https://openalex.org/W195533127', 'https://openalex.org/W222053410', 'https://openalex.org/W1567365482', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W1981617416', 'https://openalex.org/W1989658336', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103864529', 'https://openalex.org/W2115410424', 'https://openalex.org/W2117278770', 'https://openalex.org/W2117339222', 'https://openalex.org/W2122270629', 'https://openalex.org/W2123084125', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132001515', 'https://openalex.org/W2136477195', 'https://openalex.org/W2137387514', 'https://openalex.org/W2137698233', 'https://openalex.org/W2147984332', 'https://openalex.org/W2159755860', 'https://openalex.org/W2166776180', 'https://openalex.org/W2356613612', 'https://openalex.org/W3110683067', 'https://openalex.org/W3203276480', 'https://openalex.org/W3203752787']","This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. Thus, we obtain a decoding feature whose value represents the phrase pair’s closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Experiments on large scale NIST evaluation data show improvements over strong baselines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre. 1",1.0
SKG_MT_183,https://openalex.org/W2798608854,2018,19,"['https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1916559533', 'https://openalex.org/W2113104171', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2525778437', 'https://openalex.org/W2594990650', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963699608', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3211848854']","We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our model does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 1.5 BLEU improvements over the state-of-the-art baselines.",1.0
SKG_MT_186,https://openalex.org/W3018473580,2020,1,"['https://openalex.org/W22168010', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2140646431', 'https://openalex.org/W2144600658', 'https://openalex.org/W2151996595', 'https://openalex.org/W2163038970', 'https://openalex.org/W2184135559', 'https://openalex.org/W2419539795', 'https://openalex.org/W2577255746', 'https://openalex.org/W2582446770', 'https://openalex.org/W2799051177', 'https://openalex.org/W2891534142', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962882341', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2971248291', 'https://openalex.org/W2972972637']","The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.",1.0
SKG_MT_187,https://openalex.org/W2250478613,2014,2,"['https://openalex.org/W22168010', 'https://openalex.org/W23690963', 'https://openalex.org/W2046670498', 'https://openalex.org/W2124807415', 'https://openalex.org/W2398164674', 'https://openalex.org/W2595715041', 'https://openalex.org/W3209268154']","We propose a demonstration of a domainspecific terminology checking service which works on top of any generic blackbox MT, and only requires access to a bilingual terminology resource in the domain.In cases where an incorrect translation of a source term was proposed by the generic MT service, our service locates the wrong translation of the term in the target and suggests a terminologically correct translation for this term.",1.0
SKG_MT_189,https://openalex.org/W2952474700,2019,53,"['https://openalex.org/W1905522558', 'https://openalex.org/W2006969979', 'https://openalex.org/W2049633694', 'https://openalex.org/W2117278770', 'https://openalex.org/W2296073425', 'https://openalex.org/W2396575863', 'https://openalex.org/W2515631395', 'https://openalex.org/W2525778437', 'https://openalex.org/W2567571499', 'https://openalex.org/W2617128460', 'https://openalex.org/W2741838462', 'https://openalex.org/W2750588180', 'https://openalex.org/W2756978580', 'https://openalex.org/W2898846200', 'https://openalex.org/W2902918014', 'https://openalex.org/W2905933322', 'https://openalex.org/W2919188216', 'https://openalex.org/W2923622379', 'https://openalex.org/W2946379889', 'https://openalex.org/W2949920209', 'https://openalex.org/W2952650870', 'https://openalex.org/W2962890089', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963281280', 'https://openalex.org/W2963343509', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963919854', 'https://openalex.org/W2963949210', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964309657', 'https://openalex.org/W3099907503', 'https://openalex.org/W3208011254', 'https://openalex.org/W4302343710']","Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a “co-curricular learning” method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the “co-curriculum”. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",1.0
SKG_MT_190,https://openalex.org/W3089958104,2020,5,"['https://openalex.org/W1522301498', 'https://openalex.org/W1971034924', 'https://openalex.org/W2020018978', 'https://openalex.org/W2091671846', 'https://openalex.org/W2094472029', 'https://openalex.org/W2097550833', 'https://openalex.org/W2098880560', 'https://openalex.org/W2130942839', 'https://openalex.org/W2170198242', 'https://openalex.org/W2183341477', 'https://openalex.org/W2396928039', 'https://openalex.org/W2473329891', 'https://openalex.org/W2626778328', 'https://openalex.org/W2804945011', 'https://openalex.org/W2896457183', 'https://openalex.org/W2917128112', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962775474', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963974889', 'https://openalex.org/W2970157301', 'https://openalex.org/W2970849641', 'https://openalex.org/W2971167298', 'https://openalex.org/W2981852735', 'https://openalex.org/W2982399380', 'https://openalex.org/W2989539713', 'https://openalex.org/W3103469330', 'https://openalex.org/W3107826490']","Slot-filling, Translation, Intent classification, and Language identification, or STIL, is a newly-proposed task for multilingual Natural Language Understanding (NLU). By performing simultaneous slot filling and translation into a single output language (English in this case), some portion of downstream system components can be monolingual, reducing development and maintenance cost. Results are given using the multilingual BART model (Liu et al., 2020) fine-tuned on 7 languages using the MultiATIS++ dataset. When no translation is performed, mBART's performance is comparable to the current state of the art system (Cross-Lingual BERT by Xu et al. (2020)) for the languages tested, with better average intent classification accuracy (96.07% versus 95.50%) but worse average slot F1 (89.87% versus 90.81%). When simultaneous translation is performed, average intent classification accuracy degrades by only 1.7% relative and average slot F1 degrades by only 1.2% relative.",0.99644128113879
SKG_MT_194,https://openalex.org/W2137550424,2010,30,"['https://openalex.org/W24102868', 'https://openalex.org/W121569490', 'https://openalex.org/W201231365', 'https://openalex.org/W239287372', 'https://openalex.org/W1517947178', 'https://openalex.org/W1522263329', 'https://openalex.org/W1631260214', 'https://openalex.org/W1819520634', 'https://openalex.org/W1904457459', 'https://openalex.org/W1905522558', 'https://openalex.org/W1934041838', 'https://openalex.org/W1965660534', 'https://openalex.org/W1994303046', 'https://openalex.org/W1995560154', 'https://openalex.org/W2006969979', 'https://openalex.org/W2016856586', 'https://openalex.org/W2017802499', 'https://openalex.org/W2033830177', 'https://openalex.org/W2048978997', 'https://openalex.org/W2081808304', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101207453', 'https://openalex.org/W2105891181', 'https://openalex.org/W2108892043', 'https://openalex.org/W2111798208', 'https://openalex.org/W2113287090', 'https://openalex.org/W2114550122', 'https://openalex.org/W2115335620', 'https://openalex.org/W2117278770', 'https://openalex.org/W2121338597', 'https://openalex.org/W2123635983', 'https://openalex.org/W2126784811', 'https://openalex.org/W2133512280', 'https://openalex.org/W2135161317', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148675933', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154988249', 'https://openalex.org/W2156945896', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158065314', 'https://openalex.org/W2158195707', 'https://openalex.org/W2161227214', 'https://openalex.org/W2164394329', 'https://openalex.org/W2165132531', 'https://openalex.org/W2166905217', 'https://openalex.org/W2171074980', 'https://openalex.org/W2396575863', 'https://openalex.org/W2397036129', 'https://openalex.org/W2398009384', 'https://openalex.org/W2398887489', 'https://openalex.org/W2403214004', 'https://openalex.org/W2406343628', 'https://openalex.org/W2407402663', 'https://openalex.org/W2437005631', 'https://openalex.org/W2600716915', 'https://openalex.org/W2916456038', 'https://openalex.org/W2917066545', 'https://openalex.org/W3171126283', 'https://openalex.org/W3172758989']","This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation (WMT 2012). We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions. Both hierarchical and phrase-based SMT systems are applied. A number of different techniques are evaluated, including an insertion model, different lexical smoothing methods, a discriminative reordering extension for the hierarchical system, reverse translation, and system combination. By application of these methods we achieve considerable improvements over the respective baseline systems. 1",0.9818181818181818
SKG_MT_195,https://openalex.org/W2888070626,2018,91,"['https://openalex.org/W639708223', 'https://openalex.org/W1522301498', 'https://openalex.org/W1527575280', 'https://openalex.org/W1677182931', 'https://openalex.org/W1861492603', 'https://openalex.org/W1905882502', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117539524', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143623448', 'https://openalex.org/W2157331557', 'https://openalex.org/W2185175083', 'https://openalex.org/W2194775991', 'https://openalex.org/W2345720230', 'https://openalex.org/W2402302915', 'https://openalex.org/W2513263213', 'https://openalex.org/W2581101319', 'https://openalex.org/W2592831379', 'https://openalex.org/W2593341061', 'https://openalex.org/W2594229957', 'https://openalex.org/W2613718673', 'https://openalex.org/W2735217275', 'https://openalex.org/W2757877530', 'https://openalex.org/W2896234464', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963496089', 'https://openalex.org/W2963898017', 'https://openalex.org/W2963988211', 'https://openalex.org/W2964003477', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3010232603', 'https://openalex.org/W4297732549']","We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our model jointly optimizes the learning of a shared visual-language embedding and a translator. The model leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this dataset, our visual attention grounding model outperforms other methods by a large margin.",1.0
SKG_MT_196,https://openalex.org/W2147258359,2010,61,"['https://openalex.org/W2018869373', 'https://openalex.org/W2060833990', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106818711', 'https://openalex.org/W2115081467', 'https://openalex.org/W2123301721', 'https://openalex.org/W2133512280', 'https://openalex.org/W2143927888', 'https://openalex.org/W2149327368']","This paper presents METEOR-NEXT, an ex-tended version of the METEOR metric de-signed to have high correlation with post-editing measures of machine translation qual-ity. We describe changes made to the met-ric’s sentence aligner and scoring scheme as well as a method for tuning the metric’s pa-rameters to optimize correlation with human-targeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves cor-relation with HTER over baseline metrics, in-cluding earlier versions of METEOR, and ap-proaches the correlation level of a state-of-the-art metric, TER-plus (TERp). 1",1.0
SKG_MT_197,https://openalex.org/W2799920282,2018,13,"['https://openalex.org/W630532510', 'https://openalex.org/W1517289518', 'https://openalex.org/W1522301498', 'https://openalex.org/W1632114991', 'https://openalex.org/W1869752048', 'https://openalex.org/W1902237438', 'https://openalex.org/W1924770834', 'https://openalex.org/W2006969979', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2251692332', 'https://openalex.org/W2516255829', 'https://openalex.org/W2556468274', 'https://openalex.org/W2561274697', 'https://openalex.org/W2563574619', 'https://openalex.org/W2577255746', 'https://openalex.org/W2595715041', 'https://openalex.org/W2605717780', 'https://openalex.org/W2608018997', 'https://openalex.org/W2617039999', 'https://openalex.org/W2758310181', 'https://openalex.org/W2773621464', 'https://openalex.org/W2773956126', 'https://openalex.org/W2962717763', 'https://openalex.org/W2962731964', 'https://openalex.org/W2962777840', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962897020', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963355640', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964065937', 'https://openalex.org/W2964116568', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","Neural machine translation requires large amounts of parallel training text to learn a reasonable-quality translation model. This is particularly inconvenient for language pairs for which enough parallel text is not available. In this paper, we use monolingual linguistic resources in the source side to address this challenging problem based on a multi-Task learning approach. More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition. This effectively injects semantic and/or syntactic knowledge into the translation model, which would otherwise require a large amount of training bitext. We empirically evaluate and show the effectiveness of our multi-Task learning approach on three translation tasks: English-To-French, English-To-Farsi, and English-To-Vietnamese.",0.9947643979057592
SKG_MT_12,https://openalex.org/W3103169714,2020,46,"['https://openalex.org/W630532510', 'https://openalex.org/W1905522558', 'https://openalex.org/W2021618504', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2134800885', 'https://openalex.org/W2147227066', 'https://openalex.org/W2147262247', 'https://openalex.org/W2250771471', 'https://openalex.org/W2251150371', 'https://openalex.org/W2251590347', 'https://openalex.org/W2472373455', 'https://openalex.org/W2546938941', 'https://openalex.org/W2561274697', 'https://openalex.org/W2786253471', 'https://openalex.org/W2886095922', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896457183', 'https://openalex.org/W2922349260', 'https://openalex.org/W2923622379', 'https://openalex.org/W2939335894', 'https://openalex.org/W2946379889', 'https://openalex.org/W2950940239', 'https://openalex.org/W2951476960', 'https://openalex.org/W2952474700', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963281280', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963569817', 'https://openalex.org/W2970064096', 'https://openalex.org/W2970156971', 'https://openalex.org/W2971074957', 'https://openalex.org/W2971120958', 'https://openalex.org/W2971302374', 'https://openalex.org/W3035019713', 'https://openalex.org/W4298137069', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.",1.0
SKG_MT_75,https://openalex.org/W2982129078,2019,38,"['https://openalex.org/W6908809', 'https://openalex.org/W854541894', 'https://openalex.org/W1494198834', 'https://openalex.org/W1537859740', 'https://openalex.org/W1902237438', 'https://openalex.org/W2113106066', 'https://openalex.org/W2136530135', 'https://openalex.org/W2257408573', 'https://openalex.org/W2327501763', 'https://openalex.org/W2512924740', 'https://openalex.org/W2605131327', 'https://openalex.org/W2756566411', 'https://openalex.org/W2798687821', 'https://openalex.org/W2804704270', 'https://openalex.org/W2883586237', 'https://openalex.org/W2936969148', 'https://openalex.org/W2941814890', 'https://openalex.org/W2945700568', 'https://openalex.org/W2962775040', 'https://openalex.org/W2962780374', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963272440', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964104866', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964243274', 'https://openalex.org/W3012492057']","For automatic speech translation (AST), end-to-end approaches are outperformed by cascaded models that transcribe with automatic speech recognition (ASR), then trans- late with machine translation (MT). A major cause of the performance gap is that, while existing AST corpora are small, massive datasets exist for both the ASR and MT subsystems. In this work, we evaluate several data augmentation and pretraining approaches for AST, by comparing all on the same datasets. Simple data augmentation by translating ASR transcripts proves most effective on the English–French augmented LibriSpeech dataset, closing the performance gap from 8.2 to 1.4 BLEU, compared to a very strong cascade that could directly utilize copious ASR and MT data. The same end-to-end approach plus fine-tuning closes the gap on the English–Romanian MuST-C dataset from 6.7 to 3.7 BLEU. In addition to these results, we present practical rec- ommendations for augmentation and pretraining approaches. Finally, we decrease the performance gap to 0.01 BLEU us- ing a Transformer-based architecture.",0.9948717948717949
SKG_MT_95,https://openalex.org/W2970286654,2019,36,"['https://openalex.org/W1522301498', 'https://openalex.org/W1731081199', 'https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2165698076', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2567571499', 'https://openalex.org/W2581863816', 'https://openalex.org/W2740718109', 'https://openalex.org/W2740743644', 'https://openalex.org/W2744813330', 'https://openalex.org/W2750588180', 'https://openalex.org/W2756978580', 'https://openalex.org/W2757592053', 'https://openalex.org/W2760452458', 'https://openalex.org/W2785093437', 'https://openalex.org/W2802153702', 'https://openalex.org/W2803241009', 'https://openalex.org/W2805394970', 'https://openalex.org/W2886776719', 'https://openalex.org/W2892244498', 'https://openalex.org/W2905933322', 'https://openalex.org/W2946379889', 'https://openalex.org/W2952650870', 'https://openalex.org/W2954647460', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963149635', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998115938', 'https://openalex.org/W3099405210', 'https://openalex.org/W3204406378', 'https://openalex.org/W4297782088', 'https://openalex.org/W4300835687', 'https://openalex.org/W4385245566']","Jiali Zeng, Yang Liu, Jinsong Su, Yubing Ge, Yaojie Lu, Yongjing Yin, Jiebo Luo. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_150,https://openalex.org/W2250907725,2013,45,"['https://openalex.org/W24102868', 'https://openalex.org/W232191560', 'https://openalex.org/W1969974515', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095755718', 'https://openalex.org/W2096557251', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104441213', 'https://openalex.org/W2111142112', 'https://openalex.org/W2115848042', 'https://openalex.org/W2118536060', 'https://openalex.org/W2119168550', 'https://openalex.org/W2123825474', 'https://openalex.org/W2124807415', 'https://openalex.org/W2128634885', 'https://openalex.org/W2131367528', 'https://openalex.org/W2132001515', 'https://openalex.org/W2140343992', 'https://openalex.org/W2144600658', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158065314', 'https://openalex.org/W2159755860', 'https://openalex.org/W2168596788', 'https://openalex.org/W2180952760', 'https://openalex.org/W2186598481', 'https://openalex.org/W2401082558', 'https://openalex.org/W2401304182', 'https://openalex.org/W3198396151']","There have been many recent investigations into methods to tune SMT systems using large numbers of sparse features. However, there have not been nearly so many examples of helpful sparse features, especially for phrasebased systems. We use sparse features to address reordering, which is often considered a weak point of phrase-based translation. Using a hierarchical reordering model as our baseline, we show that simple features coupling phrase orientation to frequent words or wordclusters can improve translation quality, with boosts of up to 1.2 BLEU points in Chinese-English and 1.8 in Arabic-English. We compare this solution to a more traditional maximum entropy approach, where a probability model with similar features is trained on wordaligned bitext. We show that sparse decoder features outperform maximum entropy handily, indicating that there are major advantages to optimizing reordering features directly for BLEU with the decoder in the loop. 1",1.0
SKG_MT_200,https://openalex.org/W1508577659,2013,67,"['https://openalex.org/W38126138', 'https://openalex.org/W170711724', 'https://openalex.org/W1966497645', 'https://openalex.org/W1973152633', 'https://openalex.org/W2033593667', 'https://openalex.org/W2041232209', 'https://openalex.org/W2041305041', 'https://openalex.org/W2072976288', 'https://openalex.org/W2101096097', 'https://openalex.org/W2102749417', 'https://openalex.org/W2103042430', 'https://openalex.org/W2105673178', 'https://openalex.org/W2119013622', 'https://openalex.org/W2121415745', 'https://openalex.org/W2121745180', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130716237', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2140903445', 'https://openalex.org/W2144600658', 'https://openalex.org/W2144642230', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2159755860', 'https://openalex.org/W2163361328', 'https://openalex.org/W2168966090', 'https://openalex.org/W2169360026', 'https://openalex.org/W2170716095', 'https://openalex.org/W2172268343', 'https://openalex.org/W2250229103', 'https://openalex.org/W2282154246']","Statistical machine translation (SMT) performance suffers when models are trained on only small amounts of parallel data. The learned models typically have both low accuracy (incorrect translations and feature scores) and low coverage (high out-of-vocabulary rates). In this work, we use an additional data resource, comparable corpora, to improve both. Beginning with a small bitext and corresponding phrase-based SMT model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the model’s feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu,",1.0
SKG_MT_201,https://openalex.org/W2962708992,2016,199,"['https://openalex.org/W98731357', 'https://openalex.org/W206967138', 'https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W1632114991', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2006969979', 'https://openalex.org/W2064675550', 'https://openalex.org/W2078861931', 'https://openalex.org/W2095705004', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2130942839', 'https://openalex.org/W2136848157', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2169724380', 'https://openalex.org/W2220350356', 'https://openalex.org/W2250548645', 'https://openalex.org/W2437005631', 'https://openalex.org/W2576482813', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962944953', 'https://openalex.org/W2962995178', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963937700', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W4285719527']","Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.",1.0
SKG_MT_202,https://openalex.org/W2976157090,2019,4,"['https://openalex.org/W2117130368', 'https://openalex.org/W2125031621', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132339004', 'https://openalex.org/W2153579005', 'https://openalex.org/W2250539671', 'https://openalex.org/W2525778437', 'https://openalex.org/W2899423466', 'https://openalex.org/W2903188467', 'https://openalex.org/W2911489562', 'https://openalex.org/W2914056350', 'https://openalex.org/W2915756181', 'https://openalex.org/W2916835973', 'https://openalex.org/W2922293812', 'https://openalex.org/W2922709902', 'https://openalex.org/W2944815030', 'https://openalex.org/W2945969861', 'https://openalex.org/W2946417913', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964308564']","Exploiting large pretrained models for various NMT tasks have gained a lot of visibility recently. In this work we study how BERT pretrained models could be exploited for supervised Neural Machine Translation. We compare various ways to integrate pretrained BERT model with NMT model and study the impact of the monolingual data used for BERT training on the final translation quality. We use WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian datasets for these experiments. In addition to standard task test set evaluation, we perform evaluation on out-of-domain test sets and noise injected test sets, in order to assess how BERT pretrained representations affect model robustness.",1.0
SKG_MT_203,https://openalex.org/W2740087922,2017,18,"['https://openalex.org/W6908809', 'https://openalex.org/W626175318', 'https://openalex.org/W1738364571', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2028268349', 'https://openalex.org/W2080373976', 'https://openalex.org/W2097125878', 'https://openalex.org/W2097790277', 'https://openalex.org/W2099753558', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114912785', 'https://openalex.org/W2115613106', 'https://openalex.org/W2118434577', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153508793', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157331557', 'https://openalex.org/W2251634841', 'https://openalex.org/W2251849926', 'https://openalex.org/W2294059674', 'https://openalex.org/W2306852879', 'https://openalex.org/W2527133236', 'https://openalex.org/W2533220144', 'https://openalex.org/W2549599535', 'https://openalex.org/W2576482813', 'https://openalex.org/W2595715041', 'https://openalex.org/W2758567679', 'https://openalex.org/W2915926444', 'https://openalex.org/W2953384844', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962907349', 'https://openalex.org/W2963201387', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963576560', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963790827', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964335273', 'https://openalex.org/W3022187094', 'https://openalex.org/W4241645538']","Shonosuke Ishiwatari, Jingtao Yao, Shujie Liu, Mu Li, Ming Zhou, Naoki Yoshinaga, Masaru Kitsuregawa, Weijia Jia. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017.",1.0
SKG_MT_207,https://openalex.org/W3119254911,2020,1,"['https://openalex.org/W22168010', 'https://openalex.org/W630532510', 'https://openalex.org/W1819903106', 'https://openalex.org/W1905522558', 'https://openalex.org/W1973152633', 'https://openalex.org/W2065565011', 'https://openalex.org/W2124807415', 'https://openalex.org/W2250851379', 'https://openalex.org/W2294808479', 'https://openalex.org/W2575814162', 'https://openalex.org/W2806049760', 'https://openalex.org/W2902363747', 'https://openalex.org/W2902713373', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970331417', 'https://openalex.org/W2970587837', 'https://openalex.org/W2970726011', 'https://openalex.org/W2970750538', 'https://openalex.org/W2971345087', 'https://openalex.org/W2994928925', 'https://openalex.org/W3168052636']",This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. Systems were built for French to English using in-domain corpora through fine tuning and selective data training. We further explored BERT based models specifically with focus on effect of domain adaptive subword units.,1.0
SKG_MT_208,https://openalex.org/W3120329789,2020,21,"['https://openalex.org/W2250464850', 'https://openalex.org/W2741494657', 'https://openalex.org/W2772980959', 'https://openalex.org/W2773910184', 'https://openalex.org/W2787402609', 'https://openalex.org/W2884692672', 'https://openalex.org/W2885213066', 'https://openalex.org/W2885564448', 'https://openalex.org/W2885779207', 'https://openalex.org/W2885848075', 'https://openalex.org/W2885928764', 'https://openalex.org/W2887551131', 'https://openalex.org/W2933138175', 'https://openalex.org/W2938830017', 'https://openalex.org/W2945635251', 'https://openalex.org/W2948335087', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963881719', 'https://openalex.org/W2964082031', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970429618', 'https://openalex.org/W2970597249', 'https://openalex.org/W2970744242', 'https://openalex.org/W2994928925', 'https://openalex.org/W3035010485', 'https://openalex.org/W3037162118', 'https://openalex.org/W3102725307', 'https://openalex.org/W3210120707']","This paper presents the UNIPUS-Flaubert team’s hybrid system for the NLPTEA 2020 shared task of Chinese Grammatical Error Diagnosis (CGED). As a challenging NLP task, CGED has attracted increasing attention recently and has not yet fully benefited from the powerful pre-trained BERT-based models. We explore this by experimenting with three types of models. The position-tagging models and correction-tagging models are sequence tagging models fine-tuned on pre-trained BERT-based models, where the former focuses on detecting, positioning and classifying errors, and the latter aims at correcting errors. We also utilize rich representations from BERT-based models by transferring the BERT-fused models to the correction task, and further improve the performance by pre-training on a vast size of unsupervised synthetic data. To the best of our knowledge, we are the first to introduce and transfer the BERT-fused NMT model and sequence tagging model into the Chinese Grammatical Error Correction field. Our work achieved the second highest F1 score at the detecting errors, the best F1 score at correction top1 subtask and the second highest F1 score at correction top3 subtask.",1.0
SKG_MT_211,https://openalex.org/W2758640316,2017,2,"['https://openalex.org/W635530177', 'https://openalex.org/W1767788615', 'https://openalex.org/W1819903106', 'https://openalex.org/W2111666304', 'https://openalex.org/W2111885854', 'https://openalex.org/W2119727789', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2137698233', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159755860', 'https://openalex.org/W2170204377', 'https://openalex.org/W2251742274', 'https://openalex.org/W2308338022', 'https://openalex.org/W2595715041']","We describe the JAIST phrase-based machine translation systems that participated in the news translation shared task of the WMT17.In this work, we participated in the Turkish-English translation, in which only a small amount of bilingual training data is available, so that it is an example of the low-resource setting in machine translation.In order to solve the problem, we focus on two strategies: building a bilingual corpus from comparable data and exploiting existing parallel data based on phrase pivot translation.In order to utilize the strategies to enhance machine translation on the low-resource setting most effectively, we introduce a system combining the extracted corpus, the pivot translation, and the direct training data.Experimental results showed that our combined systems significantly improved the baseline models, which were trained on the small bilingual data.",1.0
SKG_MT_213,https://openalex.org/W2952153923,2019,86,"['https://openalex.org/W1533861849', 'https://openalex.org/W2119727789', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2169200297', 'https://openalex.org/W2418388682', 'https://openalex.org/W2531207078', 'https://openalex.org/W2550821151', 'https://openalex.org/W2566926700', 'https://openalex.org/W2610245951', 'https://openalex.org/W2613904329', 'https://openalex.org/W2741917668', 'https://openalex.org/W2766184602', 'https://openalex.org/W2767206889', 'https://openalex.org/W2798931235', 'https://openalex.org/W2804232614', 'https://openalex.org/W2888456631', 'https://openalex.org/W2888541716', 'https://openalex.org/W2914120296', 'https://openalex.org/W2921280978', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962830144', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.",1.0
SKG_MT_214,https://openalex.org/W2883384628,2019,8,"['https://openalex.org/W179875071', 'https://openalex.org/W1516111018', 'https://openalex.org/W1646152356', 'https://openalex.org/W1665214252', 'https://openalex.org/W1753482797', 'https://openalex.org/W1959608418', 'https://openalex.org/W1993845689', 'https://openalex.org/W1994616650', 'https://openalex.org/W2095705004', 'https://openalex.org/W2098949613', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102486516', 'https://openalex.org/W2116492146', 'https://openalex.org/W2124807415', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149327368', 'https://openalex.org/W2157331557', 'https://openalex.org/W2188365844', 'https://openalex.org/W2251367463', 'https://openalex.org/W2292999442', 'https://openalex.org/W2350508517', 'https://openalex.org/W2403717966', 'https://openalex.org/W2512924740', 'https://openalex.org/W2529194139', 'https://openalex.org/W2539350388', 'https://openalex.org/W2550821151', 'https://openalex.org/W2552839021', 'https://openalex.org/W2561274697', 'https://openalex.org/W2594155836', 'https://openalex.org/W2604977777', 'https://openalex.org/W2740433069', 'https://openalex.org/W2753738274', 'https://openalex.org/W2760656271', 'https://openalex.org/W2793214147', 'https://openalex.org/W2798493043', 'https://openalex.org/W2807995432', 'https://openalex.org/W2949335953', 'https://openalex.org/W2949888546', 'https://openalex.org/W2952584433', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962897886', 'https://openalex.org/W2962994101', 'https://openalex.org/W2963081964', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963135265', 'https://openalex.org/W2963145887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963357083', 'https://openalex.org/W2963430224', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963773425', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964076537', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W3041866211', 'https://openalex.org/W3101380508', 'https://openalex.org/W3207342693', 'https://openalex.org/W6600719526']","We present a deep generative model of bilingual sentence pairs for machine translation. The model generates source and target sentences jointly from a shared latent representation and is parameterised by neural networks. We perform efficient training using amortised variational inference and reparameterised gradients. Additionally, we discuss the statistical implications of joint modelling and propose an efficient approximation to maximum a posteriori decoding for fast test-time predictions. We demonstrate the effectiveness of our model in three machine translation scenarios: in-domain training, mixed-domain training, and learning from a mix of gold-standard and synthetic data. Our experiments show consistently that our joint formulation outperforms conditional modelling (i.e. standard neural machine translation) in all such scenarios.",1.0
SKG_MT_215,https://openalex.org/W2460130460,2016,50,"['https://openalex.org/W1570197553', 'https://openalex.org/W1591801644', 'https://openalex.org/W1753482797', 'https://openalex.org/W1800356822', 'https://openalex.org/W1821462560', 'https://openalex.org/W1825672851', 'https://openalex.org/W1841592590', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2114766824', 'https://openalex.org/W2125389748', 'https://openalex.org/W2167215970', 'https://openalex.org/W2184135559', 'https://openalex.org/W2279098554', 'https://openalex.org/W2337978879', 'https://openalex.org/W2339995566', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950967261', 'https://openalex.org/W2952432176', 'https://openalex.org/W2952533036', 'https://openalex.org/W2953071172', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963674932', 'https://openalex.org/W2964200805', 'https://openalex.org/W2964299589', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.",1.0
SKG_MT_216,https://openalex.org/W2740545630,2017,2,"['https://openalex.org/W1631260214', 'https://openalex.org/W1736600331', 'https://openalex.org/W2097927681', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113541941', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132622673', 'https://openalex.org/W2144600658', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2159755860', 'https://openalex.org/W2164415172', 'https://openalex.org/W2513529543', 'https://openalex.org/W2595715041', 'https://openalex.org/W2620000475', 'https://openalex.org/W4241645538']","In this paper, we present an improved graph-based translation model which segments an input graph into node-induced subgraphs by taking source context into consideration. Translations are generated by combining subgraph translations left-to-right using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model.",1.0
SKG_MT_217,https://openalex.org/W2251468402,2015,11,"['https://openalex.org/W1575384945', 'https://openalex.org/W1970689298', 'https://openalex.org/W1975742580', 'https://openalex.org/W2087735403', 'https://openalex.org/W2093790824', 'https://openalex.org/W2104511424', 'https://openalex.org/W2115990601', 'https://openalex.org/W2117278770', 'https://openalex.org/W2123301721', 'https://openalex.org/W2141599568', 'https://openalex.org/W2164479068', 'https://openalex.org/W2164984707', 'https://openalex.org/W2251222643', 'https://openalex.org/W2254518567', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2437096199', 'https://openalex.org/W2887933277', 'https://openalex.org/W2998704965', 'https://openalex.org/W3044695148', 'https://openalex.org/W4285719527', 'https://openalex.org/W4298302210']","We present novel features designed with a deep neural network for Machine Trans-lation (MT) Quality Estimation (QE). The features are learned with a Continuous Space Language Model to estimate the probabilities of the source and target seg-ments. These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including post-editing effort, human translation edit rate, post-editing time and METEOR. Results show significant improvements in predic-tion over the baseline, as well as over sys-tems trained on state of the art feature sets for all datasets. More notably, the addition of the newly proposed features improves over the best QE systems in WMT12 and WMT14 by a significant margin. 1",1.0
SKG_MT_220,https://openalex.org/W2251074483,2014,9,"['https://openalex.org/W1746819321', 'https://openalex.org/W1777124189', 'https://openalex.org/W2099768828', 'https://openalex.org/W2119004102', 'https://openalex.org/W2119595900', 'https://openalex.org/W2143337266', 'https://openalex.org/W2144746247', 'https://openalex.org/W2146611938', 'https://openalex.org/W2149327368', 'https://openalex.org/W2164984707', 'https://openalex.org/W2186839874', 'https://openalex.org/W2251311344', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2276605909', 'https://openalex.org/W3044695148', 'https://openalex.org/W4211049957', 'https://openalex.org/W4230031267']","We describe our systems for the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3).Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting.Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations.",0.9941520467836257
SKG_MT_221,https://openalex.org/W2250818554,2015,24,"['https://openalex.org/W22168010', 'https://openalex.org/W56220166', 'https://openalex.org/W635530177', 'https://openalex.org/W1552297528', 'https://openalex.org/W1693107767', 'https://openalex.org/W1697700638', 'https://openalex.org/W1798130076', 'https://openalex.org/W2057052429', 'https://openalex.org/W2072976288', 'https://openalex.org/W2084277454', 'https://openalex.org/W2086039194', 'https://openalex.org/W2091889711', 'https://openalex.org/W2105673178', 'https://openalex.org/W2105891181', 'https://openalex.org/W2108701407', 'https://openalex.org/W2109396985', 'https://openalex.org/W2110481933', 'https://openalex.org/W2117198860', 'https://openalex.org/W2118090838', 'https://openalex.org/W2120679544', 'https://openalex.org/W2121415745', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126725946', 'https://openalex.org/W2130571329', 'https://openalex.org/W2132019450', 'https://openalex.org/W2132446289', 'https://openalex.org/W2134800885', 'https://openalex.org/W2138974820', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2140847984', 'https://openalex.org/W2143927888', 'https://openalex.org/W2144600658', 'https://openalex.org/W2145662801', 'https://openalex.org/W2145685230', 'https://openalex.org/W2148708890', 'https://openalex.org/W2150836210', 'https://openalex.org/W2151075664', 'https://openalex.org/W2153579005', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153903004', 'https://openalex.org/W2155169493', 'https://openalex.org/W2155607551', 'https://openalex.org/W2155732278', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159712673', 'https://openalex.org/W2162507901', 'https://openalex.org/W2166776180', 'https://openalex.org/W2172268343', 'https://openalex.org/W2250229103', 'https://openalex.org/W2250445771', 'https://openalex.org/W2250952810', 'https://openalex.org/W2251302843', 'https://openalex.org/W2251780596', 'https://openalex.org/W2251855842', 'https://openalex.org/W2252053087', 'https://openalex.org/W2293547632', 'https://openalex.org/W4285719527']","The multilingual Paraphrase Database (PPDB) is a freely available automatically created resource of paraphrases in multiple languages. In statistical machine translation, paraphrases can be used to provide translation for out-of-vocabulary (OOV) phrases. In this paper, we show that a graph propagation approach that uses PPDB paraphrases can be used to improve overall translation quality. We provide an extensive comparison with previous work and show that our PPDB-based method improves the BLEU score by up to 1.79 percent points. We show that our approach improves on the state of the art in three different settings: when faced with limited amount of parallel training data; a domain shift between training and test data; and handling a morphologically complex source language. Our PPDB-based method outperforms the use of distributional profiles from monolingual source data.",1.0
SKG_MT_222,https://openalex.org/W2970231882,2019,13,"['https://openalex.org/W2101105183', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2242083635', 'https://openalex.org/W2413794162', 'https://openalex.org/W2496235729', 'https://openalex.org/W2549416390', 'https://openalex.org/W2550821151', 'https://openalex.org/W2594047108', 'https://openalex.org/W2595715041', 'https://openalex.org/W2606974598', 'https://openalex.org/W2767019613', 'https://openalex.org/W2772882909', 'https://openalex.org/W2780664814', 'https://openalex.org/W2788330850', 'https://openalex.org/W2809324505', 'https://openalex.org/W2891534142', 'https://openalex.org/W2924210975', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962964385', 'https://openalex.org/W2963062480', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964247056', 'https://openalex.org/W4293350112', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These Web pages have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from English, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 × 16 translation settings. Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and named entities. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for post-editing.",1.0
SKG_MT_223,https://openalex.org/W2803172920,2018,18,"['https://openalex.org/W1539361473', 'https://openalex.org/W1614298861', 'https://openalex.org/W1823003763', 'https://openalex.org/W1986406965', 'https://openalex.org/W1989867740', 'https://openalex.org/W2006969979', 'https://openalex.org/W2017802499', 'https://openalex.org/W2124807415', 'https://openalex.org/W2137592969', 'https://openalex.org/W2140605844', 'https://openalex.org/W2144945507', 'https://openalex.org/W2150066400', 'https://openalex.org/W2156985047', 'https://openalex.org/W2160721348', 'https://openalex.org/W2165109801', 'https://openalex.org/W2169724380', 'https://openalex.org/W2172138510', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250765246', 'https://openalex.org/W2251071803', 'https://openalex.org/W2251232800', 'https://openalex.org/W2401657597', 'https://openalex.org/W2595715041', 'https://openalex.org/W2611669587', 'https://openalex.org/W2950577311', 'https://openalex.org/W2951714314', 'https://openalex.org/W3201966201', 'https://openalex.org/W4242105955']","Nima Pourdamghani, Marjan Ghazvininejad, Kevin Knight. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 2018.",1.0
SKG_MT_224,https://openalex.org/W2120390143,2014,7,"['https://openalex.org/W1631260214', 'https://openalex.org/W1905522558', 'https://openalex.org/W1976115394', 'https://openalex.org/W2047295649', 'https://openalex.org/W2080373976', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101336838', 'https://openalex.org/W2102127584', 'https://openalex.org/W2107508692', 'https://openalex.org/W2109530867', 'https://openalex.org/W2117278770', 'https://openalex.org/W2120101509', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132532452', 'https://openalex.org/W2134800885', 'https://openalex.org/W2137387514', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147262247', 'https://openalex.org/W2152114834', 'https://openalex.org/W2153488166', 'https://openalex.org/W2156985047', 'https://openalex.org/W2595715041']","This paper describes adapting statistical machine translation (SMT) systems to medical domain using in-domain and general-domain data as well as webcrawled in-domain resources.In order to complement the limited in-domain corpora, we apply domain focused webcrawling approaches to acquire indomain monolingual data and bilingual lexicon from the Internet.The collected data is used for adapting the language model and translation model to boost the overall translation quality.Besides, we propose an alternative filtering approach to clean the crawled data and to further optimize the domain-specific SMT system.We attend the medical summary sentence unconstrained translation task of the Ninth Workshop on Statistical Machine Translation (WMT2014).Our systems achieve the second best BLEU scores for Czech-English, fourth for French-English, English-French language pairs and the third best results for reminding pairs.",1.0
SKG_MT_225,https://openalex.org/W2142333053,2013,5,"['https://openalex.org/W1510052640', 'https://openalex.org/W1555286493', 'https://openalex.org/W1631260214', 'https://openalex.org/W1828578481', 'https://openalex.org/W1954341086', 'https://openalex.org/W1955251501', 'https://openalex.org/W2030904529', 'https://openalex.org/W2060127787', 'https://openalex.org/W2092477611', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110104386', 'https://openalex.org/W2112900913', 'https://openalex.org/W2113998052', 'https://openalex.org/W2115289978', 'https://openalex.org/W2116316001', 'https://openalex.org/W2119168550', 'https://openalex.org/W2122922578', 'https://openalex.org/W2124807415', 'https://openalex.org/W2124861326', 'https://openalex.org/W2132874619', 'https://openalex.org/W2133768083', 'https://openalex.org/W2134729743', 'https://openalex.org/W2144279206', 'https://openalex.org/W2146574666', 'https://openalex.org/W2146628926', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2158388102', 'https://openalex.org/W2161227214', 'https://openalex.org/W2161586008', 'https://openalex.org/W2165666205', 'https://openalex.org/W2166905217', 'https://openalex.org/W2171421863', 'https://openalex.org/W2437005631']","We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n-gram and dependency language models. To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-todependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets. 1",1.0
SKG_MT_226,https://openalex.org/W3034742481,2020,27,"['https://openalex.org/W1902237438', 'https://openalex.org/W2036317923', 'https://openalex.org/W2099257174', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2138660131', 'https://openalex.org/W2154652894', 'https://openalex.org/W2157331557', 'https://openalex.org/W2175740768', 'https://openalex.org/W2176263492', 'https://openalex.org/W2194775991', 'https://openalex.org/W2525778437', 'https://openalex.org/W2595715041', 'https://openalex.org/W2606974598', 'https://openalex.org/W2612675303', 'https://openalex.org/W2613904329', 'https://openalex.org/W2804044248', 'https://openalex.org/W2888520903', 'https://openalex.org/W2889518897', 'https://openalex.org/W2890964657', 'https://openalex.org/W2897983179', 'https://openalex.org/W2922709902', 'https://openalex.org/W2924690340', 'https://openalex.org/W2952524847', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962785754', 'https://openalex.org/W2962931466', 'https://openalex.org/W2963088785', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963323244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963735467', 'https://openalex.org/W2963768805', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964093309', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965575120', 'https://openalex.org/W2970290486', 'https://openalex.org/W2989268779', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.",1.0
SKG_MT_230,https://openalex.org/W2475062008,2016,0,"['https://openalex.org/W22168010', 'https://openalex.org/W206967138', 'https://openalex.org/W1631260214', 'https://openalex.org/W1969974515', 'https://openalex.org/W1973923101', 'https://openalex.org/W1985514943', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104812589', 'https://openalex.org/W2105891181', 'https://openalex.org/W2119774882', 'https://openalex.org/W2130195263', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2168360976', 'https://openalex.org/W2542415155', 'https://openalex.org/W2950186769', 'https://openalex.org/W3202665693']","This paper presents an study of the use of interlocking phrases in phrase-based statistical machine translation. We examine the effect on translation quality when the translation units used in the translation hypotheses are allowed to overlap on the source side, on the target side and on both sides. A large-scale evaluation on 380 language pairs was conducted. Our results show that overall the use of overlapping phrases improved translation quality by 0.3 BLEU points on average. Further analysis revealed that language pairs requiring a larger amount of re-ordering benefited the most from our approach. When the evaluation was restricted to such pairs, the average improvement increased to up to 0.75 BLEU points with over 97% of the pairs improving. Our approach requires only a simple modification to the decoding algorithm and we believe it should be generally applicable to improve the performance of phrase-based decoders.",1.0
SKG_MT_231,https://openalex.org/W3035589854,2020,73,"['https://openalex.org/W222053410', 'https://openalex.org/W1902237438', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110485445', 'https://openalex.org/W2123442489', 'https://openalex.org/W2145251161', 'https://openalex.org/W2152224036', 'https://openalex.org/W2153508793', 'https://openalex.org/W2183341477', 'https://openalex.org/W2525778437', 'https://openalex.org/W2586559132', 'https://openalex.org/W2594047108', 'https://openalex.org/W2595715041', 'https://openalex.org/W2768763386', 'https://openalex.org/W2884083742', 'https://openalex.org/W2888539709', 'https://openalex.org/W2912351236', 'https://openalex.org/W2945059185', 'https://openalex.org/W2962788148', 'https://openalex.org/W2962811598', 'https://openalex.org/W2962911926', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963471154', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963652649', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963679688', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963888305', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964302946', 'https://openalex.org/W2970247882', 'https://openalex.org/W3103729510', 'https://openalex.org/W4297747548', 'https://openalex.org/W4385245566']","Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.",1.0
SKG_MT_232,https://openalex.org/W2951397374,2019,1,"['https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2053186076', 'https://openalex.org/W2099471712', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134563198', 'https://openalex.org/W2145406111', 'https://openalex.org/W2157331557', 'https://openalex.org/W2189508540', 'https://openalex.org/W2525778437', 'https://openalex.org/W2608029998', 'https://openalex.org/W2613904329', 'https://openalex.org/W2767019613', 'https://openalex.org/W2777551880', 'https://openalex.org/W2804371177', 'https://openalex.org/W2891534142', 'https://openalex.org/W2950527759', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962954913', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964308564', 'https://openalex.org/W4300428972', 'https://openalex.org/W4303633609', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Neural Machine Translation (NMT) has achieved notable success in recent years. Such a framework usually generates translations in isolation. In contrast, human translators often refer to reference data, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. In this paper, we propose a Reference Network to incorporate referring process into translation decoding of NMT. To construct a reference book, an intuitive way is to store the detailed translation history with extra memory, which is computationally expensive. Instead, we employ Local Coordinates Coding (LCC) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding. Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost.",1.0
SKG_MT_234,https://openalex.org/W2160131015,2012,87,"['https://openalex.org/W1508001288', 'https://openalex.org/W1848260265', 'https://openalex.org/W1880262756', 'https://openalex.org/W1905522558', 'https://openalex.org/W2004447574', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105577415', 'https://openalex.org/W2107564089', 'https://openalex.org/W2115410424', 'https://openalex.org/W2116042738', 'https://openalex.org/W2120459453', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158266063', 'https://openalex.org/W2160218441', 'https://openalex.org/W2950186769']","We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts, where topics are induced in an unsupervised way using topic models; this can be thought of as inducing subcorpora for adaptation without any human annotation. We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features. Conditioning lexical probabilities on the topic biases translations toward topicrelevant output, resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline. 1",1.0
SKG_MT_235,https://openalex.org/W2970292446,2019,12,"['https://openalex.org/W1663973292', 'https://openalex.org/W1902237438', 'https://openalex.org/W2053742104', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106751371', 'https://openalex.org/W2140343992', 'https://openalex.org/W2166905217', 'https://openalex.org/W2222235228', 'https://openalex.org/W2581624817', 'https://openalex.org/W2594047108', 'https://openalex.org/W2739894144', 'https://openalex.org/W2785093437', 'https://openalex.org/W2788277448', 'https://openalex.org/W2798465082', 'https://openalex.org/W2800490029', 'https://openalex.org/W2803369080', 'https://openalex.org/W2886026946', 'https://openalex.org/W2889404673', 'https://openalex.org/W2890908793', 'https://openalex.org/W2890909908', 'https://openalex.org/W2902441267', 'https://openalex.org/W2907222274', 'https://openalex.org/W2962717763', 'https://openalex.org/W2962811598', 'https://openalex.org/W2963011474', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564']","Xuewen Yang, Yingru Liu, Dongliang Xie, Xin Wang, Niranjan Balasubramanian. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_237,https://openalex.org/W2514698017,2016,8,"['https://openalex.org/W64777181', 'https://openalex.org/W237846910', 'https://openalex.org/W400322755', 'https://openalex.org/W645927007', 'https://openalex.org/W1522301498', 'https://openalex.org/W1815552937', 'https://openalex.org/W2006969979', 'https://openalex.org/W2037788937', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100631779', 'https://openalex.org/W2106068492', 'https://openalex.org/W2115259925', 'https://openalex.org/W2116599427', 'https://openalex.org/W2118804972', 'https://openalex.org/W2134955506', 'https://openalex.org/W2141440284', 'https://openalex.org/W2144600658', 'https://openalex.org/W2152249239', 'https://openalex.org/W2155539936', 'https://openalex.org/W2155607551', 'https://openalex.org/W2157435188', 'https://openalex.org/W2164766438', 'https://openalex.org/W2169423212', 'https://openalex.org/W2250229103', 'https://openalex.org/W2250332179', 'https://openalex.org/W2252154714', 'https://openalex.org/W2964121744', 'https://openalex.org/W4285719527']","Most machine translation systems construct translations from a closed vocabulary of target word forms, posing problems for translating into languages that have productive compounding processes.We present a simple and effective approach that deals with this problem in two phases.First, we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language.Then, for each identified span, we generate a pool of possible compounds which are added to the translation model as ""synthetic"" phrase translations.Experiments reveal that (i) we can effectively predict what spans can be compounded; (ii) our compound generation model produces good compounds; and (iii) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks.We additionally introduce KomposEval, a new multi-reference dataset of English phrases and their translations into German compounds.",1.0
SKG_MT_239,https://openalex.org/W2117717100,2012,87,"['https://openalex.org/W214028658', 'https://openalex.org/W635530177', 'https://openalex.org/W1519942606', 'https://openalex.org/W1980219648', 'https://openalex.org/W1994581546', 'https://openalex.org/W2006969979', 'https://openalex.org/W2052003572', 'https://openalex.org/W2070150502', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102443632', 'https://openalex.org/W2107221152', 'https://openalex.org/W2111666304', 'https://openalex.org/W2116599427', 'https://openalex.org/W2118947254', 'https://openalex.org/W2119727789', 'https://openalex.org/W2124807415', 'https://openalex.org/W2138753018', 'https://openalex.org/W2138934709', 'https://openalex.org/W2145685230', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2162245945', 'https://openalex.org/W2163108352', 'https://openalex.org/W2570451785']","We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline. 1",1.0
SKG_MT_240,https://openalex.org/W2890698823,2018,81,"['https://openalex.org/W90695545', 'https://openalex.org/W808583520', 'https://openalex.org/W1522301498', 'https://openalex.org/W2071643874', 'https://openalex.org/W2089629691', 'https://openalex.org/W2108325777', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143177362', 'https://openalex.org/W2145339207', 'https://openalex.org/W2251955814', 'https://openalex.org/W2419292002', 'https://openalex.org/W2529548870', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W4230563027']",Simultaneous speech translation aims to maintain translation quality while minimizing the delay between reading input and incrementally producing the output. We propose a new general-purpose prediction action which predicts future words in the input to improve quality and minimize delay in simultaneous translation. We train this agent using reinforcement learning with a novel reward function. Our agent with prediction has better translation quality and less delay compared to an agent-based simultaneous translation system without prediction.,1.0
SKG_MT_241,https://openalex.org/W2339995566,2016,102,"['https://openalex.org/W1591801644', 'https://openalex.org/W1753482797', 'https://openalex.org/W1855892484', 'https://openalex.org/W1860935423', 'https://openalex.org/W1902237438', 'https://openalex.org/W1938755728', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101609803', 'https://openalex.org/W2109886035', 'https://openalex.org/W2169724380', 'https://openalex.org/W2220350356', 'https://openalex.org/W2250342921', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251012068', 'https://openalex.org/W2259472270', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950752421', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964325005', 'https://openalex.org/W3204406378']","Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.",1.0
SKG_MT_242,https://openalex.org/W2970717663,2019,3,"['https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2963212250', 'https://openalex.org/W2970279348', 'https://openalex.org/W2977587102', 'https://openalex.org/W2978647884']","The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi ↔ Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for Nepali-Hindi, 1 PBSMT for Hindi-Nepali and 2 NMT systems were developed for Nepali↔Hindi. The results show that PBSMT could be an effective method for developing MT systems for closely-related languages. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submitted for the pair and our Nepali-Hindi PBSMTsystem was ranked 4th among the 12 systems submitted for the task.",1.0
SKG_MT_244,https://openalex.org/W3115802955,2020,0,"['https://openalex.org/W630532510', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2211796614', 'https://openalex.org/W2250289928', 'https://openalex.org/W2251366179', 'https://openalex.org/W2251627854', 'https://openalex.org/W2419539795', 'https://openalex.org/W2576482813', 'https://openalex.org/W2744813330', 'https://openalex.org/W2763856713', 'https://openalex.org/W2889326796', 'https://openalex.org/W2908336025', 'https://openalex.org/W2933138175', 'https://openalex.org/W2944815030', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971031524', 'https://openalex.org/W2982399380', 'https://openalex.org/W3031432954', 'https://openalex.org/W3035812575', 'https://openalex.org/W3037255450', 'https://openalex.org/W3037465386', 'https://openalex.org/W3107826490']","This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al.,2020). We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the models into a single model. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation.",0.9950248756218906
SKG_MT_246,https://openalex.org/W2962700074,2017,68,"['https://openalex.org/W204945112', 'https://openalex.org/W1483849869', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2050213775', 'https://openalex.org/W2111491614', 'https://openalex.org/W2117139364', 'https://openalex.org/W2118434577', 'https://openalex.org/W2122609803', 'https://openalex.org/W2128232128', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131121993', 'https://openalex.org/W2136657878', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153508793', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157167255', 'https://openalex.org/W2157331557', 'https://openalex.org/W2169242480', 'https://openalex.org/W2250849168', 'https://openalex.org/W2361821140', 'https://openalex.org/W2387720973', 'https://openalex.org/W2400065810', 'https://openalex.org/W2475913055', 'https://openalex.org/W2527845440', 'https://openalex.org/W2532807140', 'https://openalex.org/W2534200568', 'https://openalex.org/W2542860122', 'https://openalex.org/W2561274697', 'https://openalex.org/W2566564022', 'https://openalex.org/W2575629043', 'https://openalex.org/W2577335011', 'https://openalex.org/W2952479981', 'https://openalex.org/W2962780935', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962867687', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963357083', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963598809', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4307459710', 'https://openalex.org/W4390913012']","Neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chinese-to-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.",1.0
SKG_MT_247,https://openalex.org/W2400573211,2016,9,"['https://openalex.org/W2080373976', 'https://openalex.org/W2098297786', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107440414', 'https://openalex.org/W2117278770', 'https://openalex.org/W2123388068', 'https://openalex.org/W2127672659', 'https://openalex.org/W2134800885', 'https://openalex.org/W2138934709', 'https://openalex.org/W2140372282', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2159755860', 'https://openalex.org/W2170527467', 'https://openalex.org/W2175296493', 'https://openalex.org/W2180952760', 'https://openalex.org/W2250591774', 'https://openalex.org/W2250653840', 'https://openalex.org/W2251613956', 'https://openalex.org/W2251799008', 'https://openalex.org/W2321916036', 'https://openalex.org/W2410156476', 'https://openalex.org/W2950577311', 'https://openalex.org/W2963347307']","In this work, we study parameter tuning towards the M^2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M^2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M^2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M^2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M^2.",1.0
SKG_MT_248,https://openalex.org/W2899336027,2019,6,"['https://openalex.org/W19526395', 'https://openalex.org/W222053410', 'https://openalex.org/W1682403713', 'https://openalex.org/W1902237438', 'https://openalex.org/W1940872118', 'https://openalex.org/W2060277733', 'https://openalex.org/W2158899491', 'https://openalex.org/W2162245945', 'https://openalex.org/W2165698076', 'https://openalex.org/W2250214155', 'https://openalex.org/W2250333994', 'https://openalex.org/W2250859223', 'https://openalex.org/W2308486447', 'https://openalex.org/W2407892396', 'https://openalex.org/W2531207078', 'https://openalex.org/W2609130030', 'https://openalex.org/W2610748790', 'https://openalex.org/W2736422900', 'https://openalex.org/W2740106139', 'https://openalex.org/W2763176669', 'https://openalex.org/W2787514068', 'https://openalex.org/W2799266483', 'https://openalex.org/W2888740011', 'https://openalex.org/W2889191148', 'https://openalex.org/W2889511806', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963667932', 'https://openalex.org/W2963922633', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964098600', 'https://openalex.org/W2964308564']","Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on the assisting-target language pair (parent model) which is later fine-tuned for the source-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, We propose to pre-order the assisting language sentence to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality.",1.0
SKG_MT_249,https://openalex.org/W2950780414,2019,13,"['https://openalex.org/W22168010', 'https://openalex.org/W630532510', 'https://openalex.org/W1626233182', 'https://openalex.org/W2121415745', 'https://openalex.org/W2121745180', 'https://openalex.org/W2122577247', 'https://openalex.org/W2140406733', 'https://openalex.org/W2159086733', 'https://openalex.org/W2172267041', 'https://openalex.org/W2493916176', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2566926700', 'https://openalex.org/W2610245951', 'https://openalex.org/W2741917668', 'https://openalex.org/W2757281913', 'https://openalex.org/W2770394828', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798931235', 'https://openalex.org/W2804655441', 'https://openalex.org/W2890731353', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964013027', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then ‘translating’ the resulting pseudo-translation, or ‘Translationese’ into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario.",0.9939393939393939
SKG_MT_250,https://openalex.org/W3035214886,2020,119,"['https://openalex.org/W394965032', 'https://openalex.org/W648786980', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2142112143', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153804780', 'https://openalex.org/W2176263492', 'https://openalex.org/W2183341477', 'https://openalex.org/W2222235228', 'https://openalex.org/W2419539795', 'https://openalex.org/W2594229957', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2790319220', 'https://openalex.org/W2889606145', 'https://openalex.org/W2908336025', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963382396', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963665552', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963991775', 'https://openalex.org/W2964190861', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964345285', 'https://openalex.org/W2970692082', 'https://openalex.org/W2988249555', 'https://openalex.org/W3041866211', 'https://openalex.org/W4297801368', 'https://openalex.org/W4385245566']","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.",1.0
SKG_MT_251,https://openalex.org/W2950037171,2019,53,"['https://openalex.org/W25062297', 'https://openalex.org/W61894391', 'https://openalex.org/W1412698887', 'https://openalex.org/W1522301498', 'https://openalex.org/W2005828695', 'https://openalex.org/W2096204319', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102461220', 'https://openalex.org/W2104747875', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2157435188', 'https://openalex.org/W2162245945', 'https://openalex.org/W2220350356', 'https://openalex.org/W2527133236', 'https://openalex.org/W2531207078', 'https://openalex.org/W2572549015', 'https://openalex.org/W2605202026', 'https://openalex.org/W2740743644', 'https://openalex.org/W2756978580', 'https://openalex.org/W2758137671', 'https://openalex.org/W2768763386', 'https://openalex.org/W2798304389', 'https://openalex.org/W2800490029', 'https://openalex.org/W2803739890', 'https://openalex.org/W2859207840', 'https://openalex.org/W2883527841', 'https://openalex.org/W2888196092', 'https://openalex.org/W2899395607', 'https://openalex.org/W2899704967', 'https://openalex.org/W2909066711', 'https://openalex.org/W2910676115', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962811598', 'https://openalex.org/W2962904552', 'https://openalex.org/W2963021447', 'https://openalex.org/W2963022746', 'https://openalex.org/W2963201387', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963329925', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963628345', 'https://openalex.org/W2963858333', 'https://openalex.org/W2963887123', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963949210', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963997155', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352165', 'https://openalex.org/W4297733535', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder.",1.0
SKG_MT_252,https://openalex.org/W2250956062,2014,18,"['https://openalex.org/W74908938', 'https://openalex.org/W108437174', 'https://openalex.org/W287510790', 'https://openalex.org/W1551202288', 'https://openalex.org/W1631260214', 'https://openalex.org/W1986569439', 'https://openalex.org/W1995560154', 'https://openalex.org/W2000546550', 'https://openalex.org/W2016856586', 'https://openalex.org/W2054533749', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095650036', 'https://openalex.org/W2095755718', 'https://openalex.org/W2096557251', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101207453', 'https://openalex.org/W2102258849', 'https://openalex.org/W2103357334', 'https://openalex.org/W2105245376', 'https://openalex.org/W2106429429', 'https://openalex.org/W2107223151', 'https://openalex.org/W2107440414', 'https://openalex.org/W2110168585', 'https://openalex.org/W2113788796', 'https://openalex.org/W2117278770', 'https://openalex.org/W2117827367', 'https://openalex.org/W2123301721', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130094392', 'https://openalex.org/W2134800885', 'https://openalex.org/W2136657878', 'https://openalex.org/W2138706636', 'https://openalex.org/W2139621418', 'https://openalex.org/W2141440284', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150634928', 'https://openalex.org/W2152423400', 'https://openalex.org/W2153999629', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158917268', 'https://openalex.org/W2161227214', 'https://openalex.org/W2162245945', 'https://openalex.org/W2163255203', 'https://openalex.org/W2165666205', 'https://openalex.org/W2178903040', 'https://openalex.org/W2181317029', 'https://openalex.org/W2188941525', 'https://openalex.org/W2250281547', 'https://openalex.org/W2250375075', 'https://openalex.org/W2250947781', 'https://openalex.org/W2293139442', 'https://openalex.org/W2293596968', 'https://openalex.org/W2316776689', 'https://openalex.org/W2396575863', 'https://openalex.org/W2401082558', 'https://openalex.org/W2542068976', 'https://openalex.org/W2595715041', 'https://openalex.org/W2915810629', 'https://openalex.org/W2915824390', 'https://openalex.org/W2916946197', 'https://openalex.org/W2917073205', 'https://openalex.org/W3209717902', 'https://openalex.org/W4233432642']","Markus Freitag, Stephan Peitz, Joern Wuebker, Hermann Ney, Matthias Huck, Rico Sennrich, Nadir Durrani, Maria Nadejde, Philip Williams, Philipp Koehn, Teresa Herrmann, Eunah Cho, Alex Waibel. Proceedings of the Ninth Workshop on Statistical Machine Translation. 2014.",0.9879518072289156
SKG_MT_253,https://openalex.org/W2971094522,2019,21,"['https://openalex.org/W658020064', 'https://openalex.org/W1559169059', 'https://openalex.org/W1614298861', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251693562', 'https://openalex.org/W2512848817', 'https://openalex.org/W2539033431', 'https://openalex.org/W2564590796', 'https://openalex.org/W2759332014', 'https://openalex.org/W2760738985', 'https://openalex.org/W2915756181', 'https://openalex.org/W2945405384', 'https://openalex.org/W2952215760', 'https://openalex.org/W2962772361', 'https://openalex.org/W2964124576', 'https://openalex.org/W4294367149', 'https://openalex.org/W4299579390', 'https://openalex.org/W4300420473']","We propose WMDO, a metric based on distance between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation's word order have not been fully explored. Building on the Word Mover's Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics.",0.9933774834437086
SKG_MT_255,https://openalex.org/W3034982693,2020,19,"['https://openalex.org/W2121457870', 'https://openalex.org/W2132959801', 'https://openalex.org/W2251955814', 'https://openalex.org/W2529548870', 'https://openalex.org/W2890698823', 'https://openalex.org/W2930211575', 'https://openalex.org/W2946888380', 'https://openalex.org/W2951456627', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963382396', 'https://openalex.org/W2963403868', 'https://openalex.org/W2970074184', 'https://openalex.org/W2970084653', 'https://openalex.org/W2975711469', 'https://openalex.org/W2995428172', 'https://openalex.org/W3035348852', 'https://openalex.org/W3037793211', 'https://openalex.org/W3104081910', 'https://openalex.org/W4288345582', 'https://openalex.org/W4385245566']","Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.",1.0
SKG_MT_259,https://openalex.org/W2161290528,2012,5,"['https://openalex.org/W15592790', 'https://openalex.org/W1501633760', 'https://openalex.org/W1606508130', 'https://openalex.org/W1631260214', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116381553', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154988249', 'https://openalex.org/W2407351779']","In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2 % BLEU, whereas our approach yields 20.0 % with identical models. 1",1.0
SKG_MT_261,https://openalex.org/W2949745489,2019,31,"['https://openalex.org/W1522301498', 'https://openalex.org/W1880987886', 'https://openalex.org/W1969974515', 'https://openalex.org/W2006969979', 'https://openalex.org/W2036291627', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097997328', 'https://openalex.org/W2107440414', 'https://openalex.org/W2116792345', 'https://openalex.org/W2118536060', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131367528', 'https://openalex.org/W2133564696', 'https://openalex.org/W2137463156', 'https://openalex.org/W2146418175', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158065314', 'https://openalex.org/W2158388102', 'https://openalex.org/W2162245945', 'https://openalex.org/W2168184608', 'https://openalex.org/W2194775991', 'https://openalex.org/W2250714477', 'https://openalex.org/W2250831586', 'https://openalex.org/W2250907725', 'https://openalex.org/W2251048776', 'https://openalex.org/W2251369387', 'https://openalex.org/W2514458001', 'https://openalex.org/W2525778437', 'https://openalex.org/W2552839021', 'https://openalex.org/W2576482813', 'https://openalex.org/W2604241755', 'https://openalex.org/W2613904329', 'https://openalex.org/W2623037479', 'https://openalex.org/W2740743644', 'https://openalex.org/W2741986820', 'https://openalex.org/W2756978580', 'https://openalex.org/W2758137671', 'https://openalex.org/W2768231593', 'https://openalex.org/W2768763386', 'https://openalex.org/W2798304389', 'https://openalex.org/W2807352297', 'https://openalex.org/W2859207840', 'https://openalex.org/W2888196092', 'https://openalex.org/W2890244613', 'https://openalex.org/W2903193068', 'https://openalex.org/W2904829696', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962811598', 'https://openalex.org/W2962926939', 'https://openalex.org/W2963022746', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963329925', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963699608', 'https://openalex.org/W2963888305', 'https://openalex.org/W2963913268', 'https://openalex.org/W2963922633', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963949210', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3121623419', 'https://openalex.org/W3166311385', 'https://openalex.org/W4241645538', 'https://openalex.org/W4385245566']","The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT’14 English-to-German, NIST Chinese-to-English, and WAT Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer.",1.0
SKG_MT_262,https://openalex.org/W2916222345,2015,2,"['https://openalex.org/W1631260214', 'https://openalex.org/W1970689298', 'https://openalex.org/W1995560154', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095650036', 'https://openalex.org/W2097606805', 'https://openalex.org/W2103357334', 'https://openalex.org/W2105245376', 'https://openalex.org/W2108862644', 'https://openalex.org/W2111355378', 'https://openalex.org/W2114211285', 'https://openalex.org/W2120861206', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131462252', 'https://openalex.org/W2132109814', 'https://openalex.org/W2134800885', 'https://openalex.org/W2138706636', 'https://openalex.org/W2143564602', 'https://openalex.org/W2143719855', 'https://openalex.org/W2144725461', 'https://openalex.org/W2152790380', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2250375075', 'https://openalex.org/W2250905272', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251682575', 'https://openalex.org/W2401082558', 'https://openalex.org/W2542068976', 'https://openalex.org/W2595715041', 'https://openalex.org/W2998704965', 'https://openalex.org/W3209717902', 'https://openalex.org/W4241645538', 'https://openalex.org/W4285719527']","This paper presented the joined submission of KIT and LIMSI to the English to German translation task of WMT 2015.In this year submission, we integrated a neural network-based translation model into a phrase-based translation model by rescoring the n-best lists.Since the computation complexity is one of the main issues for continuous space models, we compared two techniques to reduce the computation cost.We investigated models using a structured output layer as well as models trained with noise contrastive estimation.Furthermore, we evaluated a new method to obtain the best log-linear combination in the rescoring phase.Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points.",1.0
SKG_MT_263,https://openalex.org/W2984294981,2019,8,"['https://openalex.org/W2117278770', 'https://openalex.org/W2606974598', 'https://openalex.org/W2739046565', 'https://openalex.org/W2935206035', 'https://openalex.org/W2938704169', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963592583', 'https://openalex.org/W2970752831', 'https://openalex.org/W2987188351', 'https://openalex.org/W2996287690', 'https://openalex.org/W4289298591', 'https://openalex.org/W4385245566']",LIDIAP,0.9950738916256158
SKG_MT_264,https://openalex.org/W3102816807,2020,41,"['https://openalex.org/W1522301498', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2525778437', 'https://openalex.org/W2561907692', 'https://openalex.org/W2613904329', 'https://openalex.org/W2817535134', 'https://openalex.org/W2888520903', 'https://openalex.org/W2896457183', 'https://openalex.org/W2908336025', 'https://openalex.org/W2933138175', 'https://openalex.org/W2945667196', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964088127', 'https://openalex.org/W2964093309', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965046076', 'https://openalex.org/W2970290486', 'https://openalex.org/W2970731908', 'https://openalex.org/W2972451902', 'https://openalex.org/W3034742481', 'https://openalex.org/W3035618017', 'https://openalex.org/W3035747971', 'https://openalex.org/W3103334733', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT’16 English-German and WMT’14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at https://github.com/libeineu/SDT-Training.",1.0
SKG_MT_265,https://openalex.org/W399167303,2014,24,"['https://openalex.org/W21963070', 'https://openalex.org/W111188368', 'https://openalex.org/W138108643', 'https://openalex.org/W168280322', 'https://openalex.org/W645927007', 'https://openalex.org/W1631260214', 'https://openalex.org/W1819903106', 'https://openalex.org/W1904457459', 'https://openalex.org/W2010917994', 'https://openalex.org/W2025040932', 'https://openalex.org/W2072806403', 'https://openalex.org/W2076041950', 'https://openalex.org/W2084339293', 'https://openalex.org/W2095263712', 'https://openalex.org/W2096389830', 'https://openalex.org/W2096451823', 'https://openalex.org/W2100564077', 'https://openalex.org/W2100631779', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113106066', 'https://openalex.org/W2118804972', 'https://openalex.org/W2121642367', 'https://openalex.org/W2122511433', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131694695', 'https://openalex.org/W2134756243', 'https://openalex.org/W2136530135', 'https://openalex.org/W2138974820', 'https://openalex.org/W2144600658', 'https://openalex.org/W2145685230', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148702268', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2157435188', 'https://openalex.org/W2168262690', 'https://openalex.org/W2184135559', 'https://openalex.org/W2186089609', 'https://openalex.org/W2252154714', 'https://openalex.org/W2595715041', 'https://openalex.org/W2916456038', 'https://openalex.org/W2917421659', 'https://openalex.org/W2917604489', 'https://openalex.org/W2917668649', 'https://openalex.org/W4241645538']","We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using only a source language pronunciation dictionary and language model (i.e., without an acoustic model), and use these to augment the phrase table of a standard MT system. The augmented system can thus recover from recognition errors during decoding using synthesized phrases. Using the outputs of five different English ASR systems as input, we find consistent and significant improvements in translation quality. Our proposed technique can also be used in conjunction with lattices as ASR output, leading to further improvements.",1.0
SKG_MT_266,https://openalex.org/W2250610505,2014,25,"['https://openalex.org/W179875071', 'https://openalex.org/W932413789', 'https://openalex.org/W1575384945', 'https://openalex.org/W1675954498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1965058830', 'https://openalex.org/W1965154800', 'https://openalex.org/W2013540053', 'https://openalex.org/W2020382207', 'https://openalex.org/W2026383756', 'https://openalex.org/W2058695628', 'https://openalex.org/W2099084090', 'https://openalex.org/W2100714283', 'https://openalex.org/W2105865683', 'https://openalex.org/W2107440414', 'https://openalex.org/W2114211285', 'https://openalex.org/W2121227244', 'https://openalex.org/W2124807415', 'https://openalex.org/W2136189984', 'https://openalex.org/W2141440284', 'https://openalex.org/W2141599568', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152311128', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153999629', 'https://openalex.org/W2171928131', 'https://openalex.org/W2250379827', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250651922', 'https://openalex.org/W2251098065', 'https://openalex.org/W2595715041', 'https://openalex.org/W2766736793', 'https://openalex.org/W2998704965', 'https://openalex.org/W4241645538', 'https://openalex.org/W4285719527']","We introduce recurrent neural networkbased Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts.Traditional back-off n-gram models suffer under the sparse nature of MTUs which makes estimation of highorder sequence models challenging.We tackle the sparsity problem by modeling MTUs both as bags-of-words and as a sequence of individual source and target words.Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU.",1.0
SKG_MT_268,https://openalex.org/W2113549939,2013,26,"['https://openalex.org/W16967297', 'https://openalex.org/W1515410566', 'https://openalex.org/W1686266550', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008225289', 'https://openalex.org/W2008652694', 'https://openalex.org/W2038698865', 'https://openalex.org/W2040781285', 'https://openalex.org/W2040870580', 'https://openalex.org/W2080373976', 'https://openalex.org/W2093197119', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119072456', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132217790', 'https://openalex.org/W2142112143', 'https://openalex.org/W2143564602', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165199647', 'https://openalex.org/W2171074980', 'https://openalex.org/W2171458318', 'https://openalex.org/W2177801600', 'https://openalex.org/W2180952760', 'https://openalex.org/W2184135559', 'https://openalex.org/W2567948266', 'https://openalex.org/W2892357930', 'https://openalex.org/W2950186769', 'https://openalex.org/W3167591218', 'https://openalex.org/W4285719527']","We present a novel online learning approach for statistical machine translation tailored to the computer assisted translation scenario. With the introduction of a simple online feature, we are able to adapt the translation model on the fly to the corrections made by the translators. Additionally, we do online adaption of the feature weights with a large margin algorithm. Our results show that our online adaptation technique outperforms the static phrase based statistical machine translation system by 6 BLEU points absolute, and a standard incremental adaptation approach by 2 BLEU points absolute. 1",0.957983193277311
SKG_MT_269,https://openalex.org/W3035254119,2020,54,"['https://openalex.org/W2550821151', 'https://openalex.org/W2566926700', 'https://openalex.org/W2752630748', 'https://openalex.org/W2756566411', 'https://openalex.org/W2760656271', 'https://openalex.org/W2796108585', 'https://openalex.org/W2798931235', 'https://openalex.org/W2888541716', 'https://openalex.org/W2889511806', 'https://openalex.org/W2890007195', 'https://openalex.org/W2890731353', 'https://openalex.org/W2903193068', 'https://openalex.org/W2905027511', 'https://openalex.org/W2923622379', 'https://openalex.org/W2948735718', 'https://openalex.org/W2950733326', 'https://openalex.org/W2952614664', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964343359', 'https://openalex.org/W2972534020', 'https://openalex.org/W3104652516', 'https://openalex.org/W4385245566']","Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.",0.9925925925925926
SKG_MT_271,https://openalex.org/W2970810442,2019,21,"['https://openalex.org/W22168010', 'https://openalex.org/W1522263329', 'https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2605717780', 'https://openalex.org/W2759173152', 'https://openalex.org/W2760656271', 'https://openalex.org/W2773956126', 'https://openalex.org/W2778814079', 'https://openalex.org/W2799051177', 'https://openalex.org/W2888539709', 'https://openalex.org/W2897507397', 'https://openalex.org/W2902545332', 'https://openalex.org/W2903193068', 'https://openalex.org/W2946794439', 'https://openalex.org/W2962777840', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964174820', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970810442', 'https://openalex.org/W2990555152', 'https://openalex.org/W3082674894', 'https://openalex.org/W4298170715', 'https://openalex.org/W4385245566']","Neural machine translation (NMT) has achieved new state-of-the-art performance in translating ambiguous words. However, it is still unclear which component dominates the process of disambiguation. In this paper, we explore the ability of NMT encoders and decoders to disambiguate word senses by evaluating hidden states and investigating the distributions of self-attention. We train a classifier to predict whether a translation is correct given the representation of an ambiguous noun. We find that encoder hidden states outperform word embeddings significantly which indicates that encoders adequately encode relevant information for disambiguation into hidden states. In contrast to encoders, the effect of decoder is different in models with different architectures. Moreover, the attention weights and attention entropy show that self-attention can detect ambiguous nouns and distribute more attention to the context.",1.0
SKG_MT_274,https://openalex.org/W2473954587,2016,6,"['https://openalex.org/W9489473', 'https://openalex.org/W100078778', 'https://openalex.org/W567326294', 'https://openalex.org/W1576280417', 'https://openalex.org/W2013112874', 'https://openalex.org/W2058941677', 'https://openalex.org/W2107440414', 'https://openalex.org/W2116792345', 'https://openalex.org/W2139940533', 'https://openalex.org/W2153076044', 'https://openalex.org/W2153999629', 'https://openalex.org/W2154462503', 'https://openalex.org/W2250830017', 'https://openalex.org/W2270190199', 'https://openalex.org/W2473750942']",,0.9941520467836257
SKG_MT_276,https://openalex.org/W2989819126,2019,23,"['https://openalex.org/W1553004968', 'https://openalex.org/W1902237438', 'https://openalex.org/W2009150118', 'https://openalex.org/W2024200390', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127141656', 'https://openalex.org/W2130942839', 'https://openalex.org/W2149327368', 'https://openalex.org/W2251328660', 'https://openalex.org/W2327501763', 'https://openalex.org/W2407080277', 'https://openalex.org/W2408504891', 'https://openalex.org/W2581377246', 'https://openalex.org/W2582956876', 'https://openalex.org/W2605131327', 'https://openalex.org/W2627092829', 'https://openalex.org/W2799800213', 'https://openalex.org/W2936774411', 'https://openalex.org/W2936848022', 'https://openalex.org/W2937148911', 'https://openalex.org/W2949328740', 'https://openalex.org/W2950108439', 'https://openalex.org/W2953022181', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962728618', 'https://openalex.org/W2962775040', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962826786', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964104866', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964308564', 'https://openalex.org/W2978099976', 'https://openalex.org/W3007142233', 'https://openalex.org/W3008191852']","This work investigates a simple data augmentation technique, SpecAugment, for end-to-end speech translation. SpecAugment is a low-cost implementation method applied directly to the audio input features and it consists of masking blocks of frequency channels, and/or time steps. We apply SpecAugment on end-to-end speech translation tasks and achieve up to +2.2% BLEU on LibriSpeech Audiobooks En→Fr and +1.2% on IWSLT TED-talks En→De by alleviating overfitting to some extent. We also examine the effectiveness of the method in a variety of data scenarios and show that the method also leads to significant improvements in various data conditions irrespective of the amount of training data.",1.0
SKG_MT_277,https://openalex.org/W2160993407,2013,8,"['https://openalex.org/W1631260214', 'https://openalex.org/W1736600331', 'https://openalex.org/W2006969979', 'https://openalex.org/W2038698865', 'https://openalex.org/W2087309226', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112900913', 'https://openalex.org/W2116792345', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126270798', 'https://openalex.org/W2126449874', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153204578', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154099718', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158266063', 'https://openalex.org/W2158733102', 'https://openalex.org/W2165666205', 'https://openalex.org/W2165814974', 'https://openalex.org/W2251098065']","Most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phraseinternal translation and reordering. However phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. We propose a new model to address this imbalance, based on a word-based Markov model of translation which generates target translations left-to-right. Our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1",1.0
SKG_MT_280,https://openalex.org/W2136353104,2011,97,"['https://openalex.org/W21337280', 'https://openalex.org/W143910215', 'https://openalex.org/W222053410', 'https://openalex.org/W255975419', 'https://openalex.org/W1589814219', 'https://openalex.org/W1880262756', 'https://openalex.org/W1973923101', 'https://openalex.org/W1978394996', 'https://openalex.org/W1989658336', 'https://openalex.org/W2004447574', 'https://openalex.org/W2006969979', 'https://openalex.org/W2067815623', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105577415', 'https://openalex.org/W2112874453', 'https://openalex.org/W2115526192', 'https://openalex.org/W2123635983', 'https://openalex.org/W2127836646', 'https://openalex.org/W2136477195', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2159882563', 'https://openalex.org/W2161792612', 'https://openalex.org/W2301498594', 'https://openalex.org/W2356613612']","Statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. In this paper, we propose a cache-based approach to document-level translation. Since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. In this paper, we present three kinds of caches to store relevant document-level information: 1) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document; 2) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs (i.e. source documents similar to the test document and their corresponding target documents) in the training parallel corpus; 3) a topic cache, which stores the target-side topic words related with the test document in the source-side. In particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1",1.0
SKG_MT_281,https://openalex.org/W2799051177,2018,304,"['https://openalex.org/W120531462', 'https://openalex.org/W131501223', 'https://openalex.org/W255975419', 'https://openalex.org/W1738081185', 'https://openalex.org/W2030987872', 'https://openalex.org/W2067815623', 'https://openalex.org/W2101451733', 'https://openalex.org/W2101951467', 'https://openalex.org/W2117045850', 'https://openalex.org/W2123442489', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136353104', 'https://openalex.org/W2151996595', 'https://openalex.org/W2154006786', 'https://openalex.org/W2163038970', 'https://openalex.org/W2171421863', 'https://openalex.org/W2582446770', 'https://openalex.org/W2608029998', 'https://openalex.org/W2767019613', 'https://openalex.org/W2806412155', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963695529', 'https://openalex.org/W2963925965', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964308564', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).",1.0
SKG_MT_282,https://openalex.org/W2114053509,2013,7,"['https://openalex.org/W144133692', 'https://openalex.org/W222053410', 'https://openalex.org/W1979495315', 'https://openalex.org/W2010835028', 'https://openalex.org/W2018116550', 'https://openalex.org/W2020431608', 'https://openalex.org/W2045344178', 'https://openalex.org/W2051434435', 'https://openalex.org/W2069429561', 'https://openalex.org/W2082571252', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104441213', 'https://openalex.org/W2108374515', 'https://openalex.org/W2108740414', 'https://openalex.org/W2111568643', 'https://openalex.org/W2112900913', 'https://openalex.org/W2115289978', 'https://openalex.org/W2116957398', 'https://openalex.org/W2124807415', 'https://openalex.org/W2137268927', 'https://openalex.org/W2141552696', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147448291', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157381218', 'https://openalex.org/W2158190429', 'https://openalex.org/W2158266063', 'https://openalex.org/W2163548102', 'https://openalex.org/W2165666205', 'https://openalex.org/W2166905217', 'https://openalex.org/W2169895393', 'https://openalex.org/W2180952760', 'https://openalex.org/W2251593346', 'https://openalex.org/W2252268838', 'https://openalex.org/W2293111166', 'https://openalex.org/W3037265734', 'https://openalex.org/W3145738572']","This paper proposes a nonparametric Bayesian method for inducing Part-of-Speech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilin-gual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned tar-get word, either jointly (joint model), or independently (independent model). Eval-uations of Japanese-to-English translation on the NTCIR-9 data show that our in-duced Japanese POS tags for dependency trees improve the performance of a forest-to-string SMT system. Our independent model gains over 1 point in BLEU by re-solving the sparseness problem introduced in the joint model. 1",1.0
SKG_MT_283,https://openalex.org/W2125823897,2012,8,"['https://openalex.org/W1525595230', 'https://openalex.org/W1631260214', 'https://openalex.org/W1905522558', 'https://openalex.org/W1989658336', 'https://openalex.org/W2004447574', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008652694', 'https://openalex.org/W2030760474', 'https://openalex.org/W2056260421', 'https://openalex.org/W2062270497', 'https://openalex.org/W2082718666', 'https://openalex.org/W2085086335', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102394389', 'https://openalex.org/W2103790002', 'https://openalex.org/W2109664771', 'https://openalex.org/W2116042738', 'https://openalex.org/W2117278770', 'https://openalex.org/W2119168550', 'https://openalex.org/W2120158336', 'https://openalex.org/W2132001515', 'https://openalex.org/W2136757377', 'https://openalex.org/W2139989135', 'https://openalex.org/W2145989470', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147308966', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2170120409', 'https://openalex.org/W2188911266', 'https://openalex.org/W2356613612', 'https://openalex.org/W2394860946', 'https://openalex.org/W2397909109', 'https://openalex.org/W2437005631', 'https://openalex.org/W2799061466']","In this paper, we propose a novel translation model (TM) based cross-lingual data selection model for language model (LM) adaptation in statistical machine translation (SMT), from word models to phrase models. Given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target LM training corpus is similar. Compared with the traditional approaches which utilize the first pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation. Furthermore, phrase TM based cross-lingual data selection model is more effective than the traditional approaches based on bag-ofwords models and word-based TM, because it captures contextual information in modeling the selection of phrase as a whole. Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance. 1",0.9948717948717949
SKG_MT_286,https://openalex.org/W2149971620,2011,17,"['https://openalex.org/W7542544', 'https://openalex.org/W16967297', 'https://openalex.org/W17500809', 'https://openalex.org/W140816929', 'https://openalex.org/W182840523', 'https://openalex.org/W605575788', 'https://openalex.org/W1482605500', 'https://openalex.org/W1585143568', 'https://openalex.org/W1597533204', 'https://openalex.org/W1612003148', 'https://openalex.org/W1718044877', 'https://openalex.org/W1880262756', 'https://openalex.org/W1916559533', 'https://openalex.org/W1934041838', 'https://openalex.org/W1989658336', 'https://openalex.org/W1995875735', 'https://openalex.org/W2001082470', 'https://openalex.org/W2001792610', 'https://openalex.org/W2004447574', 'https://openalex.org/W2006969979', 'https://openalex.org/W2033593667', 'https://openalex.org/W2049633694', 'https://openalex.org/W2057589672', 'https://openalex.org/W2063397738', 'https://openalex.org/W2066296458', 'https://openalex.org/W2078861931', 'https://openalex.org/W2082092506', 'https://openalex.org/W2096610504', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105577415', 'https://openalex.org/W2110591510', 'https://openalex.org/W2116229791', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127602683', 'https://openalex.org/W2132001515', 'https://openalex.org/W2134237567', 'https://openalex.org/W2137387514', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147152072', 'https://openalex.org/W2158195707', 'https://openalex.org/W2164948578', 'https://openalex.org/W2170722465', 'https://openalex.org/W2422872931', 'https://openalex.org/W2917604489', 'https://openalex.org/W3145733519']","This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training. During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language’s vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM). We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15 % reduction in perplexity and relative BLEU and NIST improvements of 3 % and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task. Our topic modeling approach is simpler to construct than its counterparts.",1.0
SKG_MT_287,https://openalex.org/W2154417380,2010,0,"['https://openalex.org/W23077562', 'https://openalex.org/W2090284840', 'https://openalex.org/W2100238596', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105891181', 'https://openalex.org/W2117086786', 'https://openalex.org/W2124797537', 'https://openalex.org/W2124807415', 'https://openalex.org/W2136657878', 'https://openalex.org/W2138414624', 'https://openalex.org/W2145765191', 'https://openalex.org/W2146574666', 'https://openalex.org/W2161227214', 'https://openalex.org/W3203109892']","This paper presents a fast consensus hypothesis regeneration approach for machine translation. It combines the advantages of feature-based fast consensus decoding and hypothesis regeneration. Our approach is more efficient than previous work on hypothesis regeneration, and it explores a wider search space than consensus decoding, resulting in improved performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-to-English NIST task. 1",1.0
SKG_MT_290,https://openalex.org/W2143002608,2013,10,"['https://openalex.org/W111475876', 'https://openalex.org/W126091597', 'https://openalex.org/W222053410', 'https://openalex.org/W1985754308', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103017773', 'https://openalex.org/W2112900913', 'https://openalex.org/W2118536060', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130988241', 'https://openalex.org/W2136575501', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2139621418', 'https://openalex.org/W2140133598', 'https://openalex.org/W2143008661', 'https://openalex.org/W2144279206', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154581043', 'https://openalex.org/W2158431714', 'https://openalex.org/W2164196204', 'https://openalex.org/W2165475302', 'https://openalex.org/W2169954957', 'https://openalex.org/W2171526525', 'https://openalex.org/W2252171041', 'https://openalex.org/W2407799926', 'https://openalex.org/W2434901392', 'https://openalex.org/W2437005631']","Predicate-argument structure (PAS) has been demonstrated to be very effective in improving SMT performance. However, since a sourceside PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. In this paper, we group PAS ambiguities into two types: role ambiguity and gap ambiguity. Then we propose two novel methods to handle the two PAS ambiguities for SMT accordingly: 1) inside context integration; 2) a novel maximum entropy PAS disambiguation (MEPD) model. In this way, we incorporate rich context information of PAS for disambiguation. Then we integrate the two methods into a PASbased translation framework. Experiments show that our approach helps to achieve significant improvements on translation quality. 1",1.0
SKG_MT_291,https://openalex.org/W3099858430,2020,2,"['https://openalex.org/W1821462560', 'https://openalex.org/W2095705004', 'https://openalex.org/W2905741102', 'https://openalex.org/W2908336025', 'https://openalex.org/W2912351236', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963766446', 'https://openalex.org/W2975381464', 'https://openalex.org/W2995999067', 'https://openalex.org/W2996159613', 'https://openalex.org/W4385245566']","The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method——LayerDrop.",1.0
SKG_MT_292,https://openalex.org/W3089067043,2019,10,[],"Supervised machine translation works well when the train and test data are sampled from the same distribution. When this is not the case, adaptation techniques help ensure that the knowledge learned from out-of-domain texts generalises to in-domain sentences. We study here a related setting, multi-domain adaptation, where the number of domains is potentially large and adapting separately to each domain would waste training resources. Our proposal transposes to neural machine translation the feature expansion technique of (Daumé III, 2007): it isolates domain-agnostic from domain-specific lexical representations, while sharing the most of the network across domains. Our experiments use two architectures and two language pairs: they show that our approach, while simple and computationally inexpensive, outperforms several strong baselines and delivers a multi-domain system that successfully translates texts from diverse sources.",1.0
SKG_MT_293,https://openalex.org/W2798761464,2018,81,"['https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2070150502', 'https://openalex.org/W2095705004', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144600658', 'https://openalex.org/W2157331557', 'https://openalex.org/W2183341477', 'https://openalex.org/W2525778437', 'https://openalex.org/W2540404261', 'https://openalex.org/W2553303224', 'https://openalex.org/W2567070169', 'https://openalex.org/W2594990650', 'https://openalex.org/W2610817424', 'https://openalex.org/W2613904329', 'https://openalex.org/W2778814079', 'https://openalex.org/W2780625659', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963778169', 'https://openalex.org/W2963970792', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964250379', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3082674894', 'https://openalex.org/W4301368689', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different architectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important on the encoder side than on the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well.",0.9947089947089947
SKG_MT_296,https://openalex.org/W2160177523,2015,9,"['https://openalex.org/W131533222', 'https://openalex.org/W594760864', 'https://openalex.org/W1493309689', 'https://openalex.org/W1533861849', 'https://openalex.org/W1976373002', 'https://openalex.org/W2032494091', 'https://openalex.org/W2081580037', 'https://openalex.org/W2099884836', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103305545', 'https://openalex.org/W2129804798', 'https://openalex.org/W2135875128', 'https://openalex.org/W2137983211', 'https://openalex.org/W2146867136', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153702313', 'https://openalex.org/W2159849140', 'https://openalex.org/W2161488606', 'https://openalex.org/W2163986298', 'https://openalex.org/W2250305120', 'https://openalex.org/W2250565861', 'https://openalex.org/W2250835812', 'https://openalex.org/W2251756164', 'https://openalex.org/W2275361309', 'https://openalex.org/W2787381839', 'https://openalex.org/W2885050925', 'https://openalex.org/W3146803896', 'https://openalex.org/W4386506836']","This paper describes the system developed by our team (HLTC-HKUST) for task 1 of SemEval 2015 workshop about paraphrase classification and semantic similarity in Twitter. We trained a neural network classifier over a range of features that includes translation metrics, lexical and syntactic similarity score and semantic features based on semantic roles. The neural network was trained taking into consideration in the objective function the six different similarity levels provided in the corpus, in order to give as output a more fine-grained estimation of the similarity level of the two sentences, as required by subtask 2. With an F-score of 0.651 in the binary paraphrase classification subtask 1, and a Pearson coefficient of 0.697 for the sentence similarity subtask 2, we achieved respectively the 6th place and the 3rd place, above the average of what obtained by the other contestants.",0.9959514170040485
SKG_MT_298,https://openalex.org/W2970797723,2019,27,[],"Context modeling is essential to generate coherent and consistent translation\nfor Document-level Neural Machine Translations. The widely used method for\ndocument-level translation usually compresses the context information into a\nrepresentation via hierarchical attention networks. However, this method\nneither considers the relationship between context words nor distinguishes the\nroles of context words. To address this problem, we propose a query-guided\ncapsule networks to cluster context information into different perspectives\nfrom which the target translation may concern. Experiment results show that our\nmethod can significantly outperform strong baselines on multiple data sets of\ndifferent domains.\n",1.0
SKG_MT_300,https://openalex.org/W2141552696,2011,14,"['https://openalex.org/W201231365', 'https://openalex.org/W1483849869', 'https://openalex.org/W1658658360', 'https://openalex.org/W1781980207', 'https://openalex.org/W1904457459', 'https://openalex.org/W1978478796', 'https://openalex.org/W1998301456', 'https://openalex.org/W2037894654', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104027215', 'https://openalex.org/W2108239140', 'https://openalex.org/W2110104386', 'https://openalex.org/W2111491614', 'https://openalex.org/W2114060876', 'https://openalex.org/W2117139364', 'https://openalex.org/W2118972857', 'https://openalex.org/W2122609803', 'https://openalex.org/W2130959832', 'https://openalex.org/W2134495021', 'https://openalex.org/W2145033586', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152096446', 'https://openalex.org/W2154757770', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157512532', 'https://openalex.org/W2159107349', 'https://openalex.org/W2159358338', 'https://openalex.org/W2161227214', 'https://openalex.org/W2162865360', 'https://openalex.org/W2163296342', 'https://openalex.org/W2169362686', 'https://openalex.org/W2171802951', 'https://openalex.org/W2437005631', 'https://openalex.org/W2594610113', 'https://openalex.org/W2911246871', 'https://openalex.org/W2988806182', 'https://openalex.org/W3003374142']","The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that con-stitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space. 1",1.0
SKG_MT_302,https://openalex.org/W3019655571,2020,2,"['https://openalex.org/W190156351', 'https://openalex.org/W630532510', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112534334', 'https://openalex.org/W2136528950', 'https://openalex.org/W2160807445', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251364186', 'https://openalex.org/W2611838487', 'https://openalex.org/W2623769564', 'https://openalex.org/W2766733559', 'https://openalex.org/W2902306948', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963494156', 'https://openalex.org/W2963500732', 'https://openalex.org/W2963708445', 'https://openalex.org/W2963979492', 'https://openalex.org/W2990352702', 'https://openalex.org/W3004726085']","Thai is a low-resource language, so it is often the case that data is not available in sufficient quantities to train an Neural Machine Translation (NMT) model which perform to a high level of quality. In addition, the Thai script does not use white spaces to delimit the boundaries between words, which adds more complexity when building sequence to sequence models. In this work, we explore how to augment a set of English--Thai parallel data by replicating sentence-pairs with different word segmentation methods on Thai, as training data for NMT model training. Using different merge operations of Byte Pair Encoding, different segmentations of Thai sentences can be obtained. The experiments show that combining these datasets, performance is improved for NMT models trained with a dataset that has been split using a supervised splitting tool.",1.0
SKG_MT_303,https://openalex.org/W2951902588,2019,74,"['https://openalex.org/W932413789', 'https://openalex.org/W1411230545', 'https://openalex.org/W1533230146', 'https://openalex.org/W2097732278', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123442489', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2138204974', 'https://openalex.org/W2153653739', 'https://openalex.org/W2184957013', 'https://openalex.org/W2340762547', 'https://openalex.org/W2525778437', 'https://openalex.org/W2574790321', 'https://openalex.org/W2787642437', 'https://openalex.org/W2799079108', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963109634', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963699608', 'https://openalex.org/W2963899908', 'https://openalex.org/W2964086597', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4299801216', 'https://openalex.org/W4385245566']","While neural machine translation (NMT) has achieved remarkable success, NMT systems are prone to make word omission errors. In this work, we propose a contrastive learning approach to reducing word omission errors in NMT. The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation, which is automatically constructed from the ground-truth translation by omitting words. We design different types of negative examples depending on the number of omitted words, word frequency, and part of speech. Experiments on Chinese-to-English, German-to-English, and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better translation performance than three baseline methods.",0.994535519125683
SKG_MT_304,https://openalex.org/W2963142052,2015,4,"['https://openalex.org/W144133692', 'https://openalex.org/W932413789', 'https://openalex.org/W1540371141', 'https://openalex.org/W1554663460', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102017903', 'https://openalex.org/W2102825444', 'https://openalex.org/W2116316001', 'https://openalex.org/W2117499988', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2158195707', 'https://openalex.org/W2158614781', 'https://openalex.org/W2160815625', 'https://openalex.org/W2161227214', 'https://openalex.org/W2163605009', 'https://openalex.org/W2180952760', 'https://openalex.org/W2250445771', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250508490', 'https://openalex.org/W2251071050', 'https://openalex.org/W2251682575', 'https://openalex.org/W2251690405', 'https://openalex.org/W2404007507', 'https://openalex.org/W4241645538', 'https://openalex.org/W4388297464']","Shujian Huang, Huadong Chen, Xin-Yu Dai, Jiajun Chen. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.",1.0
SKG_MT_305,https://openalex.org/W2251302843,2013,36,"['https://openalex.org/W22168010', 'https://openalex.org/W635530177', 'https://openalex.org/W1479807131', 'https://openalex.org/W1541966623', 'https://openalex.org/W1552297528', 'https://openalex.org/W1593239840', 'https://openalex.org/W1693107767', 'https://openalex.org/W1863309007', 'https://openalex.org/W1999806105', 'https://openalex.org/W2041232209', 'https://openalex.org/W2057052429', 'https://openalex.org/W2072976288', 'https://openalex.org/W2084277454', 'https://openalex.org/W2086039194', 'https://openalex.org/W2086511124', 'https://openalex.org/W2091889711', 'https://openalex.org/W2093641143', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102749417', 'https://openalex.org/W2105891181', 'https://openalex.org/W2109396985', 'https://openalex.org/W2116780029', 'https://openalex.org/W2120679544', 'https://openalex.org/W2121415745', 'https://openalex.org/W2122056984', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2145685230', 'https://openalex.org/W2151075664', 'https://openalex.org/W2153903004', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165404293', 'https://openalex.org/W2166776180', 'https://openalex.org/W2172268343', 'https://openalex.org/W2250413370', 'https://openalex.org/W2439017901', 'https://openalex.org/W2882319491']","Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language transla-tions. Experimental results show that our graph propagation method significantly improves per-formance over two strong baselines under intrin-sic and extrinsic evaluation metrics. 1",1.0
SKG_MT_307,https://openalex.org/W2970169061,2019,7,"['https://openalex.org/W2127863960', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2550821151', 'https://openalex.org/W2595715041', 'https://openalex.org/W2744813330', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964247056', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970558573', 'https://openalex.org/W4385245566']","In this paper we describe our neural machine translation (NMT) systems for Japanese↔English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for Japanese↔English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest.",1.0
SKG_MT_308,https://openalex.org/W2108471596,2011,3,"['https://openalex.org/W22168010', 'https://openalex.org/W300322770', 'https://openalex.org/W1631260214', 'https://openalex.org/W1854082259', 'https://openalex.org/W1972788481', 'https://openalex.org/W2006969979', 'https://openalex.org/W2038698865', 'https://openalex.org/W2101207453', 'https://openalex.org/W2118972857', 'https://openalex.org/W2124807415', 'https://openalex.org/W2171802951', 'https://openalex.org/W2181423891', 'https://openalex.org/W2237254121', 'https://openalex.org/W2437005631']",We present the DFKI hybrid translation system at the WMT workshop 2011. Three SMT and two RBMT systems are combined at the level of the final translation output. The translation results show that our hybrid system significantly outperformed individual systems by exploring strengths of both rule-based and statistical translations. 1,1.0
SKG_MT_309,https://openalex.org/W3110062857,2020,7,"['https://openalex.org/W298172948', 'https://openalex.org/W630532510', 'https://openalex.org/W635530177', 'https://openalex.org/W1205016396', 'https://openalex.org/W1625582487', 'https://openalex.org/W1682403713', 'https://openalex.org/W1753482797', 'https://openalex.org/W1966784040', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104936489', 'https://openalex.org/W2117045850', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133622676', 'https://openalex.org/W2147262247', 'https://openalex.org/W2160131015', 'https://openalex.org/W2184135559', 'https://openalex.org/W2261339088', 'https://openalex.org/W2567571499', 'https://openalex.org/W2757592053', 'https://openalex.org/W2760452458', 'https://openalex.org/W2802153702', 'https://openalex.org/W2803241009', 'https://openalex.org/W2887920589', 'https://openalex.org/W2892244498', 'https://openalex.org/W2913340405', 'https://openalex.org/W2962796826', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963841178', 'https://openalex.org/W2963897095', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964303773', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970925270', 'https://openalex.org/W3015078597', 'https://openalex.org/W3034469191', 'https://openalex.org/W3034771276', 'https://openalex.org/W3035254119', 'https://openalex.org/W3089067043', 'https://openalex.org/W3204406378', 'https://openalex.org/W3211259717']",International audience,1.0
SKG_MT_310,https://openalex.org/W3115075512,2020,37,"['https://openalex.org/W2101105183', 'https://openalex.org/W2529548870', 'https://openalex.org/W2890698823', 'https://openalex.org/W2945700568', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2963061963', 'https://openalex.org/W2963158258', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963532001', 'https://openalex.org/W2970074184', 'https://openalex.org/W2978099976', 'https://openalex.org/W2995428172', 'https://openalex.org/W3034586846', 'https://openalex.org/W3037465386', 'https://openalex.org/W3037793211']","We investigate how to adapt simultaneous text translation methods such as wait-k and monotonic multihead attention to end-to-end simultaneous speech translation by introducing a pre-decision module. A detailed analysis is provided on the latency-quality trade-offs of combining fixed and flexible pre-decision with fixed and flexible policies. We also design a novel computation-aware latency metric, adapted from Average Lagging.",0.9951690821256038
SKG_MT_312,https://openalex.org/W2466062786,2016,100,"['https://openalex.org/W6908809', 'https://openalex.org/W648786980', 'https://openalex.org/W1810943226', 'https://openalex.org/W1973923101', 'https://openalex.org/W2049872937', 'https://openalex.org/W2100128988', 'https://openalex.org/W2100664567', 'https://openalex.org/W2109942965', 'https://openalex.org/W2114396466', 'https://openalex.org/W2118434577', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134036914', 'https://openalex.org/W2160380653', 'https://openalex.org/W2169724380', 'https://openalex.org/W2250651922', 'https://openalex.org/W2251204185', 'https://openalex.org/W2251682575', 'https://openalex.org/W2293111166', 'https://openalex.org/W2460550122', 'https://openalex.org/W2595715041', 'https://openalex.org/W2952360713']","Lemao Liu, Masao Utiyama, Andrew Finch, Eiichiro Sumita. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",1.0
SKG_MT_314,https://openalex.org/W2251764313,2013,8,"['https://openalex.org/W22168010', 'https://openalex.org/W159230833', 'https://openalex.org/W193734270', 'https://openalex.org/W1508001288', 'https://openalex.org/W1556255569', 'https://openalex.org/W1880262756', 'https://openalex.org/W2020842694', 'https://openalex.org/W2024668293', 'https://openalex.org/W2024722304', 'https://openalex.org/W2033593667', 'https://openalex.org/W2042980227', 'https://openalex.org/W2072644219', 'https://openalex.org/W2078764670', 'https://openalex.org/W2115924763', 'https://openalex.org/W2142889507', 'https://openalex.org/W2147717514', 'https://openalex.org/W2152925220', 'https://openalex.org/W2159902206', 'https://openalex.org/W2160061993', 'https://openalex.org/W2161353674', 'https://openalex.org/W2165599843', 'https://openalex.org/W2166354010', 'https://openalex.org/W2171278750', 'https://openalex.org/W2171343266', 'https://openalex.org/W2397770138', 'https://openalex.org/W2950770596', 'https://openalex.org/W2953083079', 'https://openalex.org/W2962684168']","Many tasks in NLP and IR require efficient document similarity computations. Beyond their common application to exploratory data analysis, latent variable topic models have been used to represent text in a low-dimensional space, independent of vocabulary, where documents may be compared. This paper focuses on the task of searching a large multilingual collection for pairs of documents that are translations of each other. We present (1) efficient, online inference for representing documents in several languages in a common topic space and (2) fast approximations for finding near neighbors in the probability simplex. Empirical evaluations show that these methods are as accurate as—and significantly faster than— Gibbs sampling and brute-force all-pairs search. 1",1.0
SKG_MT_317,https://openalex.org/W2250508490,2014,15,"['https://openalex.org/W2966661', 'https://openalex.org/W66838807', 'https://openalex.org/W109159489', 'https://openalex.org/W222053410', 'https://openalex.org/W1423339008', 'https://openalex.org/W1753482797', 'https://openalex.org/W1848260265', 'https://openalex.org/W1916559533', 'https://openalex.org/W1966822601', 'https://openalex.org/W1973923101', 'https://openalex.org/W2100495367', 'https://openalex.org/W2104327354', 'https://openalex.org/W2116064496', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2136922672', 'https://openalex.org/W2147768505', 'https://openalex.org/W2150378737', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2154591050', 'https://openalex.org/W2168013545', 'https://openalex.org/W2180952760', 'https://openalex.org/W2181347294', 'https://openalex.org/W2184045248', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250714477', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251690405', 'https://openalex.org/W2404007507', 'https://openalex.org/W2595715041', 'https://openalex.org/W2602938505', 'https://openalex.org/W2913932916', 'https://openalex.org/W3211848854', 'https://openalex.org/W4241645538']","In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep autoencoder (DAE) paradigm for phrase-based translation model.Using the unsupervised pre-trained deep belief net (DBN) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features.Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning.On two Chinese-English tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45(IWSLT) and 0.82/1.52(NIST) BLEU points over the unsupervised DBN features and the baseline features, respectively.",1.0
SKG_MT_318,https://openalex.org/W3120734549,2020,8,"['https://openalex.org/W2251766657', 'https://openalex.org/W2293344577', 'https://openalex.org/W2509282593', 'https://openalex.org/W2593341061', 'https://openalex.org/W2807230608', 'https://openalex.org/W2888070626', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963360627', 'https://openalex.org/W2963407669', 'https://openalex.org/W2964078338', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964192290', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970074184', 'https://openalex.org/W2970084653', 'https://openalex.org/W3028630450', 'https://openalex.org/W3034982693', 'https://openalex.org/W3098507616']","Simultaneous translation involves translating a sentence before the speaker’s utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.",1.0
SKG_MT_319,https://openalex.org/W2156562229,2014,1,"['https://openalex.org/W1631260214', 'https://openalex.org/W1970689298', 'https://openalex.org/W2000546550', 'https://openalex.org/W2016856586', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095650036', 'https://openalex.org/W2097606805', 'https://openalex.org/W2103357334', 'https://openalex.org/W2105245376', 'https://openalex.org/W2117278770', 'https://openalex.org/W2117728313', 'https://openalex.org/W2117827367', 'https://openalex.org/W2124807415', 'https://openalex.org/W2143719855', 'https://openalex.org/W2144879357', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153204578', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159755860', 'https://openalex.org/W2250375075', 'https://openalex.org/W2251098065', 'https://openalex.org/W2293139442', 'https://openalex.org/W2294935421', 'https://openalex.org/W2401082558', 'https://openalex.org/W2542068976', 'https://openalex.org/W2950186769', 'https://openalex.org/W3209717902', 'https://openalex.org/W4285719527']","This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score.",1.0
SKG_MT_320,https://openalex.org/W2250296445,2015,4,"['https://openalex.org/W45109703', 'https://openalex.org/W91928571', 'https://openalex.org/W144133692', 'https://openalex.org/W179314280', 'https://openalex.org/W232191560', 'https://openalex.org/W950939656', 'https://openalex.org/W1570963478', 'https://openalex.org/W1899181831', 'https://openalex.org/W1979711143', 'https://openalex.org/W1986345088', 'https://openalex.org/W2008652694', 'https://openalex.org/W2012974849', 'https://openalex.org/W2019076594', 'https://openalex.org/W2023286866', 'https://openalex.org/W2025772227', 'https://openalex.org/W2053323879', 'https://openalex.org/W2061162698', 'https://openalex.org/W2077723394', 'https://openalex.org/W2093197119', 'https://openalex.org/W2102486516', 'https://openalex.org/W2109339818', 'https://openalex.org/W2111142112', 'https://openalex.org/W2115328410', 'https://openalex.org/W2120459453', 'https://openalex.org/W2123825474', 'https://openalex.org/W2124807415', 'https://openalex.org/W2128528573', 'https://openalex.org/W2134130436', 'https://openalex.org/W2134800885', 'https://openalex.org/W2140343992', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146540262', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147366477', 'https://openalex.org/W2148708890', 'https://openalex.org/W2149327368', 'https://openalex.org/W2155607551', 'https://openalex.org/W2158460069', 'https://openalex.org/W2158614781', 'https://openalex.org/W2159136976', 'https://openalex.org/W2160697141', 'https://openalex.org/W2162152253', 'https://openalex.org/W2166765763', 'https://openalex.org/W2250872883', 'https://openalex.org/W2251171258', 'https://openalex.org/W2251673953', 'https://openalex.org/W2251796964', 'https://openalex.org/W2252102852', 'https://openalex.org/W2252136820', 'https://openalex.org/W2312609093', 'https://openalex.org/W2319588593', 'https://openalex.org/W2595715041', 'https://openalex.org/W2953266551', 'https://openalex.org/W2989661724', 'https://openalex.org/W3170731508', 'https://openalex.org/W3202911835', 'https://openalex.org/W4243548576', 'https://openalex.org/W4292022450']","We present a theoretical analysis of online parameter tuning in statistical machine translation (SMT) from a coactive learning view.This perspective allows us to give regret and generalization bounds for latent perceptron algorithms that are common in SMT, but fall outside of the standard convex optimization scenario.Coactive learning also introduces the concept of weak feedback, which we apply in a proofof-concept experiment to SMT, showing that learning from feedback that consists of slight improvements over predictions leads to convergence in regret and translation error rate.This suggests that coactive learning might be a viable framework for interactive machine translation.Furthermore, we find that surrogate translations replacing references that are unreachable in the decoder search space can be interpreted as weak feedback and lead to convergence in learning, if they admit an underlying linear model.",1.0
SKG_MT_321,https://openalex.org/W2737114849,2017,27,"['https://openalex.org/W121023703', 'https://openalex.org/W745775011', 'https://openalex.org/W1536323281', 'https://openalex.org/W1902237438', 'https://openalex.org/W1970381522', 'https://openalex.org/W2019313057', 'https://openalex.org/W2056860548', 'https://openalex.org/W2098584016', 'https://openalex.org/W2118756286', 'https://openalex.org/W2121110499', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126714083', 'https://openalex.org/W2131988669', 'https://openalex.org/W2149764047', 'https://openalex.org/W2149941633', 'https://openalex.org/W2159035200', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250296445', 'https://openalex.org/W2250969425', 'https://openalex.org/W2251530174', 'https://openalex.org/W2251955814', 'https://openalex.org/W2260677151', 'https://openalex.org/W2278024189', 'https://openalex.org/W2322584079', 'https://openalex.org/W2329556022', 'https://openalex.org/W2410983263', 'https://openalex.org/W2411447566', 'https://openalex.org/W2461231802', 'https://openalex.org/W2487501366', 'https://openalex.org/W2513592723', 'https://openalex.org/W2539350388', 'https://openalex.org/W2553303224', 'https://openalex.org/W2609616693', 'https://openalex.org/W2905927205', 'https://openalex.org/W2952840881', 'https://openalex.org/W2952926545', 'https://openalex.org/W2963248296', 'https://openalex.org/W2964043796', 'https://openalex.org/W2964121744', 'https://openalex.org/W3041866211']","Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.",1.0
SKG_MT_322,https://openalex.org/W2984930179,2019,25,"['https://openalex.org/W1686810756', 'https://openalex.org/W1902237438', 'https://openalex.org/W2093790824', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153508793', 'https://openalex.org/W2513263213', 'https://openalex.org/W2516756687', 'https://openalex.org/W2581101319', 'https://openalex.org/W2593341061', 'https://openalex.org/W2809361596', 'https://openalex.org/W2810953419', 'https://openalex.org/W2946471547', 'https://openalex.org/W2956020289', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963212250', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970885024', 'https://openalex.org/W2977587102', 'https://openalex.org/W3105531629']","With the widespread use of Machine Trans-lation (MT) techniques, attempt to minimizecommunication gap among people from di-verse linguistic backgrounds. We have par-ticipated in Workshop on Asian Transla-tion 2019 (WAT2019) multi-modal translationtask. There are three types of submissiontrack namely, multi-modal translation, Hindi-only image captioning and text-only transla-tion for English to Hindi translation. The mainchallenge is to provide a precise MT output.The multi-modal concept incorporates textualand visual features in the translation task. Inthis work, multi-modal translation track re-lies on pre-trained convolutional neural net-works (CNN) with Visual Geometry Grouphaving 19 layered (VGG19) to extract imagefeatures and attention-based Neural MachineTranslation (NMT) system for translation.The merge-model of recurrent neural network(RNN) and CNN is used for the Hindi-onlyimage captioning. The text-only translationtrack is based on the transformer model of theNMT system. The official results evaluated atWAT2019 translation task, which shows thatour multi-modal NMT system achieved Bilin-gual Evaluation Understudy (BLEU) score20.37, Rank-based Intuitive Bilingual Eval-uation Score (RIBES) 0.642838, Adequacy-Fluency Metrics (AMFM) score 0.668260 forchallenge test data and BLEU score 40.55,RIBES 0.760080, AMFM score 0.770860 forevaluation test data in English to Hindi multi-modal translation respectively.",1.0
SKG_MT_327,https://openalex.org/W2462184968,2016,7,"['https://openalex.org/W1539361123', 'https://openalex.org/W2115081467', 'https://openalex.org/W2141799614', 'https://openalex.org/W2178149891', 'https://openalex.org/W2180554602', 'https://openalex.org/W2251994258', 'https://openalex.org/W2261755308', 'https://openalex.org/W2462305634', 'https://openalex.org/W6814641000']","We use referential translation machines (RTMs) for predicting the semantic similarity of text in both STS Core and Cross-lingual STS.RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource.RTMs become 14th out of 26 submissions in Cross-lingual STS.We also present rankings of various prediction tasks using the performance of RTM in terms of MRAER, a normalized relative absolute error metric.",0.9957805907172996
SKG_MT_328,https://openalex.org/W2986769178,2019,1,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2127863960', 'https://openalex.org/W2467834614', 'https://openalex.org/W2744813330', 'https://openalex.org/W2787384361', 'https://openalex.org/W2885950361', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2977587102', 'https://openalex.org/W4385245566']","This paper describes NHK and NHK Engineering System (NHK-ES)’s submission to the newswire translation tasks of WAT 2019 in both directions of Japanese→English and English→Japanese. In addition to the JIJI Corpus that was officially provided by the task organizer, we developed a corpus of 0.22M sentence pairs by manually, translating Japanese news sentences into English content- equivalently. The content-equivalent corpus was effective for improving translation quality, and our systems achieved the best human evaluation scores in the newswire translation tasks at WAT 2019.",1.0
SKG_MT_329,https://openalex.org/W3099665409,2020,18,"['https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2613904329', 'https://openalex.org/W2767206889', 'https://openalex.org/W2890501761', 'https://openalex.org/W2892213699', 'https://openalex.org/W2896457183', 'https://openalex.org/W2920538220', 'https://openalex.org/W2946375144', 'https://openalex.org/W2949644922', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962915948', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970832665', 'https://openalex.org/W2971167892', 'https://openalex.org/W2981648103', 'https://openalex.org/W2987188351', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990389671', 'https://openalex.org/W2995999067', 'https://openalex.org/W3034425996', 'https://openalex.org/W4289298591', 'https://openalex.org/W4385245566']","In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad et al.,2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. Our code will be released to the public.",1.0
SKG_MT_330,https://openalex.org/W3118643801,2020,1,"['https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2134800885', 'https://openalex.org/W2169200297', 'https://openalex.org/W2226734577', 'https://openalex.org/W2889326796', 'https://openalex.org/W2933138175', 'https://openalex.org/W2944815030', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2970311224', 'https://openalex.org/W2970694516']","This paper describes our submission to the WMT20 news translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. We then fine-tune the model with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, we generate final results with an ensemble model and re-rank them with averaged models and language models. Through these methods, we achieve +5.42 BLEU score compare to the baseline model.",1.0
SKG_MT_331,https://openalex.org/W2106565589,2014,27,"['https://openalex.org/W1480633635', 'https://openalex.org/W1880262756', 'https://openalex.org/W1897587124', 'https://openalex.org/W1974578407', 'https://openalex.org/W1995560154', 'https://openalex.org/W2004447574', 'https://openalex.org/W2078861931', 'https://openalex.org/W2096175520', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101454539', 'https://openalex.org/W2115161902', 'https://openalex.org/W2116229791', 'https://openalex.org/W2118536060', 'https://openalex.org/W2132001515', 'https://openalex.org/W2135399286', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148368708', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158266063', 'https://openalex.org/W2158388102', 'https://openalex.org/W2162429782', 'https://openalex.org/W2171566979', 'https://openalex.org/W2950186769']","The sense in which a word is used determines the translation of the word. In this paper, we propose a sense-based translation model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. Our method is significantly different from previous word sense disambiguation reformulatedfor machine translation in that the latter neglects word senses in nature. We test the effectiveness of the proposed sensebased translation model on a large-scale Chinese-to-English translation task. Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation.",1.0
SKG_MT_332,https://openalex.org/W577284227,2013,17,"['https://openalex.org/W93515796', 'https://openalex.org/W108437174', 'https://openalex.org/W158381997', 'https://openalex.org/W165935821', 'https://openalex.org/W350356983', 'https://openalex.org/W1551202288', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106751371', 'https://openalex.org/W2115896319', 'https://openalex.org/W2116492146', 'https://openalex.org/W2121338597', 'https://openalex.org/W2126033735', 'https://openalex.org/W2137268927', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149327368', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2168966090', 'https://openalex.org/W2437005631']",We present a new variant of the Syntax-Augmented Machine Translation (SAMT) formalism with a category-coarsening algorithm originally developed for tree-to-tree grammars. We induce bilingual labels into the,1.0
SKG_MT_333,https://openalex.org/W2741852514,2017,11,"['https://openalex.org/W1551202288', 'https://openalex.org/W1593799327', 'https://openalex.org/W1709189958', 'https://openalex.org/W1932612488', 'https://openalex.org/W2002089154', 'https://openalex.org/W2037894654', 'https://openalex.org/W2130630493', 'https://openalex.org/W2133956246', 'https://openalex.org/W2166210475', 'https://openalex.org/W2185441650', 'https://openalex.org/W2250234533', 'https://openalex.org/W2251067382', 'https://openalex.org/W2251240975', 'https://openalex.org/W2252123671', 'https://openalex.org/W2437005631', 'https://openalex.org/W2466736553', 'https://openalex.org/W2517139211', 'https://openalex.org/W4302339081']","Johannes Gontrum, Jonas Groschwitz, Alexander Koller, Christoph Teichmann. Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics. 2017.",0.9900990099009901
SKG_MT_334,https://openalex.org/W2989276524,2019,59,"['https://openalex.org/W1566289585', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144600658', 'https://openalex.org/W2257408573', 'https://openalex.org/W2525778437', 'https://openalex.org/W2555428947', 'https://openalex.org/W2561274697', 'https://openalex.org/W2567571499', 'https://openalex.org/W2581863816', 'https://openalex.org/W2889326796', 'https://openalex.org/W2905927205', 'https://openalex.org/W2914120296', 'https://openalex.org/W2923014074', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964067969', 'https://openalex.org/W2964308564', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","In this paper, a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is applied to Transformer-based neural machine translation (NMT). In contrast to monolingual tasks, the number of unlearned model parameters in an NMT decoder is as huge as the number of learned parameters in the BERT model. To train all the models appropriately, we employ two-stage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same model without pre-training. Additionally, we confirmed that NMT with the BERT encoder is more effective in low-resource settings.",1.0
SKG_MT_337,https://openalex.org/W2152423400,2013,18,"['https://openalex.org/W22168010', 'https://openalex.org/W1606076566', 'https://openalex.org/W1631260214', 'https://openalex.org/W1697844855', 'https://openalex.org/W1934041838', 'https://openalex.org/W1969974515', 'https://openalex.org/W1989292916', 'https://openalex.org/W1994303046', 'https://openalex.org/W1995560154', 'https://openalex.org/W1997420744', 'https://openalex.org/W2044804339', 'https://openalex.org/W2095755718', 'https://openalex.org/W2096384808', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101207453', 'https://openalex.org/W2105891181', 'https://openalex.org/W2115848042', 'https://openalex.org/W2121338597', 'https://openalex.org/W2124807415', 'https://openalex.org/W2137561787', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2154988249', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2165132531', 'https://openalex.org/W2186598481', 'https://openalex.org/W2403214004', 'https://openalex.org/W2437005631', 'https://openalex.org/W3166680299', 'https://openalex.org/W3172758989']","We introduce a lexicalized reordering model for hierarchical phrase-based machine translation. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by Tillmann (2004). While this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems (Koehn et al., 2007), it is however commonly not employed in hierarchical decoders. We describe how phrase orientation probabilities can be extracted from wordaligned training data for use with hierarchical phrase inventories, and show how orientations can be scored in hierarchical decoding. The model is empirically evaluated on the NIST Chinese→English translation task. We achieve a significant improvement of +1.2 %BLEU over a typical hierarchical baseline setup and an improvement of +0.7 %BLEU over a syntax-augmented hierarchical setup. On a French→German translation task, we obtain a gain of up to +0.4 %BLEU. 1",1.0
SKG_MT_338,https://openalex.org/W2738371943,2017,36,"['https://openalex.org/W1489483588', 'https://openalex.org/W1598796236', 'https://openalex.org/W1810943226', 'https://openalex.org/W1972950354', 'https://openalex.org/W1980287119', 'https://openalex.org/W2049872937', 'https://openalex.org/W2064675550', 'https://openalex.org/W2069317438', 'https://openalex.org/W2096175520', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112514265', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2154124206', 'https://openalex.org/W2222235228', 'https://openalex.org/W2353655624', 'https://openalex.org/W2418388682', 'https://openalex.org/W2519643131', 'https://openalex.org/W2557436004', 'https://openalex.org/W2577255746', 'https://openalex.org/W2962701888', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963699608', 'https://openalex.org/W2964205912', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204130541', 'https://openalex.org/W4293541669', 'https://openalex.org/W4298067517']","We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We reformulate decoding, a discrete optimization problem, into a continuous problem, such that optimization can make use of efficient gradient-based techniques. Our powerful decoding framework allows for more accurate decoding for standard neural machine translation models, as well as enabling decoding in intractable models such as intersection of several different NMT models. Our empirical results show that our decoding framework is effective, and can leads to substantial improvements in translations, especially in situations where greedy search and beam search are not feasible. Finally, we show how the technique is highly competitive with, and complementary to, reranking.",1.0
SKG_MT_339,https://openalex.org/W2797297135,2018,21,"['https://openalex.org/W132913264', 'https://openalex.org/W2137095888', 'https://openalex.org/W2153653739', 'https://openalex.org/W2413436069', 'https://openalex.org/W2525778437', 'https://openalex.org/W2949888546', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963877622', 'https://openalex.org/W2964029788']","The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithms remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.",1.0
SKG_MT_343,https://openalex.org/W3074221917,2020,1,"['https://openalex.org/W630532510', 'https://openalex.org/W1625582487', 'https://openalex.org/W2130942839', 'https://openalex.org/W2481265265', 'https://openalex.org/W2795467683', 'https://openalex.org/W2806482312', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890007195', 'https://openalex.org/W2899771611', 'https://openalex.org/W2922709902', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970331417', 'https://openalex.org/W3104526121']","Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as diaeresis, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our models have a competitive performance to state-of-the-art models while being trained on modest hardware (a single 8GB gaming GPU for nine days). Our data, models and code are available at https://github.com/unicamp-dl/Lite-T5-Translation.",1.0
SKG_MT_346,https://openalex.org/W2130988241,2012,56,"['https://openalex.org/W111475876', 'https://openalex.org/W222053410', 'https://openalex.org/W1631260214', 'https://openalex.org/W1995560154', 'https://openalex.org/W2096175520', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103017773', 'https://openalex.org/W2114067158', 'https://openalex.org/W2115557995', 'https://openalex.org/W2118536060', 'https://openalex.org/W2126242092', 'https://openalex.org/W2139621418', 'https://openalex.org/W2144995019', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158431714', 'https://openalex.org/W2252171041', 'https://openalex.org/W2407799926', 'https://openalex.org/W2434901392', 'https://openalex.org/W2437005631', 'https://openalex.org/W2586888933']","Predicate-argument structure contains rich semantic information of which statistical machine translation hasn’t taken full advantage. In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1",1.0
SKG_MT_347,https://openalex.org/W2970325945,2019,29,"['https://openalex.org/W20811212', 'https://openalex.org/W1647671624', 'https://openalex.org/W1970026646', 'https://openalex.org/W2078861931', 'https://openalex.org/W2149327368', 'https://openalex.org/W2154124206', 'https://openalex.org/W2250342921', 'https://openalex.org/W2250875036', 'https://openalex.org/W2260677151', 'https://openalex.org/W2294699749', 'https://openalex.org/W2323907496', 'https://openalex.org/W2508316494', 'https://openalex.org/W2512848817', 'https://openalex.org/W2902215807', 'https://openalex.org/W2903188467', 'https://openalex.org/W2903376039', 'https://openalex.org/W2915756181']","Over the years a number of machine translation metrics have been developed in order to evaluate the accuracy and quality of machine-generated translations. Metrics such as BLEU and TER have been used for decades. However, with the rapid progress of machine translation systems, the need for better metrics is growing. This paper proposes an extension of the edit distance, which achieves better human correlation, whilst remaining fast, flexible and easy to understand.",0.9914529914529915
SKG_MT_351,https://openalex.org/W2758310181,2017,75,"['https://openalex.org/W1522301498', 'https://openalex.org/W1533861849', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1915251500', 'https://openalex.org/W2064675550', 'https://openalex.org/W2070150502', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2125930537', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2157331557', 'https://openalex.org/W2186615578', 'https://openalex.org/W2257408573', 'https://openalex.org/W2512924740', 'https://openalex.org/W2561274697', 'https://openalex.org/W2760656271', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963504252', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564']","The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language modeling and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.",1.0
SKG_MT_353,https://openalex.org/W2944869267,2019,35,"['https://openalex.org/W1821462560', 'https://openalex.org/W2095705004', 'https://openalex.org/W2130942839', 'https://openalex.org/W2157331557', 'https://openalex.org/W2294370754', 'https://openalex.org/W2550821151', 'https://openalex.org/W2551396370', 'https://openalex.org/W2592691248', 'https://openalex.org/W2601324753', 'https://openalex.org/W2659803504', 'https://openalex.org/W2740433069', 'https://openalex.org/W2764163036', 'https://openalex.org/W2768282280', 'https://openalex.org/W2886342729', 'https://openalex.org/W2886776719', 'https://openalex.org/W2888302696', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962863357', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963631431', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963959597', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964160102', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964220233', 'https://openalex.org/W2964222566', 'https://openalex.org/W2964265128', 'https://openalex.org/W3204406378']","Hao-Ran Wei, Shujian Huang, Ran Wang, Xin-yu Dai, Jiajun Chen. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_354,https://openalex.org/W3037255450,2020,5,"['https://openalex.org/W1966253631', 'https://openalex.org/W2127863960', 'https://openalex.org/W2141440284', 'https://openalex.org/W2150903784', 'https://openalex.org/W2158874082', 'https://openalex.org/W2165199647', 'https://openalex.org/W2169200297', 'https://openalex.org/W2250289928', 'https://openalex.org/W2756566411', 'https://openalex.org/W2757592053', 'https://openalex.org/W2805394970', 'https://openalex.org/W2902918014', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963281280', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963494156', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963841178', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964343359', 'https://openalex.org/W2970152385', 'https://openalex.org/W2970279348', 'https://openalex.org/W2982924472', 'https://openalex.org/W3001350707', 'https://openalex.org/W3037465386', 'https://openalex.org/W3186200218', 'https://openalex.org/W4289291951', 'https://openalex.org/W4385245566']","This paper describes the University of Edinburgh's neural machine translation systems submitted to the IWSLT 2020 open domain Japanese↔Chinese translation task. On top of commonplace techniques like tokenisation and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU.",0.9946524064171123
SKG_MT_357,https://openalex.org/W3104318719,2020,12,"['https://openalex.org/W1522301498', 'https://openalex.org/W1584556462', 'https://openalex.org/W1599016936', 'https://openalex.org/W1608789752', 'https://openalex.org/W1964952992', 'https://openalex.org/W2055086094', 'https://openalex.org/W2073302931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2138437366', 'https://openalex.org/W2525778437', 'https://openalex.org/W2561529111', 'https://openalex.org/W2573933512', 'https://openalex.org/W2624031847', 'https://openalex.org/W2767019613', 'https://openalex.org/W2792026792', 'https://openalex.org/W2794365787', 'https://openalex.org/W2805206884', 'https://openalex.org/W2885588803', 'https://openalex.org/W2897037347', 'https://openalex.org/W2898662126', 'https://openalex.org/W2898695519', 'https://openalex.org/W2928107702', 'https://openalex.org/W2946609015', 'https://openalex.org/W2947337775', 'https://openalex.org/W2952570576', 'https://openalex.org/W2962781380', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963101081', 'https://openalex.org/W2963159690', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963995027', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970062726', 'https://openalex.org/W2970161131', 'https://openalex.org/W2970169050', 'https://openalex.org/W2970780738', 'https://openalex.org/W2970782003', 'https://openalex.org/W2980941473', 'https://openalex.org/W2989033444', 'https://openalex.org/W2991223644', 'https://openalex.org/W2995643077', 'https://openalex.org/W2996908057', 'https://openalex.org/W2998617917', 'https://openalex.org/W3099230264', 'https://openalex.org/W3105773826', 'https://openalex.org/W4288262459', 'https://openalex.org/W4385245566']","Does neural machine translation yield translations that are congenial with common sense? In this paper, we present a test suite to evaluate the commonsense reasoning capability of neural machine translation. The test suite consists of three test sets, covering lexical and contextless/contextual syntactic ambiguity that requires commonsense knowledge to resolve. We manually create 1,200 triples, each of which contain a source sentence and two contrastive translations, involving 7 different common sense types. Language models pretrained on large-scale corpora, such as BERT, GPT-2, achieve a commonsense reasoning accuracy of lower than 72% on target translations of this test suite. We conduct extensive experiments on the test suite to evaluate commonsense reasoning in neural machine translation and investigate factors that have impact on this capability. Our experiments and analyses demonstrate that neural machine translation performs poorly on commonsense reasoning of the three ambiguity types in terms of both reasoning accuracy ( 6 60.1%) and reasoning consistency (6 31%). We will release our test suite as a machine translation commonsense reasoning testbed to promote future work in this direction.",0.9940828402366864
SKG_MT_359,https://openalex.org/W2135673254,2014,4,"['https://openalex.org/W71896076', 'https://openalex.org/W2098507980', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103288873', 'https://openalex.org/W2108325777', 'https://openalex.org/W2117111101', 'https://openalex.org/W2120481102', 'https://openalex.org/W2123301721', 'https://openalex.org/W2149327368', 'https://openalex.org/W2162669657', 'https://openalex.org/W2164984707', 'https://openalex.org/W3044695148']",This paper presents the use of consensus among Machine Translation (MT) systems for the WMT14 Quality Estimation shared task.Consensus is explored here by comparing the MT system output against several alternative machine translations using standard evaluation metrics.Figures extracted from such metrics are used as features to complement baseline prediction models.The hypothesis is that knowing whether the translation of interest is similar or dissimilar to translations from multiple different MT systems can provide useful information regarding the quality of such a translation.,1.0
SKG_MT_360,https://openalex.org/W2185490708,2016,16,[],"Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.",1.0
SKG_MT_361,https://openalex.org/W3105038888,2020,58,"['https://openalex.org/W1522301498', 'https://openalex.org/W2025768430', 'https://openalex.org/W2091432990', 'https://openalex.org/W2117130368', 'https://openalex.org/W2119727789', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2151834591', 'https://openalex.org/W2183341477', 'https://openalex.org/W2251743902', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561274697', 'https://openalex.org/W2594047108', 'https://openalex.org/W2613904329', 'https://openalex.org/W2624871570', 'https://openalex.org/W2752630748', 'https://openalex.org/W2798931235', 'https://openalex.org/W2799920282', 'https://openalex.org/W2807535859', 'https://openalex.org/W2891555348', 'https://openalex.org/W2896457183', 'https://openalex.org/W2906987001', 'https://openalex.org/W2913340405', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2945383715', 'https://openalex.org/W2949911645', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962717763', 'https://openalex.org/W2962807144', 'https://openalex.org/W2962982474', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970311224', 'https://openalex.org/W2970925270', 'https://openalex.org/W2989539713', 'https://openalex.org/W2998653650', 'https://openalex.org/W3014635508', 'https://openalex.org/W3032816972', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035390927', 'https://openalex.org/W3107826490', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",1.0
SKG_MT_362,https://openalex.org/W2515541333,2016,2,"['https://openalex.org/W61355150', 'https://openalex.org/W651894412', 'https://openalex.org/W2047295649', 'https://openalex.org/W2056132907', 'https://openalex.org/W2080373976', 'https://openalex.org/W2101807845', 'https://openalex.org/W2107695330', 'https://openalex.org/W2129734311', 'https://openalex.org/W2147227066', 'https://openalex.org/W2149327368', 'https://openalex.org/W2149791639', 'https://openalex.org/W2155011048', 'https://openalex.org/W2160001241', 'https://openalex.org/W2169200297', 'https://openalex.org/W2169939314', 'https://openalex.org/W2251406574', 'https://openalex.org/W2251804196', 'https://openalex.org/W2915258134', 'https://openalex.org/W2989631226', 'https://openalex.org/W3089048834']","We address the problem of automatically cleaning a large-scale Translation Memory (TM) in a fully unsupervised fashion, i.e. without human-labelled data. We approach the task by: i) designing a set of features that capture the similarity between two text segments in different languages, ii) use them to induce reliable training labels for a subset of the translation units (TUs) contained in the TM, and iii) use the automatically labelled data to train an ensemble of binary classifiers. We apply our method to clean a test set composed of 1,000 TUs randomly extracted from the English-Italian version of MyMemory, the world’s largest public TM. Our results show competitive performance not only against a strong baseline that exploits machine translation, but also against a state-of-the-art method that relies on human-labelled data.",1.0
SKG_MT_363,https://openalex.org/W2760425631,2017,17,"['https://openalex.org/W6908809', 'https://openalex.org/W1686810756', 'https://openalex.org/W1861492603', 'https://openalex.org/W1902237438', 'https://openalex.org/W2117539524', 'https://openalex.org/W2185175083', 'https://openalex.org/W2194775991', 'https://openalex.org/W2247931231', 'https://openalex.org/W2282219577', 'https://openalex.org/W2293344577', 'https://openalex.org/W2302086703', 'https://openalex.org/W2345720230', 'https://openalex.org/W2417549359', 'https://openalex.org/W2509282593', 'https://openalex.org/W2509490957', 'https://openalex.org/W2512381898', 'https://openalex.org/W2513263213', 'https://openalex.org/W2516756687', 'https://openalex.org/W2595715041', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963344439', 'https://openalex.org/W2963778889', 'https://openalex.org/W2963909453', 'https://openalex.org/W2964018924', 'https://openalex.org/W3005285208']","This paper describes the University of Sheffield's submission to the WMT17 Multimodal Machine Translation shared task.We participated in Task 1 to develop an MT system to translate an image description from English to German and French, given its corresponding image.Our proposed systems are based on the state-of-the-art Neural Machine Translation approach.We investigate the effect of replacing the commonly-used image embeddings with an estimated posterior probability prediction for 1,000 object categories in the images.Recent work (Wu et al., 2016;You et al., 2016) exploits explicit, higher-level semantic representation of images for the tasks of image captioning and visual question answering.Instead of feeding",0.9942857142857143
SKG_MT_364,https://openalex.org/W2799708641,2018,15,['https://openalex.org/W2964029788'],"Despite the impressive quality improvements yielded by neural machine translation (NMT) systems, controlling their translation output to adhere to user-provided terminology constraints remains an open problem. We describe our approach to constrained neural decoding based on finite-state machines and multi-stack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints.",1.0
SKG_MT_365,https://openalex.org/W2467669267,2016,16,"['https://openalex.org/W68251067', 'https://openalex.org/W77450199', 'https://openalex.org/W84636783', 'https://openalex.org/W635530177', 'https://openalex.org/W1489909987', 'https://openalex.org/W1541981365', 'https://openalex.org/W1631260214', 'https://openalex.org/W2006969979', 'https://openalex.org/W2047295649', 'https://openalex.org/W2096507791', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105410942', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2129291393', 'https://openalex.org/W2131429577', 'https://openalex.org/W2134800885', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2161943765', 'https://openalex.org/W2163361328', 'https://openalex.org/W2171671120', 'https://openalex.org/W2172268343', 'https://openalex.org/W2184055828', 'https://openalex.org/W2250841445', 'https://openalex.org/W2251171258', 'https://openalex.org/W2402118343', 'https://openalex.org/W2437005631', 'https://openalex.org/W2595715041', 'https://openalex.org/W2797922696', 'https://openalex.org/W4213138647', 'https://openalex.org/W4241645538', 'https://openalex.org/W4293370400']","Akiva Miura, Graham Neubig, Michael Paul, Satoshi Nakamura. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",1.0
SKG_MT_366,https://openalex.org/W2886787567,2019,0,"['https://openalex.org/W22168010', 'https://openalex.org/W2098297786', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124725212', 'https://openalex.org/W2125616599', 'https://openalex.org/W2139621418', 'https://openalex.org/W2170527467', 'https://openalex.org/W2321916036', 'https://openalex.org/W2525778437', 'https://openalex.org/W2577255746', 'https://openalex.org/W2606308499', 'https://openalex.org/W2625092622', 'https://openalex.org/W2740839465', 'https://openalex.org/W2792296443', 'https://openalex.org/W2885950361', 'https://openalex.org/W2896060389', 'https://openalex.org/W2952566282', 'https://openalex.org/W2952688536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963109131', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963919854', 'https://openalex.org/W2964132420']","Neural Machine Translation (NMT) systems are known to degrade when confronted with noisy data, especially when the system is trained only on clean data. In this paper, we show that augmenting training data with sentences containing artificially-introduced grammatical errors can make the system more robust to such errors. In combination with an automatic grammar error correction system, we can recover 1.5 BLEU out of 2.4 BLEU lost due to grammatical errors. We also present a set of Spanish translations of the JFLEG grammar error correction corpus, which allows for testing NMT robustness to real grammatical errors.",1.0
SKG_MT_367,https://openalex.org/W2983221611,2019,6,"['https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2141440284', 'https://openalex.org/W2159755860', 'https://openalex.org/W2595715041', 'https://openalex.org/W2798931235', 'https://openalex.org/W2886288106', 'https://openalex.org/W2890007195', 'https://openalex.org/W2898785264', 'https://openalex.org/W2903193068', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964343359', 'https://openalex.org/W2971254483', 'https://openalex.org/W2977587102', 'https://openalex.org/W2977804551', 'https://openalex.org/W4385245566']","This paper presents the NICT’s supervised and unsupervised machine translation systems for the WAT2019 Myanmar-English and Khmer-English translation tasks. For all the translation directions, we built state-of-the-art supervised neural (NMT) and statistical (SMT) machine translation systems, using monolingual data cleaned and normalized. Our combination of NMT and SMT performed among the best systems for the four translation directions. We also investigated the feasibility of unsupervised machine translation for low-resource and distant language pairs and confirmed observations of previous work showing that unsupervised MT is still largely unable to deal with them.",1.0
SKG_MT_368,https://openalex.org/W2123318312,2011,62,"['https://openalex.org/W64777181', 'https://openalex.org/W222053410', 'https://openalex.org/W1506572002', 'https://openalex.org/W1522553155', 'https://openalex.org/W1586104099', 'https://openalex.org/W1635500704', 'https://openalex.org/W1957631488', 'https://openalex.org/W1977517503', 'https://openalex.org/W1987869189', 'https://openalex.org/W2061066582', 'https://openalex.org/W2101096097', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105410942', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131429577', 'https://openalex.org/W2136477195', 'https://openalex.org/W2138242477', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157963512', 'https://openalex.org/W2160571323', 'https://openalex.org/W2161943765']","We present an empirical study of instance selection techniques for machine translation. In an active learning setting, instance selection minimizes the human effort by identifying the most informative sentences for translation. In a transductive learning setting, selection of training instances relevant to the test set improves the final translation quality. After reviewing the state of the art in the field, we generalize the main ideas in a class of instance selection algorithms that use feature decay. Feature decay algorithms increase diversity of the training set by devaluing features that are already included. We show that the feature decay rate has a very strong effect on the final translation quality whereas the initial feature values, inclusion of higher order features, or sentence length normalizations do not. We evaluate the best instance selection methods using a standard Moses baseline using the whole 1.6 million sentence English-German section of the Europarl corpus. We show that selecting the best 3000 training sentences for a specific test sentence is sufficient to obtain a score within 1 BLEU of the baseline, using 5 % of the training data is sufficient to exceed the baseline, and a ∼ 2 BLEU improvement over the baseline is possible by optimally selected subset of the training data. In out-of-domain translation, we are able to reduce the training set size to about 7 % and achieve a similar performance with the baseline. 1",1.0
SKG_MT_370,https://openalex.org/W2511277317,2016,46,"['https://openalex.org/W338621447', 'https://openalex.org/W570627446', 'https://openalex.org/W1514971736', 'https://openalex.org/W1517947178', 'https://openalex.org/W1606538849', 'https://openalex.org/W1716250762', 'https://openalex.org/W1902237438', 'https://openalex.org/W2006969979', 'https://openalex.org/W2028492537', 'https://openalex.org/W2046384065', 'https://openalex.org/W2080373976', 'https://openalex.org/W2100271871', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101181502', 'https://openalex.org/W2105891181', 'https://openalex.org/W2112544287', 'https://openalex.org/W2114912785', 'https://openalex.org/W2119168550', 'https://openalex.org/W2120354757', 'https://openalex.org/W2125595887', 'https://openalex.org/W2130942839', 'https://openalex.org/W2136173435', 'https://openalex.org/W2141440284', 'https://openalex.org/W2146502635', 'https://openalex.org/W2148354767', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2161227214', 'https://openalex.org/W2177801600', 'https://openalex.org/W2183501487', 'https://openalex.org/W2250653840', 'https://openalex.org/W2251689843', 'https://openalex.org/W2251798853', 'https://openalex.org/W2252065493', 'https://openalex.org/W2257408573', 'https://openalex.org/W2962907349', 'https://openalex.org/W3146885639', 'https://openalex.org/W4241645538']","We apply phrase-based and neural models to a core task in interactive machine translation: suggesting how to complete a partial translation.For the phrase-based system, we demonstrate improvements in suggestion quality using novel objective functions, learning techniques, and inference algorithms tailored to this task.Our contributions include new tunable metrics, an improved beam search strategy, an n-best extraction method that increases suggestion diversity, and a tuning procedure for a hierarchical joint model of alignment and translation.The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5% to 41.2% in a large-scale English-German experiment.Our recurrent neural translation system increases accuracy yet further to 53.0%, but inference is two orders of magnitude slower.Manual error analysis shows the strengths and weaknesses of both approaches.",1.0
SKG_MT_371,https://openalex.org/W2250761393,2013,44,"['https://openalex.org/W34088237', 'https://openalex.org/W41621595', 'https://openalex.org/W44695385', 'https://openalex.org/W255975419', 'https://openalex.org/W1565570978', 'https://openalex.org/W1573514622', 'https://openalex.org/W1631260214', 'https://openalex.org/W1710422233', 'https://openalex.org/W1901714926', 'https://openalex.org/W2015933299', 'https://openalex.org/W2067815623', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115030595', 'https://openalex.org/W2116343275', 'https://openalex.org/W2136353104', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2140676672', 'https://openalex.org/W2144600658', 'https://openalex.org/W2159882563', 'https://openalex.org/W2162429782', 'https://openalex.org/W2166545452', 'https://openalex.org/W2176802839', 'https://openalex.org/W2258247613', 'https://openalex.org/W2399346130', 'https://openalex.org/W2437005631', 'https://openalex.org/W3203909556']","Lexical chains provide a representation of the lexical cohesion structure of a text. In this paper, we propose two lexical chain based cohesion models to incorporate lexical cohesion into document-level statistical machine translation: 1) a count cohesion model that rewards a hypothesis whenever a chain word occurs in the hypothesis, 2) and a probability cohesion model that further takes chain word translation probabilities into account. We compute lexical chains for each source document to be translated and generate target lexical chains based on the computed source chains via maximum entropy classifiers. We then use the generated target chains to provide constraints for word selection in document-level machine translation through the two proposed lexical chain based cohesion models. We verify the effectiveness of the two models using a hierarchical phrase-based translation system. Experiments on large-scale training data show that they can substantially improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices.",1.0
SKG_MT_372,https://openalex.org/W2252072825,2013,4,"['https://openalex.org/W17659133', 'https://openalex.org/W24102868', 'https://openalex.org/W39836547', 'https://openalex.org/W91928571', 'https://openalex.org/W147273232', 'https://openalex.org/W222053410', 'https://openalex.org/W232191560', 'https://openalex.org/W1528441900', 'https://openalex.org/W1631260214', 'https://openalex.org/W1763771263', 'https://openalex.org/W1961521608', 'https://openalex.org/W2020431608', 'https://openalex.org/W2096557251', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101656755', 'https://openalex.org/W2102845951', 'https://openalex.org/W2115526192', 'https://openalex.org/W2118439278', 'https://openalex.org/W2119005844', 'https://openalex.org/W2119314391', 'https://openalex.org/W2123126659', 'https://openalex.org/W2124119193', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125993116', 'https://openalex.org/W2126270798', 'https://openalex.org/W2126610017', 'https://openalex.org/W2127686544', 'https://openalex.org/W2130195263', 'https://openalex.org/W2133330531', 'https://openalex.org/W2133444727', 'https://openalex.org/W2134729743', 'https://openalex.org/W2138165032', 'https://openalex.org/W2140343992', 'https://openalex.org/W2142112143', 'https://openalex.org/W2142623206', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152213375', 'https://openalex.org/W2153653739', 'https://openalex.org/W2161792612', 'https://openalex.org/W2437005631']","Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.",1.0
SKG_MT_373,https://openalex.org/W2120459453,2012,16,"['https://openalex.org/W91928571', 'https://openalex.org/W144133692', 'https://openalex.org/W232191560', 'https://openalex.org/W1496929357', 'https://openalex.org/W1714704734', 'https://openalex.org/W2098949613', 'https://openalex.org/W2099119623', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111142112', 'https://openalex.org/W2115328410', 'https://openalex.org/W2116410915', 'https://openalex.org/W2134800885', 'https://openalex.org/W2137143056', 'https://openalex.org/W2138302120', 'https://openalex.org/W2140343992', 'https://openalex.org/W2142112143', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152382718', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158614781', 'https://openalex.org/W2159358338', 'https://openalex.org/W2159755860', 'https://openalex.org/W2160218441', 'https://openalex.org/W2160697141', 'https://openalex.org/W2180952760', 'https://openalex.org/W2429914308', 'https://openalex.org/W2950186769', 'https://openalex.org/W3202207277']","The introduction of large-margin based discriminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process. By removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features. However, these methods have not yet met with wide-spread adoption. This may be partly due to the perceived complexity of implementation, and partly due to the lack of standard methodology for applying these methods to MT. This papers aims to shed light on large-margin learning for MT, explicitly presenting the simple passive-aggressive algorithm which underlies many previous approaches, with direct application to MT, and empirically comparing several widespread optimization strategies. 1",1.0
SKG_MT_374,https://openalex.org/W2890007195,2018,324,"['https://openalex.org/W2097333193', 'https://openalex.org/W2103042430', 'https://openalex.org/W2121745180', 'https://openalex.org/W2126725946', 'https://openalex.org/W2141440284', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153579005', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2250600644', 'https://openalex.org/W2250640401', 'https://openalex.org/W2293547632', 'https://openalex.org/W2578451281', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2760424551', 'https://openalex.org/W2788353357', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964266061', 'https://openalex.org/W4241645538', 'https://openalex.org/W4294170691', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Linear temporal logic (LTL) is commonly used in model checking tasks; moreover, it is well-suited for the formalization of technical requirements. However, the correct specification and interpretation of temporal logic formulas require a strong mathematical background and can hardly be done by domain experts, who, instead, tend to rely on a natural language description of the intended system behaviour. In such situations, a system that is able to automatically translate English sentences into LTL formulas, and vice versa, would be of great help. While the task of rendering an LTL formula into a more readable English sentence may be carried out in a relatively easy way by properly parsing the formula, the converse is still an open problem, due to the inherent difficulty of interpreting free, natural language texts. Although several partial solutions have been proposed in the past, the literature still lacks a critical assessment of the work done. We address such a shortcoming, presenting the current state of the art for what concerns the English-to-LTL translation problem, and outlining some possible research directions.",1.0
SKG_MT_375,https://openalex.org/W2110295509,2013,12,"['https://openalex.org/W34088237', 'https://openalex.org/W1631260214', 'https://openalex.org/W2015933299', 'https://openalex.org/W2038721957', 'https://openalex.org/W2101105183', 'https://openalex.org/W2136353104', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154591050', 'https://openalex.org/W2156985047', 'https://openalex.org/W2166545452', 'https://openalex.org/W2399346130', 'https://openalex.org/W2437005631', 'https://openalex.org/W3203909556']","In this paper, we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets. 1",1.0
SKG_MT_376,https://openalex.org/W2155965092,2013,5,"['https://openalex.org/W91928571', 'https://openalex.org/W232191560', 'https://openalex.org/W1496929357', 'https://openalex.org/W1966771059', 'https://openalex.org/W2012715465', 'https://openalex.org/W2097341304', 'https://openalex.org/W2100564780', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103452263', 'https://openalex.org/W2111142112', 'https://openalex.org/W2115328410', 'https://openalex.org/W2120459453', 'https://openalex.org/W2122537498', 'https://openalex.org/W2123825474', 'https://openalex.org/W2126610017', 'https://openalex.org/W2137143056', 'https://openalex.org/W2138302120', 'https://openalex.org/W2140343992', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149684865', 'https://openalex.org/W2152382718', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156909104', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158614781', 'https://openalex.org/W2159358338', 'https://openalex.org/W2159755860', 'https://openalex.org/W2160218441', 'https://openalex.org/W2160648405', 'https://openalex.org/W2160697141', 'https://openalex.org/W2162828817', 'https://openalex.org/W2170494078', 'https://openalex.org/W2180952760', 'https://openalex.org/W2429914308', 'https://openalex.org/W2467575451', 'https://openalex.org/W2579923771', 'https://openalex.org/W2950186769', 'https://openalex.org/W3202207277']","Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. However, these so-lutions are impractical in complex struc-tured prediction problems such as statis-tical machine translation. We present an online gradient-based algorithm for rela-tive margin maximization, which bounds the spread of the projected data while max-imizing the margin. We evaluate our op-timizer on Chinese-English and Arabic-English translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant im-provements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art opti-mizers with the large feature set. 1",1.0
SKG_MT_377,https://openalex.org/W2950617329,2018,18,"['https://openalex.org/W606759049', 'https://openalex.org/W1991110341', 'https://openalex.org/W2018869373', 'https://openalex.org/W2149327368', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251926178', 'https://openalex.org/W2252166243', 'https://openalex.org/W2507833193', 'https://openalex.org/W2571713365', 'https://openalex.org/W2758895583', 'https://openalex.org/W2760656271', 'https://openalex.org/W2794365787', 'https://openalex.org/W2905927205', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641561', 'https://openalex.org/W3202218022', 'https://openalex.org/W3204130541']",,0.9961389961389961
SKG_MT_378,https://openalex.org/W2250280404,2015,14,"['https://openalex.org/W164335349', 'https://openalex.org/W168564468', 'https://openalex.org/W241744740', 'https://openalex.org/W338742415', 'https://openalex.org/W644067837', 'https://openalex.org/W776270814', 'https://openalex.org/W1596986901', 'https://openalex.org/W1603508585', 'https://openalex.org/W1614298861', 'https://openalex.org/W1647671624', 'https://openalex.org/W1983578042', 'https://openalex.org/W2018869373', 'https://openalex.org/W2078861931', 'https://openalex.org/W2081580037', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102163214', 'https://openalex.org/W2104511424', 'https://openalex.org/W2120654173', 'https://openalex.org/W2123724244', 'https://openalex.org/W2126400076', 'https://openalex.org/W2133459682', 'https://openalex.org/W2147192413', 'https://openalex.org/W2149327368', 'https://openalex.org/W2156099181', 'https://openalex.org/W2162130683', 'https://openalex.org/W2163986298', 'https://openalex.org/W2164019165', 'https://openalex.org/W2247119764', 'https://openalex.org/W2250599277', 'https://openalex.org/W2250612440', 'https://openalex.org/W2250731608', 'https://openalex.org/W2250819234', 'https://openalex.org/W2250861254', 'https://openalex.org/W2251251208', 'https://openalex.org/W2251335084', 'https://openalex.org/W2251476375', 'https://openalex.org/W2251797829', 'https://openalex.org/W2309409233', 'https://openalex.org/W2524964298', 'https://openalex.org/W2588877501', 'https://openalex.org/W2905171513', 'https://openalex.org/W2950577311', 'https://openalex.org/W3113525842', 'https://openalex.org/W4252434862', 'https://openalex.org/W4386506836']",This paper describes USAAR's submission to the the metrics shared task of the Workshop on Statistical Machine Translation (WMT) in 2015.The goal of our submission is to take advantage of the semantic overlap between hypothesis and reference translation for predicting MT output adequacy using language independent document embeddings.The approach presented here is learning a Bayesian Ridge Regressor using document skip-gram embeddings in order to automatically evaluate Machine Translation (MT) output by predicting semantic adequacy scores.The evaluation of our submission -measured by the correlation with human judgements -shows promising results on system-level scores.,1.0
SKG_MT_380,https://openalex.org/W2902537726,2018,33,"['https://openalex.org/W203948990', 'https://openalex.org/W1901714926', 'https://openalex.org/W2067815623', 'https://openalex.org/W2101566153', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141895568', 'https://openalex.org/W2151996595', 'https://openalex.org/W2163038970', 'https://openalex.org/W2197913429', 'https://openalex.org/W2212703438', 'https://openalex.org/W2250471514', 'https://openalex.org/W2284050935', 'https://openalex.org/W2508661145', 'https://openalex.org/W2594229957', 'https://openalex.org/W2606531491', 'https://openalex.org/W2608029998', 'https://openalex.org/W2756836964', 'https://openalex.org/W2759173152', 'https://openalex.org/W2767019613', 'https://openalex.org/W2778814079', 'https://openalex.org/W2799051177', 'https://openalex.org/W2962769558', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963167649', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963685250', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964308564', 'https://openalex.org/W3082674894', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Cross-sentence context can provide valuable information in Machine Translation and is critical for translation of anaphoric pronouns and for providing consistent translations. In this paper, we devise simple oracle experiments targeting coreference and coherence. Oracles are an easy way to evaluate the effect of different discourse-level phenomena in NMT using BLEU and eliminate the necessity to manually define challenge sets for this purpose. We propose two context-aware NMT models and compare them against models working on a concatenation of consecutive sentences. Concatenation models perform better, but are computationally expensive. We show that NMT models taking advantage of context oracle signals can achieve considerable gains in BLEU, of up to 7.02 BLEU for coreference and 1.89 BLEU for coherence on subtitles translation. Access to strong signals allows us to make clear comparisons between context-aware models.",0.9943502824858758
SKG_MT_381,https://openalex.org/W2340762547,2016,40,"['https://openalex.org/W20450839', 'https://openalex.org/W31462306', 'https://openalex.org/W150892692', 'https://openalex.org/W204341599', 'https://openalex.org/W266752158', 'https://openalex.org/W1506034704', 'https://openalex.org/W1562809004', 'https://openalex.org/W1631260214', 'https://openalex.org/W2096204319', 'https://openalex.org/W2098987612', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108188641', 'https://openalex.org/W2111889471', 'https://openalex.org/W2124807415', 'https://openalex.org/W2124818604', 'https://openalex.org/W2133258739', 'https://openalex.org/W2142131972', 'https://openalex.org/W2146574666', 'https://openalex.org/W2151996595', 'https://openalex.org/W2153579005', 'https://openalex.org/W2155069789', 'https://openalex.org/W2155541797', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157512532', 'https://openalex.org/W2158100271', 'https://openalex.org/W2166293310', 'https://openalex.org/W2250464567', 'https://openalex.org/W2250555572', 'https://openalex.org/W2250597688', 'https://openalex.org/W2251883335', 'https://openalex.org/W2397490041', 'https://openalex.org/W2399456070', 'https://openalex.org/W2595715041', 'https://openalex.org/W4294170691']","Longyue Wang, Zhaopeng Tu, Xiaojun Zhang, Hang Li, Andy Way, Qun Liu. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",1.0
SKG_MT_382,https://openalex.org/W2741049976,2017,217,"['https://openalex.org/W6908809', 'https://openalex.org/W22168010', 'https://openalex.org/W88290828', 'https://openalex.org/W1568589650', 'https://openalex.org/W1753482797', 'https://openalex.org/W1814992895', 'https://openalex.org/W1902237438', 'https://openalex.org/W1986450680', 'https://openalex.org/W2051593977', 'https://openalex.org/W2056132907', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101234009', 'https://openalex.org/W2104671598', 'https://openalex.org/W2105051853', 'https://openalex.org/W2108325777', 'https://openalex.org/W2108701407', 'https://openalex.org/W2110481933', 'https://openalex.org/W2111666304', 'https://openalex.org/W2116792345', 'https://openalex.org/W2118021410', 'https://openalex.org/W2119727789', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126034021', 'https://openalex.org/W2130571329', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131113428', 'https://openalex.org/W2131726681', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134800885', 'https://openalex.org/W2140847984', 'https://openalex.org/W2143927888', 'https://openalex.org/W2145685230', 'https://openalex.org/W2146357154', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153702313', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165979181', 'https://openalex.org/W2176263492', 'https://openalex.org/W2250225488', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251743902', 'https://openalex.org/W2251780596', 'https://openalex.org/W2251882135', 'https://openalex.org/W2251939518', 'https://openalex.org/W2252001469', 'https://openalex.org/W2252272516', 'https://openalex.org/W2443536229', 'https://openalex.org/W2539768235', 'https://openalex.org/W2595715041', 'https://openalex.org/W2949697461', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963499246', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4295803813']","Recognizing and generating paraphrases is an important component in many natural language processing applications. A well-established technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by ""pivoting"" over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based purely on neural networks. Our model represents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, and generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches.",1.0
SKG_MT_383,https://openalex.org/W3101683892,2020,61,"['https://openalex.org/W1902237438', 'https://openalex.org/W1910131649', 'https://openalex.org/W2053154970', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2253795368', 'https://openalex.org/W2561274697', 'https://openalex.org/W2608029998', 'https://openalex.org/W2669742347', 'https://openalex.org/W2767019613', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2848493808', 'https://openalex.org/W2888442053', 'https://openalex.org/W2891534142', 'https://openalex.org/W2928667520', 'https://openalex.org/W2935811960', 'https://openalex.org/W2952446148', 'https://openalex.org/W2953052971', 'https://openalex.org/W2953190730', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962943802', 'https://openalex.org/W2962985882', 'https://openalex.org/W2963084599', 'https://openalex.org/W2963123301', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970529093', 'https://openalex.org/W2971248291', 'https://openalex.org/W2971278086', 'https://openalex.org/W2971347700', 'https://openalex.org/W2977772147', 'https://openalex.org/W2983108239', 'https://openalex.org/W3007795830', 'https://openalex.org/W3034351728', 'https://openalex.org/W3034700448', 'https://openalex.org/W3035169087', 'https://openalex.org/W3035520602', 'https://openalex.org/W3102507836', 'https://openalex.org/W4385245566']","Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.",1.0
SKG_MT_384,https://openalex.org/W2963232029,2015,20,"['https://openalex.org/W1553589067', 'https://openalex.org/W1631260214', 'https://openalex.org/W1934041838', 'https://openalex.org/W2062227835', 'https://openalex.org/W2072128103', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117130368', 'https://openalex.org/W2124807415', 'https://openalex.org/W2140133598', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150378737', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154581043', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2166465139', 'https://openalex.org/W2170738476', 'https://openalex.org/W2250445771', 'https://openalex.org/W2251395256', 'https://openalex.org/W2251682575', 'https://openalex.org/W2251855842', 'https://openalex.org/W2296073425', 'https://openalex.org/W2950183041', 'https://openalex.org/W2962997665']","Baotian Hu, Zhaopeng Tu, Zhengdong Lu, Hang Li, Qingcai Chen. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2015.",1.0
SKG_MT_385,https://openalex.org/W3120758068,2020,9,"['https://openalex.org/W600197013', 'https://openalex.org/W1489525520', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2134800885', 'https://openalex.org/W2162349723', 'https://openalex.org/W2250288169', 'https://openalex.org/W2250537251', 'https://openalex.org/W2250929115', 'https://openalex.org/W2251362149', 'https://openalex.org/W2558155472', 'https://openalex.org/W2604593109', 'https://openalex.org/W2740764803', 'https://openalex.org/W2743653651', 'https://openalex.org/W2798794824', 'https://openalex.org/W2890771450', 'https://openalex.org/W2954298948', 'https://openalex.org/W2962899377', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963732644', 'https://openalex.org/W2964085347', 'https://openalex.org/W2964199361']","Building on recent advances in semantic parsing and text simplification, we investigate the use of semantic splitting of the source sentence as preprocessing for machine translation. We experiment with a Transformer model and evaluate using large-scale crowd-sourcing experiments. Results show a significant increase in fluency on long sentences on an English-to- French setting with a training corpus of 5M sentence pairs, while retaining comparable adequacy. We also perform a manual analysis which explores the tradeoff between adequacy and fluency in the case where all sentence lengths are considered.",1.0
SKG_MT_386,https://openalex.org/W3096966601,2020,59,"['https://openalex.org/W2887920589', 'https://openalex.org/W2905933322', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963211188', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964303773', 'https://openalex.org/W2970316683', 'https://openalex.org/W2970925270', 'https://openalex.org/W2971134989', 'https://openalex.org/W2996844526', 'https://openalex.org/W3017208877', 'https://openalex.org/W3023622314', 'https://openalex.org/W3035464238', 'https://openalex.org/W3099793224', 'https://openalex.org/W3102767808', 'https://openalex.org/W3105421296']","We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.",1.0
SKG_MT_389,https://openalex.org/W2154006786,2012,26,"['https://openalex.org/W23077562', 'https://openalex.org/W222053410', 'https://openalex.org/W434719956', 'https://openalex.org/W1484882168', 'https://openalex.org/W1487527466', 'https://openalex.org/W1498904163', 'https://openalex.org/W1631260214', 'https://openalex.org/W1973923101', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104595351', 'https://openalex.org/W2136353104', 'https://openalex.org/W2139939204', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2165688682', 'https://openalex.org/W2463396630', 'https://openalex.org/W3204820328']","EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference",1.0
SKG_MT_390,https://openalex.org/W2168103496,2010,20,"['https://openalex.org/W116785612', 'https://openalex.org/W1974815860', 'https://openalex.org/W2028176545', 'https://openalex.org/W2060833990', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115081467', 'https://openalex.org/W2124807415', 'https://openalex.org/W2129241332', 'https://openalex.org/W2169279899', 'https://openalex.org/W3198653489']",We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.,1.0
SKG_MT_392,https://openalex.org/W3037186962,2020,17,"['https://openalex.org/W1522301498', 'https://openalex.org/W2024200390', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101234009', 'https://openalex.org/W2127141656', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140539590', 'https://openalex.org/W2143195354', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152790380', 'https://openalex.org/W2194775991', 'https://openalex.org/W2250539671', 'https://openalex.org/W2329068866', 'https://openalex.org/W2512608784', 'https://openalex.org/W2538358357', 'https://openalex.org/W2581377246', 'https://openalex.org/W2752172973', 'https://openalex.org/W2799800213', 'https://openalex.org/W2799923439', 'https://openalex.org/W2902786662', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2951456627', 'https://openalex.org/W2956159074', 'https://openalex.org/W2962728618', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964308564', 'https://openalex.org/W2980145022', 'https://openalex.org/W3007142233', 'https://openalex.org/W3008191852', 'https://openalex.org/W3015449694', 'https://openalex.org/W3016092023', 'https://openalex.org/W3037465386', 'https://openalex.org/W3037698816', 'https://openalex.org/W4288290348', 'https://openalex.org/W4297798436', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","AppTek and RWTH Aachen University team together to participate in the offline and simultaneous speech translation tracks of IWSLT 2020. For the offline task, we create both cascaded and end-to-end speech translation systems, paying attention to careful data selection and weighting. In the cascaded approach, we combine high-quality hybrid automatic speech recognition (ASR) with the Transformer-based neural machine translation (NMT). Our end-to-end direct speech translation systems benefit from pretraining of adapted encoder and decoder components, as well as synthetic data and fine-tuning and thus are able to compete with cascaded systems in terms of MT quality. For simultaneous translation, we utilize a novel architecture that makes dynamic decisions, learned from parallel data, to determine when to continue feeding on input or generate output words. Experiments with speech and text input show that even at low latency this architecture leads to superior translation results.",0.9947089947089947
SKG_MT_393,https://openalex.org/W2888555799,2018,16,"['https://openalex.org/W1522301498', 'https://openalex.org/W1845277745', 'https://openalex.org/W2130062883', 'https://openalex.org/W2133564696', 'https://openalex.org/W2138243089', 'https://openalex.org/W2418388682', 'https://openalex.org/W2436522418', 'https://openalex.org/W2512924740', 'https://openalex.org/W2622263826', 'https://openalex.org/W2740433069', 'https://openalex.org/W2774000609', 'https://openalex.org/W2797328513', 'https://openalex.org/W2798687821', 'https://openalex.org/W2888206291', 'https://openalex.org/W2949692215', 'https://openalex.org/W2951781666', 'https://openalex.org/W2953384591', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963679688', 'https://openalex.org/W2963803379', 'https://openalex.org/W2963959597', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964343359', 'https://openalex.org/W2994779554', 'https://openalex.org/W3101036738', 'https://openalex.org/W3136506209', 'https://openalex.org/W4293569541', 'https://openalex.org/W4385245566']",In order to extract the best possible performance from asynchronous stochastic gradient descent one must increase the mini-batch size and scale the learning rate accordingly. In order to achieve further speedup we introduce a technique that delays gradient updates effectively increasing the mini-batch size. Unfortunately with the increase of mini-batch size we worsen the stale gradient problem in asynchronous stochastic gradient descent (SGD) which makes the model convergence poor. We introduce local optimizers which mitigate the stale gradient problem and together with fine tuning our momentum we are able to train a shallow machine translation system 27% faster than an optimized baseline with negligible penalty in BLEU.,1.0
SKG_MT_394,https://openalex.org/W2126129064,2014,9,"['https://openalex.org/W30193166', 'https://openalex.org/W172892979', 'https://openalex.org/W1510052640', 'https://openalex.org/W1517947178', 'https://openalex.org/W1552023264', 'https://openalex.org/W2083451366', 'https://openalex.org/W2100281225', 'https://openalex.org/W2112900913', 'https://openalex.org/W2113104171', 'https://openalex.org/W2114343221', 'https://openalex.org/W2115289978', 'https://openalex.org/W2117139364', 'https://openalex.org/W2118119027', 'https://openalex.org/W2122609803', 'https://openalex.org/W2140982523', 'https://openalex.org/W2143008661', 'https://openalex.org/W2145882814', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150378737', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155521054', 'https://openalex.org/W2168959697', 'https://openalex.org/W2538827255', 'https://openalex.org/W4241645538']","In this paper we explicitly consider sen-tence skeleton information for Machine Translation (MT). The basic idea is that we translate the key elements of the input sentence using a skeleton translation mod-el, and then cover the remain segments us-ing a full translation model. We apply our approach to a state-of-the-art phrase-based system and demonstrate very promising BLEU improvements and TER reductions on the NIST Chinese-English MT evalua-tion data. 1",1.0
SKG_MT_397,https://openalex.org/W2594047108,2017,143,"['https://openalex.org/W179875071', 'https://openalex.org/W222053410', 'https://openalex.org/W581956982', 'https://openalex.org/W1821462560', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107598941', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2134036914', 'https://openalex.org/W2153508793', 'https://openalex.org/W2157331557', 'https://openalex.org/W2158899491', 'https://openalex.org/W2175585630', 'https://openalex.org/W2402302915', 'https://openalex.org/W2407166119', 'https://openalex.org/W2463033603', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2554915555', 'https://openalex.org/W2563574619', 'https://openalex.org/W2586559132', 'https://openalex.org/W2595715041', 'https://openalex.org/W2612340881', 'https://openalex.org/W2751262944', 'https://openalex.org/W2914746235', 'https://openalex.org/W2949952998', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962819663', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963572611', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W3099884890']","There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.",1.0
SKG_MT_400,https://openalex.org/W2737711067,2017,16,"['https://openalex.org/W2194775991', 'https://openalex.org/W2282641050', 'https://openalex.org/W2473934411', 'https://openalex.org/W2525778437', 'https://openalex.org/W2626778328', 'https://openalex.org/W2949643137', 'https://openalex.org/W2949888546', 'https://openalex.org/W2964308564', 'https://openalex.org/W3037932933']","It has been shown that increasing model depth improves the quality of neural machine translation. However, different architectural variants to increase model depth have been proposed, and so far, there has been no thorough comparative study. In this work, we describe and evaluate several existing approaches to introduce depth in neural machine translation. Additionally, we explore novel architectural variants, including deep transition RNNs, and we vary how attention is used in the deep decoder. We introduce a novel ""BiDeep"" RNN architecture that combines deep transition RNNs and stacked RNNs. Our evaluation is carried out on the English to German WMT news translation dataset, using a single-GPU machine for both training and inference. We find that several of our proposed architectures improve upon existing approaches in terms of speed and translation quality. We obtain best improvements with a BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU over a strong shallow baseline. We release our code for ease of adoption.",1.0
SKG_MT_401,https://openalex.org/W2294774419,2015,414,"['https://openalex.org/W342285082', 'https://openalex.org/W1562955078', 'https://openalex.org/W1608322251', 'https://openalex.org/W1614298861', 'https://openalex.org/W2053921957', 'https://openalex.org/W2118090838', 'https://openalex.org/W2126725946', 'https://openalex.org/W2141599568', 'https://openalex.org/W2144945507', 'https://openalex.org/W2153579005', 'https://openalex.org/W2158139315', 'https://openalex.org/W2158899491', 'https://openalex.org/W2164019165', 'https://openalex.org/W2251033195', 'https://openalex.org/W2950194079', 'https://openalex.org/W2952230511', 'https://openalex.org/W2964222437', 'https://openalex.org/W2998704965', 'https://openalex.org/W4285719527', 'https://openalex.org/W4294170691']","Word embedding has been found to be highly powerful to translate words from one language to another by a simple linear transform.However, we found some inconsistence among the objective functions of the embedding and the transform learning, as well as the distance measurement.This paper proposes a solution which normalizes the word vectors on a hypersphere and constrains the linear transform as an orthogonal transform.The experimental results confirmed that the proposed solution can offer better performance on a word similarity task and an English-to-Spanish word translation task.",1.0
SKG_MT_402,https://openalex.org/W2227523508,2016,15,"['https://openalex.org/W22168010', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2006969979', 'https://openalex.org/W2038698865', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116679574', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2134800885', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2169724380', 'https://openalex.org/W2172140247', 'https://openalex.org/W2295894802', 'https://openalex.org/W2401082558', 'https://openalex.org/W2950178297', 'https://openalex.org/W2952360713', 'https://openalex.org/W2962907349', 'https://openalex.org/W2964308564']","Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.",1.0
SKG_MT_403,https://openalex.org/W2804193771,2018,4,"['https://openalex.org/W11511616', 'https://openalex.org/W808583520', 'https://openalex.org/W2089629691', 'https://openalex.org/W2121457870', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153999629', 'https://openalex.org/W2250281547', 'https://openalex.org/W2419292002', 'https://openalex.org/W2471147443', 'https://openalex.org/W2740401134', 'https://openalex.org/W2750588180', 'https://openalex.org/W2949888546', 'https://openalex.org/W2962784628', 'https://openalex.org/W3204130541', 'https://openalex.org/W3204406378']","We address the problem of simultaneous translation by modifying the Neural MT decoder to operate with dynamically built encoder and attention. We propose a tunable agent which decides the best segmentation strategy for a user-defined BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to Neural MT training to better match the incremental decoding framework.",1.0
SKG_MT_405,https://openalex.org/W2251771687,2015,9,"['https://openalex.org/W36903255', 'https://openalex.org/W932413789', 'https://openalex.org/W1575384945', 'https://openalex.org/W1631260214', 'https://openalex.org/W1934041838', 'https://openalex.org/W2026149468', 'https://openalex.org/W2058695628', 'https://openalex.org/W2091981305', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097927681', 'https://openalex.org/W2100714283', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101207453', 'https://openalex.org/W2110168585', 'https://openalex.org/W2117278770', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136016850', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158195707', 'https://openalex.org/W2171928131', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250610505', 'https://openalex.org/W2250831586', 'https://openalex.org/W2251071050', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251222643', 'https://openalex.org/W2251682575', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538']","This work explores the application of recurrent neural network (RNN) language and translation models during phrasebased decoding.Due to their use of unbounded context, the decoder integration of RNNs is more challenging compared to the integration of feedforward neural models.In this paper, we apply approximations and use caching to enable RNN decoder integration, while requiring reasonable memory and time resources.We analyze the effect of caching on translation quality and speed, and use it to integrate RNN language and translation models into a phrase-based decoder.To the best of our knowledge, no previous work has discussed the integration of RNN translation models into phrase-based decoding.We also show that a special RNN can be integrated efficiently without the need for approximations.We compare decoding using RNNs to rescoring n-best lists on two tasks: IWSLT 2013 German→English, and BOLT Arabic→English.We demonstrate that the performance of decoding with RNNs is at least as good as using them in rescoring.",1.0
SKG_MT_407,https://openalex.org/W3120749277,2020,19,"['https://openalex.org/W203948990', 'https://openalex.org/W1631260214', 'https://openalex.org/W2118972857', 'https://openalex.org/W2123318312', 'https://openalex.org/W2159755860', 'https://openalex.org/W2250969425', 'https://openalex.org/W2903490366', 'https://openalex.org/W2951604644', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962882341', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963716535', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963959336', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971257618', 'https://openalex.org/W2971287409', 'https://openalex.org/W2987998469', 'https://openalex.org/W2988975212', 'https://openalex.org/W3107826490', 'https://openalex.org/W3118680649', 'https://openalex.org/W3120168417', 'https://openalex.org/W3121049435']","This paper describes the Tencent AI Lab’s submission of the WMT 2020 shared task on chat translation in English-German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, non-autoregressive (NAT) and pretrained models. We integrate a number of advanced techniques into our systems, including data selection, back/forward translation, larger batch learning, model ensemble, finetuning as well as system combination. Specifically, we proposed a hybrid data selection method to select high-quality and in-domain sentences from out-of-domain data. To better capture the source contexts, we exploit to augment NAT models with evolved cross-attention. Furthermore, we explore to transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German-to-English primary system is ranked the second in terms of BLEU scores.",1.0
SKG_MT_408,https://openalex.org/W2741986820,2017,40,"['https://openalex.org/W6908809', 'https://openalex.org/W23077562', 'https://openalex.org/W1411230545', 'https://openalex.org/W1551202288', 'https://openalex.org/W1631260214', 'https://openalex.org/W1753482797', 'https://openalex.org/W1815076433', 'https://openalex.org/W1902237438', 'https://openalex.org/W1969974515', 'https://openalex.org/W2006969979', 'https://openalex.org/W2095705004', 'https://openalex.org/W2097997328', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116316001', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2239731672', 'https://openalex.org/W2534200568', 'https://openalex.org/W2550821151', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950527759', 'https://openalex.org/W2952479981', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962954913', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4254408171', 'https://openalex.org/W4303633609']","This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.",1.0
SKG_MT_409,https://openalex.org/W2113538861,2011,14,"['https://openalex.org/W4629839', 'https://openalex.org/W1631260214', 'https://openalex.org/W1647671624', 'https://openalex.org/W2018616927', 'https://openalex.org/W2057900969', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104812589', 'https://openalex.org/W2124769992', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132446289', 'https://openalex.org/W2133512280', 'https://openalex.org/W2134800885', 'https://openalex.org/W2137387514', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153903004', 'https://openalex.org/W2156279557', 'https://openalex.org/W2156985047', 'https://openalex.org/W2163942301', 'https://openalex.org/W2401082558', 'https://openalex.org/W2404928265', 'https://openalex.org/W3167119764']",We report results on translation of SMS messages from Haitian Creole to English. We show improvements by applying spell checking techniques to unknown words and creating a lattice with the best known spelling equivalents. We also used a small cleaned corpus to train a cleaning model that we applied to the noisy corpora. 1,1.0
SKG_MT_410,https://openalex.org/W2111560130,2012,4,"['https://openalex.org/W121569490', 'https://openalex.org/W154469489', 'https://openalex.org/W222053410', 'https://openalex.org/W1412698887', 'https://openalex.org/W1522263329', 'https://openalex.org/W1525066359', 'https://openalex.org/W1581810776', 'https://openalex.org/W1631260214', 'https://openalex.org/W2016856586', 'https://openalex.org/W2096451823', 'https://openalex.org/W2097333193', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105245376', 'https://openalex.org/W2113106066', 'https://openalex.org/W2113788796', 'https://openalex.org/W2115526192', 'https://openalex.org/W2119168550', 'https://openalex.org/W2121495627', 'https://openalex.org/W2123635983', 'https://openalex.org/W2138974820', 'https://openalex.org/W2139175732', 'https://openalex.org/W2145473818', 'https://openalex.org/W2148675933', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157435188', 'https://openalex.org/W2158195707', 'https://openalex.org/W2161792612', 'https://openalex.org/W2164766438', 'https://openalex.org/W2168103042', 'https://openalex.org/W2618735189', 'https://openalex.org/W3104884270']","In statistical machine translation, word lattices are used to represent the ambiguities in the preprocessing of the source sentence, such as word segmentation for Chinese or morphological analysis for German. Several approaches have been proposed to define the probability of different paths through the lattice with external tools like word segmenters, or by applying indicator features. We introduce a novel lattice design, which explicitly distinguishes between different preprocessing alternatives for the source sentence. It allows us to make use of specific features for each preprocessing type and to lexicalize the choice of lattice path directly in the phrase translation model. We argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously. On the newscommentary portion of the German→English WMT 2011 task we can show moderate improvements of up to 0.6 % BLEU over a stateof-the-art baseline system. 1",1.0
SKG_MT_411,https://openalex.org/W3210914950,2019,12,"['https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964110616', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564']","In this work we analyze and compare the behavior of the Transformer architecture when using different positional encoding methods. While absolute and relative positional encoding perform equally strong overall, we show that relative positional encoding is vastly superior (4.4% to 11.9% BLEU) when translating a sentence that is longer than any observed training sentence. We further propose and analyze variations of relative positional encoding and observe that the number of trainable parameters can be reduced without a performance loss, by using fixed encoding vectors or by removing some of the positional encoding vectors.",1.0
SKG_MT_412,https://openalex.org/W2889336589,2018,15,"['https://openalex.org/W21006490', 'https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124033848', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153579005', 'https://openalex.org/W2157331557', 'https://openalex.org/W2172099713', 'https://openalex.org/W2251416368', 'https://openalex.org/W2531207078', 'https://openalex.org/W2549416390', 'https://openalex.org/W2551773530', 'https://openalex.org/W2594229957', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2733128608', 'https://openalex.org/W2760656271', 'https://openalex.org/W2807910559', 'https://openalex.org/W2903193068', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962964385', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964094426', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964205912', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3104069527', 'https://openalex.org/W4293541669', 'https://openalex.org/W4294170691', 'https://openalex.org/W4385245566']","Tying the weights of the target word embeddings with the target word classifiers of neural machine translation models leads to faster training and often to better translation quality. Given the success of this parameter sharing, we investigate other forms of sharing in between no sharing and hard equality of parameters. In particular, we propose a structure-aware output layer which captures the semantic structure of the output space of words within a joint input-output embedding. The model is a generalized form of weight tying which shares parameters but allows learning a more flexible relationship with input word embeddings and allows the effective capacity of the output layer to be controlled. In addition, the model shares weights across output classifiers and translation contexts which allows it to better leverage prior knowledge about them. Our evaluation on English-to-Finnish and English-to-German datasets shows the effectiveness of the method against strong encoder-decoder baselines trained with or without weight tying.",0.994413407821229
SKG_MT_414,https://openalex.org/W3016137827,2019,24,[],"This paper describes KIT’s submission to the IWSLT 2019 Speech Translation task on two sub-tasks corresponding to two different datasets. We investigate different end-to-end architectures for the speech recognition module, including our new transformer-based architectures. Overall, our modules in the pipe-line are based on the transformer architecture which has recently achieved great results in various fields. In our systems, using transformer is also advantageous compared to traditional hybrid systems in term of simplicity while still having competent results.",1.0
SKG_MT_415,https://openalex.org/W2096426991,2012,3,"['https://openalex.org/W1510052640', 'https://openalex.org/W1535015163', 'https://openalex.org/W1934041838', 'https://openalex.org/W1979495315', 'https://openalex.org/W2000566875', 'https://openalex.org/W2037894654', 'https://openalex.org/W2103156738', 'https://openalex.org/W2106822721', 'https://openalex.org/W2110020502', 'https://openalex.org/W2110104386', 'https://openalex.org/W2112900913', 'https://openalex.org/W2115289978', 'https://openalex.org/W2126345236', 'https://openalex.org/W2139621418', 'https://openalex.org/W2146113428', 'https://openalex.org/W2146574666', 'https://openalex.org/W2146628926', 'https://openalex.org/W2158195707', 'https://openalex.org/W2162539697', 'https://openalex.org/W2168596725']","Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank. 1",0.9882352941176471
SKG_MT_418,https://openalex.org/W3119152910,2020,4,"['https://openalex.org/W2561995736', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963341956', 'https://openalex.org/W2970791445', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971120958', 'https://openalex.org/W2986143601', 'https://openalex.org/W2996403597', 'https://openalex.org/W3029985558', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035459196', 'https://openalex.org/W3120179416']","We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with contextual embed- dings extracted from different layers of two different pretrained models, multilingual BERT and XLM-RoBERTa. We also experiment with learning bilingual mappings that trans- form the vector subspace of the source language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2’s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the tar- get embedding space using a cross-lingual lin- ear projection (CLP) matrix learnt from a small development set.",1.0
SKG_MT_419,https://openalex.org/W2952367383,2019,5,"['https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2130942839', 'https://openalex.org/W2229833550', 'https://openalex.org/W2540404261', 'https://openalex.org/W2546938941', 'https://openalex.org/W2555745756', 'https://openalex.org/W2607106700', 'https://openalex.org/W2610245951', 'https://openalex.org/W2626778328', 'https://openalex.org/W2740743644', 'https://openalex.org/W2770394828', 'https://openalex.org/W2786790428', 'https://openalex.org/W2798926775', 'https://openalex.org/W2899771611', 'https://openalex.org/W2921280978', 'https://openalex.org/W2933350699', 'https://openalex.org/W2950135462', 'https://openalex.org/W2950359962', 'https://openalex.org/W2951184134', 'https://openalex.org/W2952614664', 'https://openalex.org/W2962717763', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963266340', 'https://openalex.org/W2964308564', 'https://openalex.org/W3005389111']","An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots.",1.0
SKG_MT_420,https://openalex.org/W3119935461,2020,1,"['https://openalex.org/W179314280', 'https://openalex.org/W1534477342', 'https://openalex.org/W1902237438', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2161792612', 'https://openalex.org/W2778814079', 'https://openalex.org/W2889326796', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963626623', 'https://openalex.org/W2964308564']","This review depicts our submission to the WMT20 shared news translation task. WMT is the conference to assess the level of machine translation capabilities of organizations in the word. We participated in one language pair and two language directions, from Russian to English and from English to Russian. We used official training data, 102 million parallel corpora and 10 million monolingual corpora. Our baseline systems are Transformer models trained with the Sockeye sequence modeling toolkit, supplemented by bi-text data filtering schemes, back-translations, reordering and other related processing methods. The BLEU value of our translation result from Russian to English is 35.7, ranking 5th, while from English to Russian is 39.8, ranking 2th.",1.0
SKG_MT_424,https://openalex.org/W2970780025,2019,50,"['https://openalex.org/W91588747', 'https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141467730', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250918071', 'https://openalex.org/W2291126447', 'https://openalex.org/W2398104528', 'https://openalex.org/W2408504891', 'https://openalex.org/W2413052110', 'https://openalex.org/W2419539795', 'https://openalex.org/W2512848817', 'https://openalex.org/W2537749207', 'https://openalex.org/W2580610384', 'https://openalex.org/W2581377246', 'https://openalex.org/W2799923439', 'https://openalex.org/W2887920589', 'https://openalex.org/W2891534142', 'https://openalex.org/W2938040855', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963626623', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W4297798436']","In this work, we customized a neural machine translation system for translation of subtitles in the domain of entertainment. The neural translation model was adapted to the subtitling content and style and extended by a simple, yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns. The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a recurrent neural network learned from human segmentation decisions. This model is combined with subtitle length and duration constraints established in the subtitling industry. We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a documentary and a sitcom). It showed a notable productivity increase of up to 37% as compared to translating from scratch and significant reductions in human translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model.",1.0
SKG_MT_425,https://openalex.org/W2891745105,2018,37,"['https://openalex.org/W152055444', 'https://openalex.org/W342285082', 'https://openalex.org/W1542713999', 'https://openalex.org/W1828724394', 'https://openalex.org/W2031648200', 'https://openalex.org/W2046211142', 'https://openalex.org/W2099471712', 'https://openalex.org/W2126725946', 'https://openalex.org/W2136346830', 'https://openalex.org/W2140406733', 'https://openalex.org/W2222512263', 'https://openalex.org/W2250646737', 'https://openalex.org/W2251033195', 'https://openalex.org/W2252213301', 'https://openalex.org/W2294672308', 'https://openalex.org/W2336840621', 'https://openalex.org/W2397034747', 'https://openalex.org/W2561995736', 'https://openalex.org/W2739748921', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2760424551', 'https://openalex.org/W2796093898', 'https://openalex.org/W2952037945', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963560084', 'https://openalex.org/W2963602293', 'https://openalex.org/W4232627933', 'https://openalex.org/W4292692470', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4320013936']","Word translation, or bilingual dictionary induction, is an important capability that impacts many multilingual language processing tasks. Recent research has shown that word translation can be achieved in an unsupervised manner, without parallel seed dictionaries or aligned corpora. However, state of the art methods unsupervised bilingual dictionary induction are based on generative adversarial models, and as such suffer from their well known problems of instability and hyper-parameter sensitivity. We present a statistical dependency-based approach to bilingual dictionary induction that is unsupervised – no seed dictionary or parallel corpora required; and introduces no adversary – therefore being much easier to train. Our method performs comparably to adversarial alternatives and outperforms prior non-adversarial methods.",1.0
SKG_MT_427,https://openalex.org/W2739754707,2017,1,"['https://openalex.org/W6908809', 'https://openalex.org/W1522301498', 'https://openalex.org/W1905522558', 'https://openalex.org/W1985759455', 'https://openalex.org/W2004001705', 'https://openalex.org/W2073384958', 'https://openalex.org/W2095705004', 'https://openalex.org/W2103149536', 'https://openalex.org/W2108325777', 'https://openalex.org/W2118434577', 'https://openalex.org/W2148708890', 'https://openalex.org/W2155607551', 'https://openalex.org/W2161877964', 'https://openalex.org/W2168405694', 'https://openalex.org/W2168438882', 'https://openalex.org/W2278024189', 'https://openalex.org/W2312609093', 'https://openalex.org/W2411447566', 'https://openalex.org/W2418388682', 'https://openalex.org/W2487501366', 'https://openalex.org/W2512924740', 'https://openalex.org/W2513592723', 'https://openalex.org/W2594229957', 'https://openalex.org/W2609616693', 'https://openalex.org/W2758053148', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963248296', 'https://openalex.org/W2964033619']","We introduce and describe the results of a novel shared task on bandit learning for machine translation. The task was organized jointly by Amazon and Heidelberg University for the first time at the Second Conference on Machine Translation (WMT 2017). The goal of the task is to encourage research on learning machine translation from weak user feedback instead of human references or post-edits. On each of a sequence of rounds, a machine translation system is required to propose a translation for an input, and receives a real-valued estimate of the quality of the proposed translation for learning. This paper describes the shared task's learning and evaluation setup, using services hosted on Amazon Web Services (AWS), the data and evaluation metrics, and the results of various machine translation architectures and learning protocols.",1.0
SKG_MT_428,https://openalex.org/W2250612533,2015,6,"['https://openalex.org/W222053410', 'https://openalex.org/W1593045043', 'https://openalex.org/W1631260214', 'https://openalex.org/W1753482797', 'https://openalex.org/W1901714926', 'https://openalex.org/W1995560154', 'https://openalex.org/W2085337304', 'https://openalex.org/W2097752345', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106565589', 'https://openalex.org/W2115161902', 'https://openalex.org/W2119182603', 'https://openalex.org/W2123126659', 'https://openalex.org/W2129734311', 'https://openalex.org/W2136016850', 'https://openalex.org/W2136647665', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2140133598', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154581043', 'https://openalex.org/W2154591050', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165132531', 'https://openalex.org/W2166905217', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250610505', 'https://openalex.org/W2250646384', 'https://openalex.org/W2251682575', 'https://openalex.org/W2399346130', 'https://openalex.org/W2437005631', 'https://openalex.org/W3203909556']","Lexical selection is of great importance to statistical machine translation. In this paper, we propose a graph-based framework for collective lexical selection. The framework is established on a translation graph that captures not only local associations between source-side content words and their target translations but also targetside global dependencies in terms of relatedness among target items. We also introduce a random walk style algorithm to collectively identify translations of sourceside content words that are strongly related in translation graph. We validate the effectiveness of our lexical selection framework on Chinese-English translation. Experiment results with large-scale training data show that our approach significantly improves lexical selection.",1.0
SKG_MT_429,https://openalex.org/W2251269336,2015,3,"['https://openalex.org/W115367774', 'https://openalex.org/W222053410', 'https://openalex.org/W932413789', 'https://openalex.org/W1995560154', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136016850', 'https://openalex.org/W2146574666', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250610505', 'https://openalex.org/W2251222643', 'https://openalex.org/W2251682575', 'https://openalex.org/W2252264945', 'https://openalex.org/W2293111166', 'https://openalex.org/W2295800168', 'https://openalex.org/W2595715041', 'https://openalex.org/W2964308564']","The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies.Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost.In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE.We compare the BNNJM and NNJM trained by NCE on various translation tasks.",1.0
SKG_MT_430,https://openalex.org/W3034489696,2020,12,"['https://openalex.org/W658020064', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115259925', 'https://openalex.org/W2123301721', 'https://openalex.org/W2250484373', 'https://openalex.org/W2294699749', 'https://openalex.org/W2759332014', 'https://openalex.org/W2786464815', 'https://openalex.org/W2891177506', 'https://openalex.org/W2891555348', 'https://openalex.org/W2903376039', 'https://openalex.org/W2914120296', 'https://openalex.org/W2915756181', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963644595', 'https://openalex.org/W2963918774', 'https://openalex.org/W2966292672']","We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance.",1.0
SKG_MT_431,https://openalex.org/W2507833193,2016,25,"['https://openalex.org/W606759049', 'https://openalex.org/W2021618504', 'https://openalex.org/W2037789405', 'https://openalex.org/W2053154970', 'https://openalex.org/W2080430588', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115259925', 'https://openalex.org/W2133459682', 'https://openalex.org/W2141766660', 'https://openalex.org/W2147192413', 'https://openalex.org/W2250830017', 'https://openalex.org/W2251311344', 'https://openalex.org/W2251994258', 'https://openalex.org/W2252166243', 'https://openalex.org/W2257408573', 'https://openalex.org/W3037252522']","Comunicació presentada a la 54th Annual Meeting of the Association for Computational Linguistics, celebrada del 7 al 10 d'agost de 2016 a Berlín, Alemanya.",1.0
SKG_MT_432,https://openalex.org/W2804044248,2018,36,"['https://openalex.org/W1753482797', 'https://openalex.org/W1915022094', 'https://openalex.org/W1915251500', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2176263492', 'https://openalex.org/W2194775991', 'https://openalex.org/W2525778437', 'https://openalex.org/W2559597482', 'https://openalex.org/W2567070169', 'https://openalex.org/W2613904329', 'https://openalex.org/W2739034105', 'https://openalex.org/W2936995161', 'https://openalex.org/W2949193663', 'https://openalex.org/W2953333557', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963839582', 'https://openalex.org/W2963970792', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W3041866211', 'https://openalex.org/W4302343710', 'https://openalex.org/W4385245566']","Yanyao Shen, Xu Tan, Di He, Tao Qin, Tie-Yan Liu. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",1.0
SKG_MT_434,https://openalex.org/W3115381926,2020,0,"['https://openalex.org/W6908809', 'https://openalex.org/W1994616650', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2124807415', 'https://openalex.org/W2546938941', 'https://openalex.org/W2772421198', 'https://openalex.org/W2907311798', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963333747', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3037423379']","In this paper, we propose a useful optimization method for low-resource Neural Machine Translation (NMT) by investigating the effectiveness of multiple neural network optimization algorithms. Our results confirm that applying the proposed optimization method on English-Persian translation can exceed translation quality compared to the English-Persian Statistical Machine Translation (SMT) paradigm.",0.9960159362549801
SKG_MT_436,https://openalex.org/W3121049435,2020,21,"['https://openalex.org/W2886776719', 'https://openalex.org/W2889326796', 'https://openalex.org/W2902081112', 'https://openalex.org/W2902510077', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962931466', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963411763', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963602293', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970423066', 'https://openalex.org/W2970694516', 'https://openalex.org/W2970947975', 'https://openalex.org/W2995720879', 'https://openalex.org/W2998435746', 'https://openalex.org/W3118680649']",This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English ↔ Chinese and English → German language pairs. Our systems are built on deep Transformer and several data augmentation methods. We propose a boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task.,1.0
SKG_MT_437,https://openalex.org/W2251530174,2015,25,"['https://openalex.org/W365755509', 'https://openalex.org/W419376470', 'https://openalex.org/W808583520', 'https://openalex.org/W1521626219', 'https://openalex.org/W1581774386', 'https://openalex.org/W1990611844', 'https://openalex.org/W2089629691', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108460050', 'https://openalex.org/W2116512345', 'https://openalex.org/W2121457870', 'https://openalex.org/W2124807415', 'https://openalex.org/W2153508793', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159755860', 'https://openalex.org/W2162245945', 'https://openalex.org/W2163366827', 'https://openalex.org/W2166098990', 'https://openalex.org/W2250630480', 'https://openalex.org/W2251542837', 'https://openalex.org/W2251955814', 'https://openalex.org/W2320039153', 'https://openalex.org/W2408655637', 'https://openalex.org/W2476155783', 'https://openalex.org/W2554098328', 'https://openalex.org/W2595715041', 'https://openalex.org/W2757575308', 'https://openalex.org/W2758491225', 'https://openalex.org/W3134644026', 'https://openalex.org/W3202197379']","Divergent word order between languages causes delay in simultaneous machine translation.We present a sentence rewriting method that generates more monotonic translations to improve the speedaccuracy tradeoff.We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees.We apply the rules to reference translations to make their word order closer to the source language word order.On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten, more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations.",1.0
SKG_MT_438,https://openalex.org/W3105626768,2020,41,"['https://openalex.org/W808583520', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121457870', 'https://openalex.org/W2251955814', 'https://openalex.org/W2419292002', 'https://openalex.org/W2529548870', 'https://openalex.org/W2605141709', 'https://openalex.org/W2890698823', 'https://openalex.org/W2896234185', 'https://openalex.org/W2896457183', 'https://openalex.org/W2938830017', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964078338', 'https://openalex.org/W2970074184', 'https://openalex.org/W2975711469', 'https://openalex.org/W2995428172', 'https://openalex.org/W3035348852', 'https://openalex.org/W4385245566']","Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency would probably hurt accuracy. Therefore, it is essential to segment the ASR output into appropriate units for translation. Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation. The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation. Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.",1.0
SKG_MT_439,https://openalex.org/W2982671023,2019,3,"['https://openalex.org/W1522301498', 'https://openalex.org/W1924770834', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136530135', 'https://openalex.org/W2144600658', 'https://openalex.org/W2150622104', 'https://openalex.org/W2327501763', 'https://openalex.org/W2509282593', 'https://openalex.org/W2549139847', 'https://openalex.org/W2581101319', 'https://openalex.org/W2752047430', 'https://openalex.org/W2760425631', 'https://openalex.org/W2794365787', 'https://openalex.org/W2889545026', 'https://openalex.org/W2889903020', 'https://openalex.org/W2902031175', 'https://openalex.org/W2950207430', 'https://openalex.org/W2950848235', 'https://openalex.org/W2962711930', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962934715', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963360627', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963407669', 'https://openalex.org/W2963418779', 'https://openalex.org/W2964192290', 'https://openalex.org/W2965350935']","This paper describes the cascaded multimodal speech translation systems developed by Imperial College London for the IWSLT 2019 evaluation campaign. The architecture consists of an automatic speech recognition (ASR) system followed by a Transformer-based multimodal machine translation (MMT) system. While the ASR component is identical across the experiments, the MMT model varies in terms of the way of integrating the visual context (simple conditioning vs. attention), the type of visual features exploited (pooled, convolutional, action categories) and the underlying architecture. For the latter, we explore both the canonical transformer and its deliberation version with additive and cascade variants which differ in how they integrate the textual attention. Upon conducting extensive experiments, we found that (i) the explored visual integration schemes often harm the translation performance for the transformer and additive deliberation, but considerably improve the cascade deliberation; (ii) the transformer and cascade deliberation integrate the visual modality better than the additive deliberation, as shown by the incongruence analysis.",1.0
SKG_MT_440,https://openalex.org/W2140967626,2011,11,"['https://openalex.org/W85493592', 'https://openalex.org/W91928571', 'https://openalex.org/W204009595', 'https://openalex.org/W225641137', 'https://openalex.org/W1916559533', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103149536', 'https://openalex.org/W2111142112', 'https://openalex.org/W2113788796', 'https://openalex.org/W2119774882', 'https://openalex.org/W2126610017', 'https://openalex.org/W2137143056', 'https://openalex.org/W2137387514', 'https://openalex.org/W2138302120', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152382718', 'https://openalex.org/W2156953672', 'https://openalex.org/W2160218441', 'https://openalex.org/W3202207277']","Statistical machine translation systems are normally optimised for a chosen gain function (metric) by using MERT to find the best model weights. This algorithm suffers from stability problems and cannot scale beyond 20-30 features. We present an alternative algorithm for discriminative training of phrasebased MT systems, SampleRank, which scales to hundreds of features, equals or beats MERT on both small and medium sized systems, and permits the use of sentence or document level features. SampleRank proceeds by repeatedly updating the model weights to ensure that the ranking of output sentences induced by the model is the same as that induced by the gain function. 1",1.0
SKG_MT_441,https://openalex.org/W2100664567,2015,867,"['https://openalex.org/W1571227886', 'https://openalex.org/W1606347560', 'https://openalex.org/W1614298861', 'https://openalex.org/W1753482797', 'https://openalex.org/W1916559533', 'https://openalex.org/W2097732278', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106429429', 'https://openalex.org/W2113131493', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2152790380', 'https://openalex.org/W2152808281', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250653840', 'https://openalex.org/W2250956062', 'https://openalex.org/W2316776689', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W3211848854', 'https://openalex.org/W4241645538']","Sébastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.",1.0
SKG_MT_442,https://openalex.org/W3120628641,2020,8,"['https://openalex.org/W630532510', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W1974967573', 'https://openalex.org/W2066651136', 'https://openalex.org/W2096765155', 'https://openalex.org/W2117339222', 'https://openalex.org/W2152114834', 'https://openalex.org/W2161204343', 'https://openalex.org/W2251060024', 'https://openalex.org/W2399188371', 'https://openalex.org/W2561274697', 'https://openalex.org/W2757592053', 'https://openalex.org/W2759461255', 'https://openalex.org/W2788762724', 'https://openalex.org/W2811435848', 'https://openalex.org/W2902614977', 'https://openalex.org/W2916454001', 'https://openalex.org/W2949756005', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962796826', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963708445', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970279348', 'https://openalex.org/W2985165968', 'https://openalex.org/W2988739750', 'https://openalex.org/W3000965575', 'https://openalex.org/W3072442649']","This paper describes the ADAPT Centre’s submissions to the WMT20 News translation shared task for English-to-Tamil and Tamil-to-English. We present our machine translation (MT) systems that were built using the state-of-the-art neural MT (NMT) model, Transformer. We applied various strategies in order to improve our baseline MT systems, e.g. monolingual sentence selection for creating synthetic training data, mining monolingual sentences for adapting our MT systems to the task, hyperparameters search for Transformer in low-resource scenarios. Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Tamil and Tamil-to-English translation tasks.",1.0
SKG_MT_444,https://openalex.org/W2971287409,2019,26,"['https://openalex.org/W6908809', 'https://openalex.org/W20450839', 'https://openalex.org/W150892692', 'https://openalex.org/W204341599', 'https://openalex.org/W230880734', 'https://openalex.org/W630532510', 'https://openalex.org/W1648907545', 'https://openalex.org/W1904365287', 'https://openalex.org/W1993378086', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108188641', 'https://openalex.org/W2133258739', 'https://openalex.org/W2151996595', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2250555572', 'https://openalex.org/W2296330515', 'https://openalex.org/W2340762547', 'https://openalex.org/W2394541218', 'https://openalex.org/W2519450945', 'https://openalex.org/W2608029998', 'https://openalex.org/W2613904329', 'https://openalex.org/W2759135454', 'https://openalex.org/W2759511005', 'https://openalex.org/W2767019613', 'https://openalex.org/W2783419700', 'https://openalex.org/W2787026069', 'https://openalex.org/W2799051177', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963194310', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963842551', 'https://openalex.org/W2963959336', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964265128', 'https://openalex.org/W4285719527', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","Longyue Wang, Zhaopeng Tu, Xing Wang, Shuming Shi. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",0.9921259842519685
SKG_MT_448,https://openalex.org/W2757877530,2017,9,"['https://openalex.org/W1522301498', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2293344577', 'https://openalex.org/W2345720230', 'https://openalex.org/W2417549359', 'https://openalex.org/W2512381898', 'https://openalex.org/W2513263213', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740565324', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963909453', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538']","This paper describes the NICT-NAIST system for the WMT 2017 shared multimodal machine translation task for both language pairs, English-to-German and English-to-French.We built a hierarchical phrase-based (Hiero) translation system and trained an attentional encoder-decoder neural machine translation (NMT) model to rerank the n-best output of the Hiero system, which obtained significant gains over both the Hiero system and NMT decoding alone.We also present a multimodal NMT model that integrates the target language descriptions of images that are similar to the image described by the source sentence as additional inputs of the neural networks to help the translation of the source sentence.We give detailed analysis for the results of the multimodal NMT model.Our system obtained the first place for the English-to-French task according to human evaluation.",1.0
SKG_MT_450,https://openalex.org/W3093000437,2020,28,"['https://openalex.org/W46679369', 'https://openalex.org/W222053410', 'https://openalex.org/W1604122592', 'https://openalex.org/W1995519658', 'https://openalex.org/W2028324469', 'https://openalex.org/W2101105183', 'https://openalex.org/W2148708890', 'https://openalex.org/W2250559305', 'https://openalex.org/W2252027930', 'https://openalex.org/W2289260142', 'https://openalex.org/W2504359786', 'https://openalex.org/W2511234952', 'https://openalex.org/W2562803530', 'https://openalex.org/W2741029840', 'https://openalex.org/W2806885904', 'https://openalex.org/W2891525068', 'https://openalex.org/W2950866572', 'https://openalex.org/W2952328691', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963167649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963457723', 'https://openalex.org/W2963503967', 'https://openalex.org/W2963526187', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963612262', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964222246', 'https://openalex.org/W2970583189', 'https://openalex.org/W2972654913', 'https://openalex.org/W2972680990', 'https://openalex.org/W2972735048', 'https://openalex.org/W2972972637', 'https://openalex.org/W2978094541', 'https://openalex.org/W3034458735', 'https://openalex.org/W3037109418', 'https://openalex.org/W3082674894', 'https://openalex.org/W3215160418']","When translating ""The secretary asked for details."" to a language with grammatical gender, it might be necessary to determine the gender of the subject ""secretary"". If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.",1.0
SKG_MT_452,https://openalex.org/W2799231549,2018,5,"['https://openalex.org/W1996641400', 'https://openalex.org/W2006969979', 'https://openalex.org/W2061211758', 'https://openalex.org/W2252239149', 'https://openalex.org/W2410539690', 'https://openalex.org/W2552838200', 'https://openalex.org/W2552839021', 'https://openalex.org/W2574872930', 'https://openalex.org/W2576713500', 'https://openalex.org/W2619353065', 'https://openalex.org/W2625092622', 'https://openalex.org/W2626778328', 'https://openalex.org/W2949335953', 'https://openalex.org/W2949888546', 'https://openalex.org/W2963123301', 'https://openalex.org/W2963699608', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964308564']","In NMT, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.",1.0
SKG_MT_453,https://openalex.org/W2137207778,2012,143,"['https://openalex.org/W83953607', 'https://openalex.org/W182185074', 'https://openalex.org/W1515087027', 'https://openalex.org/W1581485226', 'https://openalex.org/W1755776945', 'https://openalex.org/W1859957297', 'https://openalex.org/W1964591498', 'https://openalex.org/W1964613733', 'https://openalex.org/W1989713378', 'https://openalex.org/W1996509985', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008881528', 'https://openalex.org/W2019207508', 'https://openalex.org/W2049493498', 'https://openalex.org/W2081375810', 'https://openalex.org/W2095577376', 'https://openalex.org/W2100362224', 'https://openalex.org/W2102394389', 'https://openalex.org/W2112744748', 'https://openalex.org/W2114581066', 'https://openalex.org/W2125247664', 'https://openalex.org/W2126581182', 'https://openalex.org/W2138621811', 'https://openalex.org/W2141631351', 'https://openalex.org/W2159457224', 'https://openalex.org/W2160660844', 'https://openalex.org/W2163114435', 'https://openalex.org/W2405897923', 'https://openalex.org/W2519011775']","This paper proposes a novel approach to extract opinion targets based on word-based translation model (WTM). At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. Then, a graph-based algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. In particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora. By using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods. 1",1.0
SKG_MT_454,https://openalex.org/W2989156240,2019,42,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591706642', 'https://openalex.org/W1793121960', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2467173223', 'https://openalex.org/W2540404261', 'https://openalex.org/W2561386235', 'https://openalex.org/W2576482813', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2886845974', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963174729', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963929190', 'https://openalex.org/W2963963856', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964345285', 'https://openalex.org/W4298960041', 'https://openalex.org/W4301368689', 'https://openalex.org/W4385245566']","Long sentences have been one of the major challenges in neural machine translation (NMT). Although some approaches such as the attention mechanism have partially remedied the problem, we found that the current standard NMT model, Transformer, has difficulty in translating long sentences compared to the former standard, Recurrent Neural Network (RNN)-based model. One of the key differences of these NMT models is how the model handles position information which is essential to process sequential data. In this study, we focus on the position information type of NMT models, and hypothesize that relative position is better than absolute position. To examine the hypothesis, we propose RNN-Transformer which replaces positional encoding layer of Transformer by RNN, and then compare RNN-based model and four variants of Transformer. Experiments on ASPEC English-to-Japanese and WMT2014 English-to-German translation tasks demonstrate that relative position helps translating sentences longer than those in the training data. Further experiments on length-controlled training data reveal that absolute position actually causes overfitting to the sentence length.",1.0
SKG_MT_455,https://openalex.org/W3120880016,2020,4,"['https://openalex.org/W1566289585', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2889326796', 'https://openalex.org/W2902614977', 'https://openalex.org/W2922709902', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963708445', 'https://openalex.org/W2963716420', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2986562961', 'https://openalex.org/W2994928925', 'https://openalex.org/W2996854111', 'https://openalex.org/W3030394039', 'https://openalex.org/W3035306625']","This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, we have proposed to train a BERT-fused NMT model that leverages the use of pretrained language models. Furthermore, we have augmented the training corpus by backtranslating monolingual data. Our experiments show that NMT models in low-resource scenarios can benefit from combining these two training techniques, with improvements of up to 6.16 BLEU percentual points in the case of biomedical abstract translations.",1.0
SKG_MT_456,https://openalex.org/W3045570277,2020,3,"['https://openalex.org/W1851962382', 'https://openalex.org/W2051840895', 'https://openalex.org/W2136353104', 'https://openalex.org/W2137387514', 'https://openalex.org/W2141895568', 'https://openalex.org/W2470673105', 'https://openalex.org/W2567571499', 'https://openalex.org/W2739046565', 'https://openalex.org/W2744813330', 'https://openalex.org/W2767019613', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2885637246', 'https://openalex.org/W2891534142', 'https://openalex.org/W2962712961', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963842551', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964289193', 'https://openalex.org/W2970544750', 'https://openalex.org/W2970587837', 'https://openalex.org/W2989066524', 'https://openalex.org/W3204406378']","Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information. In general sentence-based NMT models are extended to capture contextual information from large-scale document-level corpora which are difficult to acquire. Domain adaptation on the other hand promises adapting components of already developed systems by exploiting limited in-domain data. This paper presents FJWU’s system submission at WNGT, we specifically participated in Document level MT task for German-English translation. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline. We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe systems according to the testing domain.",1.0
SKG_MT_459,https://openalex.org/W2284660317,2016,147,"['https://openalex.org/W1562498310', 'https://openalex.org/W1902237438', 'https://openalex.org/W1904365287', 'https://openalex.org/W1915022094', 'https://openalex.org/W2097333193', 'https://openalex.org/W2101456909', 'https://openalex.org/W2108677974', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2136156618', 'https://openalex.org/W2184135559', 'https://openalex.org/W2217896605', 'https://openalex.org/W2251139225', 'https://openalex.org/W2528305545', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962978267', 'https://openalex.org/W2964308564', 'https://openalex.org/W3041866211', 'https://openalex.org/W3204406378']","Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task EnglishGerman (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish-&gt;English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English-&gt;German.",1.0
SKG_MT_460,https://openalex.org/W2739738036,2017,29,"['https://openalex.org/W36903255', 'https://openalex.org/W932413789', 'https://openalex.org/W1723811852', 'https://openalex.org/W1902237438', 'https://openalex.org/W1917432393', 'https://openalex.org/W2100664567', 'https://openalex.org/W2110990863', 'https://openalex.org/W2120861206', 'https://openalex.org/W2122146326', 'https://openalex.org/W2130942839', 'https://openalex.org/W2152790380', 'https://openalex.org/W2250922215', 'https://openalex.org/W2460130460', 'https://openalex.org/W2463033603', 'https://openalex.org/W2525778437', 'https://openalex.org/W2529089661', 'https://openalex.org/W2576482813', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963643655', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963932686', 'https://openalex.org/W2963937700', 'https://openalex.org/W2963977130']","We speed up Neural Machine Translation (NMT) decoding by shrinking run-time target vocabulary. We experiment with two shrinking approaches: Locality Sensitive Hashing (LSH) and word alignments. Using the latter method, we get a 2x overall speed-up over a highly-optimized GPU implementation, without hurting BLEU. On certain low-resource language pairs, the same methods improve BLEU by 0.5 points. We also report a negative result for LSH on GPUs, due to relatively large overhead, though it was successful on CPUs. Compared with Locality Sensitive Hashing (LSH), decoding with word alignments is GPU-friendly, orthogonal to existing speedup methods and more robust across language pairs.",1.0
SKG_MT_464,https://openalex.org/W3034938700,2020,97,"['https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W1753482797', 'https://openalex.org/W1958336659', 'https://openalex.org/W1974339500', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2158139315', 'https://openalex.org/W2247359815', 'https://openalex.org/W2296073425', 'https://openalex.org/W2413794162', 'https://openalex.org/W2493916176', 'https://openalex.org/W2525778437', 'https://openalex.org/W2584268338', 'https://openalex.org/W2597655663', 'https://openalex.org/W2613904329', 'https://openalex.org/W2741838462', 'https://openalex.org/W2888520903', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890560993', 'https://openalex.org/W2898846200', 'https://openalex.org/W2907252220', 'https://openalex.org/W2919188216', 'https://openalex.org/W2923622379', 'https://openalex.org/W2946379889', 'https://openalex.org/W2951338945', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963386218', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963932569', 'https://openalex.org/W2964093309', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964343359', 'https://openalex.org/W2966610483', 'https://openalex.org/W2970290486', 'https://openalex.org/W2974635524', 'https://openalex.org/W2988975212', 'https://openalex.org/W3034201598', 'https://openalex.org/W4295680023', 'https://openalex.org/W4298201654', 'https://openalex.org/W4385245566']","A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).",1.0
SKG_MT_465,https://openalex.org/W2759168318,2017,13,"['https://openalex.org/W1522301498', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2147880316', 'https://openalex.org/W2157331557', 'https://openalex.org/W2194775991', 'https://openalex.org/W2466062786', 'https://openalex.org/W2525778437', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564']","This paper describes the Neural Machine Translation systems of Xiamen University for the translation tasks of WMT 17.Our systems are based on the Encoder-Decoder framework with attention.We participated in three directions of shared news translation tasks: English→German and Chinese↔English.We experimented with deep architectures, different segmentation models, synthetic training data and targetbidirectional translation models.Experiments show that all methods can give substantial improvements.",1.0
SKG_MT_469,https://openalex.org/W2164806437,2012,40,"['https://openalex.org/W13944552', 'https://openalex.org/W75158669', 'https://openalex.org/W92412080', 'https://openalex.org/W1631260214', 'https://openalex.org/W1973923101', 'https://openalex.org/W1989658336', 'https://openalex.org/W2000026602', 'https://openalex.org/W2016522586', 'https://openalex.org/W2062270497', 'https://openalex.org/W2085086335', 'https://openalex.org/W2093390569', 'https://openalex.org/W2097261017', 'https://openalex.org/W2097341304', 'https://openalex.org/W2100976324', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107695330', 'https://openalex.org/W2109704865', 'https://openalex.org/W2116042738', 'https://openalex.org/W2117339222', 'https://openalex.org/W2119168550', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2135161317', 'https://openalex.org/W2137387514', 'https://openalex.org/W2140826881', 'https://openalex.org/W2140903445', 'https://openalex.org/W2143539737', 'https://openalex.org/W2147308966', 'https://openalex.org/W2151913182', 'https://openalex.org/W2163361328', 'https://openalex.org/W2171421863', 'https://openalex.org/W2172268343', 'https://openalex.org/W2407127455', 'https://openalex.org/W2588710172', 'https://openalex.org/W2989631226']","Microblogging services such as Twitter have become popular media for real-time usercreated news reporting. Such communication often happens in parallel in different languages, e.g., microblog posts related to the same events of the Arab spring were written in Arabic and in English. The goal of this paper is to exploit this parallelism in order to eliminate the main bottleneck in automatic Twitter translation, namely the lack of bilingual sentence pairs for training SMT systems. We show that translation-based cross-lingual information retrieval can retrieve microblog messages across languages that are similar enough to be used to train a standard phrasebased SMT pipeline. Our method outperforms other approaches to domain adaptation for SMT such as language model adaptation, meta-parameter tuning, or self-translation. 1",1.0
SKG_MT_470,https://openalex.org/W2978378531,2019,2,"['https://openalex.org/W2127863960', 'https://openalex.org/W2153508793', 'https://openalex.org/W2157331557', 'https://openalex.org/W2531207078', 'https://openalex.org/W2611838487', 'https://openalex.org/W2626778328', 'https://openalex.org/W2888958984', 'https://openalex.org/W2949335953', 'https://openalex.org/W2949650786', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962789428', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963223057', 'https://openalex.org/W2963321862']","While translating between East Asian languages, many works have discovered clear advantages of using characters as the translation unit. Unfortunately, traditional recurrent neural machine translation systems hinder the practical usage of those character-based systems due to their architectural limitations. They are unfavorable in handling extremely long sequences as well as highly restricted in parallelizing the computations. In this paper, we demonstrate that the new transformer architecture can perform character-based translation better than the recurrent one. We conduct experiments on a low-resource language pair: Japanese-Vietnamese. Our models considerably outperform the state-of-the-art systems which employ word-based recurrent architectures.",0.9961685823754789
SKG_MT_471,https://openalex.org/W2251313925,2015,6,"['https://openalex.org/W53604701', 'https://openalex.org/W138108643', 'https://openalex.org/W1524333225', 'https://openalex.org/W1537859740', 'https://openalex.org/W2010917994', 'https://openalex.org/W2026369565', 'https://openalex.org/W2096451823', 'https://openalex.org/W2103123519', 'https://openalex.org/W2113106066', 'https://openalex.org/W2116594867', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134546430', 'https://openalex.org/W2139647714', 'https://openalex.org/W2157435188', 'https://openalex.org/W2403178545', 'https://openalex.org/W2595715041']","Speech translation is conventionally carried out by cascading an automatic speech recognition (ASR) and a statistical machine translation (SMT) system.The hypotheses chosen for translation are based on the ASR system's acoustic and language model scores, and typically optimized for word error rate, ignoring the intended downstream use: automatic translation.In this paper, we present a coarseto-fine model that uses features from the ASR and SMT systems to optimize this coupling.We demonstrate that several standard features utilized by ASR and SMT systems can be used in such a model at the speech-translation interface, and we provide empirical results on the Fisher Spanish-English speech translation corpus.",1.0
SKG_MT_472,https://openalex.org/W2888196092,2018,43,"['https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114060876', 'https://openalex.org/W2121870595', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2187089797', 'https://openalex.org/W2252077779', 'https://openalex.org/W2252186615', 'https://openalex.org/W2407166119', 'https://openalex.org/W2552839021', 'https://openalex.org/W2577255746', 'https://openalex.org/W2741823585', 'https://openalex.org/W2775767017', 'https://openalex.org/W2798608854', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963551569', 'https://openalex.org/W2964308564', 'https://openalex.org/W3102516861', 'https://openalex.org/W4241645538', 'https://openalex.org/W4385245566']","In Neural Machine Translation (NMT), the decoder can capture the features of the entire prediction history with neural connections and representations. This means that partial hypotheses with different prefixes will be regarded differently no matter how similar they are. However, this might be inefficient since some partial hypotheses can contain only local differences that will not influence future predictions. In this work, we introduce recombination in NMT decoding based on the concept of the ""equivalence"" of partial hypotheses. Heuristically, we use a simple n-gram suffix based equivalence function and adapt it into beam search decoding. Through experiments on large-scale Chinese-to-English and English-to-Germen translation tasks, we show that the proposed method can obtain similar translation quality with a smaller beam size, making NMT decoding more efficient.",1.0
SKG_MT_473,https://openalex.org/W2887966409,2018,13,"['https://openalex.org/W580074167', 'https://openalex.org/W1537058117', 'https://openalex.org/W1544827683', 'https://openalex.org/W1783519389', 'https://openalex.org/W1801721664', 'https://openalex.org/W1832693441', 'https://openalex.org/W2090243146', 'https://openalex.org/W2124672271', 'https://openalex.org/W2127589108', 'https://openalex.org/W2132324454', 'https://openalex.org/W2132339004', 'https://openalex.org/W2139645402', 'https://openalex.org/W2157364932', 'https://openalex.org/W2158199200', 'https://openalex.org/W2169045071', 'https://openalex.org/W2170240176', 'https://openalex.org/W2170738476', 'https://openalex.org/W2171590421', 'https://openalex.org/W2176685020', 'https://openalex.org/W2189364361', 'https://openalex.org/W2197450510', 'https://openalex.org/W2251033195', 'https://openalex.org/W2251765408', 'https://openalex.org/W2252212383', 'https://openalex.org/W2265846598', 'https://openalex.org/W2270364989', 'https://openalex.org/W2323116150', 'https://openalex.org/W2402089952', 'https://openalex.org/W2479919622', 'https://openalex.org/W2493916176', 'https://openalex.org/W2511678010', 'https://openalex.org/W2516125976', 'https://openalex.org/W2530571334', 'https://openalex.org/W2551361256', 'https://openalex.org/W2572757783', 'https://openalex.org/W2594021297', 'https://openalex.org/W2600111000', 'https://openalex.org/W2605089588', 'https://openalex.org/W2739533097', 'https://openalex.org/W2739533179', 'https://openalex.org/W2740132093', 'https://openalex.org/W2740883175', 'https://openalex.org/W2740885753', 'https://openalex.org/W2741602058', 'https://openalex.org/W2749078814', 'https://openalex.org/W2759098637', 'https://openalex.org/W2774523402', 'https://openalex.org/W2798093537', 'https://openalex.org/W2949615363', 'https://openalex.org/W2950682695', 'https://openalex.org/W2952190837', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963012544', 'https://openalex.org/W2963285772', 'https://openalex.org/W2963339397', 'https://openalex.org/W2963501892', 'https://openalex.org/W2963655104', 'https://openalex.org/W2963682631', 'https://openalex.org/W2963688910', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963861211', 'https://openalex.org/W2963898467', 'https://openalex.org/W2963921497', 'https://openalex.org/W2964199361', 'https://openalex.org/W4297813410', 'https://openalex.org/W4299579390', 'https://openalex.org/W4387627603']","Humans can learn multiple languages. If they know a fact in one language, they can answer a question in another language they understand. They can also answer Code-mix (CM) questions: questions which contain both languages. This behavior is attributed to the unique learning ability of humans. Our task aims to study if machines can achieve this. We demonstrate how effectively a machine can answer CM questions. In this work, we adopt a two phase approach: candidate generation and candidate re-ranking to answer questions. We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers. We show experiments on the SimpleQuestions dataset. Our network is trained only on English questions provided in this dataset and noisy Hindi translations of these questions and can answer English-Hindi CM questions effectively without the need of translation into English. Back-transliterated CM questions outperform their lexical and sentence level translated counterparts by 5% & 35% in accuracy respectively, highlighting the efficacy of our approach in a resource constrained setting.",0.9945945945945946
SKG_MT_475,https://openalex.org/W2581101319,2017,151,"['https://openalex.org/W6908809', 'https://openalex.org/W68733909', 'https://openalex.org/W648786980', 'https://openalex.org/W1514535095', 'https://openalex.org/W1527575280', 'https://openalex.org/W1686810756', 'https://openalex.org/W1753482797', 'https://openalex.org/W1810943226', 'https://openalex.org/W1895577753', 'https://openalex.org/W1902237438', 'https://openalex.org/W1934041838', 'https://openalex.org/W1947481528', 'https://openalex.org/W2064675550', 'https://openalex.org/W2081580037', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102605133', 'https://openalex.org/W2117539524', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2139501017', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2157331557', 'https://openalex.org/W2159243025', 'https://openalex.org/W2185175083', 'https://openalex.org/W2212703438', 'https://openalex.org/W2247931231', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251743902', 'https://openalex.org/W2345720230', 'https://openalex.org/W2406012631', 'https://openalex.org/W2417549359', 'https://openalex.org/W2481240925', 'https://openalex.org/W2508429489', 'https://openalex.org/W2509282593', 'https://openalex.org/W2512381898', 'https://openalex.org/W2513263213', 'https://openalex.org/W2516756687', 'https://openalex.org/W2593341061', 'https://openalex.org/W2595715041', 'https://openalex.org/W2760101375', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962835968', 'https://openalex.org/W2962953664', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963344439', 'https://openalex.org/W2963909453', 'https://openalex.org/W2963959597', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998704965', 'https://openalex.org/W4285719527']","We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. Global image features are extracted using a pre-trained convolutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate translations into English and German, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal NMT models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.",1.0
SKG_MT_476,https://openalex.org/W2141100269,2014,5,"['https://openalex.org/W22168010', 'https://openalex.org/W38462120', 'https://openalex.org/W108437174', 'https://openalex.org/W1631260214', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101234009', 'https://openalex.org/W2123100726', 'https://openalex.org/W2139621418', 'https://openalex.org/W2148922863', 'https://openalex.org/W2149327368', 'https://openalex.org/W2291445757', 'https://openalex.org/W2591804103', 'https://openalex.org/W2895810819']","In this paper we describe experiments on predicting HTER, as part of our submission in the Shared Task on Quality Estimation, in the frame of the 9th Workshop on Statistical Machine Translation. In our experiment we check whether it is possible to achieve better HTER prediction by training four individual regression models for each one of the edit types (deletions, insertions, substitutions, shifts), however no improvements were yielded. We also had no improvements when investigating the possibility of adding more data from other non-minimally post-edited and freely translated datasets. Best HTER prediction was achieved by adding deduplicated WMT13 data and additional features such as (a) rule-based language corrections (language tool) (b) PCFG parsing statistics and count of tree labels (c) position statistics of parsing labels (d) position statistics of tri-grams with low probability.",1.0
SKG_MT_477,https://openalex.org/W3105669983,2020,32,"['https://openalex.org/W53604701', 'https://openalex.org/W399167303', 'https://openalex.org/W1522301498', 'https://openalex.org/W1942035323', 'https://openalex.org/W1959608418', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113106066', 'https://openalex.org/W2127141656', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134546430', 'https://openalex.org/W2466918907', 'https://openalex.org/W2595715041', 'https://openalex.org/W2605131327', 'https://openalex.org/W2747874407', 'https://openalex.org/W2785350307', 'https://openalex.org/W2936969148', 'https://openalex.org/W2938973646', 'https://openalex.org/W2940744433', 'https://openalex.org/W2945700568', 'https://openalex.org/W2949227999', 'https://openalex.org/W2949328740', 'https://openalex.org/W2951635603', 'https://openalex.org/W2952167535', 'https://openalex.org/W2962680099', 'https://openalex.org/W2962780374', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963828549', 'https://openalex.org/W2963834942', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970777192', 'https://openalex.org/W2972448360', 'https://openalex.org/W2972514683', 'https://openalex.org/W2972818416', 'https://openalex.org/W2973048981', 'https://openalex.org/W2982129078', 'https://openalex.org/W2982727400', 'https://openalex.org/W2986963494', 'https://openalex.org/W2997436923', 'https://openalex.org/W2998386507', 'https://openalex.org/W3006988520', 'https://openalex.org/W3011384584', 'https://openalex.org/W3015703505', 'https://openalex.org/W3017643757', 'https://openalex.org/W3034571331', 'https://openalex.org/W3041700925', 'https://openalex.org/W4323654151', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394649814']","Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR encoder and apply AFS to dynamically estimate the importance of each encoded speech feature to ASR. A ST encoder, stacked on top of the ASR encoder, then receives the filtered features from the (frozen) ASR encoder. We take <i>L</i><sub>0</sub>DROP (Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech features with respect to both temporal and feature dimensions. Results on LibriSpeech En-Fr and MuST-C benchmarks show that AFS facilitates learning of ST by pruning out ∼84% temporal features, yielding an average translation gain of ∼1.3–1.6 BLEU and a decoding speedup of ∼1.4×. In particular, AFS reduces the performance gap compared to the cascade baseline, and outperforms it on LibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation).",1.0
SKG_MT_478,https://openalex.org/W2169954957,2010,15,"['https://openalex.org/W108437174', 'https://openalex.org/W222053410', 'https://openalex.org/W1571735237', 'https://openalex.org/W1965420956', 'https://openalex.org/W2096175520', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101190066', 'https://openalex.org/W2110020502', 'https://openalex.org/W2110104386', 'https://openalex.org/W2112900913', 'https://openalex.org/W2116316001', 'https://openalex.org/W2118536060', 'https://openalex.org/W2123126659', 'https://openalex.org/W2140133598', 'https://openalex.org/W2140343992', 'https://openalex.org/W2144279206', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2154581043', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158603998', 'https://openalex.org/W2165666205', 'https://openalex.org/W2166905217']","In hierarchical phrase-based SMT systems, statistical models are integrated to guide the hierarchical rule selection for better translation performance. Previous work mainly focused on the selection of either the source side of a hierarchical rule or the target side of a hierarchical rule rather than considering both of them simultaneously. This paper presents a joint model to predict the selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1",1.0
SKG_MT_479,https://openalex.org/W3034773362,2020,135,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133459682', 'https://openalex.org/W2509282593', 'https://openalex.org/W2581101319', 'https://openalex.org/W2593341061', 'https://openalex.org/W2805516822', 'https://openalex.org/W2889903020', 'https://openalex.org/W2896234464', 'https://openalex.org/W2903343986', 'https://openalex.org/W2950207430', 'https://openalex.org/W2950886580', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963909453', 'https://openalex.org/W2963988211', 'https://openalex.org/W2964121744', 'https://openalex.org/W4385245566']","Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.",1.0
SKG_MT_480,https://openalex.org/W2129336405,2012,51,"['https://openalex.org/W22168010', 'https://openalex.org/W90695545', 'https://openalex.org/W1526763309', 'https://openalex.org/W1538023239', 'https://openalex.org/W1625582487', 'https://openalex.org/W1631260214', 'https://openalex.org/W1650129375', 'https://openalex.org/W2033010331', 'https://openalex.org/W2070293423', 'https://openalex.org/W2075548498', 'https://openalex.org/W2099033952', 'https://openalex.org/W2101105183', 'https://openalex.org/W2120309221', 'https://openalex.org/W2124807415', 'https://openalex.org/W2135708429', 'https://openalex.org/W2149327368', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165861304', 'https://openalex.org/W2170120894', 'https://openalex.org/W2291598608', 'https://openalex.org/W2394530649']","In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. In this work, we address the problem of incremental speech-to-speech translation (S2S) that enables cross-lingual communication between two remote participants over a telephone. We investigate the problem in a novel real-time Session Initiation Protocol (SIP) based S2S framework. The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition. We describe the statistical models comprising the S2S system and the SIP architecture for enabling real-time two-way cross-lingual dialog. We present dialog experiments performed in this framework and study the tradeoff in accuracy versus latency in incremental speech translation. Experimental results demonstrate that high quality translations can be generated with the incremental approach with approximately half the latency associated with nonincremental approach. 1",1.0
SKG_MT_481,https://openalex.org/W2139801634,2013,9,"['https://openalex.org/W1571785066', 'https://openalex.org/W2006969979', 'https://openalex.org/W2038698865', 'https://openalex.org/W2054533749', 'https://openalex.org/W2107878390', 'https://openalex.org/W2116792345', 'https://openalex.org/W2124807415', 'https://openalex.org/W2142112143', 'https://openalex.org/W2146574666', 'https://openalex.org/W2146853049', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157136552', 'https://openalex.org/W2179206722', 'https://openalex.org/W2180952760', 'https://openalex.org/W2250281547', 'https://openalex.org/W2251912507', 'https://openalex.org/W2408503330']","This paper describes QCRI-MES’s submission on the English-Russian dataset to the Eighth Workshop on Statistical Machine Translation. We generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to GIZA++ and build a phrase-based machine translation system. For tuning, we use a variation of PRO which provides better weights by optimizing BLEU+1 at corpus-level. We transliterate out-of-vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system. For the Russian to English translation direction, we apply linguistically motivated pre-processing on the Russian side of the data.",0.9950248756218906
SKG_MT_482,https://openalex.org/W2950023669,2019,15,"['https://openalex.org/W1793121960', 'https://openalex.org/W2613904329', 'https://openalex.org/W2767019613', 'https://openalex.org/W2767989436', 'https://openalex.org/W2796108585', 'https://openalex.org/W2866343820', 'https://openalex.org/W2896457183', 'https://openalex.org/W2946794439', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963407669', 'https://openalex.org/W2964265128', 'https://openalex.org/W3104486441', 'https://openalex.org/W4288347855', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","Shohei Iida, Ryuichiro Kimura, Hongyi Cui, Po-Hsuan Hung, Takehito Utsuro, Masaaki Nagata. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. 2019.",0.9931972789115646
SKG_MT_483,https://openalex.org/W3120348349,2020,5,"['https://openalex.org/W222053410', 'https://openalex.org/W630532510', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W1967796897', 'https://openalex.org/W1974967573', 'https://openalex.org/W2066651136', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117339222', 'https://openalex.org/W2123241698', 'https://openalex.org/W2152114834', 'https://openalex.org/W2251060024', 'https://openalex.org/W2270018644', 'https://openalex.org/W2399188371', 'https://openalex.org/W2512924740', 'https://openalex.org/W2561274697', 'https://openalex.org/W2757592053', 'https://openalex.org/W2759461255', 'https://openalex.org/W2788762724', 'https://openalex.org/W2811435848', 'https://openalex.org/W2902614977', 'https://openalex.org/W2949756005', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962796826', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963708445', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970279348', 'https://openalex.org/W2985165968', 'https://openalex.org/W2988739750', 'https://openalex.org/W3072442649']","This paper describes the ADAPT Centre’s submissions to the WMT20 Biomedical Translation Shared Task for English-to-Basque. We present the machine translation (MT) systems that were built to translate scientific abstracts and terms from biomedical terminologies, and using the state-of-the-art neural MT (NMT) model: Transformer. In order to improve our baseline NMT system, we employ a number of methods, e.g. “pseudo” parallel data selection, monolingual data selection for synthetic corpus creation, mining monolingual sentences for adapting our NMT systems to this task, hyperparameters search for Transformer in low-resource scenarios. Our experiments show that systematic addition of the aforementioned techniques to the baseline yields an excellent performance in the English-to-Basque translation task.",1.0
SKG_MT_485,https://openalex.org/W2982026991,2019,33,"['https://openalex.org/W1522301498', 'https://openalex.org/W1843891098', 'https://openalex.org/W1902237438', 'https://openalex.org/W2097117768', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2337363174', 'https://openalex.org/W2467834614', 'https://openalex.org/W2525778437', 'https://openalex.org/W2528130257', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561386235', 'https://openalex.org/W2757592053', 'https://openalex.org/W2768123736', 'https://openalex.org/W2768716007', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798878995', 'https://openalex.org/W2805394970', 'https://openalex.org/W2807895655', 'https://openalex.org/W2888159079', 'https://openalex.org/W2890914727', 'https://openalex.org/W2896691342', 'https://openalex.org/W2899182573', 'https://openalex.org/W2941599692', 'https://openalex.org/W2945700568', 'https://openalex.org/W2949920209', 'https://openalex.org/W2951184134', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963382396', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963887123', 'https://openalex.org/W2964308564', 'https://openalex.org/W2989631226', 'https://openalex.org/W3204406378']","The recent advances introduced by neural machine translation (NMT) are rapidly expanding the application fields of machine translation, as well as reshaping the quality level to be targeted. In particular, if translations have to fit some given layout, quality should not only be measured in terms of adequacy and fluency, but also length. Exemplary cases are the translation of document files, subtitles, and scripts for dubbing, where the output length should ideally be as close as possible to the length of the input text. This paper addresses for the first time, to the best of our knowledge, the problem of controlling the output length in NMT. We investigate two methods for biasing the output length with a transformer architecture: i) conditioning the output to a given target-source length-ratio class and ii) enriching the transformer positional embedding with length information. Our experiments show that both methods can induce the network to generate shorter translations, as well as acquiring interpretable linguistic skills.",1.0
SKG_MT_486,https://openalex.org/W3010250255,2020,3,"['https://openalex.org/W1821462560', 'https://openalex.org/W2419539795', 'https://openalex.org/W2463507112', 'https://openalex.org/W2578338321', 'https://openalex.org/W2586087033', 'https://openalex.org/W2626778328', 'https://openalex.org/W2740718109', 'https://openalex.org/W2744813330', 'https://openalex.org/W2760452458', 'https://openalex.org/W2760656271', 'https://openalex.org/W2794561587', 'https://openalex.org/W2924902521', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964222566', 'https://openalex.org/W2970286654', 'https://openalex.org/W2970454332', 'https://openalex.org/W2978017171', 'https://openalex.org/W2978544343', 'https://openalex.org/W2982402932', 'https://openalex.org/W2986300872', 'https://openalex.org/W2991786320', 'https://openalex.org/W2994994863', 'https://openalex.org/W2997936605', 'https://openalex.org/W3008008574', 'https://openalex.org/W3204406378']","We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting. While both domain adaptation and knowledge distillation are widely-used, their interaction remains little understood. Our large-scale empirical results in machine translation (on three language pairs with three domains each) suggest distilling twice for best performance: once using general-domain data and again using in-domain data with an adapted teacher.",0.9942857142857143
SKG_MT_487,https://openalex.org/W2100175927,2014,56,"['https://openalex.org/W121281418', 'https://openalex.org/W1724972948', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2153975459', 'https://openalex.org/W2167007060', 'https://openalex.org/W2250672026', 'https://openalex.org/W2250825001', 'https://openalex.org/W2270190199', 'https://openalex.org/W2294913621', 'https://openalex.org/W2591804103', 'https://openalex.org/W2895810819']","A main output of the annual Workshop on Statistical Machine Translation (WMT) is a ranking of the systems that participated in its shared translation tasks, produced by aggregating pairwise sentencelevel comparisons collected from human judges. Over the past few years, there have been a number of tweaks to the aggregation formula in attempts to address issues arising from the inherent ambiguity and subjectivity of the task, as well as weaknesses in the proposed models and the manner of model selection. We continue this line of work by adapting the TrueSkill TM algorithm — an online approach for modeling the relative skills of players in ongoing competitions, such as Microsoft’s Xbox Live — to the human evaluation of machine translation output. Our experimental results show that TrueSkill outperforms other recently proposed models on accuracy, and also can significantly reduce the number of pairwise annotations that need to be collected by sampling non-uniformly from the space of system competitions.",1.0
SKG_MT_488,https://openalex.org/W1650129375,2010,18,"['https://openalex.org/W1498238796', 'https://openalex.org/W1514971736', 'https://openalex.org/W2028492537', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2138203921', 'https://openalex.org/W2140671896', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2161227214', 'https://openalex.org/W2164788644']","In this paper we focus on the incremental decoding for a statistical phrase-based ma-chine translation system. In incremental decoding, translations are generated incre-mentally for every word typed by a user, instead of waiting for the entire sentence as input. We introduce a novel modifi-cation to the beam-search decoding algo-rithm for phrase-based MT to address this issue, aimed at efficient computation of fu-ture costs and avoiding search errors. Our objective is to do a faster translation dur-ing incremental decoding without signifi-cant reduction in the translation quality. 1",1.0
SKG_MT_489,https://openalex.org/W3119867917,2020,10,"['https://openalex.org/W2053154970', 'https://openalex.org/W2251431759', 'https://openalex.org/W2624521690', 'https://openalex.org/W2757417237', 'https://openalex.org/W2801219566', 'https://openalex.org/W2884074763', 'https://openalex.org/W2888159079', 'https://openalex.org/W2922158773', 'https://openalex.org/W3031433088']","Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of “human parity” (Toral et al., 2018; Laubli et al., 2018) with document-level human evaluations have been published. Yet, little is known about best practices regarding human evaluation of machine translation at the document-level. This paper presents a comparison of the differences in inter-annotator agreement between quality assessments using sentence and document-level set-ups. We report results of the agreement between professional translators for fluency and adequacy scales, error annotation, and pair-wise ranking, along with the effort needed to perform the different tasks. To best of our knowledge, this is the first study of its kind.",0.995850622406639
SKG_MT_490,https://openalex.org/W3102446692,2020,50,"['https://openalex.org/W2760656271', 'https://openalex.org/W2764043458', 'https://openalex.org/W2767785892', 'https://openalex.org/W2805003733', 'https://openalex.org/W2805493160', 'https://openalex.org/W2808794272', 'https://openalex.org/W2892009249', 'https://openalex.org/W2903193068', 'https://openalex.org/W2907279971', 'https://openalex.org/W2915589364', 'https://openalex.org/W2946794439', 'https://openalex.org/W2948130861', 'https://openalex.org/W2948635472', 'https://openalex.org/W2950768109', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963643655', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963813662', 'https://openalex.org/W2964240726', 'https://openalex.org/W2964343359', 'https://openalex.org/W2965046076', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970529093', 'https://openalex.org/W2970565456', 'https://openalex.org/W2983981554', 'https://openalex.org/W2988390689', 'https://openalex.org/W2994914025', 'https://openalex.org/W3034230673', 'https://openalex.org/W3045883122', 'https://openalex.org/W4288347855', 'https://openalex.org/W4385245566']","The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish→English. The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with English→German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.",0.9942857142857143
SKG_MT_494,https://openalex.org/W2566623769,2017,38,"['https://openalex.org/W932413789', 'https://openalex.org/W1582482241', 'https://openalex.org/W1585876329', 'https://openalex.org/W1606347560', 'https://openalex.org/W1984032385', 'https://openalex.org/W2100238596', 'https://openalex.org/W2105891181', 'https://openalex.org/W2113021982', 'https://openalex.org/W2114550122', 'https://openalex.org/W2114912785', 'https://openalex.org/W2118434577', 'https://openalex.org/W2118972857', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136657878', 'https://openalex.org/W2141182089', 'https://openalex.org/W2141440284', 'https://openalex.org/W2155382668', 'https://openalex.org/W2161729680', 'https://openalex.org/W2222235228', 'https://openalex.org/W2250303366', 'https://openalex.org/W2250548645', 'https://openalex.org/W2250841445', 'https://openalex.org/W2251678492', 'https://openalex.org/W2345190899', 'https://openalex.org/W2407166119', 'https://openalex.org/W2525778437', 'https://openalex.org/W2539201987', 'https://openalex.org/W2542860122', 'https://openalex.org/W2566564022', 'https://openalex.org/W2576482813', 'https://openalex.org/W2760372894', 'https://openalex.org/W2949555952', 'https://openalex.org/W2952659248', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962907349', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963271675', 'https://openalex.org/W2963324947', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998704965', 'https://openalex.org/W4285719527']",We present a novel scheme to combine neural machine translation (NMT) with traditional statistical machine translation (SMT). Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for SMT. The NMT score is combined with the Bayes-risk of the translation according the SMT lattice. This makes our approach much more flexible than n-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on English-German and Japanese-English and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output.,1.0
SKG_MT_495,https://openalex.org/W3023986361,2020,53,"['https://openalex.org/W2101105183', 'https://openalex.org/W2170973209', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2886095922', 'https://openalex.org/W2887920589', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921280978', 'https://openalex.org/W2928941594', 'https://openalex.org/W2944815030', 'https://openalex.org/W2952468927', 'https://openalex.org/W2952729433', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964045208', 'https://openalex.org/W2970037872', 'https://openalex.org/W2970109976', 'https://openalex.org/W2982439138', 'https://openalex.org/W2983040767', 'https://openalex.org/W2985204668', 'https://openalex.org/W2991040477', 'https://openalex.org/W3035390927', 'https://openalex.org/W3082274269', 'https://openalex.org/W3203397811', 'https://openalex.org/W4232545113', 'https://openalex.org/W4288089799', 'https://openalex.org/W4288555568', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha Kudugunta, Naveen Arivazhagan, Yonghui Wu. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.",1.0
SKG_MT_496,https://openalex.org/W2102845951,2011,9,"['https://openalex.org/W91928571', 'https://openalex.org/W111246054', 'https://openalex.org/W179314280', 'https://openalex.org/W222053410', 'https://openalex.org/W1631260214', 'https://openalex.org/W1975638594', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008652694', 'https://openalex.org/W2020431608', 'https://openalex.org/W2028162617', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103149536', 'https://openalex.org/W2108452152', 'https://openalex.org/W2110104386', 'https://openalex.org/W2118439278', 'https://openalex.org/W2126270798', 'https://openalex.org/W2126610017', 'https://openalex.org/W2137143056', 'https://openalex.org/W2137807925', 'https://openalex.org/W2139183784', 'https://openalex.org/W2139403546', 'https://openalex.org/W2140343992', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148675933', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047', 'https://openalex.org/W2160697141', 'https://openalex.org/W2437005631', 'https://openalex.org/W2998215494']","Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets. 1",1.0
SKG_MT_497,https://openalex.org/W2757578620,2017,18,"['https://openalex.org/W179875071', 'https://openalex.org/W1614298861', 'https://openalex.org/W1970689298', 'https://openalex.org/W2126725946', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153579005', 'https://openalex.org/W2164984707', 'https://openalex.org/W2250771471', 'https://openalex.org/W2251333340', 'https://openalex.org/W2251468402', 'https://openalex.org/W2508963464', 'https://openalex.org/W2512232685', 'https://openalex.org/W2950577311', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998704965', 'https://openalex.org/W3014460506', 'https://openalex.org/W4285719527']","Machine translation quality estimation is a challenging task in the WMT evaluation campaign.Feature extraction plays an important role in automatic quality estimation, and in this paper, we propose neural network features, including embedding features and cross-entropy features of source sentences and machine translations, to improve machine translation quality estimation.The sentence embedding features are extracted through global average pooling from word embedding and are trained by the word2vec toolkits, while the sentence crossentropy features are calculated by the recurrent neural network language model.The experimental results on the development set of WMT17 machine translation quality estimation tasks show that the neural network features gain significant improvements over the baseline.Furthermore, when combining the neural network features and the baseline features, the system performance obtains further improvement.",1.0
SKG_MT_499,https://openalex.org/W3093206688,2020,1,"['https://openalex.org/W1821462560', 'https://openalex.org/W1905522558', 'https://openalex.org/W2147262247', 'https://openalex.org/W2148708890', 'https://openalex.org/W2159755860', 'https://openalex.org/W2194775991', 'https://openalex.org/W2587694128', 'https://openalex.org/W2889326796', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970210628', 'https://openalex.org/W2970731908', 'https://openalex.org/W2970947975', 'https://openalex.org/W3025490068', 'https://openalex.org/W3204406378']","This paper describes DiDi AI Labs' submission to the WMT2020 news translation shared task. We participate in the translation direction of Chinese-&gt;English. In this direction, we use the Transformer as our baseline model, and integrate several techniques for model enhancement, including data filtering, data selection, back-translation, fine-tuning, model ensembling, and re-ranking. As a result, our submission achieves a BLEU score of $36.6$ in Chinese-&gt;English.",1.0
SKG_MT_500,https://openalex.org/W2145662801,2013,26,"['https://openalex.org/W38126138', 'https://openalex.org/W92412080', 'https://openalex.org/W191409295', 'https://openalex.org/W222053410', 'https://openalex.org/W1631260214', 'https://openalex.org/W2041232209', 'https://openalex.org/W2082571252', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102749417', 'https://openalex.org/W2103042430', 'https://openalex.org/W2112900913', 'https://openalex.org/W2115289978', 'https://openalex.org/W2116780029', 'https://openalex.org/W2117339222', 'https://openalex.org/W2121415745', 'https://openalex.org/W2121745180', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2124935176', 'https://openalex.org/W2138753018', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2140460368', 'https://openalex.org/W2140903445', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165666205', 'https://openalex.org/W2169360026', 'https://openalex.org/W2172138510', 'https://openalex.org/W2172268343', 'https://openalex.org/W2250282568', 'https://openalex.org/W2407127455', 'https://openalex.org/W2437005631', 'https://openalex.org/W2804470177']","Currently, almost all of the statistical ma-chine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a lan-guage pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for in-ducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effec-tive method to induce a phrase-based model from the monolingual corpora given an au-tomatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1",1.0
SKG_MT_501,https://openalex.org/W2950207430,2019,82,"['https://openalex.org/W8316075', 'https://openalex.org/W44695385', 'https://openalex.org/W1514535095', 'https://openalex.org/W1522301498', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2185175083', 'https://openalex.org/W2194775991', 'https://openalex.org/W2247931231', 'https://openalex.org/W2250539671', 'https://openalex.org/W2345720230', 'https://openalex.org/W2509282593', 'https://openalex.org/W2513263213', 'https://openalex.org/W2516756687', 'https://openalex.org/W2532807140', 'https://openalex.org/W2540413092', 'https://openalex.org/W2568262903', 'https://openalex.org/W2581101319', 'https://openalex.org/W2593341061', 'https://openalex.org/W2726413184', 'https://openalex.org/W2752047430', 'https://openalex.org/W2760425631', 'https://openalex.org/W2760656271', 'https://openalex.org/W2794365787', 'https://openalex.org/W2799615702', 'https://openalex.org/W2889545026', 'https://openalex.org/W2889720786', 'https://openalex.org/W2896234464', 'https://openalex.org/W2902031175', 'https://openalex.org/W2902170865', 'https://openalex.org/W2903049941', 'https://openalex.org/W2903343986', 'https://openalex.org/W2909620036', 'https://openalex.org/W2962780935', 'https://openalex.org/W2963270032', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963349562', 'https://openalex.org/W2963360627', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963407669', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963526187', 'https://openalex.org/W2963898017', 'https://openalex.org/W2963988211', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964192290', 'https://openalex.org/W2964258722', 'https://openalex.org/W2964308564', 'https://openalex.org/W2972568911', 'https://openalex.org/W2972972637', 'https://openalex.org/W3006530332', 'https://openalex.org/W3042679443', 'https://openalex.org/W3104132024', 'https://openalex.org/W3106013275', 'https://openalex.org/W3121480429', 'https://openalex.org/W4297747548', 'https://openalex.org/W4297779694', 'https://openalex.org/W4385245566']","Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",1.0
SKG_MT_502,https://openalex.org/W2951718443,2018,31,"['https://openalex.org/W131533222', 'https://openalex.org/W174630521', 'https://openalex.org/W1522301498', 'https://openalex.org/W1814992895', 'https://openalex.org/W1854884267', 'https://openalex.org/W1980776243', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103305545', 'https://openalex.org/W2104246439', 'https://openalex.org/W2107130271', 'https://openalex.org/W2114524997', 'https://openalex.org/W2123442489', 'https://openalex.org/W2126400076', 'https://openalex.org/W2133458109', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136189984', 'https://openalex.org/W2143927888', 'https://openalex.org/W2145908128', 'https://openalex.org/W2152180407', 'https://openalex.org/W2159849140', 'https://openalex.org/W2160660844', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250565861', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251047310', 'https://openalex.org/W2251780596', 'https://openalex.org/W2251861449', 'https://openalex.org/W2251869843', 'https://openalex.org/W2251919380', 'https://openalex.org/W2251939518', 'https://openalex.org/W2289952147', 'https://openalex.org/W2384495648', 'https://openalex.org/W2462305634', 'https://openalex.org/W2605035112', 'https://openalex.org/W2606347107', 'https://openalex.org/W2607892599', 'https://openalex.org/W2611248707', 'https://openalex.org/W2622000134', 'https://openalex.org/W2739823599', 'https://openalex.org/W2740711318', 'https://openalex.org/W2741049976', 'https://openalex.org/W2741990823', 'https://openalex.org/W2751762827', 'https://openalex.org/W2752172973', 'https://openalex.org/W2798139452', 'https://openalex.org/W2899771611', 'https://openalex.org/W2949547296', 'https://openalex.org/W2949548130', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950133940', 'https://openalex.org/W2950726992', 'https://openalex.org/W2950910987', 'https://openalex.org/W2952186591', 'https://openalex.org/W2952688536', 'https://openalex.org/W2953084091', 'https://openalex.org/W2963499246', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963918774', 'https://openalex.org/W2964025273', 'https://openalex.org/W3104033643', 'https://openalex.org/W3106510757']","We describe PARANMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.",0.9952153110047847
SKG_MT_505,https://openalex.org/W2962886257,2018,33,"['https://openalex.org/W6908809', 'https://openalex.org/W1411230545', 'https://openalex.org/W1902237438', 'https://openalex.org/W1904365287', 'https://openalex.org/W1924770834', 'https://openalex.org/W2101105183', 'https://openalex.org/W2126725946', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157331557', 'https://openalex.org/W2437005631', 'https://openalex.org/W2613904329', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962834107', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","In neural machine translation, a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase. Differently from statistical machine translation, the associations between source words and their possible target counterparts are not explicitly stored. Source and target words are at the two ends of a long information processing procedure, mediated by hidden states at both the source encoding and the target decoding phases. This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language. In this paper, we seek to somewhat shorten the distance between source and target words in that procedure, and thus strengthen their association, by means of a method we term bridging source and target word embeddings. We experiment with three strategies: (1) a source-side bridging model, where source word embeddings are moved one step closer to the output target sequence; (2) a target-side bridging model, which explores the more relevant source word embeddings for the prediction of the target sequence; and (3) a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation of ones by the others. Experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly improve quality of both sentence translation, in general, and alignment and translation of individual source words with target words, in particular.",1.0
SKG_MT_506,https://openalex.org/W2950195913,2019,0,"['https://openalex.org/W22168010', 'https://openalex.org/W1902237438', 'https://openalex.org/W1972594981', 'https://openalex.org/W2068297964', 'https://openalex.org/W2071094782', 'https://openalex.org/W2143017621', 'https://openalex.org/W2149945051', 'https://openalex.org/W2153579005', 'https://openalex.org/W2250400351', 'https://openalex.org/W2624871570', 'https://openalex.org/W2883616626', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964308564', 'https://openalex.org/W3021520429']","We describe a multi-task learning approach to train a Neural Machine Translation (NMT) model with a Relevance-based Auxiliary Task (RAT) for search query translation. The translation process for Cross-lingual Information Retrieval (CLIR) task is usually treated as a black box and it is performed as an independent step. However, an NMT model trained on sentence-level parallel data is not aware of the vocabulary distribution of the retrieval corpus. We address this problem with our multi-task learning architecture that achieves 16% improvement over a strong NMT baseline on Italian-English query-document dataset. We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization effect it achieves from multi-task learning paradigm.",1.0
SKG_MT_507,https://openalex.org/W2798651744,2018,16,"['https://openalex.org/W10957333', 'https://openalex.org/W58567859', 'https://openalex.org/W410072552', 'https://openalex.org/W1492002839', 'https://openalex.org/W1518951372', 'https://openalex.org/W1591706642', 'https://openalex.org/W1796112755', 'https://openalex.org/W1902237438', 'https://openalex.org/W1995519658', 'https://openalex.org/W2038411619', 'https://openalex.org/W2051593977', 'https://openalex.org/W2075382077', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116492146', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130571329', 'https://openalex.org/W2251780596', 'https://openalex.org/W2251994258', 'https://openalex.org/W2252027930', 'https://openalex.org/W2311783643', 'https://openalex.org/W2467834614', 'https://openalex.org/W2508354372', 'https://openalex.org/W2561658355', 'https://openalex.org/W2567639685', 'https://openalex.org/W2606974598', 'https://openalex.org/W2622000134', 'https://openalex.org/W2735642330', 'https://openalex.org/W2739810148', 'https://openalex.org/W2741049976', 'https://openalex.org/W2758334418', 'https://openalex.org/W2805572053', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950178297', 'https://openalex.org/W2951184134', 'https://openalex.org/W2952250844', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963366196', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964008635', 'https://openalex.org/W2964053384', 'https://openalex.org/W2964308564']","Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.",1.0
SKG_MT_511,https://openalex.org/W2945383715,2019,124,"['https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W1844261860', 'https://openalex.org/W2111051539', 'https://openalex.org/W2113839990', 'https://openalex.org/W2124807415', 'https://openalex.org/W2474694648', 'https://openalex.org/W2560647685', 'https://openalex.org/W2567571499', 'https://openalex.org/W2595715041', 'https://openalex.org/W2735676397', 'https://openalex.org/W2744813330', 'https://openalex.org/W2760656271', 'https://openalex.org/W2778814079', 'https://openalex.org/W2783060833', 'https://openalex.org/W2794561587', 'https://openalex.org/W2803241009', 'https://openalex.org/W2806412155', 'https://openalex.org/W2886342729', 'https://openalex.org/W2949920209', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962863357', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964067969', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964309400', 'https://openalex.org/W3082674894', 'https://openalex.org/W3086499488', 'https://openalex.org/W3204406378']","Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as <i>catastrophic forgetting</i> of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)—a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading indomain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.",1.0
SKG_MT_512,https://openalex.org/W3035577668,2020,104,"['https://openalex.org/W769612788', 'https://openalex.org/W1673923490', 'https://openalex.org/W2099471712', 'https://openalex.org/W2133564696', 'https://openalex.org/W2151239833', 'https://openalex.org/W2525778437', 'https://openalex.org/W2561274697', 'https://openalex.org/W2613904329', 'https://openalex.org/W2765407302', 'https://openalex.org/W2767899794', 'https://openalex.org/W2811010710', 'https://openalex.org/W2885593519', 'https://openalex.org/W2888519496', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896388224', 'https://openalex.org/W2922293812', 'https://openalex.org/W2946817437', 'https://openalex.org/W2950651087', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963399829', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963823140', 'https://openalex.org/W2964040467', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964153729', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970558573', 'https://openalex.org/W2978426779', 'https://openalex.org/W2984051011', 'https://openalex.org/W2990812465', 'https://openalex.org/W3098341425', 'https://openalex.org/W4300996741', 'https://openalex.org/W4307459710', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566']","In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning. Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.",0.9928057553956835
SKG_MT_513,https://openalex.org/W2517953077,2016,6,"['https://openalex.org/W91928571', 'https://openalex.org/W1969974515', 'https://openalex.org/W1973923101', 'https://openalex.org/W2016522586', 'https://openalex.org/W2095650036', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111142112', 'https://openalex.org/W2113199498', 'https://openalex.org/W2114060876', 'https://openalex.org/W2117827367', 'https://openalex.org/W2123635983', 'https://openalex.org/W2124322414', 'https://openalex.org/W2124807415', 'https://openalex.org/W2136156618', 'https://openalex.org/W2143564602', 'https://openalex.org/W2156985047', 'https://openalex.org/W2161772046', 'https://openalex.org/W2161792612', 'https://openalex.org/W2171421863', 'https://openalex.org/W2180952760', 'https://openalex.org/W2183711971', 'https://openalex.org/W2595715041', 'https://openalex.org/W2796084947', 'https://openalex.org/W2989631226', 'https://openalex.org/W3100617654']","We present a novel technique for training translation models for statistical machine translation by aligning source sentences to their oracle-BLEU translations.In contrast to previous approaches which are constrained to phrase training, our method also allows the re-estimation of reordering models along with the translation model.Experiments show an improvement of up to 0.8 BLEU for our approach over a competitive Arabic-English baseline trained directly on the word-aligned bitext using heuristic extraction.As an additional benefit, the phrase table size is reduced dramatically to only 3% of the original size.",1.0
SKG_MT_515,https://openalex.org/W2251311344,2013,106,"['https://openalex.org/W105964407', 'https://openalex.org/W1746819321', 'https://openalex.org/W1897658145', 'https://openalex.org/W1914338220', 'https://openalex.org/W1970381522', 'https://openalex.org/W2021618504', 'https://openalex.org/W2043948277', 'https://openalex.org/W2057235967', 'https://openalex.org/W2083630376', 'https://openalex.org/W2087735403', 'https://openalex.org/W2098865355', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103288873', 'https://openalex.org/W2109743529', 'https://openalex.org/W2111787673', 'https://openalex.org/W2119595900', 'https://openalex.org/W2119830539', 'https://openalex.org/W2120354757', 'https://openalex.org/W2125943921', 'https://openalex.org/W2126704587', 'https://openalex.org/W2134305421', 'https://openalex.org/W2142518823', 'https://openalex.org/W2144746247', 'https://openalex.org/W2144752499', 'https://openalex.org/W2145790651', 'https://openalex.org/W2146611938', 'https://openalex.org/W2149273804', 'https://openalex.org/W2160001241', 'https://openalex.org/W2186839874', 'https://openalex.org/W2895810819']","Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and levels of consistency. We present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour. These techniques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data. Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently outperform strong baselines.",0.9957081545064378
SKG_MT_516,https://openalex.org/W4289291951,2018,1,[],"In this paper we present the ADAPT system built for the&#13;\nBasque to English Low Resource MT Evaluation Campaign.&#13;\nBasque is a low-resourced, morphologically-rich language.&#13;\nThis poses a challenge for Neural Machine Translation models which usually achieve better performance when trained&#13;\nwith large sets of data.&#13;\nAccordingly, we used synthetic data to improve the translation quality produced by a model built using only authentic&#13;\ndata. Our proposal uses back-translated data to: (a) create&#13;\nnew sentences, so the system can be trained with more data;&#13;\nand (b) translate sentences that are close to the test set, so the&#13;\nmodel can be fine-tuned to the document to be translated.",0.9879518072289156
SKG_MT_517,https://openalex.org/W2964125283,2017,49,"['https://openalex.org/W1753482797', 'https://openalex.org/W1869752048', 'https://openalex.org/W1902237438', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2576713500', 'https://openalex.org/W2951008357', 'https://openalex.org/W2962732637', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963333747', 'https://openalex.org/W2964308564']","Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer, Alex Smola. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. 2017.",1.0
SKG_MT_519,https://openalex.org/W2926977875,2019,295,"['https://openalex.org/W1498502445', 'https://openalex.org/W1933065844', 'https://openalex.org/W1962416794', 'https://openalex.org/W1966601882', 'https://openalex.org/W2113295199', 'https://openalex.org/W2117539524', 'https://openalex.org/W2118781169', 'https://openalex.org/W2132446817', 'https://openalex.org/W2155007355', 'https://openalex.org/W2176263492', 'https://openalex.org/W2189089430', 'https://openalex.org/W2194775991', 'https://openalex.org/W2236233024', 'https://openalex.org/W2342840547', 'https://openalex.org/W2593841437', 'https://openalex.org/W2609009256', 'https://openalex.org/W2612675303', 'https://openalex.org/W2636355936', 'https://openalex.org/W2774005037', 'https://openalex.org/W2783375473', 'https://openalex.org/W2805984364', 'https://openalex.org/W2884565639', 'https://openalex.org/W2886095922', 'https://openalex.org/W2888519496', 'https://openalex.org/W2889326796', 'https://openalex.org/W2909303996', 'https://openalex.org/W2951660448', 'https://openalex.org/W2962684798', 'https://openalex.org/W2962843773', 'https://openalex.org/W2962887844', 'https://openalex.org/W2962957005', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963367210', 'https://openalex.org/W2963447367', 'https://openalex.org/W2963631950', 'https://openalex.org/W2963726321', 'https://openalex.org/W2963768805', 'https://openalex.org/W2963800628', 'https://openalex.org/W2963846044', 'https://openalex.org/W2964043796', 'https://openalex.org/W2964161785', 'https://openalex.org/W2964217371', 'https://openalex.org/W2964339842', 'https://openalex.org/W2964935470', 'https://openalex.org/W3149442417', 'https://openalex.org/W3150392384', 'https://openalex.org/W4246154640']","Hao Tan, Licheng Yu, Mohit Bansal. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",0.9940828402366864
SKG_MT_521,https://openalex.org/W3119347320,2020,2,"['https://openalex.org/W1973905246', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153508793', 'https://openalex.org/W2153653739', 'https://openalex.org/W2555428947', 'https://openalex.org/W2594047108', 'https://openalex.org/W2756566411', 'https://openalex.org/W2889326796', 'https://openalex.org/W2903193068', 'https://openalex.org/W2919290281', 'https://openalex.org/W2922709902', 'https://openalex.org/W2944815030', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970247882', 'https://openalex.org/W3035464238', 'https://openalex.org/W3107826490']","In this paper, we describe IIT Delhi’s submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi   Marathi and Spanish   Portuguese. We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models. For our best submissions, we fine-tune the mBART model on the parallel data provided for the task. The pre-training is done using self-supervised objectives on a large amount of monolingual data for many languages. Overall, our models are ranked in the top four of all systems for the submitted language pairs, with first rank in Spanish -> Portuguese.",0.9949238578680203
SKG_MT_522,https://openalex.org/W2948197522,2019,58,"['https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2111142112', 'https://openalex.org/W2116316001', 'https://openalex.org/W2123442489', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144002870', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152263452', 'https://openalex.org/W2158388102', 'https://openalex.org/W2165666205', 'https://openalex.org/W2168966090', 'https://openalex.org/W2251222643', 'https://openalex.org/W2407166119', 'https://openalex.org/W2545625743', 'https://openalex.org/W2563574619', 'https://openalex.org/W2594047108', 'https://openalex.org/W2767206889', 'https://openalex.org/W2789543585', 'https://openalex.org/W2888882903', 'https://openalex.org/W2890501761', 'https://openalex.org/W2892213699', 'https://openalex.org/W2932376173', 'https://openalex.org/W2949680570', 'https://openalex.org/W2953130735', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962788148', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963126845', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963946353', 'https://openalex.org/W2964026424', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964308564', 'https://openalex.org/W4289302788', 'https://openalex.org/W4385245566']","Standard decoders for neural machine translation autoregressively generate a\nsingle target token per time step, which slows inference especially for long\noutputs. While architectural advances such as the Transformer fully parallelize\nthe decoder computations at training time, inference still proceeds\nsequentially. Recent developments in non- and semi- autoregressive decoding\nproduce multiple tokens per time step independently of the others, which\nimproves inference speed but deteriorates translation quality. In this work, we\npropose the syntactically supervised Transformer (SynST), which first\nautoregressively predicts a chunked parse tree before generating all of the\ntarget tokens in one shot conditioned on the predicted parse. A series of\ncontrolled experiments demonstrates that SynST decodes sentences ~ 5x faster\nthan the baseline autoregressive Transformer while achieving higher BLEU scores\nthan most competing methods on En-De and En-Fr datasets.\n",1.0
SKG_MT_525,https://openalex.org/W3035313462,2020,42,"['https://openalex.org/W639708223', 'https://openalex.org/W1522301498', 'https://openalex.org/W1523385540', 'https://openalex.org/W1527575280', 'https://openalex.org/W1753482797', 'https://openalex.org/W1861492603', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2509282593', 'https://openalex.org/W2513263213', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2548228487', 'https://openalex.org/W2550821151', 'https://openalex.org/W2573834658', 'https://openalex.org/W2581101319', 'https://openalex.org/W2595715041', 'https://openalex.org/W2597655663', 'https://openalex.org/W2610245951', 'https://openalex.org/W2613718673', 'https://openalex.org/W2745461083', 'https://openalex.org/W2808084195', 'https://openalex.org/W2888070626', 'https://openalex.org/W2891555348', 'https://openalex.org/W2896234464', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2944815030', 'https://openalex.org/W2950162424', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962793481', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962830144', 'https://openalex.org/W2962964995', 'https://openalex.org/W2962968835', 'https://openalex.org/W2963084599', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963360627', 'https://openalex.org/W2963386218', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963407669', 'https://openalex.org/W2963496089', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963909453', 'https://openalex.org/W2963988211', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964192290', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970632400', 'https://openalex.org/W2980216782', 'https://openalex.org/W2981473723', 'https://openalex.org/W3010232603', 'https://openalex.org/W4244167344', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.",1.0
SKG_MT_526,https://openalex.org/W3092490633,2020,0,"['https://openalex.org/W2758087204', 'https://openalex.org/W2760656271', 'https://openalex.org/W2920538220', 'https://openalex.org/W2947898088', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964026424', 'https://openalex.org/W2971167892', 'https://openalex.org/W2988975212', 'https://openalex.org/W2996843693', 'https://openalex.org/W3000840023']","Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard ""mask-predict"" algorithm, and provide analyses of its behavior on machine translation tasks.",1.0
SKG_MT_530,https://openalex.org/W2159919698,2014,3,"['https://openalex.org/W193437090', 'https://openalex.org/W2010835028', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2132069549', 'https://openalex.org/W2144636037', 'https://openalex.org/W2152803722', 'https://openalex.org/W2153508793', 'https://openalex.org/W2175969983', 'https://openalex.org/W2250947025', 'https://openalex.org/W2251997274', 'https://openalex.org/W2293111166', 'https://openalex.org/W3204893721']","Scrambling is acceptable reordering of verb arguments in languages such as Japanese and German. In automatic evaluation of translation quality, BLEU is the de facto standard method, but BLEU has only very weak correlation with human judgements in case of Japanese-toEnglish/English-to-Japanese translations. Therefore, alternative methods, IMPACT and RIBES, were proposed and they have shown much stronger correlation than BLEU. Now, RIBES is widely used in recent papers on Japanese-related translations. RIBES compares word order of MT output with manually translated reference sentences but it does not regard scrambling at all. In this paper, we present a method to enumerate scrambled sentences from dependency trees of reference sentences. Our experiments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy.",1.0
SKG_MT_531,https://openalex.org/W3034871396,2020,142,"['https://openalex.org/W639708223', 'https://openalex.org/W1501856433', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133459682', 'https://openalex.org/W2345720230', 'https://openalex.org/W2513263213', 'https://openalex.org/W2581101319', 'https://openalex.org/W2593341061', 'https://openalex.org/W2613718673', 'https://openalex.org/W2726413184', 'https://openalex.org/W2772470221', 'https://openalex.org/W2798749466', 'https://openalex.org/W2888070626', 'https://openalex.org/W2889646190', 'https://openalex.org/W2889903020', 'https://openalex.org/W2896234464', 'https://openalex.org/W2902031175', 'https://openalex.org/W2903343986', 'https://openalex.org/W2950207430', 'https://openalex.org/W2950886580', 'https://openalex.org/W2950898568', 'https://openalex.org/W2962779575', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962964995', 'https://openalex.org/W2963360627', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963497309', 'https://openalex.org/W2963521239', 'https://openalex.org/W2963532541', 'https://openalex.org/W2963858333', 'https://openalex.org/W2963898017', 'https://openalex.org/W2963988211', 'https://openalex.org/W2964015378', 'https://openalex.org/W2964113829', 'https://openalex.org/W2964192290', 'https://openalex.org/W2964345214', 'https://openalex.org/W2964400841', 'https://openalex.org/W2964972381', 'https://openalex.org/W2970231061', 'https://openalex.org/W2987734933', 'https://openalex.org/W2995306656', 'https://openalex.org/W3004349648', 'https://openalex.org/W3010232603', 'https://openalex.org/W3099405210', 'https://openalex.org/W4285723986', 'https://openalex.org/W4288318811', 'https://openalex.org/W4297734604', 'https://openalex.org/W4297780100', 'https://openalex.org/W4299522971', 'https://openalex.org/W4385245566']","Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",1.0
SKG_MT_532,https://openalex.org/W2803369080,2018,27,"['https://openalex.org/W6908809', 'https://openalex.org/W1411230545', 'https://openalex.org/W1514535095', 'https://openalex.org/W1902237438', 'https://openalex.org/W1996430422', 'https://openalex.org/W2064384528', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143104527', 'https://openalex.org/W2144169942', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2211192759', 'https://openalex.org/W2293111166', 'https://openalex.org/W2466062786', 'https://openalex.org/W2509593407', 'https://openalex.org/W2576713500', 'https://openalex.org/W2594229957', 'https://openalex.org/W2595715041', 'https://openalex.org/W2610635252', 'https://openalex.org/W2622084140', 'https://openalex.org/W2752556641', 'https://openalex.org/W2752818698', 'https://openalex.org/W2768763386', 'https://openalex.org/W2769298630', 'https://openalex.org/W2952479981', 'https://openalex.org/W2962811598', 'https://openalex.org/W2962834107', 'https://openalex.org/W2963185998', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963311117', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964097922', 'https://openalex.org/W2964308564', 'https://openalex.org/W3211259717', 'https://openalex.org/W4241645538']","Xintong Li, Lemao Liu, Zhaopeng Tu, Shuming Shi, Max Meng. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",1.0
SKG_MT_533,https://openalex.org/W2736986829,2017,3,"['https://openalex.org/W1986569439', 'https://openalex.org/W2117642127', 'https://openalex.org/W2119741585', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250332179', 'https://openalex.org/W2586524454', 'https://openalex.org/W2621587759', 'https://openalex.org/W2741026152', 'https://openalex.org/W2757041753', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2964308564']","NMT systems have problems with large vocabulary sizes. Byte-pair encoding (BPE) is a popular approach to solving this problem, but while BPE allows the system to generate any target-side word, it does not enable effective generalization over the rich vocabulary in morphologically rich languages with strong inflectional phenomena. We introduce a simple approach to overcome this problem by training a system to produce the lemma of a word and its morphologically rich POS tag, which is then followed by a deterministic generation step. We apply this strategy for English-Czech and English-German translation scenarios, obtaining improvements in both settings. We furthermore show that the improvement is not due to only adding explicit morphological information.",1.0
SKG_MT_535,https://openalex.org/W2115161902,2012,40,"['https://openalex.org/W222053410', 'https://openalex.org/W1612003148', 'https://openalex.org/W1631260214', 'https://openalex.org/W1880262756', 'https://openalex.org/W2004447574', 'https://openalex.org/W2033593667', 'https://openalex.org/W2066296458', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105577415', 'https://openalex.org/W2111142112', 'https://openalex.org/W2116229791', 'https://openalex.org/W2122270629', 'https://openalex.org/W2123126659', 'https://openalex.org/W2132001515', 'https://openalex.org/W2140460368', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149971620', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2154581043', 'https://openalex.org/W2156985047', 'https://openalex.org/W2437005631', 'https://openalex.org/W2749592723', 'https://openalex.org/W3099640513']","Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level. 1",1.0
SKG_MT_536,https://openalex.org/W3118247774,2020,3,"['https://openalex.org/W630532510', 'https://openalex.org/W2148708890', 'https://openalex.org/W2542860122', 'https://openalex.org/W2740743644', 'https://openalex.org/W2903297715', 'https://openalex.org/W2933138175', 'https://openalex.org/W2950940239', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2970059135', 'https://openalex.org/W2970295111', 'https://openalex.org/W2971345087', 'https://openalex.org/W3014114301']","This paper describes Huawei’s submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine translation performance. We utilize a transfer learning strategy through pre-trained machine translation models and extensive scope of engineering endeavors. Four of our ten submissions achieve state-of-the-art performance according to the official automatic evaluation results, namely translation directions on English French, English->German and English->Italian.",1.0
SKG_MT_539,https://openalex.org/W2970295111,2019,315,"['https://openalex.org/W2117278770', 'https://openalex.org/W2134800885', 'https://openalex.org/W2169200297', 'https://openalex.org/W2183341477', 'https://openalex.org/W2561274697', 'https://openalex.org/W2595715041', 'https://openalex.org/W2889326796', 'https://openalex.org/W2933138175', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963672008', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970694516', 'https://openalex.org/W2975153649', 'https://openalex.org/W2988451549', 'https://openalex.org/W2988739750', 'https://openalex.org/W2995746049', 'https://openalex.org/W3032816972', 'https://openalex.org/W3046368065', 'https://openalex.org/W3107826490', 'https://openalex.org/W4288400010', 'https://openalex.org/W4385245566']","This paper describes Facebook FAIR’s submission to the WMT19 shared news translation task. We participate in four language directions, English <-> German and English <-> Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system’s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English→Russian.",1.0
SKG_MT_540,https://openalex.org/W2914924671,2018,117,"['https://openalex.org/W1632114991', 'https://openalex.org/W2064675550', 'https://openalex.org/W2159636675', 'https://openalex.org/W2512924740', 'https://openalex.org/W2605717780', 'https://openalex.org/W2760656271', 'https://openalex.org/W2773956126', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962777840', 'https://openalex.org/W2963756346', 'https://openalex.org/W4302343710']","Recently, researchers have found that deep LSTMs trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech. These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives—language modeling, translation, skip-thought, and autoencoding—on their ability to induce syntactic and part-of-speech information, holding constant the quantity and genre of the training data, as well as the LSTM architecture.",0.9956331877729258
SKG_MT_541,https://openalex.org/W3035174060,2020,20,"['https://openalex.org/W2144746247', 'https://openalex.org/W2149327368', 'https://openalex.org/W2160001241', 'https://openalex.org/W2164984707', 'https://openalex.org/W2251150371', 'https://openalex.org/W2251557434', 'https://openalex.org/W2560730294', 'https://openalex.org/W2803437449', 'https://openalex.org/W2896457183', 'https://openalex.org/W2914120296', 'https://openalex.org/W2950470622', 'https://openalex.org/W2953072129', 'https://openalex.org/W2962727366', 'https://openalex.org/W2962736243', 'https://openalex.org/W2962843521', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963969878', 'https://openalex.org/W2970156971', 'https://openalex.org/W2971051558', 'https://openalex.org/W2971120958', 'https://openalex.org/W3016211260']","Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation. A carefully engineered ensemble of such models won the QE shared task at WMT19. Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels. Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively.",0.9908256880733946
SKG_MT_544,https://openalex.org/W2970231882,2019,13,"['https://openalex.org/W2101105183', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2242083635', 'https://openalex.org/W2413794162', 'https://openalex.org/W2496235729', 'https://openalex.org/W2549416390', 'https://openalex.org/W2550821151', 'https://openalex.org/W2594047108', 'https://openalex.org/W2595715041', 'https://openalex.org/W2606974598', 'https://openalex.org/W2767019613', 'https://openalex.org/W2772882909', 'https://openalex.org/W2780664814', 'https://openalex.org/W2788330850', 'https://openalex.org/W2809324505', 'https://openalex.org/W2891534142', 'https://openalex.org/W2924210975', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962964385', 'https://openalex.org/W2963062480', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964247056', 'https://openalex.org/W4293350112', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These Web pages have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from English, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 × 16 translation settings. Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and named entities. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for post-editing.",1.0
SKG_MT_545,https://openalex.org/W2250872883,2014,60,"['https://openalex.org/W11792228', 'https://openalex.org/W21337280', 'https://openalex.org/W45109703', 'https://openalex.org/W1497452857', 'https://openalex.org/W1716250762', 'https://openalex.org/W1989658336', 'https://openalex.org/W2001822005', 'https://openalex.org/W2040781285', 'https://openalex.org/W2087735403', 'https://openalex.org/W2093197119', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105891181', 'https://openalex.org/W2120459453', 'https://openalex.org/W2127836646', 'https://openalex.org/W2141440284', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148708890', 'https://openalex.org/W2154099718', 'https://openalex.org/W2155607551', 'https://openalex.org/W2158614781', 'https://openalex.org/W2158874082', 'https://openalex.org/W2160218441', 'https://openalex.org/W2177801600', 'https://openalex.org/W2180952760', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251994258', 'https://openalex.org/W2437005631', 'https://openalex.org/W2491561060', 'https://openalex.org/W2911033213', 'https://openalex.org/W3004726606', 'https://openalex.org/W3009009611', 'https://openalex.org/W3170166771', 'https://openalex.org/W3170731508', 'https://openalex.org/W3173004577', 'https://openalex.org/W3196998944', 'https://openalex.org/W3204864772', 'https://openalex.org/W4235019312']","Using machine translation output as a starting point for human translation has become an increasingly common application of MT.We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system's discriminative parameters with a MIRA step.Individually, these techniques can substantially improve MT quality, even over strong baselines.Moreover, we see super-additive improvements when all three techniques are used in tandem.",0.9942196531791907
SKG_MT_546,https://openalex.org/W2613065256,2017,7,"['https://openalex.org/W2006969979', 'https://openalex.org/W2100664567', 'https://openalex.org/W2194775991', 'https://openalex.org/W2251682575', 'https://openalex.org/W2316776689', 'https://openalex.org/W2463507112', 'https://openalex.org/W2525778437', 'https://openalex.org/W2540404261', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950580142', 'https://openalex.org/W2950855294', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964299589', 'https://openalex.org/W2964308564']","Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems. Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/throughput close to that of a phrasal decoder. We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100 words/sec on single-threaded CPU. We believe this is the best published accuracy/speed trade-off of an NMT system.",0.9947089947089947
SKG_MT_547,https://openalex.org/W2101207453,2010,79,"['https://openalex.org/W162552777', 'https://openalex.org/W647584630', 'https://openalex.org/W1606076566', 'https://openalex.org/W1631260214', 'https://openalex.org/W1995560154', 'https://openalex.org/W2000359198', 'https://openalex.org/W2006969979', 'https://openalex.org/W2049633694', 'https://openalex.org/W2061910127', 'https://openalex.org/W2105891181', 'https://openalex.org/W2115081467', 'https://openalex.org/W2115717119', 'https://openalex.org/W2115918455', 'https://openalex.org/W2121338597', 'https://openalex.org/W2121407916', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126610017', 'https://openalex.org/W2140343992', 'https://openalex.org/W2156945896', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157817599', 'https://openalex.org/W2158388102', 'https://openalex.org/W2160474663', 'https://openalex.org/W2161227214', 'https://openalex.org/W2165132531', 'https://openalex.org/W2166905217', 'https://openalex.org/W2168966090', 'https://openalex.org/W2170120409', 'https://openalex.org/W2437005631']","We present Jane, RWTH’s hierarchical phrase-based translation system, which has been open sourced for the scientific community. This system has been in development at RWTH for the last two years and has been successfully applied in different machine translation evaluations. It includes extensions to the hierarchical approach developed by RWTH as well as other research institutions. In this paper we give an overview of its main features. We also introduce a novel reordering model for the hierarchical phrase-based approach which further enhances translation performance, and analyze the effect some recent extended lexicon models have on the performance of the system. 1",0.9942196531791907
SKG_MT_548,https://openalex.org/W2970286654,2019,36,"['https://openalex.org/W1522301498', 'https://openalex.org/W1731081199', 'https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2165698076', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2567571499', 'https://openalex.org/W2581863816', 'https://openalex.org/W2740718109', 'https://openalex.org/W2740743644', 'https://openalex.org/W2744813330', 'https://openalex.org/W2750588180', 'https://openalex.org/W2756978580', 'https://openalex.org/W2757592053', 'https://openalex.org/W2760452458', 'https://openalex.org/W2785093437', 'https://openalex.org/W2802153702', 'https://openalex.org/W2803241009', 'https://openalex.org/W2805394970', 'https://openalex.org/W2886776719', 'https://openalex.org/W2892244498', 'https://openalex.org/W2905933322', 'https://openalex.org/W2946379889', 'https://openalex.org/W2952650870', 'https://openalex.org/W2954647460', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963149635', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998115938', 'https://openalex.org/W3099405210', 'https://openalex.org/W3204406378', 'https://openalex.org/W4297782088', 'https://openalex.org/W4300835687', 'https://openalex.org/W4385245566']","Jiali Zeng, Yang Liu, Jinsong Su, Yubing Ge, Yaojie Lu, Yongjing Yin, Jiebo Luo. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_549,https://openalex.org/W2626944006,2017,8,"['https://openalex.org/W1986345088', 'https://openalex.org/W2022433169', 'https://openalex.org/W2040711288', 'https://openalex.org/W2130942839', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2186839874', 'https://openalex.org/W2229833550', 'https://openalex.org/W2251333340', 'https://openalex.org/W2514694861', 'https://openalex.org/W2578839743', 'https://openalex.org/W2621975677', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963816901', 'https://openalex.org/W2964308564']","This work presents a novel approach to Automatic Post-Editing (APE) and Word-Level Quality Estimation (QE) using ensembles of specialized Neural Machine Translation (NMT) systems. Word-level features that have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis. We train a suite of NMT models that use different input representations, but share the same output space. These models are then ensembled together, and tuned for both the APE and the QE task. We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework. Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.",1.0
SKG_MT_550,https://openalex.org/W2251098065,2012,121,"['https://openalex.org/W1758382278', 'https://openalex.org/W1934041838', 'https://openalex.org/W1969974515', 'https://openalex.org/W1970689298', 'https://openalex.org/W2006969979', 'https://openalex.org/W2013540053', 'https://openalex.org/W2056250865', 'https://openalex.org/W2066558940', 'https://openalex.org/W2095650036', 'https://openalex.org/W2111355378', 'https://openalex.org/W2111798208', 'https://openalex.org/W2113788796', 'https://openalex.org/W2114211285', 'https://openalex.org/W2117827367', 'https://openalex.org/W2119168550', 'https://openalex.org/W2121227244', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131462252', 'https://openalex.org/W2132109814', 'https://openalex.org/W2132339004', 'https://openalex.org/W2143719855', 'https://openalex.org/W2144879357', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153204578', 'https://openalex.org/W2154099718', 'https://openalex.org/W2158899491', 'https://openalex.org/W2171928131', 'https://openalex.org/W2173880944', 'https://openalex.org/W2294370844', 'https://openalex.org/W2296491785', 'https://openalex.org/W2402547301', 'https://openalex.org/W2403082032', 'https://openalex.org/W2950186769']",International audience,1.0
SKG_MT_552,https://openalex.org/W2946360045,2019,0,"['https://openalex.org/W2000026602', 'https://openalex.org/W2065565011', 'https://openalex.org/W2550821151', 'https://openalex.org/W2594021297', 'https://openalex.org/W2611838487', 'https://openalex.org/W2739862013', 'https://openalex.org/W2756566411', 'https://openalex.org/W2785381227', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890007195', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963259630', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963879527', 'https://openalex.org/W2964266061', 'https://openalex.org/W3166956191']","Benjamin Marie, Atsushi Fujita. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_555,https://openalex.org/W2294699749,2015,95,"['https://openalex.org/W222053410', 'https://openalex.org/W2097137621', 'https://openalex.org/W2115081467', 'https://openalex.org/W2116492146', 'https://openalex.org/W2118593414', 'https://openalex.org/W2131062877', 'https://openalex.org/W2147192413', 'https://openalex.org/W2169279899', 'https://openalex.org/W2180484979', 'https://openalex.org/W2250234233', 'https://openalex.org/W2251431759', 'https://openalex.org/W2251610689', 'https://openalex.org/W2251926178', 'https://openalex.org/W2251994258', 'https://openalex.org/W2252166243', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W4237401773']","Evaluation of segment-level machine translation metrics is currently hampered by: (1) low inter-annotator agreement levels in human assessments; (2) lack of an effective mechanism for evaluation of translations of equal quality; and (3) lack of methods of significance testing improvements over a baseline.In this paper, we provide solutions to each of these challenges and outline a new human evaluation methodology aimed specifically at assessment of segment-level metrics.We replicate the human evaluation component of WMT-13 and reveal that the current state-of-the-art performance of segment-level metrics is better than previously believed.Three segment-level metrics -METEOR, NLEPOR and SENTBLEU-MOSES -are found to correlate with human assessment at a level not significantly outperformed by any other metric in both the individual language pair assessment for Spanish-to-English and the aggregated set of 9 language pairs.",1.0
SKG_MT_556,https://openalex.org/W2946171999,2019,2,"['https://openalex.org/W1516111018', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117539524', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133459682', 'https://openalex.org/W2150807068', 'https://openalex.org/W2183341477', 'https://openalex.org/W2185175083', 'https://openalex.org/W2188365844', 'https://openalex.org/W2194775991', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251367463', 'https://openalex.org/W2293344577', 'https://openalex.org/W2509282593', 'https://openalex.org/W2513263213', 'https://openalex.org/W2516756687', 'https://openalex.org/W2527569769', 'https://openalex.org/W2559703163', 'https://openalex.org/W2581101319', 'https://openalex.org/W2587284713', 'https://openalex.org/W2593341061', 'https://openalex.org/W2798493043', 'https://openalex.org/W2883384628', 'https://openalex.org/W2903343986', 'https://openalex.org/W2915756181', 'https://openalex.org/W2950848235', 'https://openalex.org/W2951548327', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962835968', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963331233', 'https://openalex.org/W2963344439', 'https://openalex.org/W2963407669', 'https://openalex.org/W2963594498', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963909453', 'https://openalex.org/W2963988211', 'https://openalex.org/W2964076537', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964192290', 'https://openalex.org/W2964308564']","In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and Kádár, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the minimum amount of information encoded in the latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).",1.0
SKG_MT_557,https://openalex.org/W2099607809,2011,37,"['https://openalex.org/W1631260214', 'https://openalex.org/W1984417383', 'https://openalex.org/W2038721957', 'https://openalex.org/W2040103855', 'https://openalex.org/W2061910127', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110310640', 'https://openalex.org/W2113343614', 'https://openalex.org/W2115081467', 'https://openalex.org/W2116594867', 'https://openalex.org/W2123301721', 'https://openalex.org/W2129804798', 'https://openalex.org/W2133512280', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2159107349', 'https://openalex.org/W2162499915', 'https://openalex.org/W2164454850', 'https://openalex.org/W2169724380', 'https://openalex.org/W2437005631', 'https://openalex.org/W2597684388', 'https://openalex.org/W2738633406']","Many machine translation evaluation metrics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment. It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. However, to date there has been no unambiguous report that these new metrics can improve a state-of-theart machine translation system over its BLEUtuned baseline. In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in significantly better humanjudged translation quality than the BLEUtuned baseline. TESLA-M in particular is simple and performs well in practice on large datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems. 1",1.0
SKG_MT_558,https://openalex.org/W2803484822,2018,31,"['https://openalex.org/W6908809', 'https://openalex.org/W1026270304', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2517168694', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2572549015', 'https://openalex.org/W2740433069', 'https://openalex.org/W2760452458', 'https://openalex.org/W2771979424', 'https://openalex.org/W2949643137', 'https://openalex.org/W2950621961', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963311117', 'https://openalex.org/W2963324947', 'https://openalex.org/W2964308564']","Huadong Chen, Shujian Huang, David Chiang, Xinyu Dai, Jiajun Chen. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",1.0
SKG_MT_559,https://openalex.org/W2883374288,2018,4,"['https://openalex.org/W867563', 'https://openalex.org/W9638488', 'https://openalex.org/W22168010', 'https://openalex.org/W74908938', 'https://openalex.org/W630883834', 'https://openalex.org/W1543778864', 'https://openalex.org/W1843946026', 'https://openalex.org/W2040711288', 'https://openalex.org/W2051390224', 'https://openalex.org/W2081580037', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113788796', 'https://openalex.org/W2117642127', 'https://openalex.org/W2120615054', 'https://openalex.org/W2125768350', 'https://openalex.org/W2126241965', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141599568', 'https://openalex.org/W2157331557', 'https://openalex.org/W2167723982', 'https://openalex.org/W2169599995', 'https://openalex.org/W2224490803', 'https://openalex.org/W2251495238', 'https://openalex.org/W2563574619', 'https://openalex.org/W2594047108', 'https://openalex.org/W2594229957', 'https://openalex.org/W2737638662', 'https://openalex.org/W2741239863', 'https://openalex.org/W2898200493', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964308564', 'https://openalex.org/W4386506836']","In this paper we incorporate semantic supersensetags and syntactic supertag features into EN–FR and EN–DE factored\nNMT systems. In experiments on various test sets, we observe that such features\n(and particularly when combined) help the\nNMT model training to converge faster\nand improve the model quality according\nto the BLEU scores.",0.9941520467836257
SKG_MT_560,https://openalex.org/W2250952810,2014,21,"['https://openalex.org/W22168010', 'https://openalex.org/W38126138', 'https://openalex.org/W132913264', 'https://openalex.org/W170711724', 'https://openalex.org/W1508577659', 'https://openalex.org/W2062250042', 'https://openalex.org/W2072976288', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102028293', 'https://openalex.org/W2103042430', 'https://openalex.org/W2105673178', 'https://openalex.org/W2121415745', 'https://openalex.org/W2121745180', 'https://openalex.org/W2124807415', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2140903445', 'https://openalex.org/W2143269898', 'https://openalex.org/W2144600658', 'https://openalex.org/W2144642230', 'https://openalex.org/W2145662801', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157830631', 'https://openalex.org/W2159107349', 'https://openalex.org/W2159755860', 'https://openalex.org/W2172268343', 'https://openalex.org/W2250229103', 'https://openalex.org/W2250640401', 'https://openalex.org/W2251302843', 'https://openalex.org/W2397601298', 'https://openalex.org/W2595715041', 'https://openalex.org/W4241645538']","We demonstrate that ""hallucinating"" phrasal translations can significantly improve the quality of machine translation in low resource conditions.Our hallucinated phrase tables consist of entries composed from multiple unigram translations drawn from the baseline phrase table and from translations that are induced from monolingual corpora.The hallucinated phrase table is very noisy.Its translations are low precision but high recall.We counter this by introducing 30 new feature functions (including a variety of monolinguallyestimated features) and by aggressively pruning the phrase table.Our analysis evaluates the intrinsic quality of our hallucinated phrase pairs as well as their impact in end-to-end Spanish-English and Hindi-English MT.",1.0
SKG_MT_561,https://openalex.org/W2988739750,2019,26,"['https://openalex.org/W117861810', 'https://openalex.org/W2025768430', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101210369', 'https://openalex.org/W2250653840', 'https://openalex.org/W2511852057', 'https://openalex.org/W2561274697', 'https://openalex.org/W2573761686', 'https://openalex.org/W2595715041', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798931235', 'https://openalex.org/W2903193068', 'https://openalex.org/W2905027511', 'https://openalex.org/W2913659301', 'https://openalex.org/W2914120296', 'https://openalex.org/W2933138175', 'https://openalex.org/W2948735718', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963626623', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970686691', 'https://openalex.org/W2970694516', 'https://openalex.org/W2970858854', 'https://openalex.org/W2973088264', 'https://openalex.org/W2976223659', 'https://openalex.org/W2982924472', 'https://openalex.org/W2998215494', 'https://openalex.org/W4288101931', 'https://openalex.org/W4288400010', 'https://openalex.org/W4288601832', 'https://openalex.org/W4298393544', 'https://openalex.org/W4300835687']","This paper describes Facebook AI's submission to the WAT 2019 Myanmar-English translation task. Our baseline systems are BPE-based transformer models. We explore methods to leverage monolingual data to improve generalization, including self-training, back-translation and their combination. We further improve results by using noisy channel re-ranking and ensembling. We demonstrate that these techniques can significantly improve not only a system trained with additional monolingual data, but even the baseline system trained exclusively on the provided small parallel dataset. Our system ranks first in both directions according to human evaluation and BLEU, with a gain of over 8 BLEU points above the second best system.",1.0
SKG_MT_562,https://openalex.org/W2971302374,2019,76,"['https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W2047854174', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108677974', 'https://openalex.org/W2111856253', 'https://openalex.org/W2126704587', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144746247', 'https://openalex.org/W2153653739', 'https://openalex.org/W2164411961', 'https://openalex.org/W2183341477', 'https://openalex.org/W2276605909', 'https://openalex.org/W2525778437', 'https://openalex.org/W2581133659', 'https://openalex.org/W2581377246', 'https://openalex.org/W2595715041', 'https://openalex.org/W2600383743', 'https://openalex.org/W2610245951', 'https://openalex.org/W2669742347', 'https://openalex.org/W2727300753', 'https://openalex.org/W2739602161', 'https://openalex.org/W2751802138', 'https://openalex.org/W2756566411', 'https://openalex.org/W2757980860', 'https://openalex.org/W2762464988', 'https://openalex.org/W2794365787', 'https://openalex.org/W2797913374', 'https://openalex.org/W2798931235', 'https://openalex.org/W2838081464', 'https://openalex.org/W2886095922', 'https://openalex.org/W2887516053', 'https://openalex.org/W2888808532', 'https://openalex.org/W2889326796', 'https://openalex.org/W2902608666', 'https://openalex.org/W2950896278', 'https://openalex.org/W2951266961', 'https://openalex.org/W2953072129', 'https://openalex.org/W2962696859', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963645163', 'https://openalex.org/W2963675284', 'https://openalex.org/W2963690739', 'https://openalex.org/W2963708445', 'https://openalex.org/W2964030506', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3098341425', 'https://openalex.org/W4232146016', 'https://openalex.org/W4241645538', 'https://openalex.org/W4289464898', 'https://openalex.org/W4297798436', 'https://openalex.org/W4297801368', 'https://openalex.org/W4298137069', 'https://openalex.org/W4300126339', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","Shuo Wang, Yang Liu, Chao Wang, Huanbo Luan, Maosong Sun. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_565,https://openalex.org/W2251041322,2011,7,"['https://openalex.org/W1514971736', 'https://openalex.org/W1534036668', 'https://openalex.org/W1985602840', 'https://openalex.org/W2006969979', 'https://openalex.org/W2050068097', 'https://openalex.org/W2095743640', 'https://openalex.org/W2096175520', 'https://openalex.org/W2100271871', 'https://openalex.org/W2125838338', 'https://openalex.org/W2147510700', 'https://openalex.org/W2154124206', 'https://openalex.org/W2155359448', 'https://openalex.org/W2171074980', 'https://openalex.org/W2790246984', 'https://openalex.org/W3197744324']","In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of n-grams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work. 1",1.0
SKG_MT_566,https://openalex.org/W2797371199,2018,25,"['https://openalex.org/W1521626219', 'https://openalex.org/W1522301498', 'https://openalex.org/W2098297786', 'https://openalex.org/W2123388068', 'https://openalex.org/W2124725212', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140372282', 'https://openalex.org/W2144600658', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153013403', 'https://openalex.org/W2170527467', 'https://openalex.org/W2175296493', 'https://openalex.org/W2321916036', 'https://openalex.org/W2337363174', 'https://openalex.org/W2481467102', 'https://openalex.org/W2514713644', 'https://openalex.org/W2527845440', 'https://openalex.org/W2555428947', 'https://openalex.org/W2625092622', 'https://openalex.org/W2725082186', 'https://openalex.org/W2726264694', 'https://openalex.org/W2760051178', 'https://openalex.org/W2771693927', 'https://openalex.org/W2785047343', 'https://openalex.org/W2795467683', 'https://openalex.org/W2917324689', 'https://openalex.org/W2917881729', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950577311', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963109131', 'https://openalex.org/W2963266340', 'https://openalex.org/W2964187553', 'https://openalex.org/W3203906782']","Previously, neural methods in grammatical error correction (GEC) did not reach state-of-the-art results compared to phrase-based statistical machine translation (SMT) baselines. We demonstrate parallels between neural GEC and low-resource neural MT and successfully adapt several methods from low-resource MT to neural GEC. We further establish guidelines for trustable results in neural GEC and propose a set of model-independent methods for neural GEC that can be easily applied in most GEC settings. Proposed methods include adding source-side noise, domain-adaptation techniques, a GEC-specific training-objective, transfer learning with monolingual data, and ensembling of independently trained GEC models and language models. The combined effects of these methods result in better than state-of-the-art neural GEC models that outperform previously best neural GEC systems by more than 10% M$^2$ on the CoNLL-2014 benchmark and 5.9% on the JFLEG test set. Non-neural state-of-the-art systems are outperformed by more than 2% on the CoNLL-2014 benchmark and by 4% on JFLEG.",1.0
SKG_MT_567,https://openalex.org/W2161729680,2010,15,"['https://openalex.org/W1582482241', 'https://openalex.org/W1895481600', 'https://openalex.org/W1934041838', 'https://openalex.org/W2000394794', 'https://openalex.org/W2063721546', 'https://openalex.org/W2100238596', 'https://openalex.org/W2105738468', 'https://openalex.org/W2109664771', 'https://openalex.org/W2117617819', 'https://openalex.org/W2132714218', 'https://openalex.org/W2136657878', 'https://openalex.org/W2145631726', 'https://openalex.org/W2159358338']",This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers. We introduce transducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices. 1,1.0
SKG_MT_568,https://openalex.org/W3038092046,2020,8,"['https://openalex.org/W1494198834', 'https://openalex.org/W2113106066', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143612262', 'https://openalex.org/W2157331557', 'https://openalex.org/W2466918907', 'https://openalex.org/W2604763608', 'https://openalex.org/W2806412155', 'https://openalex.org/W2936774411', 'https://openalex.org/W2945700568', 'https://openalex.org/W2949328740', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963679688', 'https://openalex.org/W2963775850', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964308564', 'https://openalex.org/W2982129078', 'https://openalex.org/W2986963494', 'https://openalex.org/W3012492057', 'https://openalex.org/W3015698636', 'https://openalex.org/W3037465386', 'https://openalex.org/W3101648800', 'https://openalex.org/W3186200218', 'https://openalex.org/W4297747548', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394642966', 'https://openalex.org/W4394649814']","In this paper, we describe the system submitted to the IWSLT 2020 Offline Speech Translation Task. We adopt the Transformer architecture coupled with the meta-learning approach to build our end-to-end Speech-to-Text Translation (ST) system. Our meta-learning approach tackles the data scarcity of the ST task by leveraging the data available from Automatic Speech Recognition (ASR) and Machine Translation (MT) tasks. The meta-learning approach combined with synthetic data augmentation techniques improves the model performance significantly and achieves BLEU scores of 24.58, 27.51, and 27.61 on IWSLT test 2015, MuST-C test, and Europarl-ST test sets respectively.",1.0
SKG_MT_570,https://openalex.org/W2970758702,2019,19,"['https://openalex.org/W31621926', 'https://openalex.org/W170721553', 'https://openalex.org/W630532510', 'https://openalex.org/W2009847538', 'https://openalex.org/W2120699290', 'https://openalex.org/W2131744502', 'https://openalex.org/W2131988669', 'https://openalex.org/W2132532452', 'https://openalex.org/W2142016317', 'https://openalex.org/W2294808479', 'https://openalex.org/W2511717148', 'https://openalex.org/W2512924740', 'https://openalex.org/W2572474373', 'https://openalex.org/W2575814162', 'https://openalex.org/W2595715041', 'https://openalex.org/W2741772806', 'https://openalex.org/W2758685863', 'https://openalex.org/W2806049760', 'https://openalex.org/W2806482312', 'https://openalex.org/W2899560680', 'https://openalex.org/W2902510077', 'https://openalex.org/W2903347069', 'https://openalex.org/W2903366149', 'https://openalex.org/W2903367671', 'https://openalex.org/W2949547296', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964343359', 'https://openalex.org/W2970726011', 'https://openalex.org/W2971345087', 'https://openalex.org/W2982629281', 'https://openalex.org/W3104526121', 'https://openalex.org/W4288359139', 'https://openalex.org/W4297747548', 'https://openalex.org/W4385245566']",International audience,0.9958847736625515
SKG_MT_572,https://openalex.org/W2251048776,2013,10,"['https://openalex.org/W23077562', 'https://openalex.org/W74075037', 'https://openalex.org/W222053410', 'https://openalex.org/W936351679', 'https://openalex.org/W1517947178', 'https://openalex.org/W1555286493', 'https://openalex.org/W1631260214', 'https://openalex.org/W1766290689', 'https://openalex.org/W1959983357', 'https://openalex.org/W2048060899', 'https://openalex.org/W2051434435', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107878631', 'https://openalex.org/W2110485445', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131774270', 'https://openalex.org/W2142632103', 'https://openalex.org/W2144499799', 'https://openalex.org/W2144879357', 'https://openalex.org/W2147880316', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2158065314', 'https://openalex.org/W2160986985', 'https://openalex.org/W2242975712', 'https://openalex.org/W2251261488', 'https://openalex.org/W2601690706', 'https://openalex.org/W2766736793', 'https://openalex.org/W3166311385']","In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. Results on five Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average. Results of comparative study with other seven widely used reordering models will also be reported. 1",1.0
SKG_MT_573,https://openalex.org/W3034407863,2020,25,"['https://openalex.org/W2101105183', 'https://openalex.org/W2194775991', 'https://openalex.org/W2531207078', 'https://openalex.org/W2550821151', 'https://openalex.org/W2572474373', 'https://openalex.org/W2946794439', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088785', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963759780', 'https://openalex.org/W2963887123', 'https://openalex.org/W2964010678', 'https://openalex.org/W2970597249', 'https://openalex.org/W4294103325', 'https://openalex.org/W4385245566']","We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.",1.0
SKG_MT_574,https://openalex.org/W2963713328,2016,191,"['https://openalex.org/W222053410', 'https://openalex.org/W592244745', 'https://openalex.org/W1753482797', 'https://openalex.org/W1850742715', 'https://openalex.org/W1902237438', 'https://openalex.org/W1909320841', 'https://openalex.org/W1959608418', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108501770', 'https://openalex.org/W2118434577', 'https://openalex.org/W2118439278', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2173681125', 'https://openalex.org/W2175740768', 'https://openalex.org/W2210838531', 'https://openalex.org/W2212846646', 'https://openalex.org/W2239731672', 'https://openalex.org/W2291126447', 'https://openalex.org/W2595715041', 'https://openalex.org/W2949416428', 'https://openalex.org/W2949888546', 'https://openalex.org/W2962741254', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963641561', 'https://openalex.org/W2964308564']","Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence.In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end.Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations.In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound.Experiments on both Chinese-English and English-German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.",1.0
SKG_MT_575,https://openalex.org/W2963949210,2018,29,"['https://openalex.org/W6908809', 'https://openalex.org/W222053410', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2524428287', 'https://openalex.org/W2525778437', 'https://openalex.org/W2537667581', 'https://openalex.org/W2540404261', 'https://openalex.org/W2584268338', 'https://openalex.org/W2594229957', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2740743644', 'https://openalex.org/W2741838462', 'https://openalex.org/W2756978580', 'https://openalex.org/W2760656271', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963643655', 'https://openalex.org/W2963932569', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W4301368689', 'https://openalex.org/W4385245566']","Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.",1.0
SKG_MT_576,https://openalex.org/W186845358,2012,3,"['https://openalex.org/W29814777', 'https://openalex.org/W131533222', 'https://openalex.org/W155101268', 'https://openalex.org/W1499795620', 'https://openalex.org/W1502916596', 'https://openalex.org/W1523962916', 'https://openalex.org/W1561908597', 'https://openalex.org/W1571278379', 'https://openalex.org/W1647729745', 'https://openalex.org/W1958499836', 'https://openalex.org/W2005754477', 'https://openalex.org/W2038721957', 'https://openalex.org/W2100935296', 'https://openalex.org/W2103362845', 'https://openalex.org/W2117805756', 'https://openalex.org/W2125885330', 'https://openalex.org/W2130158090', 'https://openalex.org/W2136480620', 'https://openalex.org/W2140887277', 'https://openalex.org/W2147422370', 'https://openalex.org/W2158710545', 'https://openalex.org/W2251861449', 'https://openalex.org/W2396767181', 'https://openalex.org/W2525127255', 'https://openalex.org/W2534712034']",This paper describes our participation in the task denominated Cross-Lingual Textual Entailment (CLTE) for content synchronization. We represent an approach to CLTE using machine translation to tackle the problem of multilinguality. Our system resides on machine learning and in the use of WordNet as semantic source knowledge. Results are very promising always achieving results above mean score.,0.9931972789115646
SKG_MT_577,https://openalex.org/W2970852256,2019,1,"['https://openalex.org/W1902237438', 'https://openalex.org/W2124807415', 'https://openalex.org/W2493916176', 'https://openalex.org/W2561274697', 'https://openalex.org/W2739978843', 'https://openalex.org/W2756566411', 'https://openalex.org/W2902614977', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963877297', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964308564']","This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi -> Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.",0.9951690821256038
SKG_MT_579,https://openalex.org/W3034325332,2020,13,"['https://openalex.org/W22168010', 'https://openalex.org/W582134693', 'https://openalex.org/W1902237438', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108677974', 'https://openalex.org/W2115259925', 'https://openalex.org/W2117670920', 'https://openalex.org/W2123301721', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2142112143', 'https://openalex.org/W2149327368', 'https://openalex.org/W2154486363', 'https://openalex.org/W2167433878', 'https://openalex.org/W2250208866', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251693562', 'https://openalex.org/W2252166243', 'https://openalex.org/W2294699749', 'https://openalex.org/W2507833193', 'https://openalex.org/W2512848817', 'https://openalex.org/W2726119835', 'https://openalex.org/W2759332014', 'https://openalex.org/W2760738985', 'https://openalex.org/W2896457183', 'https://openalex.org/W2896860804', 'https://openalex.org/W2904530326', 'https://openalex.org/W2914740698', 'https://openalex.org/W2915573484', 'https://openalex.org/W2936695845', 'https://openalex.org/W2945405384', 'https://openalex.org/W2949555952', 'https://openalex.org/W2950513705', 'https://openalex.org/W2963238274', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963675284', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964059111', 'https://openalex.org/W2964124576', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970152385', 'https://openalex.org/W2970281030', 'https://openalex.org/W2970791445', 'https://openalex.org/W2970971315', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971120622', 'https://openalex.org/W2971302374', 'https://openalex.org/W2996403597', 'https://openalex.org/W4288026164', 'https://openalex.org/W4385245566']","Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.",1.0
SKG_MT_580,https://openalex.org/W2970885024,2019,20,"['https://openalex.org/W1602767452', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2157331557', 'https://openalex.org/W2807006873', 'https://openalex.org/W2809361596', 'https://openalex.org/W2810953419', 'https://openalex.org/W2903193068', 'https://openalex.org/W2963212250', 'https://openalex.org/W2964343359', 'https://openalex.org/W2970279348']","With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available parallel data to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi ⇔ Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved BLEU score 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.",0.9873417721518988
SKG_MT_585,https://openalex.org/W1510632846,2010,6,"['https://openalex.org/W314345615', 'https://openalex.org/W1480857181', 'https://openalex.org/W1631260214', 'https://openalex.org/W1983969726', 'https://openalex.org/W2008961349', 'https://openalex.org/W2016856586', 'https://openalex.org/W2054533749', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106818711', 'https://openalex.org/W2119825066', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132446289', 'https://openalex.org/W2136657878', 'https://openalex.org/W2137387514', 'https://openalex.org/W2156985047', 'https://openalex.org/W2171421863', 'https://openalex.org/W2242975712', 'https://openalex.org/W2404928265']","In this paper we report on experiments with three preprocessing strategies for improving translation output in a statistical MT system. In training, two reordering strategies were studied: (i) reorder on the basis of the alignments from Giza++, and (ii) reorder by moving all verbs to the end of segments. In translation, out-of-vocabulary words were preprocessed in a knowledge-lite fashion to identify a likely equivalent. All three strategies were implemented for our English↔German system submitted to the WMT10 shared task. Combining them lead to improvements in both language directions.",0.9925925925925926
SKG_MT_586,https://openalex.org/W2099896673,2014,6,"['https://openalex.org/W1631260214', 'https://openalex.org/W1736600331', 'https://openalex.org/W1934041838', 'https://openalex.org/W2056250865', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116792345', 'https://openalex.org/W2124807415', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153204578', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153999629', 'https://openalex.org/W2156985047', 'https://openalex.org/W2160382364', 'https://openalex.org/W2160993407', 'https://openalex.org/W2187953100', 'https://openalex.org/W2251098065', 'https://openalex.org/W2437005631', 'https://openalex.org/W2950186769']",Phrase-based translation models usually memorize local translation literally and make independent assumption between phrases which makes it neither generalize well on unseen data nor model sentencelevel effects between phrases. In this paper we present a new method to model correlations between phrases as a Markov model and meanwhile employ a robust smoothing strategy to provide better generalization. This method defines a recursive estimation process and backs off in parallel paths to infer richer structures. Our evaluation shows an 1.1‐3.2% BLEU improvement over competitive baselines for Chinese-English and Arabic-English translation.,1.0
SKG_MT_587,https://openalex.org/W2890810021,2018,7,"['https://openalex.org/W2025772227', 'https://openalex.org/W2251798853', 'https://openalex.org/W2621526390']",We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture - combining a state-of-the-art self-attentive model with compact domain adaptation - provides high quality personalized machine translation that is both space and time efficient.,1.0
SKG_MT_589,https://openalex.org/W2962798274,2018,7,"['https://openalex.org/W1486649854', 'https://openalex.org/W1566289585', 'https://openalex.org/W1840435438', 'https://openalex.org/W1896449392', 'https://openalex.org/W2098507980', 'https://openalex.org/W2101105183', 'https://openalex.org/W2142112143', 'https://openalex.org/W2250597803', 'https://openalex.org/W2251117546', 'https://openalex.org/W2251386554', 'https://openalex.org/W2252035278', 'https://openalex.org/W2512848817', 'https://openalex.org/W2756675635', 'https://openalex.org/W2758950307', 'https://openalex.org/W2760738985', 'https://openalex.org/W2770626128', 'https://openalex.org/W2794557536', 'https://openalex.org/W2915756181', 'https://openalex.org/W2916548775', 'https://openalex.org/W2949433733', 'https://openalex.org/W2963090765', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963918774', 'https://openalex.org/W4300822525']","Sentence representations can capture a wide range of information that cannot be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of machine translation. Al-though it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.",1.0
SKG_MT_590,https://openalex.org/W2102258849,2014,10,"['https://openalex.org/W22168010', 'https://openalex.org/W55636956', 'https://openalex.org/W74908938', 'https://openalex.org/W76590478', 'https://openalex.org/W1517947178', 'https://openalex.org/W1551202288', 'https://openalex.org/W1565992329', 'https://openalex.org/W1606076566', 'https://openalex.org/W1631260214', 'https://openalex.org/W1934041838', 'https://openalex.org/W1986569439', 'https://openalex.org/W2080373976', 'https://openalex.org/W2095755718', 'https://openalex.org/W2099037496', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101456909', 'https://openalex.org/W2112900913', 'https://openalex.org/W2117873248', 'https://openalex.org/W2121338597', 'https://openalex.org/W2122270629', 'https://openalex.org/W2123635983', 'https://openalex.org/W2134800885', 'https://openalex.org/W2136612048', 'https://openalex.org/W2139621418', 'https://openalex.org/W2144279206', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156945896', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2158917268', 'https://openalex.org/W2159755860', 'https://openalex.org/W2165109801', 'https://openalex.org/W2165666205', 'https://openalex.org/W2166781037', 'https://openalex.org/W2167393476', 'https://openalex.org/W2168966090', 'https://openalex.org/W2178903040', 'https://openalex.org/W2185166416', 'https://openalex.org/W2270190199', 'https://openalex.org/W2408655637', 'https://openalex.org/W2437005631', 'https://openalex.org/W2626190081', 'https://openalex.org/W3167516426', 'https://openalex.org/W3172758989']","We present an effective technique to easily augment GHKM-style syntax-based machine translation systems (Galley et al., 2006) with phrase pairs that do not comply with any syntactic well-formedness constraints. Non-syntactic phrase pairs are distinguished from syntactic ones in order to avoid harming effects. We apply our technique in state-of-the-art string-totree and tree-to-string setups. For tree-tostring translation, we furthermore investigate novel approaches for translating with source-syntax GHKM rules in association with input tree constraints and input tree features.",1.0
SKG_MT_591,https://openalex.org/W2108188641,2013,31,"['https://openalex.org/W20450839', 'https://openalex.org/W204341599', 'https://openalex.org/W1632114991', 'https://openalex.org/W1973923101', 'https://openalex.org/W2096204319', 'https://openalex.org/W2100438903', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110416098', 'https://openalex.org/W2111889471', 'https://openalex.org/W2112900913', 'https://openalex.org/W2117238933', 'https://openalex.org/W2123311362', 'https://openalex.org/W2140270230', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155295193', 'https://openalex.org/W2155541797', 'https://openalex.org/W2158195707', 'https://openalex.org/W2162245945', 'https://openalex.org/W2180952760', 'https://openalex.org/W2963847008']","Empty categories (EC) are artificial ele-ments in Penn Treebanks motivated by the government-binding (GB) theory to ex-plain certain language phenomena such as pro-drop. ECs are ubiquitous in languages like Chinese, but they are tacitly ignored in most machine translation (MT) work because of their elusive nature. In this paper we present a comprehensive treat-ment of ECs by first recovering them with a structured MaxEnt model with a rich set of syntactic and lexical features, and then incorporating the predicted ECs into a Chinese-to-English machine translation task through multiple approaches, includ-ing the extraction of EC-specific sparse features. We show that the recovered empty categories not only improve the word alignment quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system. 1",0.9928057553956835
SKG_MT_594,https://openalex.org/W3106321930,2020,82,"['https://openalex.org/W2126725946', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251743902', 'https://openalex.org/W2270364989', 'https://openalex.org/W2294774419', 'https://openalex.org/W2462831000', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2594021297', 'https://openalex.org/W2806311723', 'https://openalex.org/W2905933322', 'https://openalex.org/W2914120296', 'https://openalex.org/W2944815030', 'https://openalex.org/W2952190837', 'https://openalex.org/W2952468927', 'https://openalex.org/W2952650870', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964114970', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970597249', 'https://openalex.org/W2970731908', 'https://openalex.org/W2971207485', 'https://openalex.org/W2971863715', 'https://openalex.org/W2996854111', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035390927', 'https://openalex.org/W3101672304', 'https://openalex.org/W3107826490', 'https://openalex.org/W4287874506', 'https://openalex.org/W4294170691', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP.",1.0
SKG_MT_595,https://openalex.org/W2158732077,2011,11,"['https://openalex.org/W165422532', 'https://openalex.org/W1506572002', 'https://openalex.org/W1506806321', 'https://openalex.org/W1510073064', 'https://openalex.org/W1554944419', 'https://openalex.org/W1663973292', 'https://openalex.org/W1957631488', 'https://openalex.org/W2061066582', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111491614', 'https://openalex.org/W2123318312', 'https://openalex.org/W2124807415', 'https://openalex.org/W2135046866', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152096938', 'https://openalex.org/W2554958826', 'https://openalex.org/W3105702746']","We present the results we obtain using our RegMT system, which uses transductive regression techniques to learn mappings between source and target features of given parallel corpora and use these mappings to generate machine translation outputs. Our training instance selection methods perform feature decay for proper selection of training instances, which plays an important role to learn correct feature mappings. RegMT uses L2 regularized regression as well as L1 regularized regression for sparse regression estimation of target features. We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F1 measure over target features as a metric for evaluating translation quality. 1",1.0
SKG_MT_597,https://openalex.org/W2951338945,2019,33,"['https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W1753482797', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2157331557', 'https://openalex.org/W2158139315', 'https://openalex.org/W2212846646', 'https://openalex.org/W2220350356', 'https://openalex.org/W2550821151', 'https://openalex.org/W2613904329', 'https://openalex.org/W2669742347', 'https://openalex.org/W2740433069', 'https://openalex.org/W2741040846', 'https://openalex.org/W2758162718', 'https://openalex.org/W2769137120', 'https://openalex.org/W2794365787', 'https://openalex.org/W2888519496', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962834107', 'https://openalex.org/W2962886257', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963547384', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963703075', 'https://openalex.org/W2963792777', 'https://openalex.org/W2963932686', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566']","Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters.",1.0
SKG_MT_598,https://openalex.org/W2252035825,2014,1,"['https://openalex.org/W76590478', 'https://openalex.org/W138910888', 'https://openalex.org/W1481626677', 'https://openalex.org/W1591659308', 'https://openalex.org/W1660460062', 'https://openalex.org/W1964388487', 'https://openalex.org/W1979711143', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008652694', 'https://openalex.org/W2037894654', 'https://openalex.org/W2040870580', 'https://openalex.org/W2061562262', 'https://openalex.org/W2061910127', 'https://openalex.org/W2104917081', 'https://openalex.org/W2112648537', 'https://openalex.org/W2116410915', 'https://openalex.org/W2125536435', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2158349948', 'https://openalex.org/W2160218441', 'https://openalex.org/W2161227214', 'https://openalex.org/W2165199647', 'https://openalex.org/W2170986599', 'https://openalex.org/W2437005631', 'https://openalex.org/W2541794668', 'https://openalex.org/W2895810819', 'https://openalex.org/W2911246871', 'https://openalex.org/W4285719527']","We present a novel Undirected Machine Translation model of Hierarchical MT that is not constrained to the standard bottomup inference order. Removing the ordering constraint makes it possible to condition on top-down structure and surrounding context. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art.",1.0
SKG_MT_602,https://openalex.org/W2984274107,2019,20,"['https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2810134635', 'https://openalex.org/W2859207840', 'https://openalex.org/W2949745489', 'https://openalex.org/W2950428495', 'https://openalex.org/W2951338945', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962822108', 'https://openalex.org/W2962931466', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963329925', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963641307', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964189376', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964302308', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971254483', 'https://openalex.org/W4385245566']","Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_603,https://openalex.org/W2166086342,2010,15,"['https://openalex.org/W35711202', 'https://openalex.org/W101793699', 'https://openalex.org/W1631260214', 'https://openalex.org/W1975638594', 'https://openalex.org/W2006969979', 'https://openalex.org/W2017802499', 'https://openalex.org/W2026864236', 'https://openalex.org/W2053306448', 'https://openalex.org/W2074546930', 'https://openalex.org/W2100397666', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112514265', 'https://openalex.org/W2116211107', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131988669', 'https://openalex.org/W2135161317', 'https://openalex.org/W2139770225', 'https://openalex.org/W2143134347', 'https://openalex.org/W2147925066', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158388102', 'https://openalex.org/W2160538511', 'https://openalex.org/W2161742089', 'https://openalex.org/W2164529863', 'https://openalex.org/W2437005631', 'https://openalex.org/W2917362697', 'https://openalex.org/W2952343510']","We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.",1.0
SKG_MT_604,https://openalex.org/W2897105542,2018,37,"['https://openalex.org/W46679369', 'https://openalex.org/W1522301498', 'https://openalex.org/W1539673959', 'https://openalex.org/W1591706642', 'https://openalex.org/W1843891098', 'https://openalex.org/W1895577753', 'https://openalex.org/W1902237438', 'https://openalex.org/W1991133427', 'https://openalex.org/W1997644175', 'https://openalex.org/W2025768430', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104246439', 'https://openalex.org/W2121879602', 'https://openalex.org/W2525778437', 'https://openalex.org/W2594978815', 'https://openalex.org/W2613904329', 'https://openalex.org/W2626778328', 'https://openalex.org/W2725082186', 'https://openalex.org/W2765961751', 'https://openalex.org/W2767899794', 'https://openalex.org/W2950448199', 'https://openalex.org/W2952288254', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963201387', 'https://openalex.org/W2963207607', 'https://openalex.org/W2964308564']","Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.",0.9949748743718593
SKG_MT_605,https://openalex.org/W2612881151,2017,78,"['https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1970849810', 'https://openalex.org/W2031196180', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110485445', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2144012961', 'https://openalex.org/W2149327368', 'https://openalex.org/W2157331557', 'https://openalex.org/W2168966090', 'https://openalex.org/W2194775991', 'https://openalex.org/W2251367463', 'https://openalex.org/W2251939518', 'https://openalex.org/W2290847742', 'https://openalex.org/W2407166119', 'https://openalex.org/W2540404261', 'https://openalex.org/W2552839021', 'https://openalex.org/W2563666160', 'https://openalex.org/W2586524454', 'https://openalex.org/W2586559132', 'https://openalex.org/W2600702321', 'https://openalex.org/W2606134370', 'https://openalex.org/W2609011624', 'https://openalex.org/W2949926081', 'https://openalex.org/W2951883919', 'https://openalex.org/W2952254971', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963451457', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964113829', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964321699']","We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.",1.0
SKG_MT_606,https://openalex.org/W168700332,2010,12,"['https://openalex.org/W23077562', 'https://openalex.org/W36709579', 'https://openalex.org/W160516373', 'https://openalex.org/W1568694948', 'https://openalex.org/W1909398668', 'https://openalex.org/W1965420956', 'https://openalex.org/W2006969979', 'https://openalex.org/W2020079054', 'https://openalex.org/W2030764196', 'https://openalex.org/W2041349462', 'https://openalex.org/W2078950386', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101454539', 'https://openalex.org/W2109613320', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153186553', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158069733', 'https://openalex.org/W2166905217', 'https://openalex.org/W3202110839']","We present a method for incorporat-ing arbitrary context-informed word at-tributes into statistical machine trans-lation by clustering attribute-qualified source words, and smoothing their word translation probabilities using bi-nary decision trees. We describe two ways in which the decision trees are used in machine translation: by us-ing the attribute-qualified source word clusters directly, or by using attribute-dependent lexical translation probabil-ities that are obtained from the trees, as a lexical smoothing feature in the de-coder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-of-speech as source word attributes, and show that the proposed method im-proves on a state-of-the-art translation system. 1",1.0
SKG_MT_609,https://openalex.org/W2978262868,2019,1,"['https://openalex.org/W222053410', 'https://openalex.org/W630532510', 'https://openalex.org/W1731081199', 'https://openalex.org/W1905522558', 'https://openalex.org/W1915251500', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2130942839', 'https://openalex.org/W2147262247', 'https://openalex.org/W2157331557', 'https://openalex.org/W2159291411', 'https://openalex.org/W2567571499', 'https://openalex.org/W2604474127', 'https://openalex.org/W2605488490', 'https://openalex.org/W2744813330', 'https://openalex.org/W2756566411', 'https://openalex.org/W2756978580', 'https://openalex.org/W2757592053', 'https://openalex.org/W2804452283', 'https://openalex.org/W2888867175', 'https://openalex.org/W2900987791', 'https://openalex.org/W2922349260', 'https://openalex.org/W2950940239', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963149635', 'https://openalex.org/W2963150697', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963446520', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","Neural networks are known to be data hungry and domain sensitive, but it is nearly impossible to obtain large quantities of labeled data for every domain we are interested in. This necessitates the use of domain adaptation strategies. One common strategy encourages generalization by aligning the global distribution statistics between source and target domains, but one drawback is that the statistics of different domains or tasks are inherently divergent, and smoothing over these differences can lead to sub-optimal performance. In this paper, we propose the framework of {\it Domain Differential Adaptation (DDA)}, where instead of smoothing over these differences we embrace them, directly modeling the difference between domains using models in a related task. We then use these learned domain differentials to adapt models for the target task accordingly. Experimental results on domain adaptation for neural machine translation demonstrate the effectiveness of this strategy, achieving consistent improvements over other alternative adaptation strategies in multiple experimental settings.",1.0
SKG_MT_610,https://openalex.org/W2759173152,2017,124,"['https://openalex.org/W153523274', 'https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1565570978', 'https://openalex.org/W1614298861', 'https://openalex.org/W1710422233', 'https://openalex.org/W2040711288', 'https://openalex.org/W2077654158', 'https://openalex.org/W2081580037', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101454539', 'https://openalex.org/W2114912785', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2179984881', 'https://openalex.org/W2250213760', 'https://openalex.org/W2497040301', 'https://openalex.org/W2533220144', 'https://openalex.org/W2572474373', 'https://openalex.org/W2594229957', 'https://openalex.org/W2760138945', 'https://openalex.org/W2760656271', 'https://openalex.org/W2885588803', 'https://openalex.org/W2950577311', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962907349', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W4386506836']","Word sense disambiguation is necessary in translation because different word senses often have different translations. Neural machine translation models learn different senses of words as part of an end-to-end translation task, and their capability to perform word sense disambiguation has so far not been quantified. We exploit the fact that neural translation models can score arbitrary translations to design a novel cross-lingual word sense disambiguation task that is tailored towards evaluating neural machine translation models. We present a test set of 7,200 lexical ambiguities for German → English, and 6,700 for German → French, and report baseline results. With 70% of lexical ambiguities correctly disambiguated, we find that word sense disambiguation remains a challenging problem for neural machine translation, especially for rare word senses. To improve word sense disambiguation in neural machine translation, we experiment with two methods to integrate sense embeddings. In a first approach we pass sense embeddings as additional input to the neural machine translation system. For the second experiment, we extract lexical chains based on sense embeddings from the document and integrate this information into the NMT model. While a baseline NMT system disambiguates frequent word senses quite reliably, the annotation with both sense labels and lexical chains improves the neural models’ performance on rare word senses.",1.0
SKG_MT_611,https://openalex.org/W3101235629,2020,6,"['https://openalex.org/W1682403713', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2560647685', 'https://openalex.org/W2567571499', 'https://openalex.org/W2740718109', 'https://openalex.org/W2744813330', 'https://openalex.org/W2750588180', 'https://openalex.org/W2757592053', 'https://openalex.org/W2760452458', 'https://openalex.org/W2794365787', 'https://openalex.org/W2803739890', 'https://openalex.org/W2892244498', 'https://openalex.org/W2911300548', 'https://openalex.org/W2945383715', 'https://openalex.org/W2947187520', 'https://openalex.org/W2950760213', 'https://openalex.org/W2952200364', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962863357', 'https://openalex.org/W2962945654', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963211188', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963841178', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964303773', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970925270', 'https://openalex.org/W2988394319', 'https://openalex.org/W2996428491', 'https://openalex.org/W3204406378', 'https://openalex.org/W4288028629', 'https://openalex.org/W4322588812', 'https://openalex.org/W4385245566']","Multi-Domain Neural Machine Translation (NMT) aims at building a single system that performs well on a range of target domains. However, along with the extreme diversity of cross-domain wording and phrasing style, the imperfections of training data distribution and the inherent defects of the current sequential learning process all contribute to making the task of multi-domain NMT very challenging. To mitigate these problems, we propose the Factorized Transformer, which consists of an in-depth factorization of the parameters of an NMT model, namely Transformer in this paper, into two categories: domain-shared ones that encode common cross-domain knowledge and domain-specific ones that are private for each constituent domain. We experiment with various designs of our model and conduct extensive validations on English to French open multi-domain dataset. Our approach achieves state-of-the-art performance and opens up new perspectives for multi-domain and open-domain applications.",1.0
SKG_MT_612,https://openalex.org/W3037460861,2020,3,"['https://openalex.org/W2107639100', 'https://openalex.org/W2308338022', 'https://openalex.org/W2550821151', 'https://openalex.org/W2610245951', 'https://openalex.org/W2739978843', 'https://openalex.org/W2949973181', 'https://openalex.org/W2962699518', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963223057', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964007535', 'https://openalex.org/W3103849166', 'https://openalex.org/W4244167344']","The primary limitation of North Korean to English translation is the lack of a parallel corpus; therefore, high translation accuracy cannot be achieved. To address this problem, we propose a zero-shot approach using South Korean data, which are remarkably similar to North Korean data. We train a neural machine translation model after tokenizing a South Korean text at the character level and decomposing characters into phonemes.We demonstrate that our method can effectively learn North Korean to English translation and improve the BLEU scores by +1.01 points in comparison with the baseline.",1.0
SKG_MT_613,https://openalex.org/W144133692,2012,19,"['https://openalex.org/W91928571', 'https://openalex.org/W147273232', 'https://openalex.org/W222053410', 'https://openalex.org/W1540198634', 'https://openalex.org/W1795026554', 'https://openalex.org/W1997859518', 'https://openalex.org/W2006969979', 'https://openalex.org/W2058475745', 'https://openalex.org/W2069317438', 'https://openalex.org/W2096175520', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102845951', 'https://openalex.org/W2103149536', 'https://openalex.org/W2111142112', 'https://openalex.org/W2112530506', 'https://openalex.org/W2115364117', 'https://openalex.org/W2118585731', 'https://openalex.org/W2121127625', 'https://openalex.org/W2124119193', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125712079', 'https://openalex.org/W2126610017', 'https://openalex.org/W2131148434', 'https://openalex.org/W2137143056', 'https://openalex.org/W2138302120', 'https://openalex.org/W2140967626', 'https://openalex.org/W2141852716', 'https://openalex.org/W2142623206', 'https://openalex.org/W2143564602', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2154124206', 'https://openalex.org/W2159358338', 'https://openalex.org/W2160218441', 'https://openalex.org/W2161227214', 'https://openalex.org/W2163548102', 'https://openalex.org/W2165966284', 'https://openalex.org/W2169819436', 'https://openalex.org/W2180952760', 'https://openalex.org/W2437005631', 'https://openalex.org/W3104615938']","We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results. 1",1.0
SKG_MT_614,https://openalex.org/W3037879901,2020,5,"['https://openalex.org/W1524786316', 'https://openalex.org/W1667342666', 'https://openalex.org/W1986345088', 'https://openalex.org/W2128070455', 'https://openalex.org/W2134670378', 'https://openalex.org/W2250599277', 'https://openalex.org/W2251375842', 'https://openalex.org/W2739766884', 'https://openalex.org/W2774332521', 'https://openalex.org/W2799493995', 'https://openalex.org/W2920839365', 'https://openalex.org/W2923362077', 'https://openalex.org/W2942274440', 'https://openalex.org/W2970587490', 'https://openalex.org/W3034673518', 'https://openalex.org/W3097330605', 'https://openalex.org/W3102818226', 'https://openalex.org/W3185596593', 'https://openalex.org/W3186483169', 'https://openalex.org/W4288262060']","Nico Herbig, Santanu Pal, Tim Düwel, Kalliopi Meladaki, Mahsa Monshizadeh, Vladislav Hnatovskiy, Antonio Krüger, Josef van Genabith. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2020.",0.9959183673469387
SKG_MT_615,https://openalex.org/W2962997665,2015,72,"['https://openalex.org/W1551202288', 'https://openalex.org/W1631260214', 'https://openalex.org/W1753482797', 'https://openalex.org/W2095690342', 'https://openalex.org/W2120615054', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152752344', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157331557', 'https://openalex.org/W2161227214', 'https://openalex.org/W2162245945', 'https://openalex.org/W2166905217', 'https://openalex.org/W2167072947', 'https://openalex.org/W2170738476', 'https://openalex.org/W2250489405', 'https://openalex.org/W2251682575', 'https://openalex.org/W2437005631', 'https://openalex.org/W2595715041', 'https://openalex.org/W2949888546', 'https://openalex.org/W2951359136', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998704965', 'https://openalex.org/W4241645538', 'https://openalex.org/W4285719527']","Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li, Wenbin Jiang, Qun Liu. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.",1.0
SKG_MT_616,https://openalex.org/W2936627440,2019,43,"['https://openalex.org/W1522301498', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2143269898', 'https://openalex.org/W2148708890', 'https://openalex.org/W2284660317', 'https://openalex.org/W2400065810', 'https://openalex.org/W2413436069', 'https://openalex.org/W2492716757', 'https://openalex.org/W2507756961', 'https://openalex.org/W2539201987', 'https://openalex.org/W2756566411', 'https://openalex.org/W2797297135', 'https://openalex.org/W2890256614', 'https://openalex.org/W2903012348', 'https://openalex.org/W2950580142', 'https://openalex.org/W2952913664', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962714778', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962944953', 'https://openalex.org/W2962972936', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963877622', 'https://openalex.org/W2964029788', 'https://openalex.org/W2964165364', 'https://openalex.org/W3204406378']","Leveraging user-provided translation to constrain NMT has practical significance. Existing methods can be classified into two main categories, namely the use of placeholder tags for lexicon words and the use of hard constraints during decoding. Both methods can hurt translation fidelity for various reasons. We investigate a data augmentation method, making code-switched training data by replacing source phrases with their target translations. Our method does not change the MNT model or decoding algorithm, allowing the model to learn lexicon translations by copying source-side target words. Extensive experiments show that our method achieves consistent improvements over existing approaches, improving translation of constrained words without hurting unconstrained words.",1.0
SKG_MT_618,https://openalex.org/W2970682957,2019,49,"['https://openalex.org/W1566289585', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2294860948', 'https://openalex.org/W2563574619', 'https://openalex.org/W2566150155', 'https://openalex.org/W2786396726', 'https://openalex.org/W2799124508', 'https://openalex.org/W2803258077', 'https://openalex.org/W2807880213', 'https://openalex.org/W2889769646', 'https://openalex.org/W2895705233', 'https://openalex.org/W2896457183', 'https://openalex.org/W2899423466', 'https://openalex.org/W2905016804', 'https://openalex.org/W2912351236', 'https://openalex.org/W2914855263', 'https://openalex.org/W2945059185', 'https://openalex.org/W2946794439', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962788148', 'https://openalex.org/W2962822108', 'https://openalex.org/W2962911926', 'https://openalex.org/W2962945603', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963406157', 'https://openalex.org/W2963641307', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2964302946', 'https://openalex.org/W2964347512', 'https://openalex.org/W2970423066', 'https://openalex.org/W4241645538', 'https://openalex.org/W4385245566']","Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, Zhaopeng Tu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_620,https://openalex.org/W2207848871,2011,6,"['https://openalex.org/W81000870', 'https://openalex.org/W222053410', 'https://openalex.org/W563039848', 'https://openalex.org/W1491975949', 'https://openalex.org/W1632114991', 'https://openalex.org/W1970849810', 'https://openalex.org/W2027979924', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113998052', 'https://openalex.org/W2122922578', 'https://openalex.org/W2131570422', 'https://openalex.org/W2131590891', 'https://openalex.org/W2152400901', 'https://openalex.org/W2153274216', 'https://openalex.org/W2170570046', 'https://openalex.org/W2171332413', 'https://openalex.org/W2405762604']","Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation. 1",1.0
SKG_MT_621,https://openalex.org/W2982162582,2019,16,"['https://openalex.org/W94670513', 'https://openalex.org/W1587490638', 'https://openalex.org/W1816313093', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2144600658', 'https://openalex.org/W2166116787', 'https://openalex.org/W2171421863', 'https://openalex.org/W2183341477', 'https://openalex.org/W2398009384', 'https://openalex.org/W2398104528', 'https://openalex.org/W2406343628', 'https://openalex.org/W2407834842', 'https://openalex.org/W2555043949', 'https://openalex.org/W2560647685', 'https://openalex.org/W2705373224', 'https://openalex.org/W2741483887', 'https://openalex.org/W2744813330', 'https://openalex.org/W2746176496', 'https://openalex.org/W2760452458', 'https://openalex.org/W2760656271', 'https://openalex.org/W2806626660', 'https://openalex.org/W2810621157', 'https://openalex.org/W2886342729', 'https://openalex.org/W2902918014', 'https://openalex.org/W2903193068', 'https://openalex.org/W2937375285', 'https://openalex.org/W2938722449', 'https://openalex.org/W2945700568', 'https://openalex.org/W2949920209', 'https://openalex.org/W2962989741', 'https://openalex.org/W2963213832', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964240726', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965976453', 'https://openalex.org/W2984051011', 'https://openalex.org/W3202802636', 'https://openalex.org/W3204406378']","Neural machine translation models have shown to achieve high quality when trained and fed with well structured and punctuated input texts. Unfortunately, the latter condition is not met in spoken language translation, where the input is generated by an automatic speech recognition (ASR) system. In this paper, we study how to adapt a strong NMT system to make it robust to typical ASR errors. As in our application scenarios transcripts might be post-edited by human experts, we propose adaptation strategies to train a single system that can translate either clean or noisy input with no supervision on the input type. Our experimental results on a public speech translation data set show that adapting a model on a significant amount of parallel data including ASR transcripts is beneficial with test data of the same type, but produces a small degradation when translating clean text. Adapting on both clean and noisy variants of the same data leads to the best results on both input types.",1.0
SKG_MT_622,https://openalex.org/W4309424007,2012,2,[],"lium.univ-lemans.fr French researchers are required to frequently translate into French the description of their work published in English. At the same time, the need for French people to access articles in English, or to international researchers to access theses or papers in French, is incorrectly resolved via the use of generic translation tools. We propose the demonstration of an end-to-end tool integrated in the HAL open archive for enabling efficient translation for scientific texts. This tool can give translation suggestions adapted to the scientific domain, improving by more than 10 points the BLEU score of a generic system. It also provides a post-edition service which captures user post-editing data that can be used to incrementally improve the translations engines. Thus it is helpful for users which need to translate or to access scientific texts. 1",1.0
SKG_MT_626,https://openalex.org/W2952524847,2019,28,"['https://openalex.org/W222053410', 'https://openalex.org/W1553589067', 'https://openalex.org/W2006258004', 'https://openalex.org/W2115161902', 'https://openalex.org/W2120615054', 'https://openalex.org/W2132001515', 'https://openalex.org/W2133564696', 'https://openalex.org/W2183893152', 'https://openalex.org/W2250473257', 'https://openalex.org/W2321897735', 'https://openalex.org/W2563574619', 'https://openalex.org/W2573728411', 'https://openalex.org/W2597655663', 'https://openalex.org/W2740743644', 'https://openalex.org/W2769298630', 'https://openalex.org/W2799124508', 'https://openalex.org/W2804044248', 'https://openalex.org/W2806591392', 'https://openalex.org/W2817535134', 'https://openalex.org/W2866343820', 'https://openalex.org/W2888520903', 'https://openalex.org/W2912351236', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962931466', 'https://openalex.org/W2962997665', 'https://openalex.org/W2963277143', 'https://openalex.org/W2963386218', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963406157', 'https://openalex.org/W2963641307', 'https://openalex.org/W2963652649', 'https://openalex.org/W2963842551', 'https://openalex.org/W2963891264', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964302946', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965575120', 'https://openalex.org/W4385245566']","In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Specifically, we show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all of the internal layers of the encoder to form a more comprehensive context representation. Experimental results on the WMT14 English-German and English-French benchmarks show that our model consistently improves performance over the strong Transformer model, demonstrating the necessity and effectiveness of exploiting sentential context for NMT.",1.0
SKG_MT_631,https://openalex.org/W3035289598,2020,55,"['https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2525778437', 'https://openalex.org/W2794365787', 'https://openalex.org/W2890220768', 'https://openalex.org/W2907945666', 'https://openalex.org/W2912937082', 'https://openalex.org/W2920538220', 'https://openalex.org/W2933138175', 'https://openalex.org/W2944815030', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963661177', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965373594', 'https://openalex.org/W2967194868', 'https://openalex.org/W2971167892', 'https://openalex.org/W2971274815', 'https://openalex.org/W2988975212', 'https://openalex.org/W2990372437', 'https://openalex.org/W2990389671', 'https://openalex.org/W2996987694', 'https://openalex.org/W2998560404', 'https://openalex.org/W3011411500']","The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.",1.0
SKG_MT_636,https://openalex.org/W2962739703,2017,32,"['https://openalex.org/W6908809', 'https://openalex.org/W46679369', 'https://openalex.org/W581956982', 'https://openalex.org/W1514535095', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1924770834', 'https://openalex.org/W2007592492', 'https://openalex.org/W2064675550', 'https://openalex.org/W2088073093', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113104171', 'https://openalex.org/W2116261113', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136848157', 'https://openalex.org/W2157331557', 'https://openalex.org/W2407166119', 'https://openalex.org/W2593341061', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963044185', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963956654', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564']","This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.",1.0
SKG_MT_640,https://openalex.org/W2954647460,2019,43,"['https://openalex.org/W1731081199', 'https://openalex.org/W1753482797', 'https://openalex.org/W1915251500', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2147192413', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2170240176', 'https://openalex.org/W2250876691', 'https://openalex.org/W2251994258', 'https://openalex.org/W2561274697', 'https://openalex.org/W2579496717', 'https://openalex.org/W2613904329', 'https://openalex.org/W2740743644', 'https://openalex.org/W2744813330', 'https://openalex.org/W2756303778', 'https://openalex.org/W2756655895', 'https://openalex.org/W2756978580', 'https://openalex.org/W2758310181', 'https://openalex.org/W2758640791', 'https://openalex.org/W2760452458', 'https://openalex.org/W2892244498', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963012544', 'https://openalex.org/W2963073511', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963355640', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964268978', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W3211259717', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394643672']","Shuhao Gu, Yang Feng, Qun Liu. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_641,https://openalex.org/W2158658733,2013,7,"['https://openalex.org/W22168010', 'https://openalex.org/W91928571', 'https://openalex.org/W130710483', 'https://openalex.org/W301824129', 'https://openalex.org/W956853814', 'https://openalex.org/W1579838312', 'https://openalex.org/W1594563152', 'https://openalex.org/W1868901511', 'https://openalex.org/W2075601665', 'https://openalex.org/W2087735403', 'https://openalex.org/W2106019990', 'https://openalex.org/W2108452152', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132728053', 'https://openalex.org/W2136657878', 'https://openalex.org/W2137143056', 'https://openalex.org/W2139761450', 'https://openalex.org/W2141440284', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156953672', 'https://openalex.org/W2161227214', 'https://openalex.org/W2164151151', 'https://openalex.org/W2169362686', 'https://openalex.org/W2437005631', 'https://openalex.org/W2752885492']","We present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling. We also present a first implementation of that method along with experimental results shedding light on some fundamental issues. In hierarchical translation, inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model. We replace this intractable distribution by a sequence of tractable upper-bounds for which exact optimisers and samplers are easy to obtain. Our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection, without recourse to pruning techniques that only provide approximate solutions. While the current implementation is limited in the size of inputs it can handle in reasonable time, our experiments provide insights towards obtaining future speedups, while staying in the same general framework. 1",1.0
SKG_MT_642,https://openalex.org/W2250831586,2015,14,"['https://openalex.org/W122999227', 'https://openalex.org/W1517947178', 'https://openalex.org/W1969974515', 'https://openalex.org/W2006969979', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097333193', 'https://openalex.org/W2097927681', 'https://openalex.org/W2098875891', 'https://openalex.org/W2099896673', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107440414', 'https://openalex.org/W2110168585', 'https://openalex.org/W2111798208', 'https://openalex.org/W2116792345', 'https://openalex.org/W2117278770', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136016850', 'https://openalex.org/W2141440284', 'https://openalex.org/W2144600658', 'https://openalex.org/W2144879357', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153999629', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2160993407', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250610505', 'https://openalex.org/W2250651922', 'https://openalex.org/W2251048776', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251203309', 'https://openalex.org/W2251369387', 'https://openalex.org/W2251682575', 'https://openalex.org/W2251771687', 'https://openalex.org/W2595715041', 'https://openalex.org/W2964308564', 'https://openalex.org/W3041866211', 'https://openalex.org/W3185386956', 'https://openalex.org/W3203276480', 'https://openalex.org/W4241645538', 'https://openalex.org/W4254408171']","We propose a conversion of bilingual sentence pairs and the corresponding word alignments into novel linear sequences.These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework.They are constructed in a simple manner while capturing multiple alignments and empty words.JTR sequences can be used to train a variety of models.We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b).Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU.",1.0
SKG_MT_645,https://openalex.org/W2756670659,2017,32,"['https://openalex.org/W1533861849', 'https://openalex.org/W1608448541', 'https://openalex.org/W1762354184', 'https://openalex.org/W1978161643', 'https://openalex.org/W2040711288', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110006374', 'https://openalex.org/W2117642127', 'https://openalex.org/W2131462252', 'https://openalex.org/W2136094405', 'https://openalex.org/W2141836429', 'https://openalex.org/W2250526231', 'https://openalex.org/W2251367463', 'https://openalex.org/W2251648400', 'https://openalex.org/W2251743902', 'https://openalex.org/W2289260142', 'https://openalex.org/W2341118687', 'https://openalex.org/W2397435450', 'https://openalex.org/W2508316494', 'https://openalex.org/W2512848817', 'https://openalex.org/W2514342461', 'https://openalex.org/W2756888147', 'https://openalex.org/W2916264126', 'https://openalex.org/W2949810612', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963011474', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W3211259717']","Translation into a morphologically rich language requires a large output vocabulary to model various morphological phenomena, which is a challenge for neural machine translation architectures. To address this issue, the present paper investigates the impact of having two output factors with a system able to generate separately two distinct representations of the target words. Within this framework, we investigate several word representations that correspond to different distributions of morpho-syntactic information across both factors. We report experiments for translation from English into two morphologically rich languages, Czech and Latvian, and show the importance of explicitly modeling target morphology.",1.0
SKG_MT_646,https://openalex.org/W1537121970,2010,17,"['https://openalex.org/W21337280', 'https://openalex.org/W143910215', 'https://openalex.org/W255975419', 'https://openalex.org/W635530177', 'https://openalex.org/W1631260214', 'https://openalex.org/W1665921526', 'https://openalex.org/W2106540279', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127836646', 'https://openalex.org/W2146574666', 'https://openalex.org/W2156985047']",We report results of our submissions to the WMT 2010 shared translation task in which we applied a system that includes adaptive language and translation models. Adaptation is implemented using exponentially decaying caches storing previous translations as the history for new predictions. Evidence from the cache is then mixed with the global background model. The main problem in this setup is error propagation and our submissions essentially failed to improve over the competitive baseline. There are slight improvements in lexical choice but the global performance decreases in terms of BLEU scores. 1,0.9945945945945946
SKG_MT_647,https://openalex.org/W2250473075,2014,44,"['https://openalex.org/W143382514', 'https://openalex.org/W1500227852', 'https://openalex.org/W1532325895', 'https://openalex.org/W1552767446', 'https://openalex.org/W1988686126', 'https://openalex.org/W1990190154', 'https://openalex.org/W2011548984', 'https://openalex.org/W2025423507', 'https://openalex.org/W2046456023', 'https://openalex.org/W2069870183', 'https://openalex.org/W2085086335', 'https://openalex.org/W2105673178', 'https://openalex.org/W2111142112', 'https://openalex.org/W2120165387', 'https://openalex.org/W2132069633', 'https://openalex.org/W2134800885', 'https://openalex.org/W2146574666', 'https://openalex.org/W2146788249', 'https://openalex.org/W2148708890', 'https://openalex.org/W2152803722', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156530810', 'https://openalex.org/W2161722485', 'https://openalex.org/W2250400351', 'https://openalex.org/W2279886070', 'https://openalex.org/W2726585279', 'https://openalex.org/W3143773373', 'https://openalex.org/W3202296894', 'https://openalex.org/W4213009331', 'https://openalex.org/W4232491506', 'https://openalex.org/W4285719527']","We present an approach to cross-language retrieval that combines dense knowledgebased features and sparse word translations.Both feature types are learned directly from relevance rankings of bilingual documents in a pairwise ranking framework.In large-scale experiments for patent prior art search and cross-lingual retrieval in Wikipedia, our approach yields considerable improvements over learningto-rank with either only dense or only sparse features, and over very competitive baselines that combine state-of-the-art machine translation and retrieval.",1.0
SKG_MT_649,https://openalex.org/W2951476960,2019,109,"['https://openalex.org/W645927007', 'https://openalex.org/W2057069782', 'https://openalex.org/W2126725946', 'https://openalex.org/W2170204377', 'https://openalex.org/W2251566419', 'https://openalex.org/W2294774419', 'https://openalex.org/W2550821151', 'https://openalex.org/W2610245951', 'https://openalex.org/W2739978843', 'https://openalex.org/W2744813330', 'https://openalex.org/W2752630748', 'https://openalex.org/W2756566411', 'https://openalex.org/W2760424551', 'https://openalex.org/W2776822439', 'https://openalex.org/W2798931235', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888173624', 'https://openalex.org/W2889191148', 'https://openalex.org/W2897058237', 'https://openalex.org/W2898799786', 'https://openalex.org/W2899490399', 'https://openalex.org/W2912095972', 'https://openalex.org/W2913659301', 'https://openalex.org/W2919290281', 'https://openalex.org/W2950733326', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963617771', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964085268', 'https://openalex.org/W3019421297', 'https://openalex.org/W4244167344', 'https://openalex.org/W4288601832', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. In this paper, we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. First, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. Second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines.",1.0
SKG_MT_650,https://openalex.org/W2608395138,2017,10,"['https://openalex.org/W6908809', 'https://openalex.org/W1534477342', 'https://openalex.org/W1606347560', 'https://openalex.org/W1821462560', 'https://openalex.org/W2059174629', 'https://openalex.org/W2113021982', 'https://openalex.org/W2114766824', 'https://openalex.org/W2114912785', 'https://openalex.org/W2125389748', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2135293965', 'https://openalex.org/W2146502635', 'https://openalex.org/W2157331557', 'https://openalex.org/W2161591461', 'https://openalex.org/W2167215970', 'https://openalex.org/W2294059674', 'https://openalex.org/W2294370754', 'https://openalex.org/W2294543795', 'https://openalex.org/W2312434537', 'https://openalex.org/W2337826259', 'https://openalex.org/W2400065810', 'https://openalex.org/W2418388682', 'https://openalex.org/W2507699225', 'https://openalex.org/W2512924740', 'https://openalex.org/W2525778437', 'https://openalex.org/W2527845440', 'https://openalex.org/W2533220144', 'https://openalex.org/W2549252813', 'https://openalex.org/W2573197213', 'https://openalex.org/W2576482813', 'https://openalex.org/W2587694128', 'https://openalex.org/W2915926444', 'https://openalex.org/W2950248853', 'https://openalex.org/W2951973027', 'https://openalex.org/W2952899695', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962867687', 'https://openalex.org/W2962902089', 'https://openalex.org/W2962907349', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963643655', 'https://openalex.org/W2963674932', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963991999', 'https://openalex.org/W2964217848', 'https://openalex.org/W2964308564', 'https://openalex.org/W4297796785', 'https://openalex.org/W4298159529']","Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.",1.0
SKG_MT_651,https://openalex.org/W2251139225,2015,7,"['https://openalex.org/W53944291', 'https://openalex.org/W76590478', 'https://openalex.org/W287510790', 'https://openalex.org/W932413789', 'https://openalex.org/W1815552937', 'https://openalex.org/W1819520634', 'https://openalex.org/W1843946026', 'https://openalex.org/W2015350341', 'https://openalex.org/W2016856586', 'https://openalex.org/W2060127787', 'https://openalex.org/W2118167915', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125768350', 'https://openalex.org/W2135413937', 'https://openalex.org/W2144600658', 'https://openalex.org/W2159755860', 'https://openalex.org/W2163353449', 'https://openalex.org/W2165666205', 'https://openalex.org/W2178903040', 'https://openalex.org/W2252272516', 'https://openalex.org/W2401109438', 'https://openalex.org/W2584764413', 'https://openalex.org/W2589089426', 'https://openalex.org/W2595715041', 'https://openalex.org/W4233432642']","When translating between two languages that differ in their degree of morphological synthesis, syntactic structures in one language may be realized as morphological structures in the other, and SMT models need a mechanism to learn such translations. Prior work has used morpheme splitting with flat representations that do not encode the hierarchical structure between morphemes, but this structure is relevant for learning morphosyntactic constraints and selectional preferences. We propose to model syntactic and morphological structure jointly in a dependency translation model, allowing the system to generalize to the level of morphemes. We present a dependency representation of German compounds and particle verbs that results in improvements in translation quality of 1.4–1.8 BLEU in the WMT English–German translation task.",1.0
SKG_MT_652,https://openalex.org/W2331726854,2014,99,"['https://openalex.org/W44695385', 'https://openalex.org/W91928571', 'https://openalex.org/W108071511', 'https://openalex.org/W120531462', 'https://openalex.org/W203948990', 'https://openalex.org/W607471960', 'https://openalex.org/W1551202288', 'https://openalex.org/W1574440611', 'https://openalex.org/W1599248181', 'https://openalex.org/W1834061219', 'https://openalex.org/W1975009259', 'https://openalex.org/W1994057144', 'https://openalex.org/W2015333112', 'https://openalex.org/W2015933299', 'https://openalex.org/W2028176545', 'https://openalex.org/W2030987872', 'https://openalex.org/W2045738181', 'https://openalex.org/W2078861931', 'https://openalex.org/W2085574295', 'https://openalex.org/W2087735403', 'https://openalex.org/W2098507980', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104950474', 'https://openalex.org/W2105156267', 'https://openalex.org/W2111142112', 'https://openalex.org/W2112706073', 'https://openalex.org/W2116492146', 'https://openalex.org/W2127713198', 'https://openalex.org/W2137901323', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2152463966', 'https://openalex.org/W2153800732', 'https://openalex.org/W2154652894', 'https://openalex.org/W2163038970', 'https://openalex.org/W2163986298', 'https://openalex.org/W2165894879', 'https://openalex.org/W2166545452', 'https://openalex.org/W2180952760', 'https://openalex.org/W2186590411', 'https://openalex.org/W2251211118', 'https://openalex.org/W2251994258', 'https://openalex.org/W2434901392', 'https://openalex.org/W2758444222', 'https://openalex.org/W2895810819']","We present experiments in using discourse structure for improving machine translation evaluation.We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory.Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment-and at the system-level.Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.",1.0
SKG_MT_654,https://openalex.org/W2916835973,2019,61,"['https://openalex.org/W22168010', 'https://openalex.org/W1673923490', 'https://openalex.org/W1862810382', 'https://openalex.org/W1905522558', 'https://openalex.org/W2101761627', 'https://openalex.org/W2250729567', 'https://openalex.org/W2756978580', 'https://openalex.org/W2757188127', 'https://openalex.org/W2767899794', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962863357', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963919854', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964132420', 'https://openalex.org/W2964153729', 'https://openalex.org/W2964247056', 'https://openalex.org/W3204406378']","Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, Graham Neubig. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_655,https://openalex.org/W2970876710,2019,17,"['https://openalex.org/W2109000768', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134800885', 'https://openalex.org/W2146574666', 'https://openalex.org/W2493916176', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2741602058', 'https://openalex.org/W2792376130', 'https://openalex.org/W2798931235', 'https://openalex.org/W2803214681', 'https://openalex.org/W2890007195', 'https://openalex.org/W2914120296', 'https://openalex.org/W2932618389', 'https://openalex.org/W2954447110', 'https://openalex.org/W2955866955', 'https://openalex.org/W2962677207', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962832505', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963418779', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963684088', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4297747548', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4299838440', 'https://openalex.org/W4385245566']","This paper describes CAiRE’s submission to the unsupervised machine translation track of the WMT’19 news shared task from German to Czech. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.",1.0
SKG_MT_656,https://openalex.org/W3034489970,2020,0,"['https://openalex.org/W1654441844', 'https://openalex.org/W2027731328', 'https://openalex.org/W2108598243', 'https://openalex.org/W2144746247', 'https://openalex.org/W2160001241', 'https://openalex.org/W2250234233', 'https://openalex.org/W2251150371', 'https://openalex.org/W2251557434', 'https://openalex.org/W2619383789', 'https://openalex.org/W2838081464', 'https://openalex.org/W2894218541', 'https://openalex.org/W2902463012', 'https://openalex.org/W2903343986', 'https://openalex.org/W2953072129', 'https://openalex.org/W2963341956', 'https://openalex.org/W2971120958', 'https://openalex.org/W3098649723', 'https://openalex.org/W4211166112', 'https://openalex.org/W4297971002']",© 2020 The Authors. Published by Association for Computational Linguistics. This is an open access article available under a Creative Commons licence.&#13;\nThe published version can be accessed at the following link on the publisher’s website: http://dx.doi.org/10.18653/v1/2020.acl-main.114,1.0
SKG_MT_658,https://openalex.org/W1816313093,2016,445,"['https://openalex.org/W6908809', 'https://openalex.org/W46679369', 'https://openalex.org/W214028658', 'https://openalex.org/W1501139663', 'https://openalex.org/W1519942606', 'https://openalex.org/W1526344253', 'https://openalex.org/W1753482797', 'https://openalex.org/W1899794420', 'https://openalex.org/W1902237438', 'https://openalex.org/W2015350341', 'https://openalex.org/W2097532276', 'https://openalex.org/W2100664567', 'https://openalex.org/W2116211107', 'https://openalex.org/W2116599427', 'https://openalex.org/W2118434577', 'https://openalex.org/W2119202242', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2143134347', 'https://openalex.org/W2148708890', 'https://openalex.org/W2220350356', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251012068', 'https://openalex.org/W2251139225', 'https://openalex.org/W2528305545', 'https://openalex.org/W2916548775', 'https://openalex.org/W2949445183', 'https://openalex.org/W2949679234', 'https://openalex.org/W2951559648', 'https://openalex.org/W2952097392', 'https://openalex.org/W2964308564']","Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",1.0
SKG_MT_659,https://openalex.org/W2899427634,2018,33,"['https://openalex.org/W648786980', 'https://openalex.org/W1533861849', 'https://openalex.org/W2064675550', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2173520492', 'https://openalex.org/W2176263492', 'https://openalex.org/W2437005631', 'https://openalex.org/W2542835211', 'https://openalex.org/W2546938941', 'https://openalex.org/W2577946330', 'https://openalex.org/W2581637843', 'https://openalex.org/W2601324753', 'https://openalex.org/W2607987856', 'https://openalex.org/W2613904329', 'https://openalex.org/W2618606525', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798931235', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963163972', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963684088', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964268978', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W4241645538', 'https://openalex.org/W4294149591', 'https://openalex.org/W4307459710', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566']","Generative Adversarial Network (GAN) has been proposed to tackle the exposure bias problem of Neural Machine Translation (NMT). However, the discriminator typically results in the instability of the GAN training due to the inadequate training problem: the search space is so huge that sampled translations are not sufficient for discriminator training. To address this issue and stabilize the GAN training, in this paper, we propose a novel Bidirectional Generative Adversarial Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator model to act as the discriminator, whereby the discriminator naturally considers the entire translation space so that the inadequate training problem can be alleviated. To satisfy this property, generator and discriminator are both designed to model the joint probability of sentence pairs, with the difference that, the generator decomposes the joint probability with a source language model and a source-to-target translation model, while the discriminator is formulated as a target language model and a target-to-source translation model. To further leverage the symmetry of them, an auxiliary GAN is introduced and adopts generator and discriminator models of original one as its own discriminator and generator respectively. Two GANs are alternately trained to update the parameters. Experiment results on German-English and Chinese-English translation tasks demonstrate that our method not only stabilizes GAN training but also achieves significant improvements over baseline systems.",1.0
SKG_MT_660,https://openalex.org/W1896570304,2010,20,"['https://openalex.org/W116785612', 'https://openalex.org/W222053410', 'https://openalex.org/W563039848', 'https://openalex.org/W1518357715', 'https://openalex.org/W1535015163', 'https://openalex.org/W1707559977', 'https://openalex.org/W1773803948', 'https://openalex.org/W1976339715', 'https://openalex.org/W1987682583', 'https://openalex.org/W2072776834', 'https://openalex.org/W2096175520', 'https://openalex.org/W2097524769', 'https://openalex.org/W2119482011', 'https://openalex.org/W2121362795', 'https://openalex.org/W2122922578', 'https://openalex.org/W2131570422', 'https://openalex.org/W2154124206', 'https://openalex.org/W2169424935']","Maximum Entropy Principle has been used successfully in various NLP tasks. In this paper we propose a forward translation model consisting of a set of maximum entropy classifiers: a separate classifier is trained for each (sufficiently frequent) source-side lemma. In this way the estimates of translation probabilities can be sensitive to a large number of features derived from the source sentence (including non-local features, features making use of sentence syntactic structure, etc.). When integrated into English-to-Czech dependency-based translation scenario implemented in the TectoMT framework, the new translation model significantly outperforms the baseline model (MLE) in terms of BLEU. The performance is further boosted in a configuration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model.",1.0
SKG_MT_662,https://openalex.org/W2949920209,2018,72,"['https://openalex.org/W22168010', 'https://openalex.org/W1522301498', 'https://openalex.org/W1848260265', 'https://openalex.org/W1902237438', 'https://openalex.org/W1905522558', 'https://openalex.org/W2038810952', 'https://openalex.org/W2094147890', 'https://openalex.org/W2115410424', 'https://openalex.org/W2117278770', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136477195', 'https://openalex.org/W2137387514', 'https://openalex.org/W2168103496', 'https://openalex.org/W2183364364', 'https://openalex.org/W2185701500', 'https://openalex.org/W2251202280', 'https://openalex.org/W2406343028', 'https://openalex.org/W2419539795', 'https://openalex.org/W2567571499', 'https://openalex.org/W2578338321', 'https://openalex.org/W2744813330', 'https://openalex.org/W2752630748', 'https://openalex.org/W2757592053', 'https://openalex.org/W2778814079', 'https://openalex.org/W2786459654', 'https://openalex.org/W2794561587', 'https://openalex.org/W2798878995', 'https://openalex.org/W2803241009', 'https://openalex.org/W2885616807', 'https://openalex.org/W2886342729', 'https://openalex.org/W2903193068', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962863357', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963303951', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963841178', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3082674894', 'https://openalex.org/W3204406378']","To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the encoder, decoder, and each embedding space) and consider each component's contribution to, and capacity for, domain adaptation. We find that freezing any single component during continued training has minimal impact on performance, and that performance is surprisingly good when a single component is adapted while holding the rest of the model fixed. We also find that continued training does not move the model very far from the out-of-domain model, compared to a sensitivity analysis metric, suggesting that the out-of-domain model can provide a good generic initialization for the new domain.",1.0
SKG_MT_665,https://openalex.org/W2963913268,2017,127,"['https://openalex.org/W6908809', 'https://openalex.org/W108437174', 'https://openalex.org/W222053410', 'https://openalex.org/W1411230545', 'https://openalex.org/W1869752048', 'https://openalex.org/W1902237438', 'https://openalex.org/W1904365287', 'https://openalex.org/W2096204319', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2150378737', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157331557', 'https://openalex.org/W2166905217', 'https://openalex.org/W2250974141', 'https://openalex.org/W2252272516', 'https://openalex.org/W2437005631', 'https://openalex.org/W2523000059', 'https://openalex.org/W2525778437', 'https://openalex.org/W2539201987', 'https://openalex.org/W2563574619', 'https://openalex.org/W2564486991', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W4392352873']","Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.",1.0
SKG_MT_668,https://openalex.org/W2104511424,2015,9,"['https://openalex.org/W113900115', 'https://openalex.org/W124954147', 'https://openalex.org/W776270814', 'https://openalex.org/W1534477342', 'https://openalex.org/W1543107604', 'https://openalex.org/W1597941798', 'https://openalex.org/W2043951469', 'https://openalex.org/W2098507980', 'https://openalex.org/W2098921539', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101234009', 'https://openalex.org/W2118296893', 'https://openalex.org/W2120654173', 'https://openalex.org/W2123301721', 'https://openalex.org/W2126400076', 'https://openalex.org/W2127331160', 'https://openalex.org/W2145865597', 'https://openalex.org/W2147258359', 'https://openalex.org/W2152180407', 'https://openalex.org/W2251251208', 'https://openalex.org/W2251861449', 'https://openalex.org/W2309409233', 'https://openalex.org/W2951299559']","This paper describes the USAAR-SHEFFIELD systems that participated in the Semantic Textual Similarity (STS) English task of SemEval-2015.We extend the work on using machine translation evaluation metrics in the STS task.Different from previous approaches, we regard the metrics' robustness across different text types and conflate the training data across different subcorpora.In addition, we introduce a novel deep regressor architecture and evaluated its efficiency in the STS task.",0.9953488372093023
SKG_MT_669,https://openalex.org/W2962931466,2018,91,"['https://openalex.org/W1753482797', 'https://openalex.org/W1849277567', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2175740768', 'https://openalex.org/W2194775991', 'https://openalex.org/W2525778437', 'https://openalex.org/W2563574619', 'https://openalex.org/W2613904329', 'https://openalex.org/W2669742347', 'https://openalex.org/W2741040846', 'https://openalex.org/W2794365787', 'https://openalex.org/W2925907129', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962822108', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963323244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964020032', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964189376', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964302946', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Advanced neural machine translation (NMT) models generally implement encoder and decoder as multiple layers, which allows systems to model complex functions and capture complicated linguistic structures. However, only the top layers of encoder and decoder are leveraged in the subsequent process, which misses the opportunity to exploit the useful information embedded in other layers. In this work, we propose to simultaneously expose all of these signals with layer aggregation and multi-layer attention mechanisms. In addition, we introduce an auxiliary regularization term to encourage different layers to capture diverse information. Experimental results on widely-used WMT14 English-German and WMT17 Chinese-English translation data demonstrate the effectiveness and universality of the proposed approach.",1.0
SKG_MT_672,https://openalex.org/W2889411721,2018,24,"['https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1869752048', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2212703438', 'https://openalex.org/W2549835527', 'https://openalex.org/W2563574619', 'https://openalex.org/W2564486991', 'https://openalex.org/W2586559132', 'https://openalex.org/W2595715041', 'https://openalex.org/W2606134370', 'https://openalex.org/W2737638662', 'https://openalex.org/W2756566411', 'https://openalex.org/W2760656271', 'https://openalex.org/W2903193068', 'https://openalex.org/W2949970154', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963407669', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564']","We introduce a novel multi-source technique for incorporating source syntax into neural machine translation using linearized parses. This is achieved by employing separate encoders for the sequential and parsed versions of the same source sentence; the resulting representations are then combined using a hierarchical attention mechanism. The proposed model improves over both seq2seq and parsed baselines by over 1 BLEU on the WMT17 English-German task. Further analysis shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines.",1.0
SKG_MT_673,https://openalex.org/W2311921240,2016,182,"['https://openalex.org/W196214544', 'https://openalex.org/W1522301498', 'https://openalex.org/W1589998400', 'https://openalex.org/W1606347560', 'https://openalex.org/W1753482797', 'https://openalex.org/W1889624880', 'https://openalex.org/W1902237438', 'https://openalex.org/W1970689298', 'https://openalex.org/W2048967369', 'https://openalex.org/W2064675550', 'https://openalex.org/W2069143585', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101609803', 'https://openalex.org/W2116599427', 'https://openalex.org/W2140679639', 'https://openalex.org/W2178903040', 'https://openalex.org/W2185720331', 'https://openalex.org/W2220350356', 'https://openalex.org/W2250956062', 'https://openalex.org/W2251012068', 'https://openalex.org/W2251069477', 'https://openalex.org/W2252335727', 'https://openalex.org/W2292633562', 'https://openalex.org/W2316776689', 'https://openalex.org/W2358307482', 'https://openalex.org/W2384495648', 'https://openalex.org/W2528305545', 'https://openalex.org/W2759607775', 'https://openalex.org/W2949563612', 'https://openalex.org/W2949679234', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950580142', 'https://openalex.org/W2950621961', 'https://openalex.org/W2951559648', 'https://openalex.org/W2953061907', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964308564', 'https://openalex.org/W3183153947']","The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.",1.0
SKG_MT_674,https://openalex.org/W2928169590,2019,9,"['https://openalex.org/W60337842', 'https://openalex.org/W648786980', 'https://openalex.org/W1522301498', 'https://openalex.org/W1931877416', 'https://openalex.org/W2024390895', 'https://openalex.org/W2037199950', 'https://openalex.org/W2121863487', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2158349948', 'https://openalex.org/W2176263492', 'https://openalex.org/W2242818861', 'https://openalex.org/W2268617045', 'https://openalex.org/W2296701362', 'https://openalex.org/W2547875792', 'https://openalex.org/W2626177496', 'https://openalex.org/W2746553466', 'https://openalex.org/W2754517384', 'https://openalex.org/W2778814079', 'https://openalex.org/W2888442053', 'https://openalex.org/W2962701888', 'https://openalex.org/W2962957031', 'https://openalex.org/W2963120839', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963580443', 'https://openalex.org/W2963962369', 'https://openalex.org/W2963991775', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W3082674894', 'https://openalex.org/W3204406378', 'https://openalex.org/W4230563027', 'https://openalex.org/W4301230920']","Weijia Xu, Xing Niu, Marine Carpuat. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_676,https://openalex.org/W2252077835,2014,0,"['https://openalex.org/W1543902963', 'https://openalex.org/W1647671624', 'https://openalex.org/W1716250762', 'https://openalex.org/W2006969979', 'https://openalex.org/W2100271871', 'https://openalex.org/W2101493333', 'https://openalex.org/W2106261033', 'https://openalex.org/W2119168550', 'https://openalex.org/W2138247936', 'https://openalex.org/W2138662031', 'https://openalex.org/W2153903004', 'https://openalex.org/W2154124206', 'https://openalex.org/W2251347599', 'https://openalex.org/W2252106864', 'https://openalex.org/W2404928265', 'https://openalex.org/W3204864772']",Interactive or Incremental Statistical Machine Translation (IMT) aims to provide a mechanism that allows the statistical models involved in the translation process to be incrementally updated and improved.The source of knowledge normally comes from users who either post-edit the entire translation or just provide the translations for wrongly translated domain-specific terminologies.Most of the existing work on IMT uses batch learning paradigm which does not allow translation systems to make use of the new input instantaneously.We introduce an adaptive MT framework with a Rule Definition Language (RDL) for users to amend MT results through translation rules or patterns.Experimental results show that our system acknowledges user feedback via RDL which improves the translations of the baseline system on three test sets for Vietnamese to English translation.,1.0
SKG_MT_677,https://openalex.org/W3118955479,2020,2,"['https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2567571499', 'https://openalex.org/W2889326796', 'https://openalex.org/W2904463355', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963708445', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970259631', 'https://openalex.org/W2978943549', 'https://openalex.org/W3204406378']","In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our post-submission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic, backtranslated corpus followed by fine-tuning on the original parallel training data.",1.0
SKG_MT_680,https://openalex.org/W2165569186,2014,5,"['https://openalex.org/W22168010', 'https://openalex.org/W99956235', 'https://openalex.org/W119387329', 'https://openalex.org/W127865248', 'https://openalex.org/W188912188', 'https://openalex.org/W1516289245', 'https://openalex.org/W1534218608', 'https://openalex.org/W1535015163', 'https://openalex.org/W2036239181', 'https://openalex.org/W2087165009', 'https://openalex.org/W2087735403', 'https://openalex.org/W2093647425', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103288873', 'https://openalex.org/W2113896283', 'https://openalex.org/W2115990601', 'https://openalex.org/W2125712079', 'https://openalex.org/W2126747756', 'https://openalex.org/W2126920742', 'https://openalex.org/W2149327368', 'https://openalex.org/W2162669657', 'https://openalex.org/W2164984707', 'https://openalex.org/W2250947781', 'https://openalex.org/W2251796964', 'https://openalex.org/W2251994258', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2460591548', 'https://openalex.org/W2771976988']","We describe the DCU-MIXED and DCU-SVR submissions to the WMT-14 Quality Estimation task 1.1, predicting sentencelevel perceived post-editing effort.Feature design focuses on target-side features as we hypothesise that the source side has little effect on the quality of human translations, which are included in task 1.1 of this year's WMT Quality Estimation shared task.We experiment with features of the QuEst framework, features of our past work, and three novel feature sets.Despite these efforts, our two systems perform poorly in the competition.Follow up experiments indicate that the poor performance is due to improperly optimised parameters.",1.0
SKG_MT_681,https://openalex.org/W2145045785,2013,7,"['https://openalex.org/W16967297', 'https://openalex.org/W132913264', 'https://openalex.org/W605575788', 'https://openalex.org/W947051507', 'https://openalex.org/W1565992329', 'https://openalex.org/W1606508130', 'https://openalex.org/W1793412061', 'https://openalex.org/W1969974515', 'https://openalex.org/W2008961349', 'https://openalex.org/W2018156128', 'https://openalex.org/W2044804339', 'https://openalex.org/W2087735403', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097997328', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105245376', 'https://openalex.org/W2111798208', 'https://openalex.org/W2113688665', 'https://openalex.org/W2123301721', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131367528', 'https://openalex.org/W2144900797', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153927306', 'https://openalex.org/W2154124206', 'https://openalex.org/W2158195707', 'https://openalex.org/W2158917268', 'https://openalex.org/W2162245945', 'https://openalex.org/W2166306133', 'https://openalex.org/W2169724380', 'https://openalex.org/W2171421863', 'https://openalex.org/W2187252394', 'https://openalex.org/W2401082558', 'https://openalex.org/W2437005631', 'https://openalex.org/W3166311385', 'https://openalex.org/W3203906782']","Despite being closely related languages, German and English are characterized by important word order differences. Longrange reordering of verbs, in particular, represents a real challenge for state-of-theart SMT systems and is one of the main reasons why translation quality is often so poor in this language pair. In this work, we review several solutions to improve the accuracy of German-English word reordering while preserving the efficiency of phrase-based decoding. Among these, we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena. Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT. 1",1.0
SKG_MT_682,https://openalex.org/W2250821182,2013,14,"['https://openalex.org/W52337014', 'https://openalex.org/W111475876', 'https://openalex.org/W150793826', 'https://openalex.org/W180782768', 'https://openalex.org/W187531367', 'https://openalex.org/W569674810', 'https://openalex.org/W1501009823', 'https://openalex.org/W1501568714', 'https://openalex.org/W1586345832', 'https://openalex.org/W1794363059', 'https://openalex.org/W1829822087', 'https://openalex.org/W1914611143', 'https://openalex.org/W2041232209', 'https://openalex.org/W2042891399', 'https://openalex.org/W2047995599', 'https://openalex.org/W2066308426', 'https://openalex.org/W2083624497', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102074916', 'https://openalex.org/W2102749417', 'https://openalex.org/W2118392464', 'https://openalex.org/W2134373401', 'https://openalex.org/W2135591120', 'https://openalex.org/W2135890475', 'https://openalex.org/W2136925175', 'https://openalex.org/W2138477841', 'https://openalex.org/W2138781677', 'https://openalex.org/W2143345705', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150028966', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155794909', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157435188', 'https://openalex.org/W2157758844', 'https://openalex.org/W2158207953', 'https://openalex.org/W2162842776', 'https://openalex.org/W2163986298', 'https://openalex.org/W2165595870', 'https://openalex.org/W2250554077', 'https://openalex.org/W2258247613', 'https://openalex.org/W2294875721', 'https://openalex.org/W2434901392', 'https://openalex.org/W2656534363', 'https://openalex.org/W2913739034', 'https://openalex.org/W2950186769', 'https://openalex.org/W3160781207']","We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline1. 1",1.0
SKG_MT_683,https://openalex.org/W2970558573,2019,56,"['https://openalex.org/W1673923490', 'https://openalex.org/W2101105183', 'https://openalex.org/W2184135559', 'https://openalex.org/W2766108848', 'https://openalex.org/W2767899794', 'https://openalex.org/W2791941932', 'https://openalex.org/W2794365787', 'https://openalex.org/W2811010710', 'https://openalex.org/W2906152891', 'https://openalex.org/W2909737760', 'https://openalex.org/W2916835973', 'https://openalex.org/W2922293812', 'https://openalex.org/W2922349260', 'https://openalex.org/W2946817437', 'https://openalex.org/W2962713901', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963062785', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963697731', 'https://openalex.org/W2963823140', 'https://openalex.org/W2963919854', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964153729', 'https://openalex.org/W2964247056', 'https://openalex.org/W2970069595', 'https://openalex.org/W2970250638', 'https://openalex.org/W2970336035', 'https://openalex.org/W2970798168', 'https://openalex.org/W2970926924', 'https://openalex.org/W2971134989', 'https://openalex.org/W2984051011', 'https://openalex.org/W2998277219', 'https://openalex.org/W4294536576', 'https://openalex.org/W4385245566', 'https://openalex.org/W6745507446']","Xian Li, Paul Michel, Antonios Anastasopoulos, Yonatan Belinkov, Nadir Durrani, Orhan Firat, Philipp Koehn, Graham Neubig, Juan Pino, Hassan Sajjad. Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1). 2019.",1.0
SKG_MT_684,https://openalex.org/W2797162333,2018,3,"['https://openalex.org/W587794757', 'https://openalex.org/W1522301498', 'https://openalex.org/W1606347560', 'https://openalex.org/W1839773802', 'https://openalex.org/W2073061372', 'https://openalex.org/W2102695322', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2172140247', 'https://openalex.org/W2577255746', 'https://openalex.org/W2587694128', 'https://openalex.org/W2594229957', 'https://openalex.org/W2613065256', 'https://openalex.org/W2625092622', 'https://openalex.org/W2964299589']","Neural machine translation has achieved levels of fluency and adequacy that would have been surprising a short time ago. Output quality is extremely relevant for industry purposes, however it is equally important to produce results in the shortest time possible, mainly for latency-sensitive applications and to control cloud hosting costs. In this paper we show the effectiveness of translating with 8-bit quantization for models that have been trained using 32-bit floating point values. Results show that 8-bit translation makes a non-negligible impact in terms of speed with no degradation in accuracy and adequacy.",0.9896907216494846
SKG_MT_685,https://openalex.org/W3117253141,2020,1,"['https://openalex.org/W612394713', 'https://openalex.org/W615146516', 'https://openalex.org/W2759607775', 'https://openalex.org/W2918362473', 'https://openalex.org/W2950940239', 'https://openalex.org/W2954647460', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963341956', 'https://openalex.org/W2969379466']","In this paper, we describe our TMU neural machine translation (NMT) system submitted for the Patent task (Korean→Japanese) of the 7th Workshop on Asian Translation (WAT 2020, Nakazawa et al., 2020). We propose a novel method to train a Korean-to-Japanese translation model. Specifically, we focus on the vocabulary overlap of Korean Hanja words and Japanese Kanji words, and propose strategies to leverage Hanja information. Our experiment shows that Hanja information is effective within a specific domain, leading to an improvement in the BLEU scores by +1.09 points compared to the baseline.",1.0
SKG_MT_687,https://openalex.org/W2971278086,2019,65,"['https://openalex.org/W131501223', 'https://openalex.org/W1522301498', 'https://openalex.org/W1993378086', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2184135559', 'https://openalex.org/W2194775991', 'https://openalex.org/W2582446770', 'https://openalex.org/W2613904329', 'https://openalex.org/W2767019613', 'https://openalex.org/W2794365787', 'https://openalex.org/W2799051177', 'https://openalex.org/W2808508619', 'https://openalex.org/W2891534142', 'https://openalex.org/W2962712961', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566']","Xin Tan, Longyin Zhang, Deyi Xiong, Guodong Zhou. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_689,https://openalex.org/W3119961061,2020,22,"['https://openalex.org/W2134800885', 'https://openalex.org/W2148708890', 'https://openalex.org/W2493916176', 'https://openalex.org/W2587694128', 'https://openalex.org/W2669742347', 'https://openalex.org/W2889326796', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963979492', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970311224', 'https://openalex.org/W2970316683', 'https://openalex.org/W2970947975']","This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh/En, Km/En, and Ps/En and in both directions under the constrained condition. We use the standard Transformer-Big model as the baseline and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our models such as Back Translation, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.",1.0
SKG_MT_690,https://openalex.org/W2989400327,2019,4,"['https://openalex.org/W2101105183', 'https://openalex.org/W2798931235', 'https://openalex.org/W2914120296', 'https://openalex.org/W2950428495', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964013027', 'https://openalex.org/W2971254483', 'https://openalex.org/W2977587102', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","This paper presents the NICT's participation (team ID: NICT) in the 6th Workshop on Asian Translation (WAT-2019) shared translation task, specifically Myanmar (Burmese) - English task in both translation directions. We built neural machine translation (NMT) systems for these tasks. Our NMT systems were trained with language model pretraining. Back-translation technology is adopted to NMT. Our NMT systems rank the third in English-to-Myanmar and the second in Myanmar-to-English according to BLEU score.",0.9947089947089947
SKG_MT_691,https://openalex.org/W2797871962,2018,14,"['https://openalex.org/W1522301498', 'https://openalex.org/W2080373976', 'https://openalex.org/W2098297786', 'https://openalex.org/W2116792345', 'https://openalex.org/W2134800885', 'https://openalex.org/W2140372282', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153013403', 'https://openalex.org/W2159755860', 'https://openalex.org/W2170527467', 'https://openalex.org/W2250591774', 'https://openalex.org/W2250653840', 'https://openalex.org/W2321916036', 'https://openalex.org/W2415893189', 'https://openalex.org/W2470324779', 'https://openalex.org/W2726264694', 'https://openalex.org/W2785047343', 'https://openalex.org/W2795467683', 'https://openalex.org/W2917881729', 'https://openalex.org/W2953320089', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963109131', 'https://openalex.org/W2963266340', 'https://openalex.org/W2964187553']","We combine two of the most popular approaches to automated Grammatical Error Correction (GEC): GEC based on Statistical Machine Translation (SMT) and GEC based on Neural Machine Translation (NMT). The hybrid system achieves new state-of-the-art results on the CoNLL-2014 and JFLEG benchmarks. This GEC system preserves the accuracy of SMT output and, at the same time, generates more fluent sentences as it typical for NMT. Our analysis shows that the created systems are closer to reaching human-level performance than any other GEC system reported so far.",1.0
SKG_MT_693,https://openalex.org/W3034324324,2020,24,"['https://openalex.org/W1787224781', 'https://openalex.org/W1821462560', 'https://openalex.org/W1902674502', 'https://openalex.org/W1916559533', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149298154', 'https://openalex.org/W2294370754', 'https://openalex.org/W2367397349', 'https://openalex.org/W2561412020', 'https://openalex.org/W2587529872', 'https://openalex.org/W2590082389', 'https://openalex.org/W2613904329', 'https://openalex.org/W2617799811', 'https://openalex.org/W2741040846', 'https://openalex.org/W2783643842', 'https://openalex.org/W2798698226', 'https://openalex.org/W2905933322', 'https://openalex.org/W2912070261', 'https://openalex.org/W2933138175', 'https://openalex.org/W2952650870', 'https://openalex.org/W2952682849', 'https://openalex.org/W2962807820', 'https://openalex.org/W2962834107', 'https://openalex.org/W2962959086', 'https://openalex.org/W2963205761', 'https://openalex.org/W2963233086', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963598809', 'https://openalex.org/W2964159778', 'https://openalex.org/W2964178496', 'https://openalex.org/W2964203871', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971154170', 'https://openalex.org/W2971296520', 'https://openalex.org/W2972384303', 'https://openalex.org/W3034917890', 'https://openalex.org/W3211848854', 'https://openalex.org/W4385245566']","Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments.",1.0
SKG_MT_695,https://openalex.org/W2594990650,2017,460,"['https://openalex.org/W1514535095', 'https://openalex.org/W1518951372', 'https://openalex.org/W1591706642', 'https://openalex.org/W1689711448', 'https://openalex.org/W1753482797', 'https://openalex.org/W1895577753', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100664567', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2194775991', 'https://openalex.org/W2341401723', 'https://openalex.org/W2402144811', 'https://openalex.org/W2481240925', 'https://openalex.org/W2525778437', 'https://openalex.org/W2552839021', 'https://openalex.org/W2557728737', 'https://openalex.org/W2953384591', 'https://openalex.org/W2953390309', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963929190', 'https://openalex.org/W2963963856', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964308564', 'https://openalex.org/W4394643672']","Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in TensorFlow to make it easy for others to reproduce our results and perform their own experiments.",1.0
SKG_MT_696,https://openalex.org/W2948845926,2019,6,"['https://openalex.org/W1524333225', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2116648050', 'https://openalex.org/W2183341477', 'https://openalex.org/W2295297373', 'https://openalex.org/W2327501763', 'https://openalex.org/W2507959295', 'https://openalex.org/W2530876040', 'https://openalex.org/W2563850823', 'https://openalex.org/W2593011301', 'https://openalex.org/W2762715843', 'https://openalex.org/W2788575190', 'https://openalex.org/W2792296443', 'https://openalex.org/W2794753807', 'https://openalex.org/W2794999731', 'https://openalex.org/W2795138957', 'https://openalex.org/W2888958984', 'https://openalex.org/W2891066653', 'https://openalex.org/W2897414631', 'https://openalex.org/W2950613790', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963946371', 'https://openalex.org/W2964121744', 'https://openalex.org/W3012492057']","Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. We see improvements of up to 5 BLEU on both our high and low resource language pairs, with a reduction in training time of 60%. Our improvements hold across multiple data sizes and two language pairs.",1.0
SKG_MT_697,https://openalex.org/W2107705235,2011,5,"['https://openalex.org/W17659133', 'https://openalex.org/W94354135', 'https://openalex.org/W110357785', 'https://openalex.org/W122584218', 'https://openalex.org/W165935821', 'https://openalex.org/W230880734', 'https://openalex.org/W1707559977', 'https://openalex.org/W1828578481', 'https://openalex.org/W1961521608', 'https://openalex.org/W1979495315', 'https://openalex.org/W2006969979', 'https://openalex.org/W2050971845', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103156738', 'https://openalex.org/W2106751371', 'https://openalex.org/W2110104386', 'https://openalex.org/W2115289978', 'https://openalex.org/W2117238933', 'https://openalex.org/W2124807415', 'https://openalex.org/W2140785880', 'https://openalex.org/W2143008661', 'https://openalex.org/W2144279206', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150378737', 'https://openalex.org/W2158364201', 'https://openalex.org/W2158388102', 'https://openalex.org/W2166905217', 'https://openalex.org/W2167393476', 'https://openalex.org/W2168966090', 'https://openalex.org/W2437005631', 'https://openalex.org/W2571124179', 'https://openalex.org/W3135099841']","We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST-08 evaluations by 1.3 absolute BLEU, which is statistically significant. 1",1.0
SKG_MT_698,https://openalex.org/W2513529543,2016,3,"['https://openalex.org/W76590478', 'https://openalex.org/W1551202288', 'https://openalex.org/W1631260214', 'https://openalex.org/W1736600331', 'https://openalex.org/W1829822087', 'https://openalex.org/W1916559533', 'https://openalex.org/W2001064229', 'https://openalex.org/W2006969979', 'https://openalex.org/W2095690342', 'https://openalex.org/W2097927681', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106310992', 'https://openalex.org/W2109776855', 'https://openalex.org/W2110104386', 'https://openalex.org/W2113541941', 'https://openalex.org/W2114396466', 'https://openalex.org/W2116492146', 'https://openalex.org/W2118536060', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131988669', 'https://openalex.org/W2132622673', 'https://openalex.org/W2144279206', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159755860', 'https://openalex.org/W2161795601', 'https://openalex.org/W2164415172', 'https://openalex.org/W2166781037', 'https://openalex.org/W2250907725', 'https://openalex.org/W2251324170', 'https://openalex.org/W2251844685', 'https://openalex.org/W2401082558', 'https://openalex.org/W2595715041', 'https://openalex.org/W2620000475', 'https://openalex.org/W2950186769', 'https://openalex.org/W3211848854', 'https://openalex.org/W4241645538']","One major drawback of phrase-based translation is that it segments an input sentence into continuous phrases.To support linguistically informed source discontinuity, in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model.The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase.We use beam search to combine translations of each subgraph left-to-right to produce a complete translation.Experiments on Chinese-English and German-English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores.By explicitly modeling the graph segmentation, our system obtains further improvement, especially on German-English.",1.0
SKG_MT_699,https://openalex.org/W2250421192,2015,51,"['https://openalex.org/W179875071', 'https://openalex.org/W1533861849', 'https://openalex.org/W1544826511', 'https://openalex.org/W1610356397', 'https://openalex.org/W1834061219', 'https://openalex.org/W1984052055', 'https://openalex.org/W1987879674', 'https://openalex.org/W2028176545', 'https://openalex.org/W2070150502', 'https://openalex.org/W2078861931', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116492146', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133280805', 'https://openalex.org/W2137901323', 'https://openalex.org/W2141599568', 'https://openalex.org/W2146502635', 'https://openalex.org/W2147192413', 'https://openalex.org/W2149327368', 'https://openalex.org/W2163582407', 'https://openalex.org/W2163986298', 'https://openalex.org/W2165894879', 'https://openalex.org/W2166545452', 'https://openalex.org/W2250188786', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250825001', 'https://openalex.org/W2251171083', 'https://openalex.org/W2251610689', 'https://openalex.org/W2251682575', 'https://openalex.org/W2251803266', 'https://openalex.org/W2251939518', 'https://openalex.org/W2257408573', 'https://openalex.org/W2331726854', 'https://openalex.org/W2895810819', 'https://openalex.org/W2917452219', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950133940', 'https://openalex.org/W2952230511', 'https://openalex.org/W2998704965', 'https://openalex.org/W4285719527', 'https://openalex.org/W4294170691']","Francisco Guzmán, Shafiq Joty, Lluís Màrquez, Preslav Nakov. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.",1.0
SKG_MT_701,https://openalex.org/W2963829526,2018,116,"['https://openalex.org/W1489834179', 'https://openalex.org/W1578564752', 'https://openalex.org/W2125536435', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2413436069', 'https://openalex.org/W2418388682', 'https://openalex.org/W2522770641', 'https://openalex.org/W2566278350', 'https://openalex.org/W2566564022', 'https://openalex.org/W2566623769', 'https://openalex.org/W2573728411', 'https://openalex.org/W2595715041', 'https://openalex.org/W2618463334', 'https://openalex.org/W2740743644', 'https://openalex.org/W2757291580', 'https://openalex.org/W2757592053', 'https://openalex.org/W2759932073', 'https://openalex.org/W2903012348', 'https://openalex.org/W2953044442', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963593215', 'https://openalex.org/W2963841178', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964334713', 'https://openalex.org/W3202105828', 'https://openalex.org/W4241645538', 'https://openalex.org/W4285719527']","Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig, Satoshi Nakamura. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",1.0
SKG_MT_702,https://openalex.org/W2952682849,2019,86,"['https://openalex.org/W1522301498', 'https://openalex.org/W1787224781', 'https://openalex.org/W1916559533', 'https://openalex.org/W2006969979', 'https://openalex.org/W2095705004', 'https://openalex.org/W2119224513', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141532438', 'https://openalex.org/W2144169942', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2590082389', 'https://openalex.org/W2605717780', 'https://openalex.org/W2613904329', 'https://openalex.org/W2741040846', 'https://openalex.org/W2803369080', 'https://openalex.org/W2896457183', 'https://openalex.org/W2946359678', 'https://openalex.org/W2946794439', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962807820', 'https://openalex.org/W2962834107', 'https://openalex.org/W2963233086', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499882', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963598809', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964178496', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3211259717', 'https://openalex.org/W3211848854', 'https://openalex.org/W4241645538', 'https://openalex.org/W4288347855', 'https://openalex.org/W4298170715', 'https://openalex.org/W4300756893', 'https://openalex.org/W4385245566']","Prior researches suggest that neural machine translation (NMT) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some NMT models. This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics.",1.0
SKG_MT_703,https://openalex.org/W2970644300,2019,7,"['https://openalex.org/W1542713999', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153579005', 'https://openalex.org/W2462831000', 'https://openalex.org/W2493916176', 'https://openalex.org/W2595715041', 'https://openalex.org/W2798931235', 'https://openalex.org/W2890007195', 'https://openalex.org/W2891081936', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902873342', 'https://openalex.org/W2914120296', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964308564', 'https://openalex.org/W4294170691', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']",We describe LMU Munich’s machine translation system for German→Czech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our model using monolingual data only from both languages. The final model is an unsupervised neural model using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.,1.0
SKG_MT_705,https://openalex.org/W2107440414,2013,56,"['https://openalex.org/W122999227', 'https://openalex.org/W142179422', 'https://openalex.org/W222053410', 'https://openalex.org/W1631260214', 'https://openalex.org/W1736600331', 'https://openalex.org/W1964279429', 'https://openalex.org/W1965058830', 'https://openalex.org/W1969974515', 'https://openalex.org/W2095650036', 'https://openalex.org/W2098032362', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107878390', 'https://openalex.org/W2111355378', 'https://openalex.org/W2116792345', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2136657878', 'https://openalex.org/W2137841811', 'https://openalex.org/W2144879357', 'https://openalex.org/W2153204578', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153999629', 'https://openalex.org/W2158733102', 'https://openalex.org/W2159755860', 'https://openalex.org/W2161227214', 'https://openalex.org/W2250651922', 'https://openalex.org/W2293596968', 'https://openalex.org/W2408503330', 'https://openalex.org/W3166311385']","The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memo-rize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrase-based search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve. 1",0.9929078014184397
SKG_MT_706,https://openalex.org/W2250640401,2013,21,"['https://openalex.org/W22168010', 'https://openalex.org/W38126138', 'https://openalex.org/W149643677', 'https://openalex.org/W161217653', 'https://openalex.org/W183066880', 'https://openalex.org/W1508577659', 'https://openalex.org/W1631260214', 'https://openalex.org/W2041232209', 'https://openalex.org/W2084277454', 'https://openalex.org/W2102028293', 'https://openalex.org/W2103042430', 'https://openalex.org/W2121415745', 'https://openalex.org/W2121745180', 'https://openalex.org/W2124807415', 'https://openalex.org/W2136346830', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2146574666', 'https://openalex.org/W2156985047', 'https://openalex.org/W2169360026', 'https://openalex.org/W2172268343', 'https://openalex.org/W2250229103', 'https://openalex.org/W2597684388', 'https://openalex.org/W2600716915', 'https://openalex.org/W3037265734']","We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%. We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets.",1.0
SKG_MT_707,https://openalex.org/W2252053087,2013,15,"['https://openalex.org/W140816929', 'https://openalex.org/W635530177', 'https://openalex.org/W1482214997', 'https://openalex.org/W1994581546', 'https://openalex.org/W2033593667', 'https://openalex.org/W2041232209', 'https://openalex.org/W2041305041', 'https://openalex.org/W2045079045', 'https://openalex.org/W2049901611', 'https://openalex.org/W2072976288', 'https://openalex.org/W2095669743', 'https://openalex.org/W2098986246', 'https://openalex.org/W2102749417', 'https://openalex.org/W2103042430', 'https://openalex.org/W2105673178', 'https://openalex.org/W2117278770', 'https://openalex.org/W2119013622', 'https://openalex.org/W2121415745', 'https://openalex.org/W2121745180', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127849236', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159755860', 'https://openalex.org/W2160218441', 'https://openalex.org/W2167265720', 'https://openalex.org/W2169360026', 'https://openalex.org/W2170571488', 'https://openalex.org/W2172268343', 'https://openalex.org/W2185701500', 'https://openalex.org/W2244987555', 'https://openalex.org/W2250229103', 'https://openalex.org/W2251302843', 'https://openalex.org/W2282154246', 'https://openalex.org/W2525721411']","When using a machine translation (MT) model trained on OLD-domain parallel data to translate NEW-domain text, one major challenge is the large number of out-of-vocabulary (OOV) and new-translation-sense words. We present a method to identify new translations of both known and unknown source language words that uses NEW-domain comparable document pairs. Starting with a joint distribution of source-target word pairs derived from the OLD-domain parallel corpus, our method recovers a new joint distribution that matches the marginal distributions of the NEW-domain comparable document pairs, while minimizing the divergence from the OLD-domain distribution. Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines.",1.0
SKG_MT_708,https://openalex.org/W2110168585,2013,41,"['https://openalex.org/W222053410', 'https://openalex.org/W1631260214', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101207453', 'https://openalex.org/W2113788796', 'https://openalex.org/W2115848042', 'https://openalex.org/W2119168550', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2163255203', 'https://openalex.org/W2181317029', 'https://openalex.org/W2250907725']","Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German!English and a larger French!German translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French!German task and 0.3% BLEU and 1.1% TER on the German!English task.",1.0
SKG_MT_709,https://openalex.org/W2891534142,2018,275,"['https://openalex.org/W112038384', 'https://openalex.org/W222053410', 'https://openalex.org/W255975419', 'https://openalex.org/W835791623', 'https://openalex.org/W2087735403', 'https://openalex.org/W2089150068', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136353104', 'https://openalex.org/W2166545452', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250761393', 'https://openalex.org/W2252050571', 'https://openalex.org/W2419539795', 'https://openalex.org/W2470673105', 'https://openalex.org/W2525778437', 'https://openalex.org/W2581969392', 'https://openalex.org/W2582446770', 'https://openalex.org/W2595715041', 'https://openalex.org/W2601662369', 'https://openalex.org/W2608029998', 'https://openalex.org/W2739563583', 'https://openalex.org/W2755908707', 'https://openalex.org/W2756773371', 'https://openalex.org/W2757417237', 'https://openalex.org/W2758358480', 'https://openalex.org/W2760138945', 'https://openalex.org/W2783419700', 'https://openalex.org/W2799051177', 'https://openalex.org/W2953320089', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962954913', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964125283', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964298349', 'https://openalex.org/W4294619053', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.",1.0
SKG_MT_710,https://openalex.org/W2250218970,2014,15,"['https://openalex.org/W1631260214', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107878390', 'https://openalex.org/W2113104171', 'https://openalex.org/W2117278770', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147262247', 'https://openalex.org/W2152263452', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2251202280', 'https://openalex.org/W2399188371', 'https://openalex.org/W2406343028']","Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest.Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus.By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model.In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection.The results show that our methods outperform previous methods.When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points.",1.0
SKG_MT_712,https://openalex.org/W2156816974,2011,15,"['https://openalex.org/W23077562', 'https://openalex.org/W222053410', 'https://openalex.org/W1535015163', 'https://openalex.org/W1631260214', 'https://openalex.org/W1632114991', 'https://openalex.org/W1969974515', 'https://openalex.org/W1970689298', 'https://openalex.org/W1974094162', 'https://openalex.org/W2016522586', 'https://openalex.org/W2056250865', 'https://openalex.org/W2097661835', 'https://openalex.org/W2097997328', 'https://openalex.org/W2098786722', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109664771', 'https://openalex.org/W2113788796', 'https://openalex.org/W2119825066', 'https://openalex.org/W2122978558', 'https://openalex.org/W2123126659', 'https://openalex.org/W2124807415', 'https://openalex.org/W2128201184', 'https://openalex.org/W2140343992', 'https://openalex.org/W2143866356', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158195707', 'https://openalex.org/W2160360506', 'https://openalex.org/W2171421863', 'https://openalex.org/W2188741930', 'https://openalex.org/W2280403519', 'https://openalex.org/W2401082558', 'https://openalex.org/W2786062171', 'https://openalex.org/W2989631226', 'https://openalex.org/W3159819647']","Part-of-speech language modeling is commonly used as a component in statistical machine translation systems, but there is mixed evidence that its usage leads to significant improvements. We argue that its limited effectiveness is due to the lack of lexicalization. We introduce a new approach that builds a separate local language model for each word and part-of-speech pair. The resulting models lead to more context-sensitive probability distributions and we also exploit the fact that different local models are used to estimate the language model probability of each word during decoding. Our approach is evaluated for Arabic- and Chinese-to-English translation. We show that it leads to statistically significant improvements for multiple test sets and also across different genres, when compared against a competitive baseline and a system using a part-of-speech model. 1",1.0
SKG_MT_714,https://openalex.org/W3035473397,2020,34,"['https://openalex.org/W84636783', 'https://openalex.org/W179391383', 'https://openalex.org/W635530177', 'https://openalex.org/W1482328859', 'https://openalex.org/W1490627662', 'https://openalex.org/W1492371337', 'https://openalex.org/W1530032857', 'https://openalex.org/W1603508585', 'https://openalex.org/W1767907121', 'https://openalex.org/W1830628992', 'https://openalex.org/W1845198550', 'https://openalex.org/W2015629112', 'https://openalex.org/W2046456023', 'https://openalex.org/W2093976457', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112890532', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132532452', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148708890', 'https://openalex.org/W2154674287', 'https://openalex.org/W2161477126', 'https://openalex.org/W2162311237', 'https://openalex.org/W2167429950', 'https://openalex.org/W2257408573', 'https://openalex.org/W2399777719', 'https://openalex.org/W2406764433', 'https://openalex.org/W2511728827', 'https://openalex.org/W2595715041', 'https://openalex.org/W2741029840', 'https://openalex.org/W2758685863', 'https://openalex.org/W2792575422', 'https://openalex.org/W2796108585', 'https://openalex.org/W2886095922', 'https://openalex.org/W2889326796', 'https://openalex.org/W2889361079', 'https://openalex.org/W2901161074', 'https://openalex.org/W2913337508', 'https://openalex.org/W2915128229', 'https://openalex.org/W2915704452', 'https://openalex.org/W2928452960', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963617771', 'https://openalex.org/W2964343359', 'https://openalex.org/W2970250706', 'https://openalex.org/W3007047534', 'https://openalex.org/W3030094404', 'https://openalex.org/W3086206064', 'https://openalex.org/W3192053425', 'https://openalex.org/W4232491506', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394127431']","We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013–2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.",1.0
SKG_MT_715,https://openalex.org/W2153508793,2010,296,"['https://openalex.org/W1489525520', 'https://openalex.org/W2001810881', 'https://openalex.org/W2018156128', 'https://openalex.org/W2088781183', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2124379907', 'https://openalex.org/W2136544838', 'https://openalex.org/W2147192413', 'https://openalex.org/W2149327368', 'https://openalex.org/W2149811020', 'https://openalex.org/W2150824314', 'https://openalex.org/W2152803722', 'https://openalex.org/W2166979461', 'https://openalex.org/W3204893721']","Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate ‘A because B ’ as ‘B because A. ’ Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1",1.0
SKG_MT_717,https://openalex.org/W2964085268,2018,323,"['https://openalex.org/W1522301498', 'https://openalex.org/W1533861849', 'https://openalex.org/W1541096673', 'https://openalex.org/W1595740843', 'https://openalex.org/W1632114991', 'https://openalex.org/W1832693441', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153579005', 'https://openalex.org/W2296283641', 'https://openalex.org/W2493916176', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555428947', 'https://openalex.org/W2594021297', 'https://openalex.org/W2762484717', 'https://openalex.org/W2780932362', 'https://openalex.org/W2786058094', 'https://openalex.org/W2952190837', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962902328', 'https://openalex.org/W2963011474', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963260927', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W4294170691', 'https://openalex.org/W4294367149', 'https://openalex.org/W4299579390', 'https://openalex.org/W4307459710']","Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, Graham Neubig. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 2018.",0.9939393939393939
SKG_MT_718,https://openalex.org/W3034508791,2020,12,"['https://openalex.org/W22168010', 'https://openalex.org/W222053410', 'https://openalex.org/W1916559533', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114719613', 'https://openalex.org/W2123301721', 'https://openalex.org/W2130942839', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153508793', 'https://openalex.org/W2166306133', 'https://openalex.org/W2183341477', 'https://openalex.org/W2624521690', 'https://openalex.org/W2727767747', 'https://openalex.org/W2736671181', 'https://openalex.org/W2739967986', 'https://openalex.org/W2803214681', 'https://openalex.org/W2892145626', 'https://openalex.org/W2903193068', 'https://openalex.org/W2933138175', 'https://openalex.org/W2953092638', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970971581', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971043182', 'https://openalex.org/W3035512170']","The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.",0.995850622406639
SKG_MT_719,https://openalex.org/W2250861721,2013,17,"['https://openalex.org/W222053410', 'https://openalex.org/W1574901103', 'https://openalex.org/W1880262756', 'https://openalex.org/W2004447574', 'https://openalex.org/W2054399842', 'https://openalex.org/W2066296458', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105577415', 'https://openalex.org/W2115410424', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132001515', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2160131015']","We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations. Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. A significant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available. Thus, our approach is well-suited to the causal constraint of spoken conversations. On an English-to-Iraqi CSLT task, the proposed approach gives significant improvements over a baseline system as measured by BLEU, TER, and NIST. Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation. 1",1.0
SKG_MT_721,https://openalex.org/W2970295111,2019,315,"['https://openalex.org/W2117278770', 'https://openalex.org/W2134800885', 'https://openalex.org/W2169200297', 'https://openalex.org/W2183341477', 'https://openalex.org/W2561274697', 'https://openalex.org/W2595715041', 'https://openalex.org/W2889326796', 'https://openalex.org/W2933138175', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963672008', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970694516', 'https://openalex.org/W2975153649', 'https://openalex.org/W2988451549', 'https://openalex.org/W2988739750', 'https://openalex.org/W2995746049', 'https://openalex.org/W3032816972', 'https://openalex.org/W3046368065', 'https://openalex.org/W3107826490', 'https://openalex.org/W4288400010', 'https://openalex.org/W4385245566']","This paper describes Facebook FAIR’s submission to the WMT19 shared news translation task. We participate in four language directions, English <-> German and English <-> Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system’s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English→Russian.",1.0
SKG_MT_724,https://openalex.org/W1804620186,2012,2,"['https://openalex.org/W110357785', 'https://openalex.org/W176283576', 'https://openalex.org/W1964591498', 'https://openalex.org/W1965058830', 'https://openalex.org/W1979495315', 'https://openalex.org/W2000359198', 'https://openalex.org/W2006969979', 'https://openalex.org/W2018156128', 'https://openalex.org/W2037894654', 'https://openalex.org/W2095755718', 'https://openalex.org/W2099084090', 'https://openalex.org/W2099224638', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108460050', 'https://openalex.org/W2114480995', 'https://openalex.org/W2118536060', 'https://openalex.org/W2119168550', 'https://openalex.org/W2119388680', 'https://openalex.org/W2123635983', 'https://openalex.org/W2124810114', 'https://openalex.org/W2124818604', 'https://openalex.org/W2140702357', 'https://openalex.org/W2144900797', 'https://openalex.org/W2146574666', 'https://openalex.org/W2158065314', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158733102', 'https://openalex.org/W2159506050', 'https://openalex.org/W2162245945', 'https://openalex.org/W2727254520', 'https://openalex.org/W3166311385', 'https://openalex.org/W3198494294', 'https://openalex.org/W4285719527']","A forced derivation tree (FDT) of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems. We first describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities. As the first step in this line of research, we verify the effectiveness of our approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality. 1",1.0
SKG_MT_725,https://openalex.org/W407827705,2013,1,"['https://openalex.org/W1464282338', 'https://openalex.org/W1631260214', 'https://openalex.org/W1966771059', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111142112', 'https://openalex.org/W2120459453', 'https://openalex.org/W2122465391', 'https://openalex.org/W2123825474', 'https://openalex.org/W2125536435', 'https://openalex.org/W2125595887', 'https://openalex.org/W2134800885', 'https://openalex.org/W2140343992', 'https://openalex.org/W2146502635', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158614781', 'https://openalex.org/W2159755860', 'https://openalex.org/W2161227214', 'https://openalex.org/W2164766438', 'https://openalex.org/W2169039522', 'https://openalex.org/W2180952760', 'https://openalex.org/W2950186769']","We present the system we developed to provide efficient large-scale feature-rich discriminative training for machine translation. We describe how we integrate with MapReduce using Hadoop streaming to allow arbitrarily scaling the tuning set and utilizing a sparse feature set. We report our findings on German-English and Russian-English translation, and discuss benefits, as well as obstacles, to tuning on larger development sets drawn from the parallel training data. 1",1.0
SKG_MT_729,https://openalex.org/W2740514420,2017,3,"['https://openalex.org/W22168010', 'https://openalex.org/W1199002571', 'https://openalex.org/W1570013475', 'https://openalex.org/W1934041838', 'https://openalex.org/W2049633694', 'https://openalex.org/W2080018251', 'https://openalex.org/W2099873701', 'https://openalex.org/W2102028293', 'https://openalex.org/W2103042430', 'https://openalex.org/W2110168585', 'https://openalex.org/W2113788796', 'https://openalex.org/W2117827367', 'https://openalex.org/W2121227244', 'https://openalex.org/W2121745180', 'https://openalex.org/W2135391077', 'https://openalex.org/W2136346830', 'https://openalex.org/W2156985047', 'https://openalex.org/W2161792612', 'https://openalex.org/W2164746297', 'https://openalex.org/W2169360026', 'https://openalex.org/W2172267041', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250600644', 'https://openalex.org/W2250640401', 'https://openalex.org/W2514378259', 'https://openalex.org/W3001379634']","We address for the first time unsupervised training for a translation task with hundreds of thousands of vocabulary words. We scale up the expectation-maximization (EM) algorithm to learn a large translation table without any parallel text or seed lexicon. First, we solve the memory bottleneck and enforce the sparsity with a simple thresholding scheme for the lexicon. Second, we initialize the lexicon training with word classes, which efficiently boosts the performance. Our methods produced promising results on two large-scale unsupervised translation tasks.",1.0
SKG_MT_730,https://openalex.org/W3093388299,2020,3,"['https://openalex.org/W1682403713', 'https://openalex.org/W2131988669', 'https://openalex.org/W2250876691', 'https://openalex.org/W2493916176', 'https://openalex.org/W2542860122', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964265128', 'https://openalex.org/W3005437029']","Despite advances in neural machine translation (NMT) quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for attaching dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.",0.9940828402366864
SKG_MT_731,https://openalex.org/W2466918907,2016,160,"['https://openalex.org/W6908809', 'https://openalex.org/W156805311', 'https://openalex.org/W854541894', 'https://openalex.org/W1514535095', 'https://openalex.org/W1556470778', 'https://openalex.org/W1586532344', 'https://openalex.org/W1885617648', 'https://openalex.org/W1902237438', 'https://openalex.org/W1973923101', 'https://openalex.org/W1994167151', 'https://openalex.org/W2006969979', 'https://openalex.org/W2038698865', 'https://openalex.org/W2063655091', 'https://openalex.org/W2064675550', 'https://openalex.org/W2090861223', 'https://openalex.org/W2095705004', 'https://openalex.org/W2107468211', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127141656', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133444727', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143612262', 'https://openalex.org/W2146502635', 'https://openalex.org/W2146939522', 'https://openalex.org/W2148708890', 'https://openalex.org/W2168310280', 'https://openalex.org/W2170353620', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251001376', 'https://openalex.org/W2252212004', 'https://openalex.org/W2293858598', 'https://openalex.org/W2327501763', 'https://openalex.org/W2490163484', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950178297', 'https://openalex.org/W2963333747', 'https://openalex.org/W2964308564', 'https://openalex.org/W3012492057']","Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, Trevor Cohn. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",1.0
SKG_MT_732,https://openalex.org/W2251032145,2014,7,"['https://openalex.org/W27623621', 'https://openalex.org/W50119190', 'https://openalex.org/W1482214997', 'https://openalex.org/W1499239271', 'https://openalex.org/W1880262756', 'https://openalex.org/W1977434113', 'https://openalex.org/W2004858782', 'https://openalex.org/W2068737686', 'https://openalex.org/W2071018679', 'https://openalex.org/W2114558668', 'https://openalex.org/W2250554077']","Creating cross-language article links among different online encyclopedias is now an important task in the unification of multilingual knowledge bases. In this paper, we propose a cross-language article linking method using a mixed-language topic model and hypernym translation features based on an SVM model to link English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China. To evaluate our approach, we compile a data set from the top 500 Baidu Baike articles and their corresponding English Wiki articles. The evaluation results show that our approach achieves 80.95% in MRR and 87.46% in recall. Our method does not heavily depend on linguistic characteristics and can be easily extended to generate crosslanguage article links among different online encyclopedias in other languages.",1.0
SKG_MT_733,https://openalex.org/W2798400986,2018,2,"['https://openalex.org/W46679369', 'https://openalex.org/W114517082', 'https://openalex.org/W1957775505', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146502635', 'https://openalex.org/W2150355110', 'https://openalex.org/W2172140247', 'https://openalex.org/W2184135559', 'https://openalex.org/W2220350356', 'https://openalex.org/W2384495648', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2532807140', 'https://openalex.org/W2594229957', 'https://openalex.org/W2611767671', 'https://openalex.org/W2621587759', 'https://openalex.org/W2757041753', 'https://openalex.org/W2886744048', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963324947', 'https://openalex.org/W2964308564', 'https://openalex.org/W3005389111']","Neural machine translation (NMT) models are typically trained with fixed-size input and output vocabularies, which creates an important bottleneck on their accuracy and generalization capability. As a solution, various studies proposed segmenting words into sub-word units and performing translation at the sub-lexical level. However, statistical word segmentation methods have recently shown to be prone to morphological errors, which can lead to inaccurate translations. In this paper, we propose to overcome this problem by replacing the source-language embedding layer of NMT with a bi-directional recurrent neural network that generates compositional representations of the input at any desired level of granularity. We test our approach in a low-resource setting with five languages from different morphological typologies, and under different composition assumptions. By training NMT to compose word representations from character n-grams, our approach consistently outperforms (from 1.71 to 2.48 BLEU points) NMT learning embeddings of statistically generated sub-word units.",1.0
SKG_MT_734,https://openalex.org/W2890244613,2018,34,"['https://openalex.org/W1902237438', 'https://openalex.org/W1910131649', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101454539', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136925175', 'https://openalex.org/W2143002608', 'https://openalex.org/W2148708890', 'https://openalex.org/W2157331557', 'https://openalex.org/W2291126447', 'https://openalex.org/W2361821140', 'https://openalex.org/W2525778437', 'https://openalex.org/W2527845440', 'https://openalex.org/W2530887700', 'https://openalex.org/W2539201987', 'https://openalex.org/W2561274697', 'https://openalex.org/W2566564022', 'https://openalex.org/W2577335011', 'https://openalex.org/W2613904329', 'https://openalex.org/W2618463334', 'https://openalex.org/W2759173152', 'https://openalex.org/W2766730959', 'https://openalex.org/W2803258077', 'https://openalex.org/W2950527759', 'https://openalex.org/W2962700074', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962945603', 'https://openalex.org/W2962954913', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963792777', 'https://openalex.org/W2963879527', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964334713', 'https://openalex.org/W4303633609', 'https://openalex.org/W4385245566']","One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different strategies to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.",1.0
SKG_MT_735,https://openalex.org/W2890908793,2018,29,"['https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1869752048', 'https://openalex.org/W1902237438', 'https://openalex.org/W1940872118', 'https://openalex.org/W2064675550', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104518905', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132726600', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134036914', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153508793', 'https://openalex.org/W2157331557', 'https://openalex.org/W2165666205', 'https://openalex.org/W2168966090', 'https://openalex.org/W2227882979', 'https://openalex.org/W2407166119', 'https://openalex.org/W2564486991', 'https://openalex.org/W2577255746', 'https://openalex.org/W2594047108', 'https://openalex.org/W2619269479', 'https://openalex.org/W2739894144', 'https://openalex.org/W2751262944', 'https://openalex.org/W2949952998', 'https://openalex.org/W2962820991', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963794306', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3099884890']","The addition of syntax-aware decoding in Neural Machine Translation (NMT) systems requires an effective tree-structured neural network, a syntax-aware attention model and a language generation model that is sensitive to sentence structure. Recent approaches resort to sequential decoding by adding additional neural network units to capture bottom-up structural information, or serialising structured data into sequence. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017) to create an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured decoding augmented with a syntax-aware attention model. Unlike previous approaches to syntax-based NMT which use dependency parsing models our method uses constituency parsing which we argue provides useful information for translation. In addition, we use the syntactic structure of the sentence to add new connections to the tree-structured decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models.",1.0
SKG_MT_737,https://openalex.org/W2586559132,2017,60,"['https://openalex.org/W71795751', 'https://openalex.org/W581956982', 'https://openalex.org/W1816079941', 'https://openalex.org/W1902237438', 'https://openalex.org/W1904365287', 'https://openalex.org/W1965893653', 'https://openalex.org/W2079735306', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108227097', 'https://openalex.org/W2114912785', 'https://openalex.org/W2116316001', 'https://openalex.org/W2116410915', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153508793', 'https://openalex.org/W2158388102', 'https://openalex.org/W2175585630', 'https://openalex.org/W2250417532', 'https://openalex.org/W2250861254', 'https://openalex.org/W2251678492', 'https://openalex.org/W2252272516', 'https://openalex.org/W2407166119', 'https://openalex.org/W2463033603', 'https://openalex.org/W2463895987', 'https://openalex.org/W2493916176', 'https://openalex.org/W2549416390', 'https://openalex.org/W2552110825', 'https://openalex.org/W2556468274', 'https://openalex.org/W2576482813', 'https://openalex.org/W2594047108', 'https://openalex.org/W2756871636', 'https://openalex.org/W2758567679', 'https://openalex.org/W2759607775', 'https://openalex.org/W2760705958', 'https://openalex.org/W2915926444', 'https://openalex.org/W2962739703', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962819663', 'https://openalex.org/W2962907349', 'https://openalex.org/W2962964385', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963451457', 'https://openalex.org/W2963571341', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963888305', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964198424', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2977587102', 'https://openalex.org/W2989631226', 'https://openalex.org/W4300450573']","This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the parser is optimized according to the translation objective. In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our model can be further improved by pre-training it with a small amount of treebank annotations. Our final ensemble model significantly outperforms the previous best models on the standard English-to-Japanese translation dataset.",1.0
SKG_MT_738,https://openalex.org/W2147227066,2014,27,"['https://openalex.org/W61355150', 'https://openalex.org/W71896076', 'https://openalex.org/W651894412', 'https://openalex.org/W1515410566', 'https://openalex.org/W1582869709', 'https://openalex.org/W1716250762', 'https://openalex.org/W1897658145', 'https://openalex.org/W2021618504', 'https://openalex.org/W2057399676', 'https://openalex.org/W2087735403', 'https://openalex.org/W2093197119', 'https://openalex.org/W2098263691', 'https://openalex.org/W2101816610', 'https://openalex.org/W2109530867', 'https://openalex.org/W2109633642', 'https://openalex.org/W2113549939', 'https://openalex.org/W2116089997', 'https://openalex.org/W2124807415', 'https://openalex.org/W2129113961', 'https://openalex.org/W2138046680', 'https://openalex.org/W2143337266', 'https://openalex.org/W2144746247', 'https://openalex.org/W2149327368', 'https://openalex.org/W2149791639', 'https://openalex.org/W2160001241', 'https://openalex.org/W2160218441', 'https://openalex.org/W2164984707', 'https://openalex.org/W2250439402', 'https://openalex.org/W2250576097', 'https://openalex.org/W2251311344', 'https://openalex.org/W2251542822', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2307270063', 'https://openalex.org/W2595715041', 'https://openalex.org/W2892357930', 'https://openalex.org/W3009009611', 'https://openalex.org/W4246994868']","The automatic estimation of machine translation (MT) output quality is a hard task in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role.When moving from controlled lab evaluations to real-life scenarios the task becomes even harder.For current MT quality estimation (QE) systems, additional complexity comes from the difficulty to model user and domain changes.Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions.To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes.Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach.",1.0
SKG_MT_740,https://openalex.org/W2917858329,2013,4,"['https://openalex.org/W1631260214', 'https://openalex.org/W1995560154', 'https://openalex.org/W2000546550', 'https://openalex.org/W2016856586', 'https://openalex.org/W2054533749', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095650036', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105245376', 'https://openalex.org/W2138706636', 'https://openalex.org/W2181317029', 'https://openalex.org/W2250375075', 'https://openalex.org/W2293139442', 'https://openalex.org/W2542068976', 'https://openalex.org/W3167516426', 'https://openalex.org/W3209717902']","In this paper, the KIT systems submitted to the Shared Translation Task are presented. We participated in two translation directions: from German to English and from English to German. Both translations are generated using phrase-based translation systems. The performance of the systems was boosted by using language models built based on different tokens such as word, part-of-speech, and automacally generated word clusters. The difference in word order between German and English is addressed by part-of-speech and syntactic tree-based reordering models. In addition to a discriminative word lexicon, we used hypothesis rescoring using the ListNet algorithm after generating the translation with the phrase-based system. We evaluated the rescoring using only the baseline features as well as using additional computational complex features.",1.0
SKG_MT_741,https://openalex.org/W2250646384,2013,20,"['https://openalex.org/W22168010', 'https://openalex.org/W114828685', 'https://openalex.org/W132913264', 'https://openalex.org/W222053410', 'https://openalex.org/W1977431865', 'https://openalex.org/W1979102019', 'https://openalex.org/W2023276762', 'https://openalex.org/W2066636486', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106261033', 'https://openalex.org/W2111666304', 'https://openalex.org/W2119727789', 'https://openalex.org/W2122837498', 'https://openalex.org/W2124202134', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127134333', 'https://openalex.org/W2143927888', 'https://openalex.org/W2146442558', 'https://openalex.org/W2153653739', 'https://openalex.org/W2160555926', 'https://openalex.org/W2165594795', 'https://openalex.org/W2169273645', 'https://openalex.org/W2250481584']","This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a bridge to generate source-target translation. However, one of the weaknesses is that some useful sourcetarget translations cannot be generated if the corresponding source phrase and target phrase connect to different pivot phrases. To alleviate the problem, we utilize Markov random walks to connect possible translation phrases between source and target language. Experimental results on European Parliament data, spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system.",1.0
SKG_MT_743,https://openalex.org/W1506572002,2010,5,"['https://openalex.org/W1480376833', 'https://openalex.org/W1506806321', 'https://openalex.org/W1510073064', 'https://openalex.org/W1554944419', 'https://openalex.org/W1663973292', 'https://openalex.org/W1957631488', 'https://openalex.org/W1986931325', 'https://openalex.org/W2061066582', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111491614', 'https://openalex.org/W2113998052', 'https://openalex.org/W2117339222', 'https://openalex.org/W2124807415', 'https://openalex.org/W2135046866', 'https://openalex.org/W2145366000', 'https://openalex.org/W2152096938', 'https://openalex.org/W2166088896', 'https://openalex.org/W2171421863', 'https://openalex.org/W2554958826', 'https://openalex.org/W3105702746']",We use L1 regularized transductive regression to learn mappings between source and target features of the training sets derived for each test sentence and use these mappings to rerank translation outputs. We compare the effectiveness of L1 regularization techniques for regression to learn mappings between features given in a sparse feature matrix. The results show the effectiveness of using L1 regularization versus L2 used in ridge regression. We show that regression mapping is effective in reranking translation outputs and in selecting the best system combinations with encouraging results on different language pairs. 1,1.0
SKG_MT_745,https://openalex.org/W2963597829,2018,15,"['https://openalex.org/W1522301498', 'https://openalex.org/W1840435438', 'https://openalex.org/W1902237438', 'https://openalex.org/W1924770834', 'https://openalex.org/W2033295622', 'https://openalex.org/W2064675550', 'https://openalex.org/W2096204319', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110485445', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250653840', 'https://openalex.org/W2250739653', 'https://openalex.org/W2251811146', 'https://openalex.org/W2252225757', 'https://openalex.org/W2267186426', 'https://openalex.org/W2308720496', 'https://openalex.org/W2415204069', 'https://openalex.org/W2525778437', 'https://openalex.org/W2552839021', 'https://openalex.org/W2553397501', 'https://openalex.org/W2593833795', 'https://openalex.org/W2610635252', 'https://openalex.org/W2613904329', 'https://openalex.org/W2618101654', 'https://openalex.org/W2741986820', 'https://openalex.org/W2751185861', 'https://openalex.org/W2755989362', 'https://openalex.org/W2769298630', 'https://openalex.org/W2799549586', 'https://openalex.org/W2952436057', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962958286', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963542836', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963714898', 'https://openalex.org/W2963899908', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964213727', 'https://openalex.org/W2964308564', 'https://openalex.org/W4254816979', 'https://openalex.org/W4297687616', 'https://openalex.org/W4297692367', 'https://openalex.org/W4299801216', 'https://openalex.org/W4385245566']","In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.",1.0
SKG_MT_746,https://openalex.org/W3092058109,2020,20,"['https://openalex.org/W630532510', 'https://openalex.org/W1489525520', 'https://openalex.org/W1763968285', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121879602', 'https://openalex.org/W2154652894', 'https://openalex.org/W2525778437', 'https://openalex.org/W2786660442', 'https://openalex.org/W2903188467', 'https://openalex.org/W2915756181', 'https://openalex.org/W2946417913', 'https://openalex.org/W2947771965', 'https://openalex.org/W2948384082', 'https://openalex.org/W2953287808', 'https://openalex.org/W2963341956', 'https://openalex.org/W2966292672', 'https://openalex.org/W2970785793', 'https://openalex.org/W2970791445', 'https://openalex.org/W2970986500', 'https://openalex.org/W2977458338', 'https://openalex.org/W2995015695', 'https://openalex.org/W2995435108', 'https://openalex.org/W2996403597', 'https://openalex.org/W3029985558', 'https://openalex.org/W3035252911', 'https://openalex.org/W3035408261', 'https://openalex.org/W3037013468', 'https://openalex.org/W3082274269', 'https://openalex.org/W3099942180', 'https://openalex.org/W3103450644', 'https://openalex.org/W3105214104']","The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on BLEURT, a previously published metric based on transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 ""zero-shot"" language pairs, for which we have no labelled examples. Additionally, we focus on English to German and demonstrate how to combine BLEURT's predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition.",0.9951219512195122
SKG_MT_747,https://openalex.org/W2612881151,2017,78,"['https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1970849810', 'https://openalex.org/W2031196180', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110485445', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2144012961', 'https://openalex.org/W2149327368', 'https://openalex.org/W2157331557', 'https://openalex.org/W2168966090', 'https://openalex.org/W2194775991', 'https://openalex.org/W2251367463', 'https://openalex.org/W2251939518', 'https://openalex.org/W2290847742', 'https://openalex.org/W2407166119', 'https://openalex.org/W2540404261', 'https://openalex.org/W2552839021', 'https://openalex.org/W2563666160', 'https://openalex.org/W2586524454', 'https://openalex.org/W2586559132', 'https://openalex.org/W2600702321', 'https://openalex.org/W2606134370', 'https://openalex.org/W2609011624', 'https://openalex.org/W2949926081', 'https://openalex.org/W2951883919', 'https://openalex.org/W2952254971', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963451457', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964113829', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964321699']","We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.",1.0
SKG_MT_748,https://openalex.org/W2250548645,2013,54,"['https://openalex.org/W76590478', 'https://openalex.org/W168425931', 'https://openalex.org/W222053410', 'https://openalex.org/W1479669738', 'https://openalex.org/W1510052640', 'https://openalex.org/W1512419429', 'https://openalex.org/W1828578481', 'https://openalex.org/W1955251501', 'https://openalex.org/W1979495315', 'https://openalex.org/W2008652694', 'https://openalex.org/W2008961349', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101752047', 'https://openalex.org/W2110104386', 'https://openalex.org/W2112900913', 'https://openalex.org/W2113104171', 'https://openalex.org/W2116316001', 'https://openalex.org/W2116679574', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2132069549', 'https://openalex.org/W2134800885', 'https://openalex.org/W2136544838', 'https://openalex.org/W2144804812', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153508793', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158614781', 'https://openalex.org/W2159358338', 'https://openalex.org/W2164301055', 'https://openalex.org/W2165666205', 'https://openalex.org/W2168966090', 'https://openalex.org/W2395054697', 'https://openalex.org/W2437005631']","In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on English-Japanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at",0.9937888198757764
SKG_MT_749,https://openalex.org/W3119078159,2020,2,"['https://openalex.org/W1985514943', 'https://openalex.org/W2095705004', 'https://openalex.org/W2948947170', 'https://openalex.org/W2963341956', 'https://openalex.org/W2970156971', 'https://openalex.org/W2970597249', 'https://openalex.org/W2971051558', 'https://openalex.org/W2986154550', 'https://openalex.org/W2996403597', 'https://openalex.org/W3035252911', 'https://openalex.org/W3119881489']","This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score).",0.989247311827957
SKG_MT_750,https://openalex.org/W2948874979,2019,4,"['https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115410424', 'https://openalex.org/W2117278770', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131988669', 'https://openalex.org/W2134800885', 'https://openalex.org/W2139989135', 'https://openalex.org/W2147262247', 'https://openalex.org/W2250861721', 'https://openalex.org/W2402144811', 'https://openalex.org/W2525778437', 'https://openalex.org/W2567571499', 'https://openalex.org/W2740718109', 'https://openalex.org/W2744813330', 'https://openalex.org/W2756978580', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","In this paper, we empirically investigate applying word-level weights to adapt neural machine translation to e-commerce domains, where small e-commerce datasets and large out-of-domain datasets are available. In order to mine in-domain like words in the out-of-domain datasets, we compute word weights by using a domain-specific and a non-domain-specific language model followed by smoothing and binary quantization. The baseline model is trained on mixed in-domain and out-of-domain datasets. Experimental results on English to Chinese e-commerce domain translation show that compared to continuing training without word weights, it improves MT quality by up to 2.11% BLEU absolute and 1.59% TER. We have also trained models using fine-tuning on the in-domain data. Pre-training a model with word weights improves fine-tuning up to 1.24% BLEU absolute and 1.64% TER, respectively.",1.0
SKG_MT_751,https://openalex.org/W2985301125,2019,52,"['https://openalex.org/W19526395', 'https://openalex.org/W22168010', 'https://openalex.org/W2041532239', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251743902', 'https://openalex.org/W2550821151', 'https://openalex.org/W2572474373', 'https://openalex.org/W2611838487', 'https://openalex.org/W2744813330', 'https://openalex.org/W2752630748', 'https://openalex.org/W2806888788', 'https://openalex.org/W2888541716', 'https://openalex.org/W2903193068', 'https://openalex.org/W2905927205', 'https://openalex.org/W2953830716', 'https://openalex.org/W2954078115', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964091381', 'https://openalex.org/W2964308564', 'https://openalex.org/W2978453854', 'https://openalex.org/W2994475016', 'https://openalex.org/W4302343710', 'https://openalex.org/W4323355575', 'https://openalex.org/W4385245566']","Raj Dabre, Atsushi Fujita, Chenhui Chu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_752,https://openalex.org/W2252177599,2013,14,"['https://openalex.org/W165935821', 'https://openalex.org/W577284227', 'https://openalex.org/W1551202288', 'https://openalex.org/W1920193847', 'https://openalex.org/W1957504465', 'https://openalex.org/W1979495315', 'https://openalex.org/W2008961349', 'https://openalex.org/W2044804339', 'https://openalex.org/W2082571252', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097997328', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108460050', 'https://openalex.org/W2113691817', 'https://openalex.org/W2121338597', 'https://openalex.org/W2123126659', 'https://openalex.org/W2130551284', 'https://openalex.org/W2130988241', 'https://openalex.org/W2140343992', 'https://openalex.org/W2144900797', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150378737', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2160382364', 'https://openalex.org/W2162245945', 'https://openalex.org/W2166905217', 'https://openalex.org/W2168966090', 'https://openalex.org/W2250907725', 'https://openalex.org/W2250974141', 'https://openalex.org/W2251163074', 'https://openalex.org/W2437005631']","This paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.",1.0
SKG_MT_754,https://openalex.org/W2888442053,2018,157,"['https://openalex.org/W45604574', 'https://openalex.org/W1522301498', 'https://openalex.org/W1524068846', 'https://openalex.org/W1665214252', 'https://openalex.org/W1757796397', 'https://openalex.org/W1777239053', 'https://openalex.org/W1993666847', 'https://openalex.org/W2096557251', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2156718681', 'https://openalex.org/W2176263492', 'https://openalex.org/W2512924740', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2561274697', 'https://openalex.org/W2581637843', 'https://openalex.org/W2597655663', 'https://openalex.org/W2607987856', 'https://openalex.org/W2613904329', 'https://openalex.org/W2740433069', 'https://openalex.org/W2740707878', 'https://openalex.org/W2751527518', 'https://openalex.org/W2752047430', 'https://openalex.org/W2754517384', 'https://openalex.org/W2774334615', 'https://openalex.org/W2785093437', 'https://openalex.org/W2787802257', 'https://openalex.org/W2794365787', 'https://openalex.org/W2889606145', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962955856', 'https://openalex.org/W2963120839', 'https://openalex.org/W2963163972', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963386218', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964190861', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W4298857966', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.",1.0
SKG_MT_756,https://openalex.org/W2977183928,2019,9,"['https://openalex.org/W399167303', 'https://openalex.org/W1484504603', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2186089609', 'https://openalex.org/W2466918907', 'https://openalex.org/W2593011301', 'https://openalex.org/W2605131327', 'https://openalex.org/W2605202026', 'https://openalex.org/W2745785989', 'https://openalex.org/W2746176496', 'https://openalex.org/W2890448475', 'https://openalex.org/W2898989428', 'https://openalex.org/W2945700568', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963288440', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963418779', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963777589', 'https://openalex.org/W2963918197', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964104866', 'https://openalex.org/W2964268978', 'https://openalex.org/W3012492057', 'https://openalex.org/W3032598645']","In a pipeline speech translation system, automatic speech recognition (ASR) system will transmit errors in recognition to the downstream machine translation (MT) system. A standard machine translation system is usually trained on parallel corpus composed of clean text and will perform poorly on text with recognition noise, a gap well known in speech translation community. In this paper, we propose a training architecture which aims at making a neural machine translation model more robust against speech recognition errors. Our approach addresses the encoder and the decoder simultaneously using adversarial learning and data augmentation, respectively. Experimental results on IWSLT2018 speech translation task show that our approach can bridge the gap between the ASR output and the MT input, outperforms the baseline by up to 2.83 BLEU on noisy ASR output, while maintaining close performance on clean text.",0.9947089947089947
SKG_MT_757,https://openalex.org/W2887838996,2018,143,"['https://openalex.org/W1614298861', 'https://openalex.org/W1626233182', 'https://openalex.org/W1968691112', 'https://openalex.org/W2041232209', 'https://openalex.org/W2058659201', 'https://openalex.org/W2072976288', 'https://openalex.org/W2099471712', 'https://openalex.org/W2102749417', 'https://openalex.org/W2121415745', 'https://openalex.org/W2126725946', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2250229103', 'https://openalex.org/W2294774419', 'https://openalex.org/W2331128040', 'https://openalex.org/W2493916176', 'https://openalex.org/W2546938941', 'https://openalex.org/W2594021297', 'https://openalex.org/W2737057113', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2760424551', 'https://openalex.org/W2762484717', 'https://openalex.org/W2786378416', 'https://openalex.org/W2950577311', 'https://openalex.org/W2951939904', 'https://openalex.org/W2952190837', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963376432', 'https://openalex.org/W2963522749', 'https://openalex.org/W4299579390', 'https://openalex.org/W4302780227', 'https://openalex.org/W4320013936']","Unsupervised word translation from non-parallel inter-lingual corpora has attracted much research interest. Very recently, neural network methods trained with adversarial loss functions achieved high accuracy on this task. Despite the impressive success of the recent techniques, they suffer from the typical drawbacks of generative adversarial models: sensitivity to hyper-parameters, long training time and lack of interpretability. In this paper, we make the observation that two sufficiently similar distributions can be aligned correctly with iterative matching methods. We present a novel method that first aligns the second moment of the word distributions of the two languages and then iteratively refines the alignment. Extensive experiments on word translation of European and Non-European languages show that our method achieves better performance than recent state-of-the-art deep adversarial approaches and is competitive with the supervised baseline. It is also efficient, easy to parallelize on CPU and interpretable.",1.0
SKG_MT_758,https://openalex.org/W2964084720,2017,13,"['https://openalex.org/W24166807', 'https://openalex.org/W36903255', 'https://openalex.org/W192515608', 'https://openalex.org/W1522301498', 'https://openalex.org/W1676820704', 'https://openalex.org/W1848903538', 'https://openalex.org/W1902237438', 'https://openalex.org/W1991133427', 'https://openalex.org/W1995875735', 'https://openalex.org/W2026357342', 'https://openalex.org/W2060108852', 'https://openalex.org/W2079656678', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116261113', 'https://openalex.org/W2120861206', 'https://openalex.org/W2121227244', 'https://openalex.org/W2127863960', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136848157', 'https://openalex.org/W2153579005', 'https://openalex.org/W2220350356', 'https://openalex.org/W2250303366', 'https://openalex.org/W2404212720', 'https://openalex.org/W2529089661', 'https://openalex.org/W2576482813', 'https://openalex.org/W2577255746', 'https://openalex.org/W2595715041', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963932686', 'https://openalex.org/W2963937700', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2993383518', 'https://openalex.org/W3119473442', 'https://openalex.org/W4285719527', 'https://openalex.org/W4294170691']","In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.",1.0
SKG_MT_759,https://openalex.org/W2888958984,2018,15,"['https://openalex.org/W1522301498', 'https://openalex.org/W2220350356', 'https://openalex.org/W2242818861', 'https://openalex.org/W2292633562', 'https://openalex.org/W2327501763', 'https://openalex.org/W2339995566', 'https://openalex.org/W2410539690', 'https://openalex.org/W2510842514', 'https://openalex.org/W2531207078', 'https://openalex.org/W2740787117', 'https://openalex.org/W2789543585', 'https://openalex.org/W2798362442', 'https://openalex.org/W2804713552', 'https://openalex.org/W2896060389', 'https://openalex.org/W2952436057', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963979492']","Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.",1.0
SKG_MT_760,https://openalex.org/W3034719878,2020,35,"['https://openalex.org/W2130942839', 'https://openalex.org/W2251743902', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2798931235', 'https://openalex.org/W2807535859', 'https://openalex.org/W2891924676', 'https://openalex.org/W2899015110', 'https://openalex.org/W2912095972', 'https://openalex.org/W2921280978', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962807144', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964073484', 'https://openalex.org/W2964108048', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.",1.0
SKG_MT_761,https://openalex.org/W2970520743,2019,12,"['https://openalex.org/W24656680', 'https://openalex.org/W1821462560', 'https://openalex.org/W1972188389', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2176263492', 'https://openalex.org/W2466062786', 'https://openalex.org/W2546938941', 'https://openalex.org/W2587694128', 'https://openalex.org/W2613904329', 'https://openalex.org/W2733239165', 'https://openalex.org/W2747909401', 'https://openalex.org/W2798761464', 'https://openalex.org/W2803569830', 'https://openalex.org/W2807912816', 'https://openalex.org/W2833393231', 'https://openalex.org/W2885588803', 'https://openalex.org/W2886776719', 'https://openalex.org/W2888539709', 'https://openalex.org/W2889326796', 'https://openalex.org/W2906987001', 'https://openalex.org/W2908336025', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963205761', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963774520', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964220233', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2999635570', 'https://openalex.org/W4289670464', 'https://openalex.org/W4298159529', 'https://openalex.org/W4385245566']","Tianchi Bi, Hao Xiong, Zhongjun He, Hua Wu, Haifeng Wang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_764,https://openalex.org/W2803805552,2018,6,"['https://openalex.org/W168564468', 'https://openalex.org/W1582482241', 'https://openalex.org/W2018869373', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103593018', 'https://openalex.org/W2115259925', 'https://openalex.org/W2147258359', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153579005', 'https://openalex.org/W2163986298', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251117546', 'https://openalex.org/W2251882135', 'https://openalex.org/W2252024278', 'https://openalex.org/W2252035278', 'https://openalex.org/W2260677151', 'https://openalex.org/W2293778248', 'https://openalex.org/W2508316494', 'https://openalex.org/W2511254834', 'https://openalex.org/W2950577311']",International audience,1.0
SKG_MT_765,https://openalex.org/W2952406142,2019,25,"['https://openalex.org/W1777239053', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2133564696', 'https://openalex.org/W2147527908', 'https://openalex.org/W2525778437', 'https://openalex.org/W2613904329', 'https://openalex.org/W2740707878', 'https://openalex.org/W2741263286', 'https://openalex.org/W2794365787', 'https://openalex.org/W2888442053', 'https://openalex.org/W2890498499', 'https://openalex.org/W2951527505', 'https://openalex.org/W2962729168', 'https://openalex.org/W2962972512', 'https://openalex.org/W2963061963', 'https://openalex.org/W2963233086', 'https://openalex.org/W2963363070', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963418779', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964190861', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964302946', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W4297747548', 'https://openalex.org/W4385245566']","Soft-attention based Neural Machine Translation (NMT) models have achieved promising results on several translation tasks. These models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. In this work, we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it. Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German (EN-DE) and English-French (ENFR) translation tasks compared to the soft attention based NMT.",0.9924812030075187
SKG_MT_766,https://openalex.org/W3102852174,2020,7,"['https://openalex.org/W179875071', 'https://openalex.org/W1522301498', 'https://openalex.org/W2119717200', 'https://openalex.org/W2133564696', 'https://openalex.org/W2526471240', 'https://openalex.org/W2546938941', 'https://openalex.org/W2547875792', 'https://openalex.org/W2754517384', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798931235', 'https://openalex.org/W2804232614', 'https://openalex.org/W2886095922', 'https://openalex.org/W2888442053', 'https://openalex.org/W2889326796', 'https://openalex.org/W2903193068', 'https://openalex.org/W2903399267', 'https://openalex.org/W2906987001', 'https://openalex.org/W2927431361', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963120839', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963780864', 'https://openalex.org/W2963987720', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2995197202', 'https://openalex.org/W3004665584', 'https://openalex.org/W3005389111', 'https://openalex.org/W3005577593', 'https://openalex.org/W4298137069', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.",0.9942857142857143
SKG_MT_767,https://openalex.org/W2903012348,2017,59,"['https://openalex.org/W1522301498', 'https://openalex.org/W1591801644', 'https://openalex.org/W1815076433', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2159755860', 'https://openalex.org/W2168231600', 'https://openalex.org/W2278108219', 'https://openalex.org/W2466062786', 'https://openalex.org/W2492716757', 'https://openalex.org/W2525778437', 'https://openalex.org/W2537667581', 'https://openalex.org/W2542860122', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4394666973']","We describe the Sogou neural machine translation systems for the WMT 2017 Chinese↔English news translation tasks.Our systems are based on a multilayer encoder-decoder architecture with attention mechanism.The best translation is obtained with ensemble and reranking techniques.We also propose an approach to improve the named entity translation problem.Our Chi-nese→English system achieved the highest cased BLEU among all 20 submitted systems, and our English→Chinese system ranked the third out of 16 submitted systems. 1",1.0
SKG_MT_769,https://openalex.org/W3103544486,2020,21,"['https://openalex.org/W630532510', 'https://openalex.org/W2041532239', 'https://openalex.org/W2105767494', 'https://openalex.org/W2111666304', 'https://openalex.org/W2118974695', 'https://openalex.org/W2119727789', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2162245945', 'https://openalex.org/W2251654371', 'https://openalex.org/W2419539795', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2572474373', 'https://openalex.org/W2613904329', 'https://openalex.org/W2733239165', 'https://openalex.org/W2739810937', 'https://openalex.org/W2739978843', 'https://openalex.org/W2741602058', 'https://openalex.org/W2798304389', 'https://openalex.org/W2882669297', 'https://openalex.org/W2890176538', 'https://openalex.org/W2909389168', 'https://openalex.org/W2914120296', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921280978', 'https://openalex.org/W2944815030', 'https://openalex.org/W2950428495', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963021447', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532104', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963774520', 'https://openalex.org/W2964047576', 'https://openalex.org/W2964073484', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970109976', 'https://openalex.org/W2995460523', 'https://openalex.org/W2997518171', 'https://openalex.org/W2998230451', 'https://openalex.org/W2998335012', 'https://openalex.org/W3015566765', 'https://openalex.org/W3034906024', 'https://openalex.org/W3101672304', 'https://openalex.org/W3105940451', 'https://openalex.org/W3107826490', 'https://openalex.org/W4244167344', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Exploiting a common language as an auxiliary for better translation has a long tradition in machine translation and lets supervised learning-based machine translation enjoy the enhancement delivered by the well-used pivot language in the absence of a source language to target language parallel corpus. The rise of unsupervised neural machine translation (UNMT) almost completely relieves the parallel corpus curse, though UNMT is still subject to unsatisfactory performance due to the vagueness of the clues available for its core back-translation training. Further enriching the idea of pivot translation by extending the use of parallel corpora beyond the source-target paradigm, we propose a new reference language-based framework for UNMT, RUNMT, in which the reference language only shares a parallel corpus with the source, but this corpus still indicates a signal clear enough to help the reconstruction training of UNMT through a proposed reference agreement mechanism. Experimental results show that our methods improve the quality of UNMT over that of a strong baseline that uses only one auxiliary language, demonstrating the usefulness of the proposed reference language-based UNMT and establishing a good start for the community.",1.0
SKG_MT_770,https://openalex.org/W2251943262,2014,9,"['https://openalex.org/W108437174', 'https://openalex.org/W110357785', 'https://openalex.org/W111475876', 'https://openalex.org/W222053410', 'https://openalex.org/W1743517014', 'https://openalex.org/W1838574496', 'https://openalex.org/W1920193847', 'https://openalex.org/W1973923101', 'https://openalex.org/W2008961349', 'https://openalex.org/W2096204319', 'https://openalex.org/W2097927681', 'https://openalex.org/W2100281225', 'https://openalex.org/W2100565866', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108460050', 'https://openalex.org/W2111142112', 'https://openalex.org/W2114737762', 'https://openalex.org/W2115557995', 'https://openalex.org/W2130551284', 'https://openalex.org/W2130988241', 'https://openalex.org/W2131148434', 'https://openalex.org/W2140906828', 'https://openalex.org/W2143002608', 'https://openalex.org/W2144900797', 'https://openalex.org/W2144995019', 'https://openalex.org/W2146685010', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152423400', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2158603998', 'https://openalex.org/W2158847908', 'https://openalex.org/W2162245945', 'https://openalex.org/W2169039522', 'https://openalex.org/W2242975712', 'https://openalex.org/W2250319048', 'https://openalex.org/W2250338395', 'https://openalex.org/W2250907725', 'https://openalex.org/W2250974141', 'https://openalex.org/W2251048776', 'https://openalex.org/W2251843378', 'https://openalex.org/W2252177599', 'https://openalex.org/W2434901392', 'https://openalex.org/W2437005631', 'https://openalex.org/W2467575451', 'https://openalex.org/W2950186769', 'https://openalex.org/W4241645538']","This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system: 1) a syntactic reordering model that explores reorderings for context free grammar rules; and 2) a semantic reordering model that focuses on the reordering of predicate-argument structures.We develop novel features based on both models and use them as soft constraints to guide the translation process.Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system.However, the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model, and we therefore provide a detailed analysis of the behavior differences between the two.",1.0
SKG_MT_771,https://openalex.org/W2131367528,2010,32,"['https://openalex.org/W30655990', 'https://openalex.org/W222053410', 'https://openalex.org/W1484082930', 'https://openalex.org/W1542491098', 'https://openalex.org/W1555531993', 'https://openalex.org/W1592796124', 'https://openalex.org/W1606508130', 'https://openalex.org/W1969974515', 'https://openalex.org/W1983599491', 'https://openalex.org/W1984566630', 'https://openalex.org/W1996430422', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097997328', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105245376', 'https://openalex.org/W2105891181', 'https://openalex.org/W2119168550', 'https://openalex.org/W2121404172', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126241965', 'https://openalex.org/W2135161317', 'https://openalex.org/W2143263475', 'https://openalex.org/W2145493314', 'https://openalex.org/W2146418175', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150378737', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157956963', 'https://openalex.org/W2158065314', 'https://openalex.org/W2162245945', 'https://openalex.org/W2167302977', 'https://openalex.org/W2168479559', 'https://openalex.org/W2169724380', 'https://openalex.org/W2171421863', 'https://openalex.org/W2215355985', 'https://openalex.org/W2437005631', 'https://openalex.org/W2591728348', 'https://openalex.org/W3203149905']","The distortion cost function used in Moses-style machine translation systems has two flaws. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, all distortion is penalized linearly, even when appropriate re-orderings are performed. Because the cost function does not effectively constrain search, translation quality decreases at higher distortion limits, which are often needed when translating between languages of different typologies such as Arabic and English. To address these problems, we introduce a method for estimating future linear distortion cost, and a new discriminative distortion model that predicts word movement during translation. In combination, these extensions give a statistically significant improvement over a baseline distortion parameterization. When we triple the distortion limit, our model achieves a +2.32 BLEU average gain over Moses.",1.0
SKG_MT_773,https://openalex.org/W2970520743,2019,12,"['https://openalex.org/W24656680', 'https://openalex.org/W1821462560', 'https://openalex.org/W1972188389', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2176263492', 'https://openalex.org/W2466062786', 'https://openalex.org/W2546938941', 'https://openalex.org/W2587694128', 'https://openalex.org/W2613904329', 'https://openalex.org/W2733239165', 'https://openalex.org/W2747909401', 'https://openalex.org/W2798761464', 'https://openalex.org/W2803569830', 'https://openalex.org/W2807912816', 'https://openalex.org/W2833393231', 'https://openalex.org/W2885588803', 'https://openalex.org/W2886776719', 'https://openalex.org/W2888539709', 'https://openalex.org/W2889326796', 'https://openalex.org/W2906987001', 'https://openalex.org/W2908336025', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963205761', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963774520', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964220233', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2999635570', 'https://openalex.org/W4289670464', 'https://openalex.org/W4298159529', 'https://openalex.org/W4385245566']","Tianchi Bi, Hao Xiong, Zhongjun He, Hua Wu, Haifeng Wang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_774,https://openalex.org/W3140210878,2020,2,"['https://openalex.org/W2587694128', 'https://openalex.org/W2902503418', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964265128', 'https://openalex.org/W2970295111', 'https://openalex.org/W2970752831', 'https://openalex.org/W2970947975', 'https://openalex.org/W3034742481']","In this paper we introduce the systems IIE submitted for the WMT20 shared task on German-French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data selection, back translation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our German-to-French system achieved 35.0 BLEU and ranked the second among all anonymous submissions, and our French-to-German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions.",1.0
SKG_MT_775,https://openalex.org/W3101095987,2020,19,"['https://openalex.org/W1505878979', 'https://openalex.org/W1522301498', 'https://openalex.org/W1802356529', 'https://openalex.org/W2013035813', 'https://openalex.org/W2120340025', 'https://openalex.org/W2120513984', 'https://openalex.org/W2161914416', 'https://openalex.org/W2539809671', 'https://openalex.org/W2540413092', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548228487', 'https://openalex.org/W2553718801', 'https://openalex.org/W2614634292', 'https://openalex.org/W2661761953', 'https://openalex.org/W2738371943', 'https://openalex.org/W2740094762', 'https://openalex.org/W2767206889', 'https://openalex.org/W2769196856', 'https://openalex.org/W2788330850', 'https://openalex.org/W2789543585', 'https://openalex.org/W2804821933', 'https://openalex.org/W2885421725', 'https://openalex.org/W2890397703', 'https://openalex.org/W2933138175', 'https://openalex.org/W2947898088', 'https://openalex.org/W2948629866', 'https://openalex.org/W2951352467', 'https://openalex.org/W2959300817', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964026424', 'https://openalex.org/W2964076774', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964205912', 'https://openalex.org/W2970832665', 'https://openalex.org/W2971034910', 'https://openalex.org/W2985694911', 'https://openalex.org/W2988975212', 'https://openalex.org/W2996843693', 'https://openalex.org/W3004970274', 'https://openalex.org/W3007103623', 'https://openalex.org/W3022372506', 'https://openalex.org/W3034425996', 'https://openalex.org/W3098269892', 'https://openalex.org/W3099073165', 'https://openalex.org/W4293052541', 'https://openalex.org/W4293541669', 'https://openalex.org/W4293865263', 'https://openalex.org/W4297779694', 'https://openalex.org/W4385245566', 'https://openalex.org/W4391602018']","We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT’14 En→De, WMT’16 Ro→En and IWSLT’16 De→En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT’14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).",1.0
SKG_MT_776,https://openalex.org/W3102315351,2020,20,"['https://openalex.org/W2103328396', 'https://openalex.org/W2108501770', 'https://openalex.org/W2126610017', 'https://openalex.org/W2133564696', 'https://openalex.org/W2250473257', 'https://openalex.org/W2467575451', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2594978815', 'https://openalex.org/W2595715041', 'https://openalex.org/W2600383743', 'https://openalex.org/W2807753879', 'https://openalex.org/W2886095922', 'https://openalex.org/W2887516053', 'https://openalex.org/W2888442053', 'https://openalex.org/W2888519496', 'https://openalex.org/W2888808532', 'https://openalex.org/W2889326796', 'https://openalex.org/W2905555874', 'https://openalex.org/W2946068894', 'https://openalex.org/W2951476960', 'https://openalex.org/W2952208079', 'https://openalex.org/W2952638483', 'https://openalex.org/W2953173959', 'https://openalex.org/W2962717182', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963357083', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963443335', 'https://openalex.org/W2963520382', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963823140', 'https://openalex.org/W2964030506', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964076537', 'https://openalex.org/W2964144363', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964348886', 'https://openalex.org/W2969999977', 'https://openalex.org/W2971302374', 'https://openalex.org/W2984413306', 'https://openalex.org/W3023856002', 'https://openalex.org/W3098341425', 'https://openalex.org/W3118942129', 'https://openalex.org/W4297801368', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.",1.0
SKG_MT_777,https://openalex.org/W2888541716,2018,315,"['https://openalex.org/W1522301498', 'https://openalex.org/W1915251500', 'https://openalex.org/W1994616650', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2194321275', 'https://openalex.org/W2493916176', 'https://openalex.org/W2531207078', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561274697', 'https://openalex.org/W2566926700', 'https://openalex.org/W2594021297', 'https://openalex.org/W2601450892', 'https://openalex.org/W2604763608', 'https://openalex.org/W2610245951', 'https://openalex.org/W2613904329', 'https://openalex.org/W2734377693', 'https://openalex.org/W2741602058', 'https://openalex.org/W2751304263', 'https://openalex.org/W2760424551', 'https://openalex.org/W2766184602', 'https://openalex.org/W2767206889', 'https://openalex.org/W2952190837', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962830144', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963341924', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963775850', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394642966']","In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn, et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem where we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (~600 parallel sentences)",1.0
SKG_MT_780,https://openalex.org/W2970999217,2019,3,"['https://openalex.org/W46679369', 'https://openalex.org/W214028658', 'https://openalex.org/W309335912', 'https://openalex.org/W1905100302', 'https://openalex.org/W1987408080', 'https://openalex.org/W2032942114', 'https://openalex.org/W2042783153', 'https://openalex.org/W2053306448', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117621558', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152561112', 'https://openalex.org/W2511950321', 'https://openalex.org/W2512848817', 'https://openalex.org/W2531207078', 'https://openalex.org/W2538358357', 'https://openalex.org/W2714176837', 'https://openalex.org/W2757141711', 'https://openalex.org/W2757281913', 'https://openalex.org/W2759409226', 'https://openalex.org/W2808154809', 'https://openalex.org/W2888893419', 'https://openalex.org/W2902827059', 'https://openalex.org/W2954254257', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963715460', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964053711', 'https://openalex.org/W2977881703', 'https://openalex.org/W4249773576', 'https://openalex.org/W4385245566']","This paper describes the University of Helsinki Language Technology group’s participation in the WMT 2019 similar language translation task. We trained neural machine translation models for the language pairs Czech <-> Polish and Spanish <-> Portuguese. Our experiments focused on different subword segmentation methods, and in particular on the comparison of a cognate-aware segmentation method, Cognate Morfessor, with character segmentation and unsupervised segmentation methods for which the data from different languages were simply concatenated. We did not observe major benefits from cognate-aware segmentation methods, but further research may be needed to explore larger parts of the parameter space. Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding.",1.0
SKG_MT_781,https://openalex.org/W2756566411,2017,220,"['https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1915251500', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134800885', 'https://openalex.org/W2157331557', 'https://openalex.org/W2212703438', 'https://openalex.org/W2252272516', 'https://openalex.org/W2418388682', 'https://openalex.org/W2512924740', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2561274697', 'https://openalex.org/W2594229957', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W4307459710']","We train a neural machine translation (NMT) system to both translate sourcelanguage text and copy target-language text, thereby exploiting monolingual corpora in the target language.Specifically, we create a bitext from the monolingual text in the target language so that each source sentence is identical to the target sentence.This copied data is then mixed with the parallel corpus and the NMT system is trained like normal, with no metadata to distinguish the two input languages.Our proposed method proves to be an effective way of incorporating monolingual data into low-resource NMT.On Turkish↔English and Romanian↔English translation tasks, we see gains of up to 1.2 BLEU over a strong baseline with back-translation.Further analysis shows that the linguistic phenomena behind these gains are different from and largely orthogonal to back-translation, with our copied corpus method improving accuracy on named entities and other words that should remain identical between the source and target languages.",1.0
SKG_MT_782,https://openalex.org/W3015423925,2020,6,"['https://openalex.org/W2101105183', 'https://openalex.org/W2183341477', 'https://openalex.org/W2512593609', 'https://openalex.org/W2529548870', 'https://openalex.org/W2946888380', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964078338', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970074184', 'https://openalex.org/W2970084653', 'https://openalex.org/W2995428172']","There has been great progress in improving streaming machine translation, a simultaneous paradigm where the system appends to a growing hypothesis as more source content becomes available. We study a related problem in which revisions to the hypothesis beyond strictly appending words are permitted. This is suitable for applications such as live captioning an audio feed. In this setting, we compare custom streaming approaches to re-translation, a straightforward strategy where each new source token triggers a distinct translation from scratch. We find re-translation to be as good or better than state-of-the-art streaming systems, even when operating under constraints that allow very few revisions. We attribute much of this success to a previously proposed data-augmentation technique that adds prefix-pairs to the training data, which alongside wait-k inference forms a strong baseline for streaming translation. We also highlight re-translation's ability to wrap arbitrarily powerful MT systems with an experiment showing large improvements from an upgrade to its base model.",1.0
SKG_MT_783,https://openalex.org/W2971036337,2019,32,"['https://openalex.org/W1500281234', 'https://openalex.org/W2013196554', 'https://openalex.org/W2038721957', 'https://openalex.org/W2070150502', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116492146', 'https://openalex.org/W2123301721', 'https://openalex.org/W2133459682', 'https://openalex.org/W2147258359', 'https://openalex.org/W2149327368', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251882135', 'https://openalex.org/W2252166243', 'https://openalex.org/W2902449872']","This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task. The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge. However, it only focuses on the lexical level and utilizes consecutive n-grams paraphrases. In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes may be skip-grams. We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based metrics. Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed metric outperforms all previous versions of Meteor.",0.994535519125683
SKG_MT_784,https://openalex.org/W3045818398,2020,11,"['https://openalex.org/W1522301498', 'https://openalex.org/W1605005685', 'https://openalex.org/W2101105183', 'https://openalex.org/W2242818861', 'https://openalex.org/W2267635276', 'https://openalex.org/W2286365479', 'https://openalex.org/W2291160084', 'https://openalex.org/W2407022425', 'https://openalex.org/W2524428287', 'https://openalex.org/W2740433069', 'https://openalex.org/W2805493160', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962944188', 'https://openalex.org/W2963122961', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963631431', 'https://openalex.org/W2963643655', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964299589', 'https://openalex.org/W2985517040', 'https://openalex.org/W2991040477', 'https://openalex.org/W4295262505', 'https://openalex.org/W4297813615', 'https://openalex.org/W4385245566']","Neural Machine Translation (NMT) is resource-intensive. We design a quantization procedure to compress fit NMT models better for devices with limited hardware capability. We use logarithmic quantization, instead of the more commonly used fixed-point quantization, based on the empirical fact that parameters distribution is not uniform. We find that biases do not take a lot of memory and show that biases can be left uncompressed to improve the overall quality without affecting the compression rate. We also propose to use an error-feedback mechanism during retraining, to preserve the compressed model as a stale gradient. We empirically show that NMT models based on Transformer or RNN architecture can be compressed up to 4-bit precision without any noticeable quality degradation. Models can be compressed up to binary precision, albeit with lower quality. RNN architecture seems to be more robust towards compression, compared to the Transformer.",1.0
SKG_MT_788,https://openalex.org/W43226814,2012,15,"['https://openalex.org/W16967297', 'https://openalex.org/W22168010', 'https://openalex.org/W145987047', 'https://openalex.org/W1989658336', 'https://openalex.org/W2047092297', 'https://openalex.org/W2095252775', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101456909', 'https://openalex.org/W2104936489', 'https://openalex.org/W2110518760', 'https://openalex.org/W2116042738', 'https://openalex.org/W2117339222', 'https://openalex.org/W2120354757', 'https://openalex.org/W2120587290', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131148434', 'https://openalex.org/W2132001515', 'https://openalex.org/W2134800885', 'https://openalex.org/W2137387514', 'https://openalex.org/W2137986108', 'https://openalex.org/W2143104527', 'https://openalex.org/W2143564602', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148334774', 'https://openalex.org/W2154590816', 'https://openalex.org/W2159514083', 'https://openalex.org/W2171421863', 'https://openalex.org/W2406343028', 'https://openalex.org/W2473743559', 'https://openalex.org/W3169012378', 'https://openalex.org/W3202296894']","Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents. In this paper we analyze patents along the orthogonal dimensions of topic and textual structure. We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance. We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning. We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters. A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs.",1.0
SKG_MT_789,https://openalex.org/W2149747182,2011,2,"['https://openalex.org/W165935821', 'https://openalex.org/W222053410', 'https://openalex.org/W230880734', 'https://openalex.org/W1510052640', 'https://openalex.org/W1551202288', 'https://openalex.org/W1596525713', 'https://openalex.org/W1979495315', 'https://openalex.org/W2100281225', 'https://openalex.org/W2103156738', 'https://openalex.org/W2110104386', 'https://openalex.org/W2112777616', 'https://openalex.org/W2112900913', 'https://openalex.org/W2121338597', 'https://openalex.org/W2139621418', 'https://openalex.org/W2140343992', 'https://openalex.org/W2143008661', 'https://openalex.org/W2144279206', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152263452', 'https://openalex.org/W2161227214']","To address the parse error issue for tree-tostring translation, this paper proposes a similarity-based decoding generation (SDG) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. Our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models. 1",1.0
SKG_MT_790,https://openalex.org/W2250699071,2014,11,"['https://openalex.org/W232191560', 'https://openalex.org/W1258814950', 'https://openalex.org/W1481820510', 'https://openalex.org/W1559723967', 'https://openalex.org/W1716250762', 'https://openalex.org/W1830628992', 'https://openalex.org/W1978381081', 'https://openalex.org/W2008652694', 'https://openalex.org/W2016522586', 'https://openalex.org/W2023286866', 'https://openalex.org/W2046384065', 'https://openalex.org/W2093197119', 'https://openalex.org/W2100271871', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102258316', 'https://openalex.org/W2103866953', 'https://openalex.org/W2115328410', 'https://openalex.org/W2118781169', 'https://openalex.org/W2123825474', 'https://openalex.org/W2124204950', 'https://openalex.org/W2125536435', 'https://openalex.org/W2125993116', 'https://openalex.org/W2142623206', 'https://openalex.org/W2142898321', 'https://openalex.org/W2147898188', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156050092', 'https://openalex.org/W2163561827', 'https://openalex.org/W2170716095', 'https://openalex.org/W2184656097', 'https://openalex.org/W2250872883', 'https://openalex.org/W2251673953', 'https://openalex.org/W2251912507', 'https://openalex.org/W2252136820', 'https://openalex.org/W2790246984', 'https://openalex.org/W2989631226', 'https://openalex.org/W3170166771', 'https://openalex.org/W3170731508', 'https://openalex.org/W3202911835', 'https://openalex.org/W3203768450']","We propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input. We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database. Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for responsebased learning over learning from references only on returning the correct answer from a semantic parse of a translated query. In general, our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT.",1.0
SKG_MT_791,https://openalex.org/W2798465082,2018,57,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1632114991', 'https://openalex.org/W1869752048', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2157331557', 'https://openalex.org/W2550821151', 'https://openalex.org/W2561274697', 'https://openalex.org/W2577255746', 'https://openalex.org/W2595715041', 'https://openalex.org/W2617039999', 'https://openalex.org/W2758310181', 'https://openalex.org/W2773621464', 'https://openalex.org/W2799920282', 'https://openalex.org/W2952339051', 'https://openalex.org/W2962717763', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963393838', 'https://openalex.org/W2963506925', 'https://openalex.org/W2964116568', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W4293718192', 'https://openalex.org/W4297730607']","Neural Machine Translation (NMT) is notorious for its need for large amounts of bilingual data. An effective approach to compensate for this requirement is Multi-Task Learning (MTL) to leverage different linguistic resources as a source of inductive bias. Current MTL architectures are based on the Seq2Seq transduction, and (partially) share different components of the models among the tasks. However, this MTL approach often suffers from task interference and is not able to fully capture commonalities among subsets of tasks. We address this issue by extending the recurrent units with multiple “blocks” along with a trainable “routing network”. The routing network enables adaptive collaboration by dynamic sharing of blocks conditioned on the task at hand, input, and model state. Empirical evaluation of two low-resource translation tasks, English to Vietnamese and Farsi, show +1 BLEU score improvements compared to strong baselines.",0.9949748743718593
SKG_MT_792,https://openalex.org/W2951065878,2019,32,"['https://openalex.org/W22168010', 'https://openalex.org/W630532510', 'https://openalex.org/W1905522558', 'https://openalex.org/W2117278770', 'https://openalex.org/W2130942839', 'https://openalex.org/W2144600658', 'https://openalex.org/W2147262247', 'https://openalex.org/W2740718109', 'https://openalex.org/W2740743644', 'https://openalex.org/W2752630748', 'https://openalex.org/W2756566411', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888519496', 'https://openalex.org/W2912095972', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964085268']","To improve low-resource Neural Machine Translation (NMT) with multilingual corpus, training on the most related high-resource language only is generally more effective than us- ing all data available (Neubig and Hu, 2018). However, it remains a question whether a smart data selection strategy can further improve low-resource NMT with data from other auxiliary languages. In this paper, we seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose and efficient algorithm, (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show TCS brings significant gains of up to 2 BLEU improvements on three of four languages we test, with minimal training overhead.",0.9948717948717949
SKG_MT_795,https://openalex.org/W1792850142,2010,35,"['https://openalex.org/W224499596', 'https://openalex.org/W1479669738', 'https://openalex.org/W1551202288', 'https://openalex.org/W1629870923', 'https://openalex.org/W1965893653', 'https://openalex.org/W1969974515', 'https://openalex.org/W1994919150', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008961349', 'https://openalex.org/W2044804339', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108460050', 'https://openalex.org/W2112900913', 'https://openalex.org/W2116316001', 'https://openalex.org/W2118612506', 'https://openalex.org/W2124807415', 'https://openalex.org/W2139823104', 'https://openalex.org/W2145096378', 'https://openalex.org/W2146685010', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158388102', 'https://openalex.org/W2162245945', 'https://openalex.org/W2166210475', 'https://openalex.org/W2401082558', 'https://openalex.org/W2405762604', 'https://openalex.org/W2437005631']","This paper proposes a novel method for long distance, clause-level reordering in statistical machine translation (SMT). The proposed method separately translates clauses in the source sentence and recon-structs the target sentence using the clause translations with non-terminals. The non-terminals are placeholders of embedded clauses, by which we reduce complicated clause-level reordering into simple word-level reordering. Its translation model is trained using a bilingual corpus with clause-level alignment, which can be au-tomatically annotated by our alignment algorithm with a syntactic parser in the source language. We achieved signifi-cant improvements of 1.4 % in BLEU and 1.3 % in TER by using Moses, and 2.2% in BLEU and 3.5 % in TER by using our hierarchical phrase-based SMT, for the English-to-Japanese translation of re-search paper abstracts in the medical do-main. 1",0.994475138121547
SKG_MT_796,https://openalex.org/W3091924335,2020,1,"['https://openalex.org/W22168010', 'https://openalex.org/W222053410', 'https://openalex.org/W1249771036', 'https://openalex.org/W1602368597', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112076978', 'https://openalex.org/W2122922389', 'https://openalex.org/W2132984949', 'https://openalex.org/W2133512280', 'https://openalex.org/W2134800885', 'https://openalex.org/W2149327368', 'https://openalex.org/W2151834591', 'https://openalex.org/W2153579005', 'https://openalex.org/W2157331557', 'https://openalex.org/W2163922914', 'https://openalex.org/W2296073425', 'https://openalex.org/W2609278920', 'https://openalex.org/W2741602058', 'https://openalex.org/W2741838462', 'https://openalex.org/W2889326796', 'https://openalex.org/W2890007195', 'https://openalex.org/W2898846200', 'https://openalex.org/W2919188216', 'https://openalex.org/W2923622379', 'https://openalex.org/W2926516160', 'https://openalex.org/W2944815030', 'https://openalex.org/W2946379889', 'https://openalex.org/W2949405462', 'https://openalex.org/W2950485982', 'https://openalex.org/W2952474700', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962696263', 'https://openalex.org/W2962735107', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962890089', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963443683', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964122237', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970686691', 'https://openalex.org/W3001434439', 'https://openalex.org/W3022961397', 'https://openalex.org/W3030163527']","Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.",1.0
SKG_MT_799,https://openalex.org/W2888010645,2018,20,"['https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2534200568', 'https://openalex.org/W2659803504', 'https://openalex.org/W2787534827', 'https://openalex.org/W2806591392', 'https://openalex.org/W2905927205', 'https://openalex.org/W2952479981', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962954913', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963541420', 'https://openalex.org/W2963699608', 'https://openalex.org/W2963891264', 'https://openalex.org/W2963970666', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W4385245566']","Most of the Neural Machine Translation (NMT) models are based on the sequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped with the attention mechanism. However, the conventional attention mechanism treats the decoding at each time step equally with the same matrix, which is problematic since the softness of the attention for different types of words (e.g. content words and function words) should differ. Therefore, we propose a new model with a mechanism called Self-Adaptive Control of Temperature (SACT) to control the softness of attention by means of an attention temperature. Experimental results on the Chinese-English translation and English-Vietnamese translation demonstrate that our model outperforms the baseline models, and the analysis and the case study show that our model can attend to the most relevant elements in the source-side contexts and generate the translation of high quality.",0.9956709956709957
SKG_MT_800,https://openalex.org/W2155539936,2011,12,"['https://openalex.org/W22168010', 'https://openalex.org/W129135300', 'https://openalex.org/W167345539', 'https://openalex.org/W294504433', 'https://openalex.org/W314345615', 'https://openalex.org/W1480857181', 'https://openalex.org/W1819520634', 'https://openalex.org/W1868901511', 'https://openalex.org/W2008652694', 'https://openalex.org/W2016856586', 'https://openalex.org/W2078861931', 'https://openalex.org/W2080012968', 'https://openalex.org/W2091542740', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113788796', 'https://openalex.org/W2117745860', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125838338', 'https://openalex.org/W2129158994', 'https://openalex.org/W2139770225', 'https://openalex.org/W2143134347', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147880316', 'https://openalex.org/W2171421863']","In many languages the use of compound words is very productive. A common practice to reduce sparsity consists in splitting compounds in the training data. When this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order. Furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order. We also propose new heuristic methods for merging components that outperform all known methods, and a learning-based method that has similar accuracy as the heuristic method, is better at producing novel compounds, and can operate with no background linguistic resources. 1",1.0
SKG_MT_804,https://openalex.org/W2946068894,2019,116,"['https://openalex.org/W1522301498', 'https://openalex.org/W1915251500', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2163605009', 'https://openalex.org/W2250473257', 'https://openalex.org/W2546938941', 'https://openalex.org/W2594978815', 'https://openalex.org/W2613904329', 'https://openalex.org/W2765961751', 'https://openalex.org/W2766182427', 'https://openalex.org/W2804047946', 'https://openalex.org/W2886095922', 'https://openalex.org/W2888519496', 'https://openalex.org/W2905266130', 'https://openalex.org/W2950359962', 'https://openalex.org/W2952148143', 'https://openalex.org/W2956130159', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963545917', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3098341425', 'https://openalex.org/W4295727797', 'https://openalex.org/W4298393544', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation.Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines.",1.0
SKG_MT_807,https://openalex.org/W2806311723,2018,80,"['https://openalex.org/W1518951372', 'https://openalex.org/W1825672851', 'https://openalex.org/W2095705004', 'https://openalex.org/W2124807415', 'https://openalex.org/W2149428536', 'https://openalex.org/W2162390675', 'https://openalex.org/W2168231600', 'https://openalex.org/W2399880602', 'https://openalex.org/W2606974598', 'https://openalex.org/W2612675303', 'https://openalex.org/W2613904329', 'https://openalex.org/W2622263826', 'https://openalex.org/W2626778328', 'https://openalex.org/W2763421725', 'https://openalex.org/W2767989436', 'https://openalex.org/W2773493195', 'https://openalex.org/W2789541106', 'https://openalex.org/W2790319220', 'https://openalex.org/W2791110811', 'https://openalex.org/W2797162333', 'https://openalex.org/W2896060389', 'https://openalex.org/W2949650786', 'https://openalex.org/W2952754453', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963702144', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564']","Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT'14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",1.0
SKG_MT_808,https://openalex.org/W2970682420,2019,18,"['https://openalex.org/W309335912', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2250342921', 'https://openalex.org/W2250761258', 'https://openalex.org/W2251610689', 'https://openalex.org/W2508907594', 'https://openalex.org/W2550821151', 'https://openalex.org/W2563351168', 'https://openalex.org/W2595715041', 'https://openalex.org/W2887920589', 'https://openalex.org/W2902918014', 'https://openalex.org/W2916548775', 'https://openalex.org/W2962735107', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963626623', 'https://openalex.org/W2964053711', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964343359', 'https://openalex.org/W2973088264', 'https://openalex.org/W4385245566']","This paper presents the systems submitted by the University of Groningen to the English-Kazakh language pair (both translation directions) for the WMT 2019 news translation task. We explore the potential benefits of (i) morphological segmentation (both unsupervised and rule-based), given the agglutinative nature of Kazakh, (ii) data from two additional languages (Turkish and Russian), given the scarcity of English-Kazakh data and (iii) synthetic data, both for the source and for the target language. Our best sub- missions ranked second for Kazakh-English and third for English-Kazakh in terms of the BLEU automatic evaluation metric.",1.0
SKG_MT_809,https://openalex.org/W1902237438,2015,8489,"['https://openalex.org/W1514535095', 'https://openalex.org/W1586532344', 'https://openalex.org/W1591801644', 'https://openalex.org/W1753482797', 'https://openalex.org/W2086202918', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2147527908', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2169724380', 'https://openalex.org/W2250653840', 'https://openalex.org/W2950178297', 'https://openalex.org/W2951527505', 'https://openalex.org/W2962741254', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538']","An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.However, there has been little work exploring useful architectures for attention-based NMT.This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",1.0
SKG_MT_810,https://openalex.org/W2108967571,2015,5,"['https://openalex.org/W5939791', 'https://openalex.org/W125693536', 'https://openalex.org/W1423339008', 'https://openalex.org/W1753482797', 'https://openalex.org/W1909398668', 'https://openalex.org/W1969974515', 'https://openalex.org/W1976252502', 'https://openalex.org/W2006969979', 'https://openalex.org/W2056894934', 'https://openalex.org/W2060127787', 'https://openalex.org/W2077054525', 'https://openalex.org/W2100183594', 'https://openalex.org/W2108597378', 'https://openalex.org/W2116042738', 'https://openalex.org/W2117130368', 'https://openalex.org/W2127426251', 'https://openalex.org/W2132339004', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136016850', 'https://openalex.org/W2136925175', 'https://openalex.org/W2140343992', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158899491', 'https://openalex.org/W2250338395', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250732891', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251222643', 'https://openalex.org/W2251682575', 'https://openalex.org/W2252177599', 'https://openalex.org/W2252225757', 'https://openalex.org/W2913340405', 'https://openalex.org/W2949300694', 'https://openalex.org/W2949888546']","We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various non-local translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese-English translation over a state-of-the-art system that already includes neural network features.",1.0
SKG_MT_811,https://openalex.org/W3099744315,2020,67,"['https://openalex.org/W630532510', 'https://openalex.org/W1840435438', 'https://openalex.org/W2006832571', 'https://openalex.org/W2148708890', 'https://openalex.org/W2169200297', 'https://openalex.org/W2250653840', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2742113707', 'https://openalex.org/W2798665661', 'https://openalex.org/W2889326796', 'https://openalex.org/W2891555348', 'https://openalex.org/W2914120296', 'https://openalex.org/W2948017315', 'https://openalex.org/W2951286828', 'https://openalex.org/W2952103439', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962727366', 'https://openalex.org/W2962736243', 'https://openalex.org/W2962843521', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963394326', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963721344', 'https://openalex.org/W2963748441', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963969878', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970752815', 'https://openalex.org/W2971043182', 'https://openalex.org/W2971207485', 'https://openalex.org/W2977458338', 'https://openalex.org/W2995541765', 'https://openalex.org/W3013840636', 'https://openalex.org/W3034469191', 'https://openalex.org/W3034716087', 'https://openalex.org/W3034850762', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035497479', 'https://openalex.org/W3045462440', 'https://openalex.org/W4288284086', 'https://openalex.org/W4385245566']","Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",1.0
SKG_MT_813,https://openalex.org/W2148168985,2011,9,"['https://openalex.org/W1551202288', 'https://openalex.org/W1631260214', 'https://openalex.org/W1965893653', 'https://openalex.org/W1979495315', 'https://openalex.org/W2010835028', 'https://openalex.org/W2034660568', 'https://openalex.org/W2037894654', 'https://openalex.org/W2038248725', 'https://openalex.org/W2049633694', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108374515', 'https://openalex.org/W2108460050', 'https://openalex.org/W2110104386', 'https://openalex.org/W2112900913', 'https://openalex.org/W2119314391', 'https://openalex.org/W2124807415', 'https://openalex.org/W2137268927', 'https://openalex.org/W2144804812', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165003622', 'https://openalex.org/W2165666205', 'https://openalex.org/W2167393476', 'https://openalex.org/W2167723982', 'https://openalex.org/W3202296894', 'https://openalex.org/W3202429746']","In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-to-Japanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1",1.0
SKG_MT_814,https://openalex.org/W3034789084,2020,33,"['https://openalex.org/W1899794420', 'https://openalex.org/W2100664567', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2525778437', 'https://openalex.org/W2530486890', 'https://openalex.org/W2531207078', 'https://openalex.org/W2592647456', 'https://openalex.org/W2896457183', 'https://openalex.org/W2952288254', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962935543', 'https://openalex.org/W2963077280', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963742216', 'https://openalex.org/W2963887123', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964308564', 'https://openalex.org/W3035207248', 'https://openalex.org/W3035445001', 'https://openalex.org/W3100753857', 'https://openalex.org/W4289440825', 'https://openalex.org/W4385245566']","This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English &lt;=&gt; (German, Romanian, Estonian, Finnish, Hungarian).",1.0
SKG_MT_815,https://openalex.org/W2250849168,2015,10,"['https://openalex.org/W43079696', 'https://openalex.org/W125693536', 'https://openalex.org/W149848483', 'https://openalex.org/W201231365', 'https://openalex.org/W1497117093', 'https://openalex.org/W1663287312', 'https://openalex.org/W1981116971', 'https://openalex.org/W2032494091', 'https://openalex.org/W2062953001', 'https://openalex.org/W2076749875', 'https://openalex.org/W2110660056', 'https://openalex.org/W2113287090', 'https://openalex.org/W2118972857', 'https://openalex.org/W2122609803', 'https://openalex.org/W2128232128', 'https://openalex.org/W2136657878', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150634928', 'https://openalex.org/W2157512532', 'https://openalex.org/W2169242480', 'https://openalex.org/W2171802951', 'https://openalex.org/W2250761024', 'https://openalex.org/W2252065493', 'https://openalex.org/W2437005631']","In this paper, we propose a paraphrasing model to address the task of system combination for machine translation.We dynamically learn hierarchical paraphrases from target hypotheses and form a synchronous context-free grammar to guide a series of transformations of target hypotheses into fused translations.The model is able to exploit phrasal and structural system-weighted consensus and also to utilize existing information about word ordering present in the target hypotheses.In addition, to consider a diverse set of plausible fused translations, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations.Our experimental results show that our approach can achieve a significant improvement over combination baselines.",1.0
SKG_MT_818,https://openalex.org/W2798931235,2018,232,"['https://openalex.org/W8895266', 'https://openalex.org/W38126138', 'https://openalex.org/W630532510', 'https://openalex.org/W1508577659', 'https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1816313093', 'https://openalex.org/W1915251500', 'https://openalex.org/W2025768430', 'https://openalex.org/W2064675550', 'https://openalex.org/W2121745180', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2153653739', 'https://openalex.org/W2166880518', 'https://openalex.org/W2250952810', 'https://openalex.org/W2251222643', 'https://openalex.org/W2252106004', 'https://openalex.org/W2418388682', 'https://openalex.org/W2425903306', 'https://openalex.org/W2493916176', 'https://openalex.org/W2514713644', 'https://openalex.org/W2608615602', 'https://openalex.org/W2610245951', 'https://openalex.org/W2625092622', 'https://openalex.org/W2626778328', 'https://openalex.org/W2741602058', 'https://openalex.org/W2741917668', 'https://openalex.org/W2785093437', 'https://openalex.org/W2786790428', 'https://openalex.org/W2794365787', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950133940', 'https://openalex.org/W2950359962', 'https://openalex.org/W2951184134', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964308564']","Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.",0.9908256880733946
SKG_MT_819,https://openalex.org/W2963888305,2017,129,"['https://openalex.org/W6908809', 'https://openalex.org/W108437174', 'https://openalex.org/W1816079941', 'https://openalex.org/W2052816566', 'https://openalex.org/W2096204319', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112900913', 'https://openalex.org/W2116316001', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2160382364', 'https://openalex.org/W2437005631', 'https://openalex.org/W2549259847', 'https://openalex.org/W2563574619', 'https://openalex.org/W2576713500', 'https://openalex.org/W2586050494', 'https://openalex.org/W2949847915', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963699608', 'https://openalex.org/W2963956654', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564']","Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.",1.0
SKG_MT_820,https://openalex.org/W2915824390,2011,14,"['https://openalex.org/W50182133', 'https://openalex.org/W1578737154', 'https://openalex.org/W1631260214', 'https://openalex.org/W2016856586', 'https://openalex.org/W2054533749', 'https://openalex.org/W2072336782', 'https://openalex.org/W2095650036', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103357334', 'https://openalex.org/W2105245376', 'https://openalex.org/W2107695330', 'https://openalex.org/W2138706636', 'https://openalex.org/W2542068976', 'https://openalex.org/W3148153833', 'https://openalex.org/W3209717902']","In this paper, we present the KIT systems participating in the Shared Translation Task translating between English↔German and English↔French. All translations are generated using phrase-based translation systems, using different kinds of word-based, part-of-speech-based and cluster-based language models trained on the provided data. Additional models include bilingual language models, reordering models based on part-of-speech tags and syntactic parse trees, as well as a lexicalized reordering model. In order to make use of noisy web-crawled data, we apply filtering and data selection methods for language modeling. A discriminative word lexicon using source context information proved beneficial for all translation directions.",1.0
SKG_MT_821,https://openalex.org/W2962890089,2018,70,"['https://openalex.org/W222053410', 'https://openalex.org/W1905522558', 'https://openalex.org/W2113290770', 'https://openalex.org/W2115410424', 'https://openalex.org/W2117278770', 'https://openalex.org/W2132984949', 'https://openalex.org/W2296073425', 'https://openalex.org/W2396575863', 'https://openalex.org/W2515631395', 'https://openalex.org/W2525778437', 'https://openalex.org/W2613904329', 'https://openalex.org/W2743320544', 'https://openalex.org/W2750588180', 'https://openalex.org/W2756978580', 'https://openalex.org/W2760452458', 'https://openalex.org/W2903158431', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963919854', 'https://openalex.org/W2964022663', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964265128', 'https://openalex.org/W3208011254', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of data quality and tries to reduce the negative impact of data noise on MT training, in particular, neural MT (NMT) training. This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training. The proposed approach uses trusted data and a denoising curriculum realized by online data selection. Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise.",1.0
SKG_MT_822,https://openalex.org/W2891896107,2018,344,"['https://openalex.org/W1533303231', 'https://openalex.org/W1542713999', 'https://openalex.org/W1614298861', 'https://openalex.org/W2126725946', 'https://openalex.org/W2155870214', 'https://openalex.org/W2167623372', 'https://openalex.org/W2251033195', 'https://openalex.org/W2294774419', 'https://openalex.org/W2296319761', 'https://openalex.org/W2493916176', 'https://openalex.org/W2561995736', 'https://openalex.org/W2594021297', 'https://openalex.org/W2741602058', 'https://openalex.org/W2784235818', 'https://openalex.org/W2950577311', 'https://openalex.org/W2952190837', 'https://openalex.org/W2962772361', 'https://openalex.org/W2962844668', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W4294367149', 'https://openalex.org/W4299579390', 'https://openalex.org/W4302571896']","Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a quadratic problem to learn a orthogonal matrix aligning a bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.",0.9936305732484076
SKG_MT_824,https://openalex.org/W2251362467,2013,15,"['https://openalex.org/W10455605', 'https://openalex.org/W20811212', 'https://openalex.org/W61355150', 'https://openalex.org/W155101268', 'https://openalex.org/W349770100', 'https://openalex.org/W1489409710', 'https://openalex.org/W1529616844', 'https://openalex.org/W1603508585', 'https://openalex.org/W2021618504', 'https://openalex.org/W2032494091', 'https://openalex.org/W2067341777', 'https://openalex.org/W2078861931', 'https://openalex.org/W2087735403', 'https://openalex.org/W2097688447', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104858828', 'https://openalex.org/W2114584278', 'https://openalex.org/W2121771196', 'https://openalex.org/W2122422241', 'https://openalex.org/W2129804798', 'https://openalex.org/W2132446289', 'https://openalex.org/W2133512280', 'https://openalex.org/W2133990480', 'https://openalex.org/W2139212933', 'https://openalex.org/W2139621418', 'https://openalex.org/W2147880316', 'https://openalex.org/W2150955799', 'https://openalex.org/W2154486363', 'https://openalex.org/W2162499915', 'https://openalex.org/W2164441316', 'https://openalex.org/W2165213242', 'https://openalex.org/W2166545452', 'https://openalex.org/W2167679664', 'https://openalex.org/W2170074965', 'https://openalex.org/W2251251208', 'https://openalex.org/W2251763817', 'https://openalex.org/W3170960752', 'https://openalex.org/W3208474812']","This paper is to introduce our participation in the WMT13 shared tasks on Quality Estimation for machine translation without using reference translations. We submitted the results for Task 1.1 (sentence-level quality estimation), Task 1.2 (system selection) and Task 2 (word-level quality estimation). In Task 1.1, we used an enhanced version of BLEU metric without using reference translations to evaluate the translation quality. In Task 1.2, we utilized a probability model Naïve Bayes (NB) as a classification algorithm with the features borrowed from the traditional evaluation metrics. In Task 2, to take the contextual information into account, we employed a discriminative undirected probabilistic graphical model Conditional random field (CRF), in addition to the NB algorithm. The training experiments on the past WMT corpora showed that the designed methods of this paper yielded promising results especially the statistical models of CRF and NB. The official results show that our CRF model achieved the highest F-score 0.8297 in binary classification of Task 2. 1",1.0
SKG_MT_825,https://openalex.org/W2898793635,2018,9,"['https://openalex.org/W1522301498', 'https://openalex.org/W1903029394', 'https://openalex.org/W2101105183', 'https://openalex.org/W2184135559', 'https://openalex.org/W2222949842', 'https://openalex.org/W2229833550', 'https://openalex.org/W2251743902', 'https://openalex.org/W2337363174', 'https://openalex.org/W2443536229', 'https://openalex.org/W2467834614', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2626778328', 'https://openalex.org/W2725082186', 'https://openalex.org/W2740711318', 'https://openalex.org/W2744813330', 'https://openalex.org/W2752630748', 'https://openalex.org/W2765961751', 'https://openalex.org/W2786790428', 'https://openalex.org/W2787560479', 'https://openalex.org/W2798081680', 'https://openalex.org/W2798878995', 'https://openalex.org/W2803241009', 'https://openalex.org/W2953391683', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963027654', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963563735']","We propose a method to transfer knowledge across neural machine translation (NMT) models by means of a shared dynamic vocabulary. Our approach allows to extend an initial model for a given language pair to cover new languages by adapting its vocabulary as long as new data become available (i.e., introducing new vocabulary items if they are not included in the initial model). The parameter transfer mechanism is evaluated in two scenarios: i) to adapt a trained single language NMT system to work with a new language pair and ii) to continuously add new language pairs to grow to a multilingual NMT system. In both the scenarios our goal is to improve the translation performance, while minimizing the training convergence time. Preliminary experiments spanning five languages with different training data sizes (i.e., 5k and 50k parallel sentences) show a significant performance gain ranging from +3.85 up to +13.63 BLEU in different language directions. Moreover, when compared with training an NMT model from scratch, our transfer-learning approach allows us to reach higher performance after training up to 4% of the total training steps.",1.0
SKG_MT_826,https://openalex.org/W3116878451,2020,3,"['https://openalex.org/W90695545', 'https://openalex.org/W808583520', 'https://openalex.org/W1777239053', 'https://openalex.org/W2089629691', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108325777', 'https://openalex.org/W2121457870', 'https://openalex.org/W2132959801', 'https://openalex.org/W2155027007', 'https://openalex.org/W2251955814', 'https://openalex.org/W2419292002', 'https://openalex.org/W2512593609', 'https://openalex.org/W2529548870', 'https://openalex.org/W2610245951', 'https://openalex.org/W2805523732', 'https://openalex.org/W2886751231', 'https://openalex.org/W2890698823', 'https://openalex.org/W2896234185', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964078338', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970074184', 'https://openalex.org/W3015583403']","Despite the success of neural machine translation (NMT), simultaneous neural machine translation (SNMT), the task of translating in real time before a full sentence has been observed, remains challenging due to the syntactic structure difference and simultaneity requirements. In this paper, we propose a general framework for adapting neural machine translation to translate simultaneously. Our framework contains two parts: prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation. Experiments on three translation corpora and two language pairs show the efficacy of the proposed framework on balancing the quality and latency in adapting NMT to perform simultaneous translation.",1.0
SKG_MT_827,https://openalex.org/W2251894068,2014,1,"['https://openalex.org/W17659133', 'https://openalex.org/W165283731', 'https://openalex.org/W222053410', 'https://openalex.org/W1508977358', 'https://openalex.org/W1631260214', 'https://openalex.org/W1768003599', 'https://openalex.org/W1792850142', 'https://openalex.org/W1965893653', 'https://openalex.org/W1990174463', 'https://openalex.org/W2008652694', 'https://openalex.org/W2010835028', 'https://openalex.org/W2037894654', 'https://openalex.org/W2095690342', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108460050', 'https://openalex.org/W2112900913', 'https://openalex.org/W2116316001', 'https://openalex.org/W2118536060', 'https://openalex.org/W2119168550', 'https://openalex.org/W2121435899', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133444727', 'https://openalex.org/W2136544838', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149747182', 'https://openalex.org/W2152803722', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2156641381', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158953777', 'https://openalex.org/W2175969983', 'https://openalex.org/W2250333994', 'https://openalex.org/W2252045625', 'https://openalex.org/W2352162662', 'https://openalex.org/W2437005631', 'https://openalex.org/W2463396630', 'https://openalex.org/W2502430711', 'https://openalex.org/W2595715041', 'https://openalex.org/W4241645538']","Word reordering is a crucial technique in statistical machine translation in which syntactic information plays an important role.Synchronous context-free grammar has typically been used for this purpose with various modifications for adding flexibilities to its synchronized tree generation.We permit further flexibilities in the synchronous context-free grammar in order to translate between languages with drastically different word order.Our method pre-processes a parallel corpus by abstracting source-side dependency trees, and performs long-distance reordering on top of an off-the-shelf phrase-based system.Experimental results show that our method significantly outperforms previous phrase-based and syntax-based models for translation between English and Japanese.",1.0
SKG_MT_828,https://openalex.org/W2137894131,2014,16,"['https://openalex.org/W1631260214', 'https://openalex.org/W2060786818', 'https://openalex.org/W2101816610', 'https://openalex.org/W2113131493', 'https://openalex.org/W2123318312', 'https://openalex.org/W2124807415', 'https://openalex.org/W2151976760', 'https://openalex.org/W2156985047', 'https://openalex.org/W2251749997', 'https://openalex.org/W2257408573', 'https://openalex.org/W2604377206', 'https://openalex.org/W2752885492', 'https://openalex.org/W3104921985', 'https://openalex.org/W3145128584']","We use parallel FDA5, an efficiently pa-rameterized and optimized parallel im-plementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a day for each translation di-rection. We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of 3.49 BLEU points difference using significantly less resources for training and development. 1",1.0
SKG_MT_829,https://openalex.org/W2251682543,2013,5,"['https://openalex.org/W132913264', 'https://openalex.org/W2096557251', 'https://openalex.org/W2098601596', 'https://openalex.org/W2107564089', 'https://openalex.org/W2111142112', 'https://openalex.org/W2123825474', 'https://openalex.org/W2124807415', 'https://openalex.org/W2129262273', 'https://openalex.org/W2136135294', 'https://openalex.org/W2137227862', 'https://openalex.org/W2141852716', 'https://openalex.org/W2143269898', 'https://openalex.org/W2154124206', 'https://openalex.org/W2180952760', 'https://openalex.org/W2399967715']","Until recently, the application of discriminative training to log linear-based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters. In this paper, we propose to scale up discriminative training of (He and Deng, 2012) to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning. The experimental results confirm the effectiveness of our proposals on NIST MT06 set over a strong baseline. 1",1.0
SKG_MT_830,https://openalex.org/W3208326910,2019,2,[],"This paper describes our end-to-end speech translation system for the speech translation task of lectures and TED talks from English to German for IWSLT Evaluation 2019. We propose layer-tied self-attention for end-to-end speech translation. Our method takes advantage of sharing weights of speech encoder and text decoder. The representation of source speech and the representation of target text are coordinated layer by layer, so that the speech and text can learn a better alignment during the training procedure. We also adopt data augmentation to enhance the parallel speech-text corpus. The En-De experimental results show that our best model achieves 17.68 on tst2015. Our ASR achieves WER of 6.6% on TED-LIUM test set. The En-Pt model can achieve about 11.83 on the MuST-C dev set.",1.0
SKG_MT_831,https://openalex.org/W2964048171,2018,178,"['https://openalex.org/W769612788', 'https://openalex.org/W1522301498', 'https://openalex.org/W1673923490', 'https://openalex.org/W1832693441', 'https://openalex.org/W1902237438', 'https://openalex.org/W1904365287', 'https://openalex.org/W1945616565', 'https://openalex.org/W2099471712', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2173520492', 'https://openalex.org/W2194775991', 'https://openalex.org/W2342045095', 'https://openalex.org/W2525778437', 'https://openalex.org/W2540404261', 'https://openalex.org/W2542835211', 'https://openalex.org/W2546938941', 'https://openalex.org/W2561274697', 'https://openalex.org/W2581637843', 'https://openalex.org/W2601324753', 'https://openalex.org/W2610245951', 'https://openalex.org/W2613904329', 'https://openalex.org/W2739978843', 'https://openalex.org/W2741917668', 'https://openalex.org/W2767899794', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963207607', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963373786', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963684088', 'https://openalex.org/W2964040467', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964153729', 'https://openalex.org/W2964253222', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4244167344', 'https://openalex.org/W4293846201', 'https://openalex.org/W4294149591', 'https://openalex.org/W4298393544', 'https://openalex.org/W4301368689', 'https://openalex.org/W4307459710', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.",1.0
SKG_MT_832,https://openalex.org/W2151594415,2012,25,"['https://openalex.org/W635530177', 'https://openalex.org/W1550982044', 'https://openalex.org/W1631260214', 'https://openalex.org/W1665921526', 'https://openalex.org/W1848260265', 'https://openalex.org/W1973923101', 'https://openalex.org/W1978630505', 'https://openalex.org/W2043832764', 'https://openalex.org/W2072867742', 'https://openalex.org/W2079182758', 'https://openalex.org/W2105891181', 'https://openalex.org/W2111142112', 'https://openalex.org/W2111362445', 'https://openalex.org/W2117339222', 'https://openalex.org/W2120708938', 'https://openalex.org/W2122270629', 'https://openalex.org/W2122373020', 'https://openalex.org/W2130959832', 'https://openalex.org/W2132001515', 'https://openalex.org/W2132726600', 'https://openalex.org/W2137387514', 'https://openalex.org/W2138309071', 'https://openalex.org/W2144600658', 'https://openalex.org/W2145033586', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2167216307', 'https://openalex.org/W2171119909', 'https://openalex.org/W2356613612', 'https://openalex.org/W2394860946']","Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation. 1",1.0
SKG_MT_833,https://openalex.org/W3035511085,2020,19,"['https://openalex.org/W135467536', 'https://openalex.org/W1522301498', 'https://openalex.org/W1526096287', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102486516', 'https://openalex.org/W2130942839', 'https://openalex.org/W2149741699', 'https://openalex.org/W2164948578', 'https://openalex.org/W2170716095', 'https://openalex.org/W2188365844', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548228487', 'https://openalex.org/W2561274697', 'https://openalex.org/W2567948266', 'https://openalex.org/W2572816092', 'https://openalex.org/W2626792426', 'https://openalex.org/W2756566411', 'https://openalex.org/W2767206889', 'https://openalex.org/W2773493195', 'https://openalex.org/W2789478061', 'https://openalex.org/W2798493043', 'https://openalex.org/W2807753879', 'https://openalex.org/W2903782687', 'https://openalex.org/W2910135751', 'https://openalex.org/W2915573484', 'https://openalex.org/W2950513705', 'https://openalex.org/W2953046278', 'https://openalex.org/W2962738009', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963090522', 'https://openalex.org/W2963135265', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963311117', 'https://openalex.org/W2963357083', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963522047', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963537482', 'https://openalex.org/W2963665552', 'https://openalex.org/W2963714898', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963918167', 'https://openalex.org/W2963919854', 'https://openalex.org/W2963987720', 'https://openalex.org/W2964000524', 'https://openalex.org/W2964076537', 'https://openalex.org/W2964106094', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964168257', 'https://openalex.org/W2964247056', 'https://openalex.org/W2970311224', 'https://openalex.org/W2970646865', 'https://openalex.org/W2971120622', 'https://openalex.org/W4293411878', 'https://openalex.org/W4295177495', 'https://openalex.org/W4297801368', 'https://openalex.org/W4298343152', 'https://openalex.org/W4385245566', 'https://openalex.org/W4391602018']","This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro↔En and De↔En. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).",1.0
SKG_MT_835,https://openalex.org/W2250240265,2012,1,"['https://openalex.org/W143478578', 'https://openalex.org/W1483201900', 'https://openalex.org/W1551202288', 'https://openalex.org/W1964591498', 'https://openalex.org/W2071606701', 'https://openalex.org/W2103349228', 'https://openalex.org/W2113104171', 'https://openalex.org/W2119005844', 'https://openalex.org/W2119224513', 'https://openalex.org/W2119314391', 'https://openalex.org/W2119388680', 'https://openalex.org/W2126946601', 'https://openalex.org/W2133229022', 'https://openalex.org/W2137592969', 'https://openalex.org/W2139621418', 'https://openalex.org/W2140605844', 'https://openalex.org/W2144279206', 'https://openalex.org/W2146574666', 'https://openalex.org/W2154563865', 'https://openalex.org/W2161227214', 'https://openalex.org/W2165109801']",This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1,1.0
SKG_MT_836,https://openalex.org/W2251659871,2014,9,"['https://openalex.org/W1631260214', 'https://openalex.org/W1905522558', 'https://openalex.org/W1976115394', 'https://openalex.org/W2080373976', 'https://openalex.org/W2107508692', 'https://openalex.org/W2109530867', 'https://openalex.org/W2117278770', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2137387514', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147262247', 'https://openalex.org/W2152114834', 'https://openalex.org/W2156985047']","This paper explores a number of simple and effective techniques to adapt statistical machine translation (SMT) systems in the medical domain. Comparative experiments are conducted on large corpora for six language pairs. We not only compare each adapted system with the baseline, but also combine them to further improve the domain-specific systems. Finally, we attend the WMT2014 medical summary sentence translation constrained task and our systems achieve the best BLEU scores for Czech-English, EnglishGerman, French-English language pairs and the second best BLEU scores for reminding pairs.",1.0
SKG_MT_837,https://openalex.org/W3207986056,2019,3,[],"This paper describes the ESPnet submissions to the How2 Speech Translation task at IWSLT2019. In this year, we mainly build our systems based on Transformer architectures in all tasks and focus on the end-to-end speech translation (E2E-ST). We first compare RNN-based models and Transformer, and then confirm Transformer models significantly and consistently outperform RNN models in all tasks and corpora. Next, we investigate pre-training of E2E-ST models with the ASR and MT tasks. On top of the pre-training, we further explore knowledge distillation from the NMT model and the deeper speech encoder, and confirm drastic improvements over the baseline model. All of our codes are publicly available in ESPnet.",0.9953488372093023
SKG_MT_838,https://openalex.org/W2148861942,2010,88,"['https://openalex.org/W94749751', 'https://openalex.org/W202303397', 'https://openalex.org/W1517932739', 'https://openalex.org/W1550206324', 'https://openalex.org/W1648885110', 'https://openalex.org/W1966497645', 'https://openalex.org/W2006969979', 'https://openalex.org/W2024181699', 'https://openalex.org/W2037603696', 'https://openalex.org/W2048679005', 'https://openalex.org/W2049633694', 'https://openalex.org/W2064885744', 'https://openalex.org/W2081795963', 'https://openalex.org/W2096152098', 'https://openalex.org/W2096175520', 'https://openalex.org/W2110302976', 'https://openalex.org/W2114524997', 'https://openalex.org/W2115191221', 'https://openalex.org/W2138247936', 'https://openalex.org/W2142262074', 'https://openalex.org/W2143954309', 'https://openalex.org/W2145251161', 'https://openalex.org/W2145262681', 'https://openalex.org/W2149684865', 'https://openalex.org/W2150749667', 'https://openalex.org/W2156909104', 'https://openalex.org/W2167660864']","In this paper, we introduce a method that automatically builds text classifiers in a new language by training on already labeled data in another language. Our method transfers the classification knowledge across languages by translating the model features and by using an Expectation Maximization (EM) algorithm that naturally takes into account the ambiguity associated with the translation of a word. We further exploit the readily available unlabeled data in the target language via semisupervised learning, and adapt the translated model to better fit the data distribution of the target language. 1",1.0
SKG_MT_839,https://openalex.org/W2252101403,2015,5,"['https://openalex.org/W23077562', 'https://openalex.org/W45604574', 'https://openalex.org/W1901714926', 'https://openalex.org/W2003458432', 'https://openalex.org/W2034991102', 'https://openalex.org/W2090284840', 'https://openalex.org/W2096253869', 'https://openalex.org/W2111856253', 'https://openalex.org/W2124807415', 'https://openalex.org/W2147984332', 'https://openalex.org/W2159755860', 'https://openalex.org/W2168733445', 'https://openalex.org/W2250715607', 'https://openalex.org/W2251098065', 'https://openalex.org/W2252065493']","Benjamin Marie, Aurélien Max. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2015.",1.0
SKG_MT_840,https://openalex.org/W2118206542,2014,21,"['https://openalex.org/W110357785', 'https://openalex.org/W176283576', 'https://openalex.org/W1498502445', 'https://openalex.org/W1568793342', 'https://openalex.org/W1743517014', 'https://openalex.org/W1899100928', 'https://openalex.org/W2008961349', 'https://openalex.org/W2044804339', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095755718', 'https://openalex.org/W2103890644', 'https://openalex.org/W2108460050', 'https://openalex.org/W2109680024', 'https://openalex.org/W2118585731', 'https://openalex.org/W2130551284', 'https://openalex.org/W2136544838', 'https://openalex.org/W2144385980', 'https://openalex.org/W2144900797', 'https://openalex.org/W2144995019', 'https://openalex.org/W2146685010', 'https://openalex.org/W2147930801', 'https://openalex.org/W2151738913', 'https://openalex.org/W2153927306', 'https://openalex.org/W2162245945', 'https://openalex.org/W2175969983', 'https://openalex.org/W2242975712', 'https://openalex.org/W2250963088', 'https://openalex.org/W2251843378', 'https://openalex.org/W2399720833', 'https://openalex.org/W3203149905', 'https://openalex.org/W3209717902']","We present a simple preordering approach for machine translation based on a featurerich logistic regression model to predict whether two children of the same node in the source-side parse tree should be swapped or not.Given the pair-wise children regression scores we conduct an efficient depth-first branch-and-bound search through the space of possible children permutations, avoiding using a cascade of classifiers or limiting the list of possible ordering outcomes.We report experiments in translating English to Japanese and Korean, demonstrating superior performance as (a) the number of crossing links drops by more than 10% absolute with respect to other state-of-the-art preordering approaches, (b) BLEU scores improve on 2.2 points over the baseline with lexicalised reordering model, and (c) decoding can be carried out 80 times faster.",1.0
SKG_MT_842,https://openalex.org/W2760101375,2017,5,"['https://openalex.org/W1686810756', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117352571', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2157331557', 'https://openalex.org/W2194775991', 'https://openalex.org/W2212703438', 'https://openalex.org/W2251994258', 'https://openalex.org/W2513263213', 'https://openalex.org/W2581101319', 'https://openalex.org/W2593341061', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963909453', 'https://openalex.org/W2964308564']","We report experiments with multi-modal\nneural machine translation models that incorporate global visual features in different parts of the encoder and decoder, and\nuse the VGG19 network to extract features for all images. In our experiments,\nwe explore both different strategies to include global image features and also how\nensembling different models at inference\ntime impact translations. Our submissions\nranked 3rd best for translating from English into French, always improving considerably over an neural machine translation baseline across all language pair evaluated, e.g. an increase of 7.0–9.2 METEOR points.",1.0
SKG_MT_843,https://openalex.org/W2970624737,2019,7,"['https://openalex.org/W21337280', 'https://openalex.org/W77450199', 'https://openalex.org/W630532510', 'https://openalex.org/W1998045382', 'https://openalex.org/W2042296808', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107695330', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2163327848', 'https://openalex.org/W2413534664', 'https://openalex.org/W2613846746', 'https://openalex.org/W2740743644', 'https://openalex.org/W2744813330', 'https://openalex.org/W2752172973', 'https://openalex.org/W2760327630', 'https://openalex.org/W2767784948', 'https://openalex.org/W2800233718', 'https://openalex.org/W2843176975', 'https://openalex.org/W2903158431', 'https://openalex.org/W2907233999', 'https://openalex.org/W2942274440', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962863357', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963915737', 'https://openalex.org/W3137086296', 'https://openalex.org/W4248960356', 'https://openalex.org/W4293863842']","We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or model training. In this paper, we propose a novel query strategy for selecting “unlabeled” samples from a new domain based on sentence embeddings for Arabic. We accelerate the fine-tuning process of the generic model to the target domain. Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their sentence embeddings to embeddings from the generic domain. We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling baseline with our new approach, similar to active learning, using two small update sets for simulating the work of human translators. For the prescribed setting we can save more than 50% of the annotation costs without loss in quality, demonstrating the effectiveness of our approach.",1.0
SKG_MT_845,https://openalex.org/W2169218969,2013,9,"['https://openalex.org/W1650129375', 'https://openalex.org/W1736600331', 'https://openalex.org/W1828578481', 'https://openalex.org/W2063698930', 'https://openalex.org/W2087735403', 'https://openalex.org/W2105338474', 'https://openalex.org/W2122373020', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125536435', 'https://openalex.org/W2133768083', 'https://openalex.org/W2134495021', 'https://openalex.org/W2134800885', 'https://openalex.org/W2141972832', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2159136976', 'https://openalex.org/W2161227214', 'https://openalex.org/W2250319048', 'https://openalex.org/W2251832051', 'https://openalex.org/W2395054697', 'https://openalex.org/W2437005631']","Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n 2 b) for input of n words and beam sizeb, compared toO(n 3 ) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) obtained phrase-based and CKY Hiero with the same translation model.",1.0
SKG_MT_846,https://openalex.org/W2963088657,2019,5,"['https://openalex.org/W1523296404', 'https://openalex.org/W2114013702', 'https://openalex.org/W2131698806', 'https://openalex.org/W2144746247', 'https://openalex.org/W2146574666', 'https://openalex.org/W2160218441', 'https://openalex.org/W2514694861', 'https://openalex.org/W2621975677', 'https://openalex.org/W2757980860', 'https://openalex.org/W2884519246', 'https://openalex.org/W2894218541', 'https://openalex.org/W2902608666', 'https://openalex.org/W2902702034', 'https://openalex.org/W2914120296', 'https://openalex.org/W2949733514', 'https://openalex.org/W2953072129', 'https://openalex.org/W2962678612', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963341956', 'https://openalex.org/W3005347330']","We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: we combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.",1.0
SKG_MT_848,https://openalex.org/W3106124812,2020,52,"['https://openalex.org/W1496189301', 'https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2008400752', 'https://openalex.org/W2047237057', 'https://openalex.org/W2105717194', 'https://openalex.org/W2105842272', 'https://openalex.org/W2123442489', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131962941', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143612262', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251349042', 'https://openalex.org/W2302963717', 'https://openalex.org/W2475046758', 'https://openalex.org/W2606780347', 'https://openalex.org/W2751262944', 'https://openalex.org/W2786865417', 'https://openalex.org/W2788411903', 'https://openalex.org/W2788802500', 'https://openalex.org/W2796167946', 'https://openalex.org/W2798296074', 'https://openalex.org/W2798749466', 'https://openalex.org/W2799079975', 'https://openalex.org/W2805019413', 'https://openalex.org/W2888128175', 'https://openalex.org/W2890194927', 'https://openalex.org/W2890585661', 'https://openalex.org/W2890908793', 'https://openalex.org/W2897767292', 'https://openalex.org/W2898951026', 'https://openalex.org/W2905122540', 'https://openalex.org/W2949384567', 'https://openalex.org/W2950898568', 'https://openalex.org/W2951107864', 'https://openalex.org/W2951790836', 'https://openalex.org/W2951939640', 'https://openalex.org/W2962728167', 'https://openalex.org/W2962767366', 'https://openalex.org/W2962800603', 'https://openalex.org/W2962822108', 'https://openalex.org/W2963266334', 'https://openalex.org/W2963357517', 'https://openalex.org/W2963374482', 'https://openalex.org/W2963510263', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963617989', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963655793', 'https://openalex.org/W2963794306', 'https://openalex.org/W2963858333', 'https://openalex.org/W2963965612', 'https://openalex.org/W2964015378', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964325845', 'https://openalex.org/W2964710271', 'https://openalex.org/W2966750840', 'https://openalex.org/W2967827612', 'https://openalex.org/W2970308008', 'https://openalex.org/W2994900646', 'https://openalex.org/W3034712001', 'https://openalex.org/W3128464707', 'https://openalex.org/W4294558607', 'https://openalex.org/W4297716178', 'https://openalex.org/W4297733535']","The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural information for encoding, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our model for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.",1.0
SKG_MT_850,https://openalex.org/W2177801600,2010,63,"['https://openalex.org/W1534448508', 'https://openalex.org/W1686266550', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008225289', 'https://openalex.org/W2038698865', 'https://openalex.org/W2040781285', 'https://openalex.org/W2049633694', 'https://openalex.org/W2061910127', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109664771', 'https://openalex.org/W2122270629', 'https://openalex.org/W2137387514', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150903784', 'https://openalex.org/W2152420937', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158874082', 'https://openalex.org/W2165199647', 'https://openalex.org/W2171458318', 'https://openalex.org/W2437005631', 'https://openalex.org/W3146209407']","Typical statistical machine translation systems are trained with static parallel corpora. Here we account for scenarios with a continuous incoming stream of parallel training data. Such scenarios include daily governmental proceedings, sustained output from translation agencies, or crowd-sourced translations. We show incorporating recent sentence pairs from the stream improves performance compared with a static baseline. Since frequent batch retraining is computationally demanding we introduce a fast incremental alternative using an online version of the EM algorithm. To bound our memory requirements we use a novel data-structure and associated training regime. When compared to frequent batch retraining, our online time and space-bounded model achieves the same performance with significantly less computational overhead. 1",1.0
SKG_MT_852,https://openalex.org/W2952443824,2019,22,"['https://openalex.org/W174630521', 'https://openalex.org/W1493309689', 'https://openalex.org/W1523296404', 'https://openalex.org/W1615290298', 'https://openalex.org/W2016221228', 'https://openalex.org/W2079145130', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107130271', 'https://openalex.org/W2113343614', 'https://openalex.org/W2118021410', 'https://openalex.org/W2131726681', 'https://openalex.org/W2145685230', 'https://openalex.org/W2165199647', 'https://openalex.org/W2250523604', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251743902', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2739823599', 'https://openalex.org/W2741049976', 'https://openalex.org/W2767206889', 'https://openalex.org/W2786553814', 'https://openalex.org/W2787087032', 'https://openalex.org/W2787811729', 'https://openalex.org/W2963208801', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216505', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963571015', 'https://openalex.org/W2963732644', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964053384', 'https://openalex.org/W2964170290', 'https://openalex.org/W3098341425']","Paraphrases, rewordings of the same semantic meaning, are useful for improving generalization and translation. Unlike previous works that only explore paraphrases at the word or phrase level, we use different translations of the whole training data that are consistent in structure as paraphrases at the corpus level. We treat paraphrases as foreign languages, tag source sentences with paraphrase labels, and train on parallel paraphrases in the style of multilingual Neural Machine Translation (NMT). Our multi-paraphrase NMT that trains only on two languages outperforms the multilingual baselines. Adding paraphrases improves the rare word translation and increases entropy and diversity in lexical choice. Adding the source paraphrases boosts performance better than adding the target ones, while adding both lifts performance further. We achieve a BLEU score of 57.2 for French-to-English translation using 24 corpus-level paraphrases of the Bible, which outperforms the multilingual baselines and is +34.7 above the single-source single-target NMT baseline.",1.0
SKG_MT_853,https://openalex.org/W102083554,2010,5,"['https://openalex.org/W1647671624', 'https://openalex.org/W2092689630', 'https://openalex.org/W2128514324', 'https://openalex.org/W2153225416', 'https://openalex.org/W2179984881', 'https://openalex.org/W2597886012', 'https://openalex.org/W2760736243']","This paper presents the participation of the University of Bari (UBA) at the SemEval-2010 Cross-Lingual Lexical Substitution Task. The goal of the task is to substitute a word in a language Ls, which occurs in a particular context, by providing the best synonyms in a different language Lt which fit in that context. This task has a strict relation with the task of automatic machine translation, but there are some differences: Cross-lingual lexical substitution targets one word at a time and the main goal is to find as many good translations as possible for the given target word. Moreover, there are some connections with Word Sense Disambiguation (WSD) algorithms. Indeed, understanding the meaning of the target word is necessary to find the best substitutions. An important aspect of this kind of task is the possibility of finding synonyms without using a particular sense inventory or a specific parallel corpus, thus allowing the participation of unsupervised approaches. UBA proposes two systems: the former is based on an automatic translation system which exploits Google Translator, the latter is based on a parallel corpus approach which relies on Wikipedia in order to find the best substitutions. 1",0.9940828402366864
SKG_MT_854,https://openalex.org/W2250685329,2015,2,"['https://openalex.org/W5056303', 'https://openalex.org/W74908938', 'https://openalex.org/W222053410', 'https://openalex.org/W635530177', 'https://openalex.org/W1510052640', 'https://openalex.org/W1551202288', 'https://openalex.org/W1606076566', 'https://openalex.org/W1631260214', 'https://openalex.org/W1697844855', 'https://openalex.org/W1708005048', 'https://openalex.org/W2034639451', 'https://openalex.org/W2095669743', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112900913', 'https://openalex.org/W2115081467', 'https://openalex.org/W2124935176', 'https://openalex.org/W2139621418', 'https://openalex.org/W2140133598', 'https://openalex.org/W2143002608', 'https://openalex.org/W2154581043', 'https://openalex.org/W2158917268', 'https://openalex.org/W2159755860', 'https://openalex.org/W2165666205', 'https://openalex.org/W2166781037', 'https://openalex.org/W2167447786', 'https://openalex.org/W2169954957', 'https://openalex.org/W2252177599', 'https://openalex.org/W2257408573', 'https://openalex.org/W2437005631', 'https://openalex.org/W2584764413', 'https://openalex.org/W3203940212']","In syntax-based machine translation, rule selection is the task of choosing the correct target side of a translation rule among rules with the same source side.We define a discriminative rule selection model for systems that have syntactic annotation on the target language side (stringto-tree).This is a new and clean way to integrate soft source syntactic constraints into string-to-tree systems as features of the rule selection model.We release our implementation as part of Moses.",1.0
SKG_MT_855,https://openalex.org/W2798047776,2018,9,"['https://openalex.org/W635530177', 'https://openalex.org/W1514587017', 'https://openalex.org/W1832693441', 'https://openalex.org/W1975879668', 'https://openalex.org/W2100664567', 'https://openalex.org/W2113065326', 'https://openalex.org/W2124807415', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149327368', 'https://openalex.org/W2188353343', 'https://openalex.org/W2293743194', 'https://openalex.org/W2513592723', 'https://openalex.org/W2735665712', 'https://openalex.org/W2740565324', 'https://openalex.org/W2768398531', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962955856', 'https://openalex.org/W2963053969', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963463964', 'https://openalex.org/W2964033619', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964123521', 'https://openalex.org/W2964297722', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W3204406378']","We present the first real-world application of methods for improving neural machine translation (NMT) with human reinforcement, based on explicit and implicit user feedback collected on the eBay e-commerce platform. Previous work has been confined to simulation experiments, whereas in this paper we work with real logged feedback for offline bandit learning of NMT parameters. We conduct a thorough analysis of the available explicit user judgments---five-star ratings of translation quality---and show that they are not reliable enough to yield significant improvements in bandit learning. In contrast, we successfully utilize implicit task-based feedback collected in a cross-lingual search task to improve task-specific and machine translation quality metrics.",0.991869918699187
SKG_MT_856,https://openalex.org/W2976965654,2019,64,"['https://openalex.org/W1690739335', 'https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2183341477', 'https://openalex.org/W2194775991', 'https://openalex.org/W2487501366', 'https://openalex.org/W2525778437', 'https://openalex.org/W2540404261', 'https://openalex.org/W2552839021', 'https://openalex.org/W2613904329', 'https://openalex.org/W2659803504', 'https://openalex.org/W2767206889', 'https://openalex.org/W2789543585', 'https://openalex.org/W2963350559', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964118293', 'https://openalex.org/W2964190861', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W4297747548', 'https://openalex.org/W4301368689', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Zhuohan Li, Zi Lin, Di He, Fei Tian, Tao Qin, Liwei Wang, Tie-Yan Liu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_857,https://openalex.org/W3088348768,2019,7,[],"This work is inspired by a typical machine translation industry scenario in which translators make use of in-domain data for facilitating translation of similar or repeating sentences. We introduce a generic framework applied at inference in which a subset of segment pairs are first extracted from training data according to their similarity to the input sentences. These segments are then used to dynamically update the parameters of a generic NMT network, thus performing a lexical micro-adaptation. Our approach demonstrates strong adaptation performance to new and existing datasets including pseudo in-domain data. We evaluate our approach on a heterogeneous English-French training dataset showing accuracy gains on all evaluated domains when compared to strong adaptation baselines.",1.0
SKG_MT_859,https://openalex.org/W3098593077,2020,14,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1905522558', 'https://openalex.org/W1915251500', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2123318312', 'https://openalex.org/W2134800885', 'https://openalex.org/W2147262247', 'https://openalex.org/W2525778437', 'https://openalex.org/W2546938941', 'https://openalex.org/W2561274697', 'https://openalex.org/W2740718109', 'https://openalex.org/W2756566411', 'https://openalex.org/W2757592053', 'https://openalex.org/W2759461255', 'https://openalex.org/W2794365787', 'https://openalex.org/W2805394970', 'https://openalex.org/W2886095922', 'https://openalex.org/W2887516053', 'https://openalex.org/W2889326796', 'https://openalex.org/W2922349260', 'https://openalex.org/W2933138175', 'https://openalex.org/W2950940239', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963149635', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970409072', 'https://openalex.org/W2987154291', 'https://openalex.org/W2996854111', 'https://openalex.org/W2997518171', 'https://openalex.org/W2997763445', 'https://openalex.org/W3103169714', 'https://openalex.org/W3204406378', 'https://openalex.org/W4287874506', 'https://openalex.org/W4288253296', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.",1.0
SKG_MT_861,https://openalex.org/W2902740013,2017,0,"['https://openalex.org/W1522301498', 'https://openalex.org/W1677182931', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117539524', 'https://openalex.org/W2133512280', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144600658', 'https://openalex.org/W2194775991', 'https://openalex.org/W2509282593', 'https://openalex.org/W2514713644', 'https://openalex.org/W2552124255', 'https://openalex.org/W2594229957', 'https://openalex.org/W2608030593', 'https://openalex.org/W2950178297', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963954913']",This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convolutional features and refine them using encoder-side information. Our final constrained submissions ranked first for English-French and second for English-German language pairs among the constrained submissions according to the automatic evaluation metric METEOR.,1.0
SKG_MT_862,https://openalex.org/W2891924676,2018,84,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121879602', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2251743902', 'https://openalex.org/W2413436069', 'https://openalex.org/W2525778437', 'https://openalex.org/W2539201987', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561274697', 'https://openalex.org/W2610245951', 'https://openalex.org/W2613904329', 'https://openalex.org/W2766730959', 'https://openalex.org/W2794365787', 'https://openalex.org/W2803258077', 'https://openalex.org/W2807535859', 'https://openalex.org/W2809456172', 'https://openalex.org/W2962700074', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962807144', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499433', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963879527', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566']","Due to the benefits of model compactness, multilingual translation (including many-to-one, many-to-many and one-to-many) based on a universal encoder-decoder architecture attracts more and more attention. However, previous studies show that one-to-many translation based on this framework cannot perform on par with the individually trained models. In this work, we introduce three strategies to improve one-to-many multilingual translation by balancing the shared and unique features. Within the architecture of one decoder for all target languages, we first exploit the use of unique initial states for different target languages. Then, we employ language-dependent positional embeddings. Finally and especially, we propose to divide the hidden cells of the decoder into shared and language-dependent ones. The extensive experiments demonstrate that our proposed methods can obtain remarkable improvements over the strong baselines. Moreover, our strategies can achieve comparable or even better performance than the individually trained translation models.",1.0
SKG_MT_864,https://openalex.org/W2970947975,2019,35,"['https://openalex.org/W2111142112', 'https://openalex.org/W2148708890', 'https://openalex.org/W2159755860', 'https://openalex.org/W2176263492', 'https://openalex.org/W2270070752', 'https://openalex.org/W2466062786', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798761464', 'https://openalex.org/W2888539709', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902510077', 'https://openalex.org/W2903193068', 'https://openalex.org/W2912351236', 'https://openalex.org/W2938830017', 'https://openalex.org/W2950399211', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964110616', 'https://openalex.org/W3015703505', 'https://openalex.org/W3204406378', 'https://openalex.org/W4385245566']","In this paper we introduce the systems Baidu submitted for the WMT19 shared task on Chinese<->English news translation. Our systems are based on the Transformer architecture with some effective improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our Chinese->English system achieved the highest case-sensitive BLEU score among all constrained submissions, and our English->Chinese system ranked the second in all submissions.",1.0
SKG_MT_865,https://openalex.org/W2251832051,2012,4,"['https://openalex.org/W1978630505', 'https://openalex.org/W2053306448', 'https://openalex.org/W2122373020', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2146574666', 'https://openalex.org/W2151594415', 'https://openalex.org/W2152249239', 'https://openalex.org/W2437005631']","This paper describes our submissions for the WMT-12 translation task using Kriya- our hierarchical phrase-based system. We submitted systems in French-English and English-Czech language pairs. In addition to the baseline system following the standard MT pipeline, we tried ensemble decoding for French-English. The ensemble decoding method improved the BLEU score by 0.4 points over the baseline in newstest-2011. For English-Czech, we segmented the Czech side of the corpora and trained two different segmented models in addition to our baseline system. 1 Baseline Systems",1.0
SKG_MT_868,https://openalex.org/W2250644439,2014,162,"['https://openalex.org/W100623710', 'https://openalex.org/W179875071', 'https://openalex.org/W222053410', 'https://openalex.org/W1423339008', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118536060', 'https://openalex.org/W2119168550', 'https://openalex.org/W2123635983', 'https://openalex.org/W2133280805', 'https://openalex.org/W2136922672', 'https://openalex.org/W2143564602', 'https://openalex.org/W2147768505', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158899491', 'https://openalex.org/W2163605009', 'https://openalex.org/W2169488311', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250714477', 'https://openalex.org/W2251395256', 'https://openalex.org/W2251690405', 'https://openalex.org/W2252102852', 'https://openalex.org/W2952230511', 'https://openalex.org/W4233906699']","In this paper, we propose a novel recursive recurrent neural network (R 2 NN) to model the end-to-end decoding process for statistical machine translation.R 2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner.A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly.Experiments on a Chinese to English translation task show that our proposed R 2 NN can outperform the stateof-the-art baseline by about 1.5 points in BLEU.",1.0
SKG_MT_869,https://openalex.org/W2250338219,2013,2,"['https://openalex.org/W170711724', 'https://openalex.org/W1522553155', 'https://openalex.org/W1970381522', 'https://openalex.org/W1988511622', 'https://openalex.org/W2058179030', 'https://openalex.org/W2104907655', 'https://openalex.org/W2123301721', 'https://openalex.org/W2127849236', 'https://openalex.org/W2143539737', 'https://openalex.org/W2145350077', 'https://openalex.org/W2251988367']","Large amount of parallel corpora is required for building Statistical Machine Translation (SMT) systems. We describe the TransDoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation of constituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence translations from phrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work.",0.993103448275862
SKG_MT_870,https://openalex.org/W2759274242,2017,5,"['https://openalex.org/W1479669738', 'https://openalex.org/W1605903931', 'https://openalex.org/W1606076566', 'https://openalex.org/W2006969979', 'https://openalex.org/W2076749875', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119727789', 'https://openalex.org/W2127797489', 'https://openalex.org/W2127863960', 'https://openalex.org/W2134800885', 'https://openalex.org/W2153653739', 'https://openalex.org/W2170204377', 'https://openalex.org/W2250548645', 'https://openalex.org/W2250841445', 'https://openalex.org/W2251349730', 'https://openalex.org/W2437005631', 'https://openalex.org/W2550821151', 'https://openalex.org/W2572474373', 'https://openalex.org/W2595715041', 'https://openalex.org/W2596163066', 'https://openalex.org/W2964007535', 'https://openalex.org/W4241645538']","Pivot translation is a useful method for translating between languages with little or no parallel data by utilizing parallel data in an intermediate language such as English.A popular approach for pivot translation used in phrase-based or tree-based translation models combines source-pivot and pivot-target translation models into a source-target model, as known as triangulation.However, this combination is based on the constituent words' surface forms and often produces incorrect source-target phrase pairs due to semantic ambiguity in the pivot language, and interlingual differences.This degrades translation accuracy.In this paper, we propose a approach for the triangulation using syntactic subtrees in the pivot language to distinguish pivot language words by their syntactic roles to avoid incorrect phrase combinations.Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points. 1",0.9921259842519685
SKG_MT_871,https://openalex.org/W2988680494,2019,0,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2555428947', 'https://openalex.org/W2914120296', 'https://openalex.org/W2945700568', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2965373594', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970597249']","In this paper, we present our submission for the English to Czech Text Translation Task of IWSLT 2019. Our system aims to study how pre-trained language models, used as input embeddings, can improve a specialized machine translation system trained on few data. Therefore, we implemented a Transformer-based encoder-decoder neural system which is able to use the output of a pre-trained language model as input embeddings, and we compared its performance under three configurations: 1) without any pre-trained language model (constrained), 2) using a language model trained on the monolingual parts of the allowed English-Czech data (constrained), and 3) using a language model trained on a large quantity of external monolingual data (unconstrained). We used BERT as external pre-trained language model (configuration 3), and BERT architecture for training our own language model (configuration 2). Regarding the training data, we trained our MT system on a small quantity of parallel text: one set only consists of the provided MuST-C corpus, and the other set consists of the MuST-C corpus and the News Commentary corpus from WMT. We observed that using the external pre-trained BERT improves the scores of our system by +0.8 to +1.5 of BLEU on our development set, and +0.97 to +1.94 of BLEU on the test set. However, using our own language model trained only on the allowed parallel data seems to improve the machine translation performances only when the system is trained on the smallest dataset.",1.0
SKG_MT_872,https://openalex.org/W2739894144,2017,115,"['https://openalex.org/W6908809', 'https://openalex.org/W222053410', 'https://openalex.org/W1533861849', 'https://openalex.org/W1598566484', 'https://openalex.org/W1902237438', 'https://openalex.org/W1934041838', 'https://openalex.org/W2030904529', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114887620', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134036914', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153508793', 'https://openalex.org/W2157331557', 'https://openalex.org/W2165666205', 'https://openalex.org/W2166905217', 'https://openalex.org/W2525778437', 'https://openalex.org/W2533220144', 'https://openalex.org/W2573523139', 'https://openalex.org/W2576482813', 'https://openalex.org/W2915926444', 'https://openalex.org/W2949952998', 'https://openalex.org/W2950635152', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963713328', 'https://openalex.org/W2964308564']","Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and Japanese-English translation tasks.",1.0
SKG_MT_873,https://openalex.org/W2963593215,2017,62,"['https://openalex.org/W6908809', 'https://openalex.org/W1964952992', 'https://openalex.org/W2006969979', 'https://openalex.org/W2064675550', 'https://openalex.org/W2113788796', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2154368244', 'https://openalex.org/W2157331557', 'https://openalex.org/W2410217169', 'https://openalex.org/W2413436069', 'https://openalex.org/W2430909973', 'https://openalex.org/W2525778437', 'https://openalex.org/W2527845440', 'https://openalex.org/W2539201987', 'https://openalex.org/W2576713500', 'https://openalex.org/W2950635152', 'https://openalex.org/W2952659248', 'https://openalex.org/W2962708992', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963696609', 'https://openalex.org/W2963699608', 'https://openalex.org/W3010865323', 'https://openalex.org/W4241645538']","Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning processing of the neural translation model. Experiments on Chinese-English dataset show that our approach leads to significant improvements.",1.0
SKG_MT_875,https://openalex.org/W2979478220,2019,9,"['https://openalex.org/W48935546', 'https://openalex.org/W2742141468']","Among the six challenges of neural machine translation (NMT) coined by (Koehn and Knowles, 2017), rare-word problem is considered the most severe one, especially in translation of low-resource languages. In this paper, we propose three solutions to address the rare words in neural machine translation systems. First, we enhance source context to predict the target words by connecting directly the source embeddings to the output of the attention component in NMT. Second, we propose an algorithm to learn morphology of unknown words for English in supervised way in order to minimize the adverse effect of rare-word problem. Finally, we exploit synonymous relation from the WordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our approaches on two low-resource language pairs: English-Vietnamese and Japanese-Vietnamese. In our experiments, we have achieved significant improvements of up to roughly +1.0 BLEU points in both language pairs.",1.0
SKG_MT_876,https://openalex.org/W2950940239,2019,58,"['https://openalex.org/W1915251500', 'https://openalex.org/W2101456909', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136156618', 'https://openalex.org/W2156985047', 'https://openalex.org/W2172268343', 'https://openalex.org/W2251335508', 'https://openalex.org/W2294774419', 'https://openalex.org/W2493916176', 'https://openalex.org/W2555428947', 'https://openalex.org/W2561274697', 'https://openalex.org/W2567571499', 'https://openalex.org/W2740743644', 'https://openalex.org/W2753639998', 'https://openalex.org/W2756566411', 'https://openalex.org/W2758310181', 'https://openalex.org/W2759461255', 'https://openalex.org/W2760452458', 'https://openalex.org/W2798931235', 'https://openalex.org/W2805394970', 'https://openalex.org/W2885616807', 'https://openalex.org/W2890244613', 'https://openalex.org/W2903193068', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964308564', 'https://openalex.org/W3098341425', 'https://openalex.org/W3204406378', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",1.0
SKG_MT_881,https://openalex.org/W2131969445,2011,43,"['https://openalex.org/W67353944', 'https://openalex.org/W832270446', 'https://openalex.org/W1618905105', 'https://openalex.org/W1631260214', 'https://openalex.org/W1647671624', 'https://openalex.org/W1934041838', 'https://openalex.org/W2040781285', 'https://openalex.org/W2056983531', 'https://openalex.org/W2067341777', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119821739', 'https://openalex.org/W2123301721', 'https://openalex.org/W2124807415', 'https://openalex.org/W2145790651', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153635508', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2276605909', 'https://openalex.org/W2434153183', 'https://openalex.org/W2989631226', 'https://openalex.org/W3197274356']","We present a discriminative learning method to improve the consistency of translations in phrase-based Statistical Machine Translation (SMT) systems. Our method is inspired by Translation Memory (TM) systems which are widely used by human translators in industrial settings. We constrain the translation of an input sentence using the most similar ‘translation example ’ retrieved from the TM. Differently from previous research which used simple fuzzy match thresholds, these constraints are imposed using discriminative learning to optimise the translation performance. We observe that using this method can benefit the SMT system by not only producing consistent translations, but also improved translation outputs. We report a 0.9 point improvement in terms of BLEU score on English–Chinese technical documents. 1",1.0
SKG_MT_883,https://openalex.org/W2970155008,2019,17,"['https://openalex.org/W2101105183', 'https://openalex.org/W2108765529', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2176263492', 'https://openalex.org/W2250243742', 'https://openalex.org/W2250969425', 'https://openalex.org/W2251689237', 'https://openalex.org/W2274912527', 'https://openalex.org/W2295710275', 'https://openalex.org/W2460474657', 'https://openalex.org/W2475024326', 'https://openalex.org/W2539350388', 'https://openalex.org/W2622801600', 'https://openalex.org/W2888442053', 'https://openalex.org/W2891844856', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962955856', 'https://openalex.org/W2963218093', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963506925', 'https://openalex.org/W2964123521', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W2972972637', 'https://openalex.org/W4385245566']","Amirhossein Tebbifakhr, Luisa Bentivogli, Matteo Negri, Marco Turchi. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",0.9929078014184397
SKG_MT_885,https://openalex.org/W2250822480,2013,3,"['https://openalex.org/W23077562', 'https://openalex.org/W49976080', 'https://openalex.org/W201231365', 'https://openalex.org/W1528941926', 'https://openalex.org/W2005505724', 'https://openalex.org/W2087735403', 'https://openalex.org/W2110660056', 'https://openalex.org/W2118972857', 'https://openalex.org/W2121380975', 'https://openalex.org/W2122609803', 'https://openalex.org/W2130630493', 'https://openalex.org/W2131986285', 'https://openalex.org/W2138046680', 'https://openalex.org/W2153800732', 'https://openalex.org/W2162010692', 'https://openalex.org/W2165666205', 'https://openalex.org/W2168733445', 'https://openalex.org/W2169599995', 'https://openalex.org/W2786062171', 'https://openalex.org/W3037557769']","We present a novel, structured language model- Supertagged Dependency Language Model to model the syntactic dependencies between words. The goal is to identify ungrammatical hypotheses from a set of candidate translations in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination. 1",1.0
SKG_MT_887,https://openalex.org/W2561274697,2016,307,"['https://openalex.org/W1486649854', 'https://openalex.org/W1553589067', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1915251500', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117339222', 'https://openalex.org/W2118434577', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2139621418', 'https://openalex.org/W2140460368', 'https://openalex.org/W2146502635', 'https://openalex.org/W2157331557', 'https://openalex.org/W2212846646', 'https://openalex.org/W2239731672', 'https://openalex.org/W2242975712', 'https://openalex.org/W2251743902', 'https://openalex.org/W2251855842', 'https://openalex.org/W2280395961', 'https://openalex.org/W2291126447', 'https://openalex.org/W2361821140', 'https://openalex.org/W2437005631', 'https://openalex.org/W2577335011', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963325132', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963357083', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964308564']",,1.0
SKG_MT_889,https://openalex.org/W3118540863,2020,3,"['https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2184135559', 'https://openalex.org/W2799051177', 'https://openalex.org/W2933138175', 'https://openalex.org/W2944815030', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962943802', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964202354', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965373594', 'https://openalex.org/W2987998469', 'https://openalex.org/W2994928925', 'https://openalex.org/W2996428491', 'https://openalex.org/W2996854111', 'https://openalex.org/W2997763445', 'https://openalex.org/W3035317912', 'https://openalex.org/W3102507836']","Large-scale pre-trained representations such as BERT have been widely used in many natural language understanding tasks. The methods of incorporating BERT into document-level machine translation are still being explored. BERT is able to understand sentence relationships since BERT is pre-trained using the next sentence prediction task. In our work, we leverage this property to improve document-level machine translation. In our proposed model, BERT performs as a context encoder to achieve document-level contextual information, which is then integrated into both the encoder and decoder. Experiment results show that our proposed method can significantly outperform strong document-level machine translation baselines on BLEU score. Moreover, the ablation study shows our method can capture document-level context information to boost translation performance.",1.0
SKG_MT_891,https://openalex.org/W2251742274,2013,32,"['https://openalex.org/W52051954', 'https://openalex.org/W171476473', 'https://openalex.org/W201141796', 'https://openalex.org/W222053410', 'https://openalex.org/W1493788606', 'https://openalex.org/W1550053614', 'https://openalex.org/W1631260214', 'https://openalex.org/W2006617528', 'https://openalex.org/W2042987798', 'https://openalex.org/W2098057009', 'https://openalex.org/W2098603082', 'https://openalex.org/W2100976324', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103593018', 'https://openalex.org/W2119727789', 'https://openalex.org/W2124807415', 'https://openalex.org/W2129124601', 'https://openalex.org/W2133512280', 'https://openalex.org/W2134800885', 'https://openalex.org/W2135161317', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147272182', 'https://openalex.org/W2156985047', 'https://openalex.org/W2169273645', 'https://openalex.org/W2170204377', 'https://openalex.org/W2250323038', 'https://openalex.org/W2736671181', 'https://openalex.org/W3170093816']","An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study. 1",1.0
SKG_MT_895,https://openalex.org/W2963088995,2016,732,"['https://openalex.org/W87617161', 'https://openalex.org/W581956982', 'https://openalex.org/W932413789', 'https://openalex.org/W1551202288', 'https://openalex.org/W1591801644', 'https://openalex.org/W1902237438', 'https://openalex.org/W1917432393', 'https://openalex.org/W1998582365', 'https://openalex.org/W2116261113', 'https://openalex.org/W2118434577', 'https://openalex.org/W2120861206', 'https://openalex.org/W2130942839', 'https://openalex.org/W2136848157', 'https://openalex.org/W2140343992', 'https://openalex.org/W2150355110', 'https://openalex.org/W2165666205', 'https://openalex.org/W2165698076', 'https://openalex.org/W2169724380', 'https://openalex.org/W2172166122', 'https://openalex.org/W2251743902', 'https://openalex.org/W2251994258', 'https://openalex.org/W2278264165', 'https://openalex.org/W2295582178', 'https://openalex.org/W2963084471', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963522845', 'https://openalex.org/W2964308564']","The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.",1.0
SKG_MT_896,https://openalex.org/W2787384361,2019,31,"['https://openalex.org/W1902237438', 'https://openalex.org/W2127863960', 'https://openalex.org/W2576482813', 'https://openalex.org/W2758567679', 'https://openalex.org/W2759088880', 'https://openalex.org/W2915926444', 'https://openalex.org/W2922349260', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963551569', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","In this year, we participated in four translation subtasks at WAT 2017. Our model structure is quite simple but we used it with well-tuned hyper-parameters, leading to a significant improvement compared to the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by back-translating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation.",1.0
SKG_MT_897,https://openalex.org/W2460423734,2016,51,"['https://openalex.org/W68132019', 'https://openalex.org/W205829674', 'https://openalex.org/W1699691160', 'https://openalex.org/W1756422141', 'https://openalex.org/W1942169943', 'https://openalex.org/W2022166150', 'https://openalex.org/W2029249040', 'https://openalex.org/W2081580037', 'https://openalex.org/W2094728533', 'https://openalex.org/W2097286355', 'https://openalex.org/W2101802482', 'https://openalex.org/W2101848544', 'https://openalex.org/W2127426251', 'https://openalex.org/W2127795553', 'https://openalex.org/W2138204945', 'https://openalex.org/W2184957013', 'https://openalex.org/W2247119764', 'https://openalex.org/W2250342289', 'https://openalex.org/W2250376704', 'https://openalex.org/W2250601658', 'https://openalex.org/W2283196293', 'https://openalex.org/W4300062145', 'https://openalex.org/W4386506836']","Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, Se-Young Park. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",1.0
SKG_MT_898,https://openalex.org/W3034771276,2020,26,"['https://openalex.org/W22168010', 'https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2183341477', 'https://openalex.org/W2250876691', 'https://openalex.org/W2251202280', 'https://openalex.org/W2467575451', 'https://openalex.org/W2525778437', 'https://openalex.org/W2581863816', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740553716', 'https://openalex.org/W2740718109', 'https://openalex.org/W2756978580', 'https://openalex.org/W2758685863', 'https://openalex.org/W2760452458', 'https://openalex.org/W2760705958', 'https://openalex.org/W2798931235', 'https://openalex.org/W2803739890', 'https://openalex.org/W2892244498', 'https://openalex.org/W2896457183', 'https://openalex.org/W2903533908', 'https://openalex.org/W2933138175', 'https://openalex.org/W2948874979', 'https://openalex.org/W2954647460', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970409072', 'https://openalex.org/W2990804422', 'https://openalex.org/W2994689640', 'https://openalex.org/W3005389111', 'https://openalex.org/W3041866211', 'https://openalex.org/W3089067043', 'https://openalex.org/W4297782088', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566', 'https://openalex.org/W6679436768', 'https://openalex.org/W6691454830', 'https://openalex.org/W6739901393', 'https://openalex.org/W6744595604', 'https://openalex.org/W6774062504', 'https://openalex.org/W6780039898', 'https://openalex.org/W6898505805']","Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",1.0
SKG_MT_900,https://openalex.org/W2251044602,2015,20,"['https://openalex.org/W1631260214', 'https://openalex.org/W1798130076', 'https://openalex.org/W1901714926', 'https://openalex.org/W2010856309', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110310640', 'https://openalex.org/W2113199498', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126704587', 'https://openalex.org/W2149327368', 'https://openalex.org/W2159755860', 'https://openalex.org/W2164788644', 'https://openalex.org/W2250872883', 'https://openalex.org/W2251171258', 'https://openalex.org/W2251347599', 'https://openalex.org/W2595715041', 'https://openalex.org/W2678635060', 'https://openalex.org/W2790246984', 'https://openalex.org/W2911033213', 'https://openalex.org/W3204081637']","We introduce pre-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing.We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, confirming the promising prospects of the novel translation scenarios offered by our approach.",1.0
SKG_MT_901,https://openalex.org/W2565643881,2016,1,"['https://openalex.org/W55636956', 'https://openalex.org/W331019419', 'https://openalex.org/W1510052640', 'https://openalex.org/W1575907248', 'https://openalex.org/W1631260214', 'https://openalex.org/W1632114991', 'https://openalex.org/W1632379906', 'https://openalex.org/W1990760950', 'https://openalex.org/W1999993003', 'https://openalex.org/W2015238682', 'https://openalex.org/W2018116550', 'https://openalex.org/W2052449326', 'https://openalex.org/W2095690342', 'https://openalex.org/W2096204319', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106751371', 'https://openalex.org/W2110104386', 'https://openalex.org/W2116410915', 'https://openalex.org/W2116957398', 'https://openalex.org/W2137854946', 'https://openalex.org/W2143995218', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2156985047', 'https://openalex.org/W2164415172', 'https://openalex.org/W2165666205', 'https://openalex.org/W2165698076', 'https://openalex.org/W2166905217', 'https://openalex.org/W2167706101', 'https://openalex.org/W2251537766', 'https://openalex.org/W2437005631', 'https://openalex.org/W2564674211', 'https://openalex.org/W2950415592', 'https://openalex.org/W3172849569', 'https://openalex.org/W3201784606', 'https://openalex.org/W4241645538', 'https://openalex.org/W4253646471']","Structural isomorphism between languages benefits the performance of cross-lingual applications.We propose an automatic algorithm for cross-lingual similarization of dependency grammars, which automatically learns grammars with high cross-lingual similarity.The algorithm similarizes the annotation styles of the dependency grammars for two languages in the level of classification decisions, and gradually improves the cross-lingual similarity without losing linguistic knowledge resorting to iterative crosslingual cooperative learning.The dependency grammars given by cross-lingual similarization have much higher cross-lingual similarity while maintaining non-triviality.As applications, the cross-lingually similarized grammars significantly improve the performance of dependency tree-based machine translation.",1.0
SKG_MT_902,https://openalex.org/W2919290281,2019,403,"['https://openalex.org/W1753482797', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2251743902', 'https://openalex.org/W2512924740', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2752630748', 'https://openalex.org/W2760656271', 'https://openalex.org/W2785847164', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798465082', 'https://openalex.org/W2807535859', 'https://openalex.org/W2809456172', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888456631', 'https://openalex.org/W2891844856', 'https://openalex.org/W2891924676', 'https://openalex.org/W2903193068', 'https://openalex.org/W2912095972', 'https://openalex.org/W2928941594', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962807144', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499433', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963826397', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963983698', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964308564', 'https://openalex.org/W2973088264', 'https://openalex.org/W3005389111', 'https://openalex.org/W4294389941', 'https://openalex.org/W4302343710', 'https://openalex.org/W4385245566']","Roee Aharoni, Melvin Johnson, Orhan Firat. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_904,https://openalex.org/W2970925677,2019,108,"['https://openalex.org/W1495697223', 'https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W1902237438', 'https://openalex.org/W1965933598', 'https://openalex.org/W1966961660', 'https://openalex.org/W2056540093', 'https://openalex.org/W2091966899', 'https://openalex.org/W2098707894', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2251743902', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2566957588', 'https://openalex.org/W2613904329', 'https://openalex.org/W2739967986', 'https://openalex.org/W2785389859', 'https://openalex.org/W2794365787', 'https://openalex.org/W2804044248', 'https://openalex.org/W2888456631', 'https://openalex.org/W2889606145', 'https://openalex.org/W2890964657', 'https://openalex.org/W2905933322', 'https://openalex.org/W2944815030', 'https://openalex.org/W2950577311', 'https://openalex.org/W2952468927', 'https://openalex.org/W2952650870', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532104', 'https://openalex.org/W2963826397', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2968447912', 'https://openalex.org/W4297747548', 'https://openalex.org/W4385245566']","Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, Tie-Yan Liu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_905,https://openalex.org/W2471679455,2016,38,"['https://openalex.org/W23077562', 'https://openalex.org/W179314280', 'https://openalex.org/W1966771059', 'https://openalex.org/W1979711143', 'https://openalex.org/W2034991102', 'https://openalex.org/W2035718809', 'https://openalex.org/W2058475745', 'https://openalex.org/W2098297786', 'https://openalex.org/W2102294561', 'https://openalex.org/W2118599227', 'https://openalex.org/W2123388068', 'https://openalex.org/W2127672659', 'https://openalex.org/W2140372282', 'https://openalex.org/W2144950812', 'https://openalex.org/W2153013403', 'https://openalex.org/W2155870214', 'https://openalex.org/W2158025800', 'https://openalex.org/W2168420336', 'https://openalex.org/W2170527467', 'https://openalex.org/W2175296493', 'https://openalex.org/W2251613956', 'https://openalex.org/W2251799008', 'https://openalex.org/W2251862950', 'https://openalex.org/W2315316408', 'https://openalex.org/W3167803720']","Research on grammatical error correction has received considerable attention. For dealing with all types of errors, grammatical error correction methods that employ statistical machine translation (SMT) have been proposed in recent years. An SMT system generates candidates with scores for all candidates and selects the sentence with the highest score as the correction result. However, the 1-best result of an SMT system is not always the best result. Thus, we propose a reranking approach for grammatical error correction. The reranking approach is used to re-score N-best results of the SMT and reorder the results. Our experiments show that our reranking system using parts of speech and syntactic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0.",1.0
SKG_MT_908,https://openalex.org/W3097426566,2020,0,"['https://openalex.org/W1522301498', 'https://openalex.org/W1904365287', 'https://openalex.org/W2788988616', 'https://openalex.org/W2902510077', 'https://openalex.org/W2908336025', 'https://openalex.org/W2950169745', 'https://openalex.org/W2952444318', 'https://openalex.org/W2952509486', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963807318', 'https://openalex.org/W2967985939', 'https://openalex.org/W2970752831', 'https://openalex.org/W2970947975', 'https://openalex.org/W3006610127', 'https://openalex.org/W3092327118']","This paper describes our VolcTrans system on WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer, with several variants (wider or deeper Transformers, dynamic convolutions). The final system includes text pre-process, data selection, synthetic data generation, advanced model ensemble, and multilingual pre-training.",1.0
SKG_MT_909,https://openalex.org/W2601324753,2018,162,"['https://openalex.org/W1489525520', 'https://openalex.org/W1753482797', 'https://openalex.org/W1836465849', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134298462', 'https://openalex.org/W2157331557', 'https://openalex.org/W2173520492', 'https://openalex.org/W2176263492', 'https://openalex.org/W2251912507', 'https://openalex.org/W2413332972', 'https://openalex.org/W2521028896', 'https://openalex.org/W2525778437', 'https://openalex.org/W2542835211', 'https://openalex.org/W2550821151', 'https://openalex.org/W2554423077', 'https://openalex.org/W2570431255', 'https://openalex.org/W2572549015', 'https://openalex.org/W2581637843', 'https://openalex.org/W2607987856', 'https://openalex.org/W2613904329', 'https://openalex.org/W2618625858', 'https://openalex.org/W2626778328', 'https://openalex.org/W2949117887', 'https://openalex.org/W2951184134', 'https://openalex.org/W2952745707', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963163972', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963684088', 'https://openalex.org/W2963729324', 'https://openalex.org/W2964268978', 'https://openalex.org/W2964308564', 'https://openalex.org/W4294149591', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394670483']","Zhen Yang, Wei Chen, Feng Wang, Bo Xu. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",1.0
SKG_MT_910,https://openalex.org/W2609193491,2017,65,"['https://openalex.org/W6908809', 'https://openalex.org/W38739846', 'https://openalex.org/W606036854', 'https://openalex.org/W831466375', 'https://openalex.org/W996287990', 'https://openalex.org/W1487926025', 'https://openalex.org/W1580265784', 'https://openalex.org/W1603598191', 'https://openalex.org/W1787338159', 'https://openalex.org/W1797402740', 'https://openalex.org/W1932968309', 'https://openalex.org/W1989713378', 'https://openalex.org/W2002722895', 'https://openalex.org/W2042061820', 'https://openalex.org/W2078861931', 'https://openalex.org/W2079398999', 'https://openalex.org/W2097726431', 'https://openalex.org/W2099653665', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102381086', 'https://openalex.org/W2116492146', 'https://openalex.org/W2122639307', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133341045', 'https://openalex.org/W2134800885', 'https://openalex.org/W2143927888', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157331557', 'https://openalex.org/W2159849140', 'https://openalex.org/W2164290393', 'https://openalex.org/W2165044314', 'https://openalex.org/W2250489604', 'https://openalex.org/W2250710744', 'https://openalex.org/W2251379416', 'https://openalex.org/W2251771443', 'https://openalex.org/W2251958472', 'https://openalex.org/W2263859238', 'https://openalex.org/W2595715041', 'https://openalex.org/W3000470214', 'https://openalex.org/W4241645538']","Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, ""Sarcasm is the giant chasm between what I say, and the person who doesn't get it."". In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN's interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new task.",0.9942196531791907
SKG_MT_911,https://openalex.org/W2739833967,2017,19,"['https://openalex.org/W22168010', 'https://openalex.org/W137989762', 'https://openalex.org/W201141796', 'https://openalex.org/W245306944', 'https://openalex.org/W1543107604', 'https://openalex.org/W1625582487', 'https://openalex.org/W1981287915', 'https://openalex.org/W2006617528', 'https://openalex.org/W2006832571', 'https://openalex.org/W2040573798', 'https://openalex.org/W2051303884', 'https://openalex.org/W2098603082', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119727789', 'https://openalex.org/W2124807415', 'https://openalex.org/W2147272182', 'https://openalex.org/W2166306133', 'https://openalex.org/W2250816155', 'https://openalex.org/W2251742274', 'https://openalex.org/W2362401466', 'https://openalex.org/W2460354904', 'https://openalex.org/W2550821151', 'https://openalex.org/W2595715041', 'https://openalex.org/W2736671181', 'https://openalex.org/W2790246984', 'https://openalex.org/W2951166594', 'https://openalex.org/W2951184134', 'https://openalex.org/W3204172911']","Nizar Habash, Nasser Zalmout, Dima Taji, Hieu Hoang, Maverick Alzate. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. 2017.",1.0
SKG_MT_913,https://openalex.org/W2250188786,2014,39,"['https://openalex.org/W91928571', 'https://openalex.org/W95780973', 'https://openalex.org/W108071511', 'https://openalex.org/W299621969', 'https://openalex.org/W1489525520', 'https://openalex.org/W2015333112', 'https://openalex.org/W2045738181', 'https://openalex.org/W2070150502', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111142112', 'https://openalex.org/W2112706073', 'https://openalex.org/W2127713198', 'https://openalex.org/W2133512280', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2180952760', 'https://openalex.org/W2251610689', 'https://openalex.org/W2331726854', 'https://openalex.org/W2552556052']","We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with five transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add other metrics from the ASIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level.",0.9927007299270073
SKG_MT_916,https://openalex.org/W2902614977,2018,119,"['https://openalex.org/W11511616', 'https://openalex.org/W22168010', 'https://openalex.org/W211509693', 'https://openalex.org/W338621447', 'https://openalex.org/W1522301498', 'https://openalex.org/W1916559533', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123442489', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2149811020', 'https://openalex.org/W2251367463', 'https://openalex.org/W2512848817', 'https://openalex.org/W2542835211', 'https://openalex.org/W2546938941', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561274697', 'https://openalex.org/W2580262340', 'https://openalex.org/W2594229957', 'https://openalex.org/W2597891111', 'https://openalex.org/W2604275745', 'https://openalex.org/W2756566411', 'https://openalex.org/W2758310181', 'https://openalex.org/W2759461255', 'https://openalex.org/W2765961751', 'https://openalex.org/W2797913374', 'https://openalex.org/W2950359962', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963708445', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W3211848854', 'https://openalex.org/W4294149591', 'https://openalex.org/W4298393544', 'https://openalex.org/W4307459710', 'https://openalex.org/W4320013936']","Neural Machine Translation (MT) has radically changed the way systems are\ndeveloped. A major difference with the previous generation (Phrase-Based MT) is\nthe way monolingual target data, which often abounds, is used in these two\nparadigms. While Phrase-Based MT can seamlessly integrate very large language\nmodels trained on billions of sentences, the best option for Neural MT\ndevelopers seems to be the generation of artificial parallel data through\n\\textsl{back-translation} - a technique that fails to fully take advantage of\nexisting datasets. In this paper, we conduct a systematic study of\nback-translation, comparing alternative uses of monolingual data, as well as\nmultiple data generation procedures. Our findings confirm that back-translation\nis very effective and give new explanations as to why this is the case. We also\nintroduce new data simulation techniques that are almost as effective, yet much\ncheaper to implement.\n",0.993006993006993
SKG_MT_918,https://openalex.org/W2982587176,2019,0,"['https://openalex.org/W1521413921', 'https://openalex.org/W1570031249', 'https://openalex.org/W1645937837', 'https://openalex.org/W2097828466', 'https://openalex.org/W2157812664', 'https://openalex.org/W2169200297', 'https://openalex.org/W2739046565', 'https://openalex.org/W2889326796', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962905474', 'https://openalex.org/W2963091658', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963592583', 'https://openalex.org/W2963807318', 'https://openalex.org/W2970295111', 'https://openalex.org/W2971134989', 'https://openalex.org/W2987188351']","Recently, neural models led to significant improvements in both machine translation (MT) and natural language generation tasks (NLG). However, generation of long descriptive summaries conditioned on structured data remains an open challenge. Likewise, MT that goes beyond sentence-level context is still an open issue (e.g., document-level MT or MT with metadata). To address these challenges, we propose to leverage data from both tasks and do transfer learning between MT, NLG, and MT with source-side metadata (MT+NLG). First, we train document-based MT systems with large amounts of parallel data. Then, we adapt these models to pure NLG and MT+NLG tasks by fine-tuning with smaller amounts of domain-specific data. This end-to-end NLG approach, without data selection and planning, outperforms the previous state of the art on the Rotowire NLG task. We participated to the ""Document Generation and Translation"" task at WNGT 2019, and ranked first in all tracks.",1.0
SKG_MT_919,https://openalex.org/W2150453030,2011,8,"['https://openalex.org/W7542544', 'https://openalex.org/W145738457', 'https://openalex.org/W166614460', 'https://openalex.org/W179875071', 'https://openalex.org/W222053410', 'https://openalex.org/W1483677181', 'https://openalex.org/W1491175367', 'https://openalex.org/W1508165687', 'https://openalex.org/W1537309404', 'https://openalex.org/W1551893515', 'https://openalex.org/W1554944419', 'https://openalex.org/W1573082642', 'https://openalex.org/W1597533204', 'https://openalex.org/W1631260214', 'https://openalex.org/W1644652583', 'https://openalex.org/W1649222155', 'https://openalex.org/W1674813824', 'https://openalex.org/W1768003599', 'https://openalex.org/W1880262756', 'https://openalex.org/W1934041838', 'https://openalex.org/W1974515274', 'https://openalex.org/W1978470410', 'https://openalex.org/W1984635093', 'https://openalex.org/W1989705153', 'https://openalex.org/W1991133427', 'https://openalex.org/W1991995555', 'https://openalex.org/W1996903695', 'https://openalex.org/W1997885189', 'https://openalex.org/W2012115736', 'https://openalex.org/W2017708149', 'https://openalex.org/W2018482254', 'https://openalex.org/W2020073413', 'https://openalex.org/W2022995612', 'https://openalex.org/W2032293014', 'https://openalex.org/W2038614136', 'https://openalex.org/W2040335964', 'https://openalex.org/W2045675590', 'https://openalex.org/W2047706513', 'https://openalex.org/W2053742104', 'https://openalex.org/W2056250865', 'https://openalex.org/W2066495029', 'https://openalex.org/W2068017609', 'https://openalex.org/W2079164875', 'https://openalex.org/W2087309226', 'https://openalex.org/W2096175520', 'https://openalex.org/W2096253869', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103018059', 'https://openalex.org/W2104210067', 'https://openalex.org/W2105891181', 'https://openalex.org/W2109664771', 'https://openalex.org/W2111305191', 'https://openalex.org/W2114858359', 'https://openalex.org/W2116316001', 'https://openalex.org/W2117444496', 'https://openalex.org/W2118714763', 'https://openalex.org/W2122410182', 'https://openalex.org/W2123893795', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126399065', 'https://openalex.org/W2132339004', 'https://openalex.org/W2134731454', 'https://openalex.org/W2142763781', 'https://openalex.org/W2145833060', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148603752', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154099718', 'https://openalex.org/W2154124206', 'https://openalex.org/W2155693943', 'https://openalex.org/W2159863202', 'https://openalex.org/W2160645305', 'https://openalex.org/W2160842254', 'https://openalex.org/W2173213060', 'https://openalex.org/W2437005631', 'https://openalex.org/W2621752948', 'https://openalex.org/W2949237929', 'https://openalex.org/W2950186769', 'https://openalex.org/W3021713638', 'https://openalex.org/W3129711340', 'https://openalex.org/W3198494294']","This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the BLEU score and “readability ” when applied to the task of re-ranking the N-best list from a state-of-theart parsing-based machine translation system. 1",1.0
SKG_MT_920,https://openalex.org/W2798474427,2018,28,"['https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2271840356', 'https://openalex.org/W2400065810', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740240682', 'https://openalex.org/W2778814079', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964335273', 'https://openalex.org/W3082674894']","Attention-based neural machine translation (NMT) models selectively focus on specific source positions to produce a translation, which brings significant improvements over pure encoder-decoder sequence-to-sequence models. This work investigates NMT while replacing the attention component. We study a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models, which are trained jointly using the forward-backward algorithm. We show that the attention component can be effectively replaced by the neural network alignment model and the neural HMM approach is able to provide comparable performance with the state-of-the-art attention-based models on the WMT 2017 German↔English and Chinese→English translation tasks.",1.0
SKG_MT_921,https://openalex.org/W2971254483,2019,25,"['https://openalex.org/W2124807415', 'https://openalex.org/W2145094598', 'https://openalex.org/W2159755860', 'https://openalex.org/W2546938941', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740743644', 'https://openalex.org/W2744813330', 'https://openalex.org/W2756978580', 'https://openalex.org/W2788353357', 'https://openalex.org/W2805394970', 'https://openalex.org/W2886288106', 'https://openalex.org/W2890007195', 'https://openalex.org/W2898785264', 'https://openalex.org/W2903193068', 'https://openalex.org/W2914120296', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963329925', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964343359', 'https://openalex.org/W2970279348', 'https://openalex.org/W2997574889', 'https://openalex.org/W3204406378', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","Benjamin Marie, Haipeng Sun, Rui Wang, Kehai Chen, Atsushi Fujita, Masao Utiyama, Eiichiro Sumita. Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1). 2019.",1.0
SKG_MT_922,https://openalex.org/W2126712675,2013,32,"['https://openalex.org/W67353944', 'https://openalex.org/W222053410', 'https://openalex.org/W832270446', 'https://openalex.org/W1489834179', 'https://openalex.org/W1631260214', 'https://openalex.org/W1647671624', 'https://openalex.org/W1934041838', 'https://openalex.org/W1982426321', 'https://openalex.org/W1984566630', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113199498', 'https://openalex.org/W2113641473', 'https://openalex.org/W2121404172', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131969445', 'https://openalex.org/W2135546538', 'https://openalex.org/W2145790651', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2187442798', 'https://openalex.org/W2187953100', 'https://openalex.org/W2434153183', 'https://openalex.org/W3197274356', 'https://openalex.org/W3201792308']","Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associat-ed with each phrase at SMT decoding. On a Chinese–English TM database, our experi-ments show that the proposed integrated Mod-el-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Be-sides, the proposed models also outperform previous approaches significantly. 1",1.0
SKG_MT_924,https://openalex.org/W2757592053,2017,123,"['https://openalex.org/W46679369', 'https://openalex.org/W1483313504', 'https://openalex.org/W1987869189', 'https://openalex.org/W1989658336', 'https://openalex.org/W2101105183', 'https://openalex.org/W2132001515', 'https://openalex.org/W2133564696', 'https://openalex.org/W2137387514', 'https://openalex.org/W2141440284', 'https://openalex.org/W2143177362', 'https://openalex.org/W2146502635', 'https://openalex.org/W2148708890', 'https://openalex.org/W2157331557', 'https://openalex.org/W2159755860', 'https://openalex.org/W2356613612', 'https://openalex.org/W2394860946', 'https://openalex.org/W2522770641', 'https://openalex.org/W2557283755', 'https://openalex.org/W2595715041', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963841178', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","We investigate the application of Neural Machine Translation (NMT) under the following three conditions posed by realworld application scenarios.First, we operate with an input stream of sentences coming from many different domains and with no predefined order.Second, the sentences are presented without domain information.Third, the input stream should be processed by a single generic NMT model.To tackle the weaknesses of current NMT technology in this unsupervised multi-domain setting, we explore an efficient instance-based adaptation method that, by exploiting the similarity between the training instances and each test sentence, dynamically sets the hyperparameters of the learning algorithm and updates the generic model on-the-fly.The results of our experiments with multi-domain data show that local adaptation outperforms not only the original generic NMT system, but also a strong phrase-based system and even single-domain NMT models specifically optimized on each domain and applicable only by violating two of our aforementioned assumptions.",1.0
SKG_MT_925,https://openalex.org/W2806311723,2018,80,"['https://openalex.org/W1518951372', 'https://openalex.org/W1825672851', 'https://openalex.org/W2095705004', 'https://openalex.org/W2124807415', 'https://openalex.org/W2149428536', 'https://openalex.org/W2162390675', 'https://openalex.org/W2168231600', 'https://openalex.org/W2399880602', 'https://openalex.org/W2606974598', 'https://openalex.org/W2612675303', 'https://openalex.org/W2613904329', 'https://openalex.org/W2622263826', 'https://openalex.org/W2626778328', 'https://openalex.org/W2763421725', 'https://openalex.org/W2767989436', 'https://openalex.org/W2773493195', 'https://openalex.org/W2789541106', 'https://openalex.org/W2790319220', 'https://openalex.org/W2791110811', 'https://openalex.org/W2797162333', 'https://openalex.org/W2896060389', 'https://openalex.org/W2949650786', 'https://openalex.org/W2952754453', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963702144', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564']","Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. 1 On WMT'14 English-German translation, we match the accuracy of Vaswani et al. ( We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",1.0
SKG_MT_931,https://openalex.org/W2119202242,2014,89,"['https://openalex.org/W12911848', 'https://openalex.org/W78623039', 'https://openalex.org/W128271544', 'https://openalex.org/W170711724', 'https://openalex.org/W1551794154', 'https://openalex.org/W1628767011', 'https://openalex.org/W1893161951', 'https://openalex.org/W1968084137', 'https://openalex.org/W2075381214', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107440414', 'https://openalex.org/W2110388853', 'https://openalex.org/W2114879013', 'https://openalex.org/W2117717100', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2135161317', 'https://openalex.org/W2136657878', 'https://openalex.org/W2138477841', 'https://openalex.org/W2139604620', 'https://openalex.org/W2146853049', 'https://openalex.org/W2150028966', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157136552', 'https://openalex.org/W2159755860', 'https://openalex.org/W2161227214', 'https://openalex.org/W2163069610', 'https://openalex.org/W2166202273', 'https://openalex.org/W2170096403', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250793660', 'https://openalex.org/W2250947781', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2293596968', 'https://openalex.org/W2595715041', 'https://openalex.org/W2917073205']","We investigate three methods for integrating an unsupervised transliteration model into an end-to-end SMT system.We induce a transliteration model from parallel data and use it to translate OOV words.Our approach is fully unsupervised and language independent.In the methods to integrate transliterations, we observed improvements from 0.23-0.75(∆ 0.41) BLEU points across 7 language pairs.We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora.",1.0
SKG_MT_933,https://openalex.org/W2963777589,2019,56,"['https://openalex.org/W1673923490', 'https://openalex.org/W1902237438', 'https://openalex.org/W2096384330', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2187089797', 'https://openalex.org/W2613904329', 'https://openalex.org/W2746176496', 'https://openalex.org/W2767899794', 'https://openalex.org/W2807253849', 'https://openalex.org/W2886248719', 'https://openalex.org/W2896234185', 'https://openalex.org/W2898989428', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963661177', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964153729', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3101868899', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566']","Neural machine translation (NMT) is notoriously sensitive to noises, but noises are almost inevitable in practice. One special kind of noise is the homophone noise, where words are replaced by other words with similar pronunciations. We propose to improve the robustness of NMT to homophone noises by 1) jointly embedding both textual and phonetic information of source sentences, and 2) augmenting the training dataset with homophone noises. Interestingly, to achieve better translation quality and more robustness, we found that most (though not all) weights should be put on the phonetic rather than textual information. Experiments show that our method not only significantly improves the robustness of NMT to homophone noises, but also surprisingly improves the translation quality on some clean test sets.",1.0
SKG_MT_934,https://openalex.org/W2735139879,2017,0,"['https://openalex.org/W1522301498', 'https://openalex.org/W1978630505', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103078213', 'https://openalex.org/W2110006374', 'https://openalex.org/W2130942839', 'https://openalex.org/W2289260142', 'https://openalex.org/W2549416390', 'https://openalex.org/W2756888147', 'https://openalex.org/W2949810612', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963347649', 'https://openalex.org/W2964308564']","This paper describes LIUM submissions to WMT17 News Translation Task for English-German, English-Turkish, English-Czech and English-Latvian language pairs. We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework. Competitive scores were obtained by ensembling various systems and exploiting the availability of target monolingual corpora for back-translation. The impact of back-translation quantity and quality is also analyzed for English-Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.",1.0
SKG_MT_935,https://openalex.org/W2983991354,2019,6,"['https://openalex.org/W1902237438', 'https://openalex.org/W2093790824', 'https://openalex.org/W2133564696', 'https://openalex.org/W2346934282', 'https://openalex.org/W2758172851', 'https://openalex.org/W2787384361', 'https://openalex.org/W2904817833', 'https://openalex.org/W2915926444', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2964308564', 'https://openalex.org/W2989027560']","This paper describes the UCSYNLP-Lab submission to WAT 2019 for Myanmar-English translation tasks in both direction. We have used the neural machine translation systems with attention model and utilized the UCSY-corpus and ALT corpus. In NMT with attention model, we use the word segmentation level as well as syllable segmentation level. Especially, we made the UCSY-corpus to be cleaned in WAT 2019. Therefore, the UCSY corpus for WAT 2019 is not identical to those used in WAT 2018. Experiments show that the translation systems can produce the substantial improvements.",1.0
SKG_MT_937,https://openalex.org/W3116748060,2020,4,"['https://openalex.org/W1902237438', 'https://openalex.org/W2095705004', 'https://openalex.org/W2130942839', 'https://openalex.org/W2194775991', 'https://openalex.org/W2509282593', 'https://openalex.org/W2553522418', 'https://openalex.org/W2581101319', 'https://openalex.org/W2593341061', 'https://openalex.org/W2803237843', 'https://openalex.org/W2889326796', 'https://openalex.org/W2954996726', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963407669', 'https://openalex.org/W2963459241', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964192290', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2994928925', 'https://openalex.org/W3098341425', 'https://openalex.org/W3098507616']","We introduce our TMU system submitted to the Japanese English Multimodal Task (constrained) for WAT 2020 (Nakazawa et al., 2020). This task aims to improve translation performance with the help of another modality (images) associated with the input sentences. In a multimodal translation task, the dataset is, by its nature, a low-resource one. Our method used herein augments the data by generating noisy translations and adding noise to existing training images. Subsequently, we pretrain a translation model on the augmented noisy data, and then fine-tune it on the clean data. We also examine the probabilistic dropping of either the textual or visual context vector in the decoder. This aims to regularize the network to make use of both features while training. The experimental results indicate that translation performance can be improved using our method of textual data augmentation with noising on the target side and probabilistic dropping of either context vector.",1.0
SKG_MT_939,https://openalex.org/W2739664242,2017,6,"['https://openalex.org/W108460122', 'https://openalex.org/W342285082', 'https://openalex.org/W1532624895', 'https://openalex.org/W1542713999', 'https://openalex.org/W1614298861', 'https://openalex.org/W1828724394', 'https://openalex.org/W2098940361', 'https://openalex.org/W2126725946', 'https://openalex.org/W2138575863', 'https://openalex.org/W2139812240', 'https://openalex.org/W2155603234', 'https://openalex.org/W2250229103', 'https://openalex.org/W2250642041', 'https://openalex.org/W2251765408', 'https://openalex.org/W2252212383', 'https://openalex.org/W2406688488', 'https://openalex.org/W2572114121', 'https://openalex.org/W2950577311', 'https://openalex.org/W2952037945', 'https://openalex.org/W2962795068', 'https://openalex.org/W2963002901', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963915291', 'https://openalex.org/W3203627005']",We investigate the reranking of the output of several distributional approaches on the Bilingual Lexicon Induction task. We show that reranking an n-best list produced by any of those approaches leads to very substantial improvements. We further demonstrate that combining several n-best lists by reranking is an effective way of further boosting performance.,1.0
SKG_MT_940,https://openalex.org/W2252213522,2013,6,"['https://openalex.org/W91928571', 'https://openalex.org/W298172948', 'https://openalex.org/W1594031697', 'https://openalex.org/W1678356000', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104327354', 'https://openalex.org/W2111142112', 'https://openalex.org/W2140343992', 'https://openalex.org/W2141852716', 'https://openalex.org/W2143331230', 'https://openalex.org/W2154124206', 'https://openalex.org/W2159755860', 'https://openalex.org/W2162059449', 'https://openalex.org/W2172268343', 'https://openalex.org/W2180952760', 'https://openalex.org/W2767905780', 'https://openalex.org/W3085162807']","In this paper we show how to automatically induce non-linear features for machine translation. The new features are selected to approximately maximize a BLEU-related objective and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase. We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 1",1.0
SKG_MT_941,https://openalex.org/W2806311723,2018,80,"['https://openalex.org/W1518951372', 'https://openalex.org/W1825672851', 'https://openalex.org/W2095705004', 'https://openalex.org/W2124807415', 'https://openalex.org/W2149428536', 'https://openalex.org/W2162390675', 'https://openalex.org/W2168231600', 'https://openalex.org/W2399880602', 'https://openalex.org/W2606974598', 'https://openalex.org/W2612675303', 'https://openalex.org/W2613904329', 'https://openalex.org/W2622263826', 'https://openalex.org/W2626778328', 'https://openalex.org/W2763421725', 'https://openalex.org/W2767989436', 'https://openalex.org/W2773493195', 'https://openalex.org/W2789541106', 'https://openalex.org/W2790319220', 'https://openalex.org/W2791110811', 'https://openalex.org/W2797162333', 'https://openalex.org/W2896060389', 'https://openalex.org/W2949650786', 'https://openalex.org/W2952754453', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963702144', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564']","Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. 1 On WMT'14 English-German translation, we match the accuracy of Vaswani et al. ( We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",1.0
SKG_MT_943,https://openalex.org/W3093907478,2020,2,"['https://openalex.org/W2101105183', 'https://openalex.org/W2149327368', 'https://openalex.org/W2251367463', 'https://openalex.org/W2512848817', 'https://openalex.org/W2890007195', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962911098', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964121744', 'https://openalex.org/W3104652516']","This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between German and Upper Sorbian. We experimented with training on synthetic data and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on transfer learning from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.",1.0
SKG_MT_945,https://openalex.org/W2139635790,2012,3,"['https://openalex.org/W1965660534', 'https://openalex.org/W2062250042', 'https://openalex.org/W2093710484', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114550122', 'https://openalex.org/W2121404172', 'https://openalex.org/W2122373020', 'https://openalex.org/W2126270798', 'https://openalex.org/W2126784652', 'https://openalex.org/W2131932654', 'https://openalex.org/W2152263452', 'https://openalex.org/W2437005631']","Shallow-n grammars (de Gispert et al., 2010) were introduced to reduce over-generation in the Hiero translation model (Chiang, 2005) resulting in much faster decoding and restricting reordering to a desired level for specific language pairs. However, Shallow-n grammars require parameters which cannot be directly optimized using minimum error-rate tuning by the decoder. This paper introduces some novel improvements to the translation model for Shallow-n grammars. We introduce two rules: a BITG-style reordering glue rule and a simpler monotonic concatenation rule. We use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights. We show this formulation of Shallow-n hierarchical phrasebased translation is comparable in translation quality to full Hiero-style decoding (without shallow rules) while at the same time being considerably faster. 1",1.0
SKG_MT_948,https://openalex.org/W3106403484,2020,3,"['https://openalex.org/W1902237438', 'https://openalex.org/W1938755728', 'https://openalex.org/W2130942839', 'https://openalex.org/W2220350356', 'https://openalex.org/W2463895987', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888541716', 'https://openalex.org/W2912095972', 'https://openalex.org/W2919290281', 'https://openalex.org/W2922349260', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949973181', 'https://openalex.org/W2950733326', 'https://openalex.org/W2951065878', 'https://openalex.org/W2951559648', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962732637', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963547384', 'https://openalex.org/W2963831310', 'https://openalex.org/W2964085268', 'https://openalex.org/W3127887696', 'https://openalex.org/W4385245566']","To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate into the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.",1.0
SKG_MT_949,https://openalex.org/W3035019713,2020,73,"['https://openalex.org/W47568227', 'https://openalex.org/W2033593667', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124659975', 'https://openalex.org/W2148861942', 'https://openalex.org/W2161044106', 'https://openalex.org/W2251033195', 'https://openalex.org/W2251743902', 'https://openalex.org/W2550821151', 'https://openalex.org/W2566957588', 'https://openalex.org/W2604763608', 'https://openalex.org/W2752630748', 'https://openalex.org/W2767434619', 'https://openalex.org/W2795282075', 'https://openalex.org/W2798465082', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888541716', 'https://openalex.org/W2889191148', 'https://openalex.org/W2891924676', 'https://openalex.org/W2912095972', 'https://openalex.org/W2919290281', 'https://openalex.org/W2933138175', 'https://openalex.org/W2950733326', 'https://openalex.org/W2951065878', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962743139', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963371670', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963577254', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963677766', 'https://openalex.org/W2963729324', 'https://openalex.org/W2963826397', 'https://openalex.org/W2963831310', 'https://openalex.org/W2964084097', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964090065', 'https://openalex.org/W2970854433', 'https://openalex.org/W2979636403', 'https://openalex.org/W2983040767', 'https://openalex.org/W2989650185', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035608860', 'https://openalex.org/W4230563027', 'https://openalex.org/W4288400010', 'https://openalex.org/W4288555568', 'https://openalex.org/W4293402910', 'https://openalex.org/W4385245566']","When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.",1.0
SKG_MT_951,https://openalex.org/W2985204668,2019,21,"['https://openalex.org/W1915251500', 'https://openalex.org/W2124807415', 'https://openalex.org/W2512924740', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561274697', 'https://openalex.org/W2595715041', 'https://openalex.org/W2604275745', 'https://openalex.org/W2756566411', 'https://openalex.org/W2778814079', 'https://openalex.org/W2794365787', 'https://openalex.org/W2886095922', 'https://openalex.org/W2888456631', 'https://openalex.org/W2889326796', 'https://openalex.org/W2899490399', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921280978', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964007535', 'https://openalex.org/W3019421297', 'https://openalex.org/W3082674894', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","Zero-shot neural machine translation (NMT) is a framework that uses source-pivot and target-pivot parallel data to train a source-target NMT system. An extension to zero-shot NMT is zero-resource NMT, which generates pseudo-parallel corpora using a zero-shot system and further trains the zero-shot system on that data. In this paper, we expand on zero-resource NMT by incorporating monolingual data in the pivot language into training; since the pivot language is usually the highest-resource language of the three, we expect monolingual pivot-language data to be most abundant. We propose methods for generating pseudo-parallel corpora using pivot-language monolingual data and for leveraging the pseudo-parallel corpora to improve the zero-shot NMT system. We evaluate these methods for a high-resource language pair (German-Russian) using English as the pivot. We show that our proposed methods yield consistent improvements over strong zero-shot and zero-resource baselines and even catch up to pivot-based models in BLEU (while not requiring the two-pass inference that pivot models require).",1.0
SKG_MT_952,https://openalex.org/W2962712961,2018,257,"['https://openalex.org/W44695385', 'https://openalex.org/W1851962382', 'https://openalex.org/W2100664567', 'https://openalex.org/W2130942839', 'https://openalex.org/W2136353104', 'https://openalex.org/W2184135559', 'https://openalex.org/W2194775991', 'https://openalex.org/W2250761393', 'https://openalex.org/W2399346130', 'https://openalex.org/W2525778437', 'https://openalex.org/W2608029998', 'https://openalex.org/W2613904329', 'https://openalex.org/W2669742347', 'https://openalex.org/W2767019613', 'https://openalex.org/W2774875515', 'https://openalex.org/W2799051177', 'https://openalex.org/W2799253188', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963084773', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964289193', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.",1.0
SKG_MT_953,https://openalex.org/W2899207798,2019,8,[],"In this study, we first investigate a novel capsule network with dynamic\nrouting for linear time Neural Machine Translation (NMT), referred as\n\\textsc{CapsNMT}. \\textsc{CapsNMT} uses an aggregation mechanism to map the\nsource sentence into a matrix with pre-determined size, and then applys a deep\nLSTM network to decode the target sequence from the source representation.\nUnlike the previous work \\cite{sutskever2014sequence} to store the source\nsentence with a passive and bottom-up way, the dynamic routing policy encodes\nthe source sentence with an iterative process to decide the credit attribution\nbetween nodes from lower and higher layers. \\textsc{CapsNMT} has two core\nproperties: it runs in time that is linear in the length of the sequences and\nprovides a more flexible way to select, represent and aggregates the part-whole\ninformation of the source sentence. On WMT14 English-German task and a larger\nWMT14 English-French task, \\textsc{CapsNMT} achieves comparable results with\nthe state-of-the-art NMT systems. To the best of our knowledge, this is the\nfirst work that capsule networks have been empirically investigated for\nsequence to sequence problems.\n",1.0
SKG_MT_955,https://openalex.org/W2251416744,2013,1,"['https://openalex.org/W4629839', 'https://openalex.org/W35711202', 'https://openalex.org/W42820592', 'https://openalex.org/W201532657', 'https://openalex.org/W222053410', 'https://openalex.org/W1605282883', 'https://openalex.org/W1631260214', 'https://openalex.org/W1972640883', 'https://openalex.org/W1978394996', 'https://openalex.org/W2006969979', 'https://openalex.org/W2017802499', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112900913', 'https://openalex.org/W2113788796', 'https://openalex.org/W2119168550', 'https://openalex.org/W2135161317', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2140110038', 'https://openalex.org/W2143134347', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2154124206', 'https://openalex.org/W2154581043', 'https://openalex.org/W2165666205', 'https://openalex.org/W2169954957', 'https://openalex.org/W2170464899', 'https://openalex.org/W2319213760', 'https://openalex.org/W2342339894', 'https://openalex.org/W3204394734']","Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the atomic translation unit to alleviate data spareness. In addition, we associate each stemgranularity translation rule with a distribution of related affixes, and select desirable rules according to the similarity of their affix distributions with given spans to be translated. Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese. 1",1.0
SKG_MT_959,https://openalex.org/W2251743902,2015,612,"['https://openalex.org/W6908809', 'https://openalex.org/W40379961', 'https://openalex.org/W1498238796', 'https://openalex.org/W1606347560', 'https://openalex.org/W1753482797', 'https://openalex.org/W2003304759', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111666304', 'https://openalex.org/W2130903752', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2133622676', 'https://openalex.org/W2136016850', 'https://openalex.org/W2158899491', 'https://openalex.org/W2170204377', 'https://openalex.org/W2172140247', 'https://openalex.org/W2250445771', 'https://openalex.org/W2250644439', 'https://openalex.org/W2250747734', 'https://openalex.org/W2251682575', 'https://openalex.org/W2952230511', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W3028642772']","Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, Haifeng Wang. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.",1.0
SKG_MT_960,https://openalex.org/W2963699608,2016,131,"['https://openalex.org/W6908809', 'https://openalex.org/W1498238796', 'https://openalex.org/W1902237438', 'https://openalex.org/W2100664567', 'https://openalex.org/W2117238933', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145033586', 'https://openalex.org/W2148708890', 'https://openalex.org/W2162245945', 'https://openalex.org/W2180952760', 'https://openalex.org/W2239731672', 'https://openalex.org/W2251299219', 'https://openalex.org/W2291126447', 'https://openalex.org/W2561274697', 'https://openalex.org/W2577335011', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963598809', 'https://openalex.org/W2963937700', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564']","In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT.For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes.Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.",1.0
SKG_MT_962,https://openalex.org/W2102394389,2011,139,"['https://openalex.org/W50534376', 'https://openalex.org/W1482214997', 'https://openalex.org/W1525595230', 'https://openalex.org/W1975809876', 'https://openalex.org/W2006969979', 'https://openalex.org/W2035363410', 'https://openalex.org/W2056260421', 'https://openalex.org/W2062270497', 'https://openalex.org/W2065240770', 'https://openalex.org/W2082718666', 'https://openalex.org/W2093390569', 'https://openalex.org/W2119168550', 'https://openalex.org/W2135739396', 'https://openalex.org/W2136542423', 'https://openalex.org/W2147308966', 'https://openalex.org/W2151429169', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2162355876', 'https://openalex.org/W2165612380', 'https://openalex.org/W2170120409']","Community-based question answer (Q&amp;amp;A) has become an important issue due to the popularity of Q&amp;amp;A archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&amp;amp;A archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&amp;amp;A data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model. 1",1.0
SKG_MT_963,https://openalex.org/W3091881170,2020,1,"['https://openalex.org/W1602395960', 'https://openalex.org/W1968625547', 'https://openalex.org/W2144600658', 'https://openalex.org/W2251322177', 'https://openalex.org/W2530647954', 'https://openalex.org/W2757222607', 'https://openalex.org/W2910092637', 'https://openalex.org/W2922349260', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963267799', 'https://openalex.org/W2963351448', 'https://openalex.org/W2963382396', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964085268', 'https://openalex.org/W2964335437', 'https://openalex.org/W2970206392', 'https://openalex.org/W2986378306', 'https://openalex.org/W2994928925', 'https://openalex.org/W2995197345', 'https://openalex.org/W2996403597', 'https://openalex.org/W3005389111', 'https://openalex.org/W3035512170', 'https://openalex.org/W3035618147', 'https://openalex.org/W3041866211']","State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation (MT) datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.",1.0
SKG_MT_964,https://openalex.org/W2120360483,2011,4,"['https://openalex.org/W1631260214', 'https://openalex.org/W2101105183', 'https://openalex.org/W2125536435', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2161227214', 'https://openalex.org/W2164766438', 'https://openalex.org/W2181577198', 'https://openalex.org/W2437005631', 'https://openalex.org/W2571124179', 'https://openalex.org/W2950186769']","This paper presents the system we developed for the 2011 WMT Haitian Creole–English SMS featured translation task. Applying stan-dard statistical machine translation methods to noisy real-world SMS data in a low-density language setting such as Haitian Creole poses a unique set of challenges, which we attempt to address in this work. Along with techniques to better exploit the limited available train-ing data, we explore the benefits of several methods for alleviating the additional noise inherent in the SMS and transforming it to better suite the assumptions of our hierarchi-cal phrase-based model system. We show that these methods lead to significant improve-ments in BLEU score over the baseline. 1",1.0
SKG_MT_965,https://openalex.org/W3106185885,2020,18,"['https://openalex.org/W1522301498', 'https://openalex.org/W1816313093', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2172589779', 'https://openalex.org/W2251743902', 'https://openalex.org/W2437005631', 'https://openalex.org/W2561274697', 'https://openalex.org/W2752047430', 'https://openalex.org/W2767206889', 'https://openalex.org/W2769298630', 'https://openalex.org/W2896457183', 'https://openalex.org/W2949745489', 'https://openalex.org/W2950855294', 'https://openalex.org/W2962926939', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963311117', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963699608', 'https://openalex.org/W2964093309', 'https://openalex.org/W2970045405', 'https://openalex.org/W2970287057', 'https://openalex.org/W2970798168', 'https://openalex.org/W2970849705', 'https://openalex.org/W2996766022', 'https://openalex.org/W2998655810', 'https://openalex.org/W3034742481', 'https://openalex.org/W3127825258']","Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.",1.0
SKG_MT_966,https://openalex.org/W2252106864,2011,17,"['https://openalex.org/W21337280', 'https://openalex.org/W22168010', 'https://openalex.org/W1514971736', 'https://openalex.org/W1716250762', 'https://openalex.org/W1982816048', 'https://openalex.org/W2031535158', 'https://openalex.org/W2100271871', 'https://openalex.org/W2140671896', 'https://openalex.org/W2154124206', 'https://openalex.org/W2567948266']","State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework, where the knowledge of a human translator is combined with the MT system. We present a statistical IMT system able to learn from user feedback by means of the application of online learning techniques. These techniques allow the MT system to update the parameters of the underlying models in real time. According to empirical results, our system outperforms the results of conventional IMT systems. To the best of our knowledge, this online learning capability has never been provided by previous IMT systems. Our IMT system is implemented in C++, JavaScript, and ActionScript; and is publicly available on the Web. 1",1.0
SKG_MT_967,https://openalex.org/W3101706601,2020,5,"['https://openalex.org/W1821462560', 'https://openalex.org/W2099471712', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108207895', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2183341477', 'https://openalex.org/W2294370754', 'https://openalex.org/W2402040300', 'https://openalex.org/W2546938941', 'https://openalex.org/W2581377246', 'https://openalex.org/W2610245951', 'https://openalex.org/W2613904329', 'https://openalex.org/W2620998106', 'https://openalex.org/W2795867901', 'https://openalex.org/W2889326796', 'https://openalex.org/W2906987001', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936252403', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962894366', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964190861', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970520743', 'https://openalex.org/W3095794553', 'https://openalex.org/W3115333065', 'https://openalex.org/W4255975151', 'https://openalex.org/W4297798436', 'https://openalex.org/W4320013936', 'https://openalex.org/W4385245566']","Mutual learning, where multiple agents learn collaboratively and teach one another, has been shown to be an effective way to distill knowledge for image classification tasks. In this paper, we extend mutual learning to the machine translation task and operate at both the sentence-level and the token-level. Firstly, we co-train multiple agents by using the same parallel corpora. After convergence, each agent selects and learns its poorly predicted tokens from other agents. The poorly predicted tokens are determined by the acceptance-rejection sampling algorithm. Our experiments show that sequential mutual learning at the sentence-level and the token-level improves the results cumulatively. Absolute improvements compared to strong baselines are obtained on various translation tasks. On the IWSLT'14 German-English task, we get a new state-of-the-art BLEU score of 37.0. We also report a competitive result, 29.9 BLEU score, on the WMT'14 English-German task.",1.0
SKG_MT_968,https://openalex.org/W141243353,2010,7,"['https://openalex.org/W132913264', 'https://openalex.org/W1494864219', 'https://openalex.org/W1631260214', 'https://openalex.org/W1970689298', 'https://openalex.org/W1973152633', 'https://openalex.org/W2006969979', 'https://openalex.org/W2061528982', 'https://openalex.org/W2080373976', 'https://openalex.org/W2101456909', 'https://openalex.org/W2115081467', 'https://openalex.org/W2116337133', 'https://openalex.org/W2124807415', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047']","This paper describes the development of French–English and English–French ma-chine translation systems for the 2010 WMT shared task evaluation. These sys-tems were standard phrase-based statisti-cal systems based on the Moses decoder, trained on the provided data only. Most of our efforts were devoted to the choice and extraction of bilingual data used for training. We filtered out some bilingual corpora and pruned the phrase table. We also investigated the impact of adding two types of additional bilingual texts, ex-tracted automatically from the available monolingual data. We first collected bilin-gual data by performing automatic trans-lations of monolingual texts. The second type of bilingual text was harvested from comparable corpora with Information Re-trieval techniques. 1",1.0
SKG_MT_970,https://openalex.org/W2930211575,2019,7,"['https://openalex.org/W648786980', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146502635', 'https://openalex.org/W2147880316', 'https://openalex.org/W2176263492', 'https://openalex.org/W2268617045', 'https://openalex.org/W2574872930', 'https://openalex.org/W2613904329', 'https://openalex.org/W2757222607', 'https://openalex.org/W2896234185', 'https://openalex.org/W2949641365', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962934837', 'https://openalex.org/W2963084599', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963317148', 'https://openalex.org/W2963382396', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963620441', 'https://openalex.org/W2963665552', 'https://openalex.org/W2963950336', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3041866211', 'https://openalex.org/W4385245566']","Mingbo Ma, Renjie Zheng, Liang Huang. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_971,https://openalex.org/W2250210685,2015,4,"['https://openalex.org/W71795751', 'https://openalex.org/W75781568', 'https://openalex.org/W165935821', 'https://openalex.org/W577284227', 'https://openalex.org/W1565992329', 'https://openalex.org/W1614298861', 'https://openalex.org/W1631260214', 'https://openalex.org/W1828724394', 'https://openalex.org/W1857937273', 'https://openalex.org/W2001064229', 'https://openalex.org/W2100281225', 'https://openalex.org/W2100565866', 'https://openalex.org/W2107038437', 'https://openalex.org/W2109169419', 'https://openalex.org/W2111742432', 'https://openalex.org/W2120615054', 'https://openalex.org/W2121338597', 'https://openalex.org/W2123126659', 'https://openalex.org/W2126725946', 'https://openalex.org/W2137607259', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153579005', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158139315', 'https://openalex.org/W2158199200', 'https://openalex.org/W2158953777', 'https://openalex.org/W2168966090', 'https://openalex.org/W2250550978', 'https://openalex.org/W2252177599', 'https://openalex.org/W2437005631', 'https://openalex.org/W2950577311', 'https://openalex.org/W2952037945', 'https://openalex.org/W4241645538', 'https://openalex.org/W4294170691']","In hierarchical phrase-based translation, coarse-grained nonterminal Xs may generate inappropriate translations due to the lack of sufficient information for phrasal substitution.In this paper we propose a framework to refine nonterminals in hierarchical translation rules with real-valued semantic representations.The semantic representations are learned via a weighted mean value and a minimum distance method using phrase vector representations obtained from large scale monolingual corpus.Based on the learned semantic vectors, we build a semantic nonterminal refinement model to measure semantic similarities between phrasal substitutions and nonterminal Xs in translation rules.Experiment results on Chinese-English translation show that the proposed model significantly improves translation quality on NIST test sets.",1.0
SKG_MT_972,https://openalex.org/W2296562594,2015,13,"['https://openalex.org/W137989762', 'https://openalex.org/W170711724', 'https://openalex.org/W1617082848', 'https://openalex.org/W1970381522', 'https://openalex.org/W2051821822', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104907655', 'https://openalex.org/W2115915304', 'https://openalex.org/W2116594867', 'https://openalex.org/W2125943921', 'https://openalex.org/W2127849236', 'https://openalex.org/W2146574666', 'https://openalex.org/W2251089262', 'https://openalex.org/W2251339511']",,0.9942196531791907
SKG_MT_973,https://openalex.org/W2170006268,2013,2,"['https://openalex.org/W1631260214', 'https://openalex.org/W2087309226', 'https://openalex.org/W2087937280', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101451733', 'https://openalex.org/W2101656755', 'https://openalex.org/W2103731025', 'https://openalex.org/W2107468211', 'https://openalex.org/W2108847444', 'https://openalex.org/W2109664771', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126610017', 'https://openalex.org/W2132001515', 'https://openalex.org/W2133444727', 'https://openalex.org/W2136477195', 'https://openalex.org/W2144600658', 'https://openalex.org/W2152213375', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154099718', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158388102', 'https://openalex.org/W2159755860', 'https://openalex.org/W2160245618', 'https://openalex.org/W2163407492', 'https://openalex.org/W2171458318', 'https://openalex.org/W2177801600', 'https://openalex.org/W2340221426', 'https://openalex.org/W2408503330', 'https://openalex.org/W3203347510']","Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel. 1",1.0
SKG_MT_974,https://openalex.org/W3091448919,2020,1,"['https://openalex.org/W222053410', 'https://openalex.org/W2101105183', 'https://openalex.org/W2250342921', 'https://openalex.org/W2804732445', 'https://openalex.org/W2807253849', 'https://openalex.org/W2952614664', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964121744', 'https://openalex.org/W2971081268', 'https://openalex.org/W2972534020', 'https://openalex.org/W2972978035', 'https://openalex.org/W2978123282', 'https://openalex.org/W2994475016', 'https://openalex.org/W3026538704', 'https://openalex.org/W3035254119', 'https://openalex.org/W3035464238', 'https://openalex.org/W3082805191', 'https://openalex.org/W3104652516', 'https://openalex.org/W3203397811']","Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.",1.0
SKG_MT_976,https://openalex.org/W2250993060,2015,10,"['https://openalex.org/W91928571', 'https://openalex.org/W232191560', 'https://openalex.org/W1551202288', 'https://openalex.org/W1862810382', 'https://openalex.org/W1901440691', 'https://openalex.org/W1997859518', 'https://openalex.org/W2013138596', 'https://openalex.org/W2061910127', 'https://openalex.org/W2070150502', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101207453', 'https://openalex.org/W2111142112', 'https://openalex.org/W2115328410', 'https://openalex.org/W2119706326', 'https://openalex.org/W2123825474', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126784811', 'https://openalex.org/W2134800885', 'https://openalex.org/W2140343992', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2155607551', 'https://openalex.org/W2159755860', 'https://openalex.org/W2162245945', 'https://openalex.org/W2169755259', 'https://openalex.org/W2180952760', 'https://openalex.org/W2250847409', 'https://openalex.org/W2251912507', 'https://openalex.org/W2294378299', 'https://openalex.org/W2401082558', 'https://openalex.org/W2595715041', 'https://openalex.org/W2738633406', 'https://openalex.org/W3203906782', 'https://openalex.org/W4241645538']","We study the impact of source length and verbosity of the tuning dataset on the performance of parameter optimizers such as MERT and PRO for statistical machine translation.In particular, we test whether the verbosity of the resulting translations can be modified by varying the length or the verbosity of the tuning sentences.We find that MERT learns the tuning set verbosity very well, while PRO is sensitive to both the verbosity and the length of the source sentences in the tuning set; yet, overall PRO learns best from highverbosity tuning datasets.Given these dependencies, and potentially some other such as amount of reordering, number of unknown words, syntactic complexity, and evaluation measure, to mention just a few, we argue for the need of controlled evaluation scenarios, so that the selection of tuning set and optimization strategy does not overshadow scientific advances in modeling or decoding.In the mean time, until we develop such controlled scenarios, we recommend using PRO with a large verbosity tuning set, which, in our experiments, yields highest BLEU across datasets and language pairs.",0.9949748743718593
SKG_MT_978,https://openalex.org/W3118554287,2020,3,"['https://openalex.org/W2124807415', 'https://openalex.org/W2902466572', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963672008', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964108048', 'https://openalex.org/W2971255462', 'https://openalex.org/W3017208877', 'https://openalex.org/W3034719878']","In this article, we describe the TALP-UPC participation in the WMT20 news translation shared task for Tamil-English. Given the low amount of parallel training data, we resort to adapt the task to a multilingual system to benefit from the positive transfer from high resource languages. We use iterative backtranslation to fine-tune the system and benefit from the monolingual data available. In order to measure the effectivity of such methods, we compare our results to a bilingual baseline system.",0.9953488372093023
SKG_MT_981,https://openalex.org/W2107016235,2012,1,"['https://openalex.org/W121228829', 'https://openalex.org/W161530955', 'https://openalex.org/W205180317', 'https://openalex.org/W1967768610', 'https://openalex.org/W1997306212', 'https://openalex.org/W2143543488', 'https://openalex.org/W2282891966', 'https://openalex.org/W2587282545', 'https://openalex.org/W2913513779']","This paper demonstrates a novel distributed architecture to facilitate the acquisition of Language Resources. We build a factory that automates the stages involved in the acquisition, production, updating and maintenance of these resources. The factory is designed as a platform where functionalities are deployed as web services, which can be combined in complex acquisition chains using workflows. We show a case study, which acquires a Translation Memory for a given pair of languages and a domain using web services for crawling, sentence alignment and conversion to TMX. 1",0.9937888198757764
SKG_MT_982,https://openalex.org/W2552839021,2017,429,"['https://openalex.org/W753012316', 'https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1793121960', 'https://openalex.org/W1815076433', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2100664567', 'https://openalex.org/W2118434577', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132043663', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2157331557', 'https://openalex.org/W2158899491', 'https://openalex.org/W2165115000', 'https://openalex.org/W2176263492', 'https://openalex.org/W2194775991', 'https://openalex.org/W2252272516', 'https://openalex.org/W2311921240', 'https://openalex.org/W2418388682', 'https://openalex.org/W2518157461', 'https://openalex.org/W2525778437', 'https://openalex.org/W2527845440', 'https://openalex.org/W2529089661', 'https://openalex.org/W2540404261', 'https://openalex.org/W2566563465', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950344723', 'https://openalex.org/W2951008357', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962997665', 'https://openalex.org/W2963232029', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963937700', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964125283', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204130541', 'https://openalex.org/W4301368689']",The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. We present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT’16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and on WMT’15 English-German we outperform several recently published results. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT’14 English-French translation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM.,1.0
SKG_MT_985,https://openalex.org/W2890501761,2018,79,"['https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2194775991', 'https://openalex.org/W2251994258', 'https://openalex.org/W2311921240', 'https://openalex.org/W2402144811', 'https://openalex.org/W2519091744', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2613904329', 'https://openalex.org/W2740087922', 'https://openalex.org/W2767206889', 'https://openalex.org/W2769810959', 'https://openalex.org/W2789543585', 'https://openalex.org/W2949382160', 'https://openalex.org/W2953384591', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963069107', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964084720', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W4294619240', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Existing approaches to neural machine translation are typically autoregressive models. While these models attain state-of-the-art translation quality, they are suffering from low parallelizability and thus slow at decoding long sequences. In this paper, we propose a novel model for fast sequence generation — the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive property in global but relieves in local and thus are able to produce multiple successive words in parallel at each time step. Experiments conducted on English-German and Chinese-English translation tasks show that the SAT achieves a good balance between translation quality and decoding speed. On WMT’14 English-German translation, the SAT achieves 5.58× speedup while maintaining 88% translation quality, significantly better than the previous non-autoregressive methods. When produces two words at each time step, the SAT is almost lossless (only 1% degeneration in BLEU score).",1.0
SKG_MT_987,https://openalex.org/W2970009562,2019,22,"['https://openalex.org/W22168010', 'https://openalex.org/W630532510', 'https://openalex.org/W1625582487', 'https://openalex.org/W1869752048', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2131095505', 'https://openalex.org/W2133564696', 'https://openalex.org/W2757281913', 'https://openalex.org/W2757291580', 'https://openalex.org/W2782917850', 'https://openalex.org/W2798931235', 'https://openalex.org/W2886095922', 'https://openalex.org/W2888539709', 'https://openalex.org/W2889326796', 'https://openalex.org/W2898785264', 'https://openalex.org/W2903120920', 'https://openalex.org/W2903193068', 'https://openalex.org/W2916877561', 'https://openalex.org/W2951166594', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970279348', 'https://openalex.org/W3196558007', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566']","We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of neural machine translation on three low-resource, similar language pairs: Spanish – Portuguese, Czech – Polish, and Hindi – Nepali. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish – Portuguese and Czech – Polish translation, whereas LSTMs with global attention worked best on Hindi – Nepali translation.",1.0
SKG_MT_990,https://openalex.org/W3120567826,2020,4,"['https://openalex.org/W309335912', 'https://openalex.org/W1902237438', 'https://openalex.org/W1916559533', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964308564', 'https://openalex.org/W3099107321']","This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use of different linguistic features like POS and Morph along with back translation for Hindi-Marathi and Marathi-Hindi machine translation.",1.0
SKG_MT_991,https://openalex.org/W2798742628,2018,15,"['https://openalex.org/W1828163288', 'https://openalex.org/W1969483458', 'https://openalex.org/W1981031568', 'https://openalex.org/W2095705004', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131241448', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2222235228', 'https://openalex.org/W2525778437', 'https://openalex.org/W2557436004', 'https://openalex.org/W2576482813', 'https://openalex.org/W2580192806', 'https://openalex.org/W2581624817', 'https://openalex.org/W2613904329', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963620441', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2969945254', 'https://openalex.org/W3196603613', 'https://openalex.org/W4293718192']","To achieve high translation performance, neural machine translation models usually rely on the beam search algorithm for decoding sentences. The beam search finds good candidate translations by considering multiple hypotheses of translations simultaneously. However, as the algorithm produces hypotheses in a monotonic left-to-right order, a hypothesis can not be revisited once it is discarded. We found such monotonicity forces the algorithm to sacrifice some good decoding paths. To mitigate this problem, we relax the monotonic constraint of the beam search by maintaining all found hypotheses in a single priority queue and using a universal score function for hypothesis selection. The proposed algorithm allows discarded hypotheses to be recovered in a later step. Despite its simplicity, we show that the proposed decoding algorithm enhances the quality of selected hypotheses and improve the translations even for high-performance models in English-Japanese translation task.",1.0
SKG_MT_992,https://openalex.org/W2970849705,2019,37,"['https://openalex.org/W2966661', 'https://openalex.org/W1902237438', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2525778437', 'https://openalex.org/W2580192806', 'https://openalex.org/W2613904329', 'https://openalex.org/W2751527518', 'https://openalex.org/W2752047430', 'https://openalex.org/W2767206889', 'https://openalex.org/W2769298630', 'https://openalex.org/W2783831488', 'https://openalex.org/W2785994986', 'https://openalex.org/W2798608854', 'https://openalex.org/W2806591392', 'https://openalex.org/W2807940204', 'https://openalex.org/W2898972706', 'https://openalex.org/W2935811960', 'https://openalex.org/W2955347425', 'https://openalex.org/W2962853356', 'https://openalex.org/W2962926939', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963311117', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963406157', 'https://openalex.org/W2963499089', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963699608', 'https://openalex.org/W2963703618', 'https://openalex.org/W2963891264', 'https://openalex.org/W2963905071', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W2970911692', 'https://openalex.org/W3105409430', 'https://openalex.org/W4385245566']","Zaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-Yu Dai, Jiajun Chen. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_994,https://openalex.org/W2963260202,2016,643,"['https://openalex.org/W1411230545', 'https://openalex.org/W1514535095', 'https://openalex.org/W1902237438', 'https://openalex.org/W1924770834', 'https://openalex.org/W2006969979', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2169724380', 'https://openalex.org/W2212846646', 'https://openalex.org/W2239731672', 'https://openalex.org/W2437005631', 'https://openalex.org/W2595715041', 'https://openalex.org/W2949335953', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950178297', 'https://openalex.org/W2950635152', 'https://openalex.org/W2952360713', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963463964', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998704965', 'https://openalex.org/W4241645538', 'https://openalex.org/W4285719527']","Attention mechanism has enhanced stateof-the-art Neural Machine Translation (NMT) by jointly learning to align and translate.It tends to ignore past alignment information, however, which often leads to over-translation and under-translation.To address this problem, we propose coverage-based NMT in this paper.We maintain a coverage vector to keep track of the attention history.The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words.Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT. 1",1.0
SKG_MT_995,https://openalex.org/W2737175496,2017,7,"['https://openalex.org/W7636100', 'https://openalex.org/W22168010', 'https://openalex.org/W160516373', 'https://openalex.org/W787069592', 'https://openalex.org/W1555354714', 'https://openalex.org/W1868323616', 'https://openalex.org/W2038721957', 'https://openalex.org/W2055518963', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101234009', 'https://openalex.org/W2101454539', 'https://openalex.org/W2106565589', 'https://openalex.org/W2113788796', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126241965', 'https://openalex.org/W2127218421', 'https://openalex.org/W2135399286', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2153579005', 'https://openalex.org/W2159974671', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250612533', 'https://openalex.org/W2280403519', 'https://openalex.org/W2460354904', 'https://openalex.org/W2573981993', 'https://openalex.org/W2595715041', 'https://openalex.org/W4294170691']","Statistical machine translation (SMT) systems use local cues from n-gram translation and language models to select the translation of each source word.Such systems do not explicitly perform word sense disambiguation (WSD), although this would enable them to select translations depending on the hypothesized sense of each word.Previous attempts to constrain word translations based on the results of generic WSD systems have suffered from their limited accuracy.We demonstrate that WSD systems can be adapted to help SMT, thanks to three key achievements: (1) we consider a larger context for WSD than SMT can afford to consider; (2) we adapt the number of senses per word to the ones observed in the training data using clustering-based WSD with K-means; and (3) we initialize senseclustering with definitions or examples extracted from WordNet.Our WSD system is competitive, and in combination with a factored SMT system improves noun and verb translation from English to Chinese, Dutch, French, German, and Spanish.",1.0
SKG_MT_996,https://openalex.org/W3098989509,2020,3,"['https://openalex.org/W222053410', 'https://openalex.org/W1945616565', 'https://openalex.org/W1991133427', 'https://openalex.org/W2008225289', 'https://openalex.org/W2049633694', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121879602', 'https://openalex.org/W2157381218', 'https://openalex.org/W2250303366', 'https://openalex.org/W2325237720', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2613904329', 'https://openalex.org/W2735135478', 'https://openalex.org/W2767899794', 'https://openalex.org/W2799194071', 'https://openalex.org/W2896457183', 'https://openalex.org/W2896691342', 'https://openalex.org/W2912095972', 'https://openalex.org/W2922293812', 'https://openalex.org/W2933138175', 'https://openalex.org/W2938348542', 'https://openalex.org/W2946232455', 'https://openalex.org/W2950651087', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963207607', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963823140', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963887123', 'https://openalex.org/W2963969878', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964247056', 'https://openalex.org/W2964265128', 'https://openalex.org/W2970510999', 'https://openalex.org/W2970558573', 'https://openalex.org/W2970597249', 'https://openalex.org/W2971134989', 'https://openalex.org/W2982434686', 'https://openalex.org/W2982756474', 'https://openalex.org/W2984051011', 'https://openalex.org/W2988249555', 'https://openalex.org/W3001816066', 'https://openalex.org/W3035207248', 'https://openalex.org/W3105718208', 'https://openalex.org/W4288333985', 'https://openalex.org/W4289440907', 'https://openalex.org/W4300996741', 'https://openalex.org/W4385245566']","Exposing diverse subword segmentations to neural machine translation (NMT) models often improves the robustness of machine translation as NMT models can experience various subword candidates. However, the diversification of subword segmentations mostly relies on the pre-trained subword language models from which erroneous segmentations of unseen words are less likely to be sampled. In this paper, we present adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations. We experimentally show that our model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of NMT models in low-resource and out-domain datasets.",1.0
SKG_MT_997,https://openalex.org/W3088815688,2020,1,"['https://openalex.org/W67937665', 'https://openalex.org/W630532510', 'https://openalex.org/W1588612820', 'https://openalex.org/W2041532239', 'https://openalex.org/W2096369514', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116492146', 'https://openalex.org/W2124807415', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149327368', 'https://openalex.org/W2496235729', 'https://openalex.org/W2550821151', 'https://openalex.org/W2567571499', 'https://openalex.org/W2625023053', 'https://openalex.org/W2626122427', 'https://openalex.org/W2741157881', 'https://openalex.org/W2765448416', 'https://openalex.org/W2786520898', 'https://openalex.org/W2794365787', 'https://openalex.org/W2805910418', 'https://openalex.org/W2810296466', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964053711', 'https://openalex.org/W2964098600', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2978820841', 'https://openalex.org/W2995616860']","Although there are several sources where to find historical texts, they usually are available in the original language that makes them generally inaccessible. This paper presents the development of state-of-the-art Neural Machine Systems for the low-resourced Latin-Spanish language pair. First, we build a Transformer-based Machine Translation system on the Bible parallel corpus. Then, we build a comparable corpus from Saint Augustine texts and their translations. We use this corpus to study the domain adaptation case from the Bible texts to Saint Augustine’s works. Results show the difficulties of handling a low-resourced language as Latin. First, we noticed the importance of having enough data, since the systems do not achieve high BLEU scores. Regarding domain adaptation, results show how using in-domain data helps systems to achieve a better quality translation. Also, we observed that it is needed a higher amount of data to perform an effective vocabulary extension that includes in-domain vocabulary.",0.9932885906040269
SKG_MT_998,https://openalex.org/W2949938546,2019,37,"['https://openalex.org/W2097606805', 'https://openalex.org/W2103305545', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2493916176', 'https://openalex.org/W2557436004', 'https://openalex.org/W2576482813', 'https://openalex.org/W2595715041', 'https://openalex.org/W2607059968', 'https://openalex.org/W2785779000', 'https://openalex.org/W2885514718', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898658996', 'https://openalex.org/W2950513705', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W4385245566']","Users of machine translation systems may desire to obtain multiple candidates translated in different ways. In this work, we attempt to obtain diverse translations by using sentence codes to condition the sentence generation. We describe two methods to extract the codes, either with or without the help of syntax information. For diverse generation, we sample multiple candidates, each of which conditioned on a unique code. Experiments show that the sampled translations have much higher diversity scores when using reasonable sentence codes, where the translation quality is still on par with the baselines even under strong constraint imposed by the codes. In qualitative analysis, we show that our method is able to generate paraphrase translations with drastically different structures. The proposed approach can be easily adopted to existing translation systems as no modification to the model is required.",1.0
SKG_MT_1002,https://openalex.org/W3095478399,2020,6,"['https://openalex.org/W211509693', 'https://openalex.org/W2101105183', 'https://openalex.org/W2122270629', 'https://openalex.org/W2914120296', 'https://openalex.org/W2948335087', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963281280', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964121744', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970152385', 'https://openalex.org/W2983981554', 'https://openalex.org/W3035254119', 'https://openalex.org/W3104652516', 'https://openalex.org/W3107826490']",International audience,1.0
SKG_MT_1003,https://openalex.org/W2136757377,2011,6,"['https://openalex.org/W1989658336', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116042738', 'https://openalex.org/W2124807415', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2188911266']","The language model (LM) is a critical component in most statistical machine translation (SMT) systems, serving to establish a probability distribution over the hypothesis space. Most SMT systems use a static LM, independent of the source language input. While previous work has shown that adapting LMs based on the input improves SMT performance, none of the techniques has thus far been shown to be feasible for on-line systems. In this paper, we develop a novel measure of cross-lingual similarity for biasing the LM based on the test input. We also illustrate an efficient on-line implementation that supports integration with on-line SMT systems by transferring much of the computational load off-line. Our approach yields significant reductions in target perplexity compared to the static LM, as well as consistent improvements in SMT performance across language pairs (English-Dari and English-Pashto). 1",1.0
SKG_MT_1004,https://openalex.org/W2251333340,2015,66,"['https://openalex.org/W22168010', 'https://openalex.org/W78356000', 'https://openalex.org/W168564468', 'https://openalex.org/W179875071', 'https://openalex.org/W204967857', 'https://openalex.org/W1498436455', 'https://openalex.org/W1614298861', 'https://openalex.org/W1753482797', 'https://openalex.org/W2025423507', 'https://openalex.org/W2064111571', 'https://openalex.org/W2072128103', 'https://openalex.org/W2110660056', 'https://openalex.org/W2115780035', 'https://openalex.org/W2120165387', 'https://openalex.org/W2133564696', 'https://openalex.org/W2137983211', 'https://openalex.org/W2138857742', 'https://openalex.org/W2141599568', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153579005', 'https://openalex.org/W2155607551', 'https://openalex.org/W2158899491', 'https://openalex.org/W2250473075', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251994258', 'https://openalex.org/W2257408573', 'https://openalex.org/W2952230511', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998704965', 'https://openalex.org/W3146803896', 'https://openalex.org/W4231109964', 'https://openalex.org/W4285719527', 'https://openalex.org/W4294170691']","This paper describes the system submitted by the University of Heidelberg to the Shared Task on Word-level Quality Estimation at the 2015 Workshop on Statistical Machine Translation.The submitted system combines a continuous space deep neural network, that learns a bilingual feature representation from scratch, with a linear combination of the manually defined baseline features provided by the task organizers.A combination of these orthogonal information sources shows significant improvements over the combined systems, and produces very competitive F 1 -scores for predicting word-level translation quality.",0.9950248756218906
SKG_MT_1008,https://openalex.org/W3101096852,2020,24,"['https://openalex.org/W11012074', 'https://openalex.org/W194878845', 'https://openalex.org/W280003846', 'https://openalex.org/W1419602491', 'https://openalex.org/W1484504603', 'https://openalex.org/W1504308419', 'https://openalex.org/W1631260214', 'https://openalex.org/W2089629691', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121457870', 'https://openalex.org/W2133564696', 'https://openalex.org/W2135708429', 'https://openalex.org/W2172126254', 'https://openalex.org/W2184135559', 'https://openalex.org/W2186089609', 'https://openalex.org/W2327501763', 'https://openalex.org/W2401969231', 'https://openalex.org/W2407834842', 'https://openalex.org/W2407874404', 'https://openalex.org/W2471933213', 'https://openalex.org/W2512608784', 'https://openalex.org/W2605131327', 'https://openalex.org/W2745785989', 'https://openalex.org/W2759128986', 'https://openalex.org/W2886751231', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2943845043', 'https://openalex.org/W2951456627', 'https://openalex.org/W2952079278', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970074184', 'https://openalex.org/W2972495969', 'https://openalex.org/W2972528057', 'https://openalex.org/W2978099976', 'https://openalex.org/W2978992435', 'https://openalex.org/W2982129078', 'https://openalex.org/W3007142233', 'https://openalex.org/W3015698636', 'https://openalex.org/W3015703505', 'https://openalex.org/W3015926140', 'https://openalex.org/W3015927303', 'https://openalex.org/W3202802636', 'https://openalex.org/W4300558631', 'https://openalex.org/W4300987104', 'https://openalex.org/W4385245566']","Javier Iranzo-Sánchez, Adrià Giménez Pastor, Joan Albert Silvestre-Cerdà, Pau Baquero-Arnal, Jorge Civera Saiz, Alfons Juan. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.",1.0
SKG_MT_1009,https://openalex.org/W650948056,2014,73,"['https://openalex.org/W22168010', 'https://openalex.org/W331019419', 'https://openalex.org/W1419521840', 'https://openalex.org/W1480519300', 'https://openalex.org/W1508977358', 'https://openalex.org/W1558777661', 'https://openalex.org/W1568793342', 'https://openalex.org/W1591659308', 'https://openalex.org/W1670472234', 'https://openalex.org/W1818534184', 'https://openalex.org/W2016630033', 'https://openalex.org/W2027979924', 'https://openalex.org/W2114887620', 'https://openalex.org/W2115774663', 'https://openalex.org/W2124807415', 'https://openalex.org/W2128342468', 'https://openalex.org/W2141440284', 'https://openalex.org/W2142493409', 'https://openalex.org/W2143954309', 'https://openalex.org/W2143995218', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152691628', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156064360', 'https://openalex.org/W2156985047', 'https://openalex.org/W2161044106', 'https://openalex.org/W2165256480', 'https://openalex.org/W2184466497', 'https://openalex.org/W2569308312']","Cross-lingual learning has become a popular approach to facilitate the development of resources and tools for low density languages. Its underlying idea is to make use of existing tools and annotations in resource-rich languages to create similar tools and resources for resource-poor languages. Typically, this is achieved by either projecting annotations across parallel corpora, or by transferring models from one or more source languages to a target language. In this paper, we explore a third strategy by using machine translation to create synthetic training data from the original source-side annotations. Specifically, we apply this technique to dependency parsing, using a cross-lingually unified treebank for adequate evaluation. Our approach draws on annotation projection but avoids the use of noisy source-side annotation of an unrelated parallel corpus and instead relies on manual treebank annotation in combination with statistical machine translation, which makes it possible to train fully lexicalized parsers. We show that this approach significantly outperforms delexicalized transfer parsing.% despite the error-prone translation step.",1.0
SKG_MT_1011,https://openalex.org/W2951635603,2019,55,"['https://openalex.org/W2064675550', 'https://openalex.org/W2125838338', 'https://openalex.org/W2130942839', 'https://openalex.org/W2194775991', 'https://openalex.org/W2513832136', 'https://openalex.org/W2527133236', 'https://openalex.org/W2605202026', 'https://openalex.org/W2760656271', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798858969', 'https://openalex.org/W2798931235', 'https://openalex.org/W2886776719', 'https://openalex.org/W2896457183', 'https://openalex.org/W2903193068', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962904552', 'https://openalex.org/W2963201387', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963824830', 'https://openalex.org/W2963831883', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963951265', 'https://openalex.org/W2963979492', 'https://openalex.org/W2980216782', 'https://openalex.org/W3012492057', 'https://openalex.org/W3015703505', 'https://openalex.org/W4253573210', 'https://openalex.org/W4295253143', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Recent advances in sequence modeling have highlighted the strengths of the transformer architecture, especially in achieving state-of-the-art machine translation results. However, depending on the up-stream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly. The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input. We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (ASR) which contains multiple paths and posterior scores. To leverage the extra information from the lattice structure, we develop a novel controllable lattice attention mechanism to obtain latent representations. On the LDC Spanish-English speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM. Additionally, we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmentations. In this task, we also observe the improvements over strong baselines.",1.0
SKG_MT_1012,https://openalex.org/W2963991775,2019,22,"['https://openalex.org/W1522301498', 'https://openalex.org/W2016589492', 'https://openalex.org/W2025768430', 'https://openalex.org/W2099471712', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157575844', 'https://openalex.org/W2170716095', 'https://openalex.org/W2212703438', 'https://openalex.org/W2242818861', 'https://openalex.org/W2296701362', 'https://openalex.org/W2546938941', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548228487', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555428947', 'https://openalex.org/W2565378226', 'https://openalex.org/W2661761953', 'https://openalex.org/W2740702290', 'https://openalex.org/W2778814079', 'https://openalex.org/W2783419700', 'https://openalex.org/W2785093437', 'https://openalex.org/W2889326796', 'https://openalex.org/W2903193068', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963194310', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963551569', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963580443', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963681240', 'https://openalex.org/W2963959336', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3082674894', 'https://openalex.org/W4294294142', 'https://openalex.org/W4297801368', 'https://openalex.org/W4298393544', 'https://openalex.org/W4307459710', 'https://openalex.org/W4320013936']","Xing Niu, Weijia Xu, Marine Carpuat. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_1013,https://openalex.org/W2737638662,2017,72,"['https://openalex.org/W74908938', 'https://openalex.org/W287510790', 'https://openalex.org/W1522301498', 'https://openalex.org/W1551202288', 'https://openalex.org/W1753482797', 'https://openalex.org/W1843946026', 'https://openalex.org/W2006019948', 'https://openalex.org/W2099037496', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2171421863', 'https://openalex.org/W2251495238', 'https://openalex.org/W2280403519', 'https://openalex.org/W2418388682', 'https://openalex.org/W2437005631', 'https://openalex.org/W2512924740', 'https://openalex.org/W2514342461', 'https://openalex.org/W2515056335', 'https://openalex.org/W2527845440', 'https://openalex.org/W2563574619', 'https://openalex.org/W2580422047', 'https://openalex.org/W2594047108', 'https://openalex.org/W2594229957', 'https://openalex.org/W2949810612', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962867687', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963572611', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963876447', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564']","Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG super tags in the decoder, by interleaving the target super tags with the word sequence. Our results on WMT data show that explicitly modeling target syntax improves machine translation quality for German→English, a high-resource pair, and for Romanian→English, a low resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English.",1.0
SKG_MT_1014,https://openalex.org/W3035459196,2020,52,"['https://openalex.org/W22168010', 'https://openalex.org/W66597527', 'https://openalex.org/W658020064', 'https://openalex.org/W2021618504', 'https://openalex.org/W2057069782', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115259925', 'https://openalex.org/W2126725946', 'https://openalex.org/W2133512280', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2150824314', 'https://openalex.org/W2150955799', 'https://openalex.org/W2153579005', 'https://openalex.org/W2164984707', 'https://openalex.org/W2251033195', 'https://openalex.org/W2294774419', 'https://openalex.org/W2462305634', 'https://openalex.org/W2507833193', 'https://openalex.org/W2550821151', 'https://openalex.org/W2595715041', 'https://openalex.org/W2741375528', 'https://openalex.org/W2758950307', 'https://openalex.org/W2793381978', 'https://openalex.org/W2891896107', 'https://openalex.org/W2903188467', 'https://openalex.org/W2903376039', 'https://openalex.org/W2907069963', 'https://openalex.org/W2913897682', 'https://openalex.org/W2915128308', 'https://openalex.org/W2915756181', 'https://openalex.org/W2936695845', 'https://openalex.org/W2947771965', 'https://openalex.org/W2952638691', 'https://openalex.org/W2953287808', 'https://openalex.org/W2961915345', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963004259', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963165489', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964308564', 'https://openalex.org/W2967600676', 'https://openalex.org/W2970037872', 'https://openalex.org/W2970785793', 'https://openalex.org/W2970791445', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971036337', 'https://openalex.org/W2971148473', 'https://openalex.org/W2973088264', 'https://openalex.org/W2995015695', 'https://openalex.org/W2995118574', 'https://openalex.org/W2995230342', 'https://openalex.org/W2996403597', 'https://openalex.org/W2998463583', 'https://openalex.org/W3013840636', 'https://openalex.org/W3022891705', 'https://openalex.org/W3034402523', 'https://openalex.org/W3034716087', 'https://openalex.org/W3035390927', 'https://openalex.org/W3038033387', 'https://openalex.org/W3104033643', 'https://openalex.org/W3144078281', 'https://openalex.org/W4287811180', 'https://openalex.org/W4294170691', 'https://openalex.org/W4299589691']","Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish ""translationese"", i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points. We make our MT evaluation code available.",1.0
SKG_MT_1015,https://openalex.org/W3093559770,2020,4,"['https://openalex.org/W131501223', 'https://openalex.org/W1522301498', 'https://openalex.org/W1738081185', 'https://openalex.org/W2051840895', 'https://openalex.org/W2067815623', 'https://openalex.org/W2101105183', 'https://openalex.org/W2151996595', 'https://openalex.org/W2163038970', 'https://openalex.org/W2574872930', 'https://openalex.org/W2582446770', 'https://openalex.org/W2606032440', 'https://openalex.org/W2608029998', 'https://openalex.org/W2626778328', 'https://openalex.org/W2787560479', 'https://openalex.org/W2799051177', 'https://openalex.org/W2806987872', 'https://openalex.org/W2888159079', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902582221', 'https://openalex.org/W2902614977', 'https://openalex.org/W2923779212', 'https://openalex.org/W2949888546', 'https://openalex.org/W2952446148', 'https://openalex.org/W2956301977', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963842551', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964308564', 'https://openalex.org/W2971347700', 'https://openalex.org/W3006381853', 'https://openalex.org/W3018473580', 'https://openalex.org/W3023598618', 'https://openalex.org/W3045523169', 'https://openalex.org/W3210914950']","Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.",1.0
SKG_MT_1016,https://openalex.org/W2970247882,2019,60,"['https://openalex.org/W22168010', 'https://openalex.org/W1632114991', 'https://openalex.org/W1869752048', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2512924740', 'https://openalex.org/W2550821151', 'https://openalex.org/W2564486991', 'https://openalex.org/W2595715041', 'https://openalex.org/W2737638662', 'https://openalex.org/W2760656271', 'https://openalex.org/W2778814079', 'https://openalex.org/W2803214681', 'https://openalex.org/W2884083742', 'https://openalex.org/W2888539709', 'https://openalex.org/W2889411721', 'https://openalex.org/W2903193068', 'https://openalex.org/W2912351236', 'https://openalex.org/W2945059185', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962911926', 'https://openalex.org/W2962982474', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964308564', 'https://openalex.org/W3082674894', 'https://openalex.org/W4385245566']","Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it. We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation.",1.0
SKG_MT_1018,https://openalex.org/W3006801027,2020,24,"['https://openalex.org/W222053410', 'https://openalex.org/W630532510', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2157331557', 'https://openalex.org/W2549835527', 'https://openalex.org/W2740107491', 'https://openalex.org/W2759173152', 'https://openalex.org/W2786396726', 'https://openalex.org/W2887751429', 'https://openalex.org/W2888539709', 'https://openalex.org/W2894627709', 'https://openalex.org/W2899423466', 'https://openalex.org/W2905927205', 'https://openalex.org/W2908336025', 'https://openalex.org/W2922349260', 'https://openalex.org/W2940744433', 'https://openalex.org/W2946417913', 'https://openalex.org/W2946462349', 'https://openalex.org/W2946567085', 'https://openalex.org/W2946794439', 'https://openalex.org/W2948947170', 'https://openalex.org/W2950339735', 'https://openalex.org/W2951563833', 'https://openalex.org/W2952406142', 'https://openalex.org/W2962697716', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962788148', 'https://openalex.org/W2962911926', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964110616', 'https://openalex.org/W2964302946', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970045405', 'https://openalex.org/W2970120757', 'https://openalex.org/W2970247882', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970565456', 'https://openalex.org/W2970777192', 'https://openalex.org/W2970810442', 'https://openalex.org/W2970820321', 'https://openalex.org/W2971016963', 'https://openalex.org/W2971033911', 'https://openalex.org/W2971141904', 'https://openalex.org/W2972324944', 'https://openalex.org/W2972342261', 'https://openalex.org/W2972498556', 'https://openalex.org/W2973154008', 'https://openalex.org/W2978017171', 'https://openalex.org/W2986267869', 'https://openalex.org/W2988309730', 'https://openalex.org/W2994673210', 'https://openalex.org/W2995446988', 'https://openalex.org/W3006881356', 'https://openalex.org/W3021293129', 'https://openalex.org/W3034955736', 'https://openalex.org/W3041866211', 'https://openalex.org/W3088581407', 'https://openalex.org/W3103729510', 'https://openalex.org/W3202218022']","Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed -- non-learnable -- attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.",1.0
SKG_MT_1020,https://openalex.org/W2890220768,2018,36,"['https://openalex.org/W6908809', 'https://openalex.org/W648786980', 'https://openalex.org/W1753482797', 'https://openalex.org/W2016589492', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2155027007', 'https://openalex.org/W2157331557', 'https://openalex.org/W2176263492', 'https://openalex.org/W2268617045', 'https://openalex.org/W2487501366', 'https://openalex.org/W2525778437', 'https://openalex.org/W2542835211', 'https://openalex.org/W2546938941', 'https://openalex.org/W2601324753', 'https://openalex.org/W2607987856', 'https://openalex.org/W2613904329', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798931235', 'https://openalex.org/W2896060389', 'https://openalex.org/W2963141266', 'https://openalex.org/W2963163972', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963246629', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W4294149591', 'https://openalex.org/W4385245566']","Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.",1.0
SKG_MT_1021,https://openalex.org/W3093157554,2020,6,"['https://openalex.org/W1572579572', 'https://openalex.org/W2038721957', 'https://openalex.org/W2098162425', 'https://openalex.org/W2101105183', 'https://openalex.org/W2132622898', 'https://openalex.org/W2133459682', 'https://openalex.org/W2148922863', 'https://openalex.org/W2163986298', 'https://openalex.org/W2250513563', 'https://openalex.org/W2250537251', 'https://openalex.org/W2252123671', 'https://openalex.org/W2471690557', 'https://openalex.org/W2508316494', 'https://openalex.org/W2604593109', 'https://openalex.org/W2804491852', 'https://openalex.org/W2902449872', 'https://openalex.org/W2915756181', 'https://openalex.org/W2916548775', 'https://openalex.org/W2964165364', 'https://openalex.org/W2971036337']","Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on lexical similarity.",1.0
SKG_MT_1022,https://openalex.org/W2986712369,2019,28,"['https://openalex.org/W1647671624', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2184135559', 'https://openalex.org/W2580723344', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2767899794', 'https://openalex.org/W2945700568', 'https://openalex.org/W2952317054', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963823140', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964247056', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970169061', 'https://openalex.org/W2970250638', 'https://openalex.org/W2970336035', 'https://openalex.org/W2970558573', 'https://openalex.org/W2970798168', 'https://openalex.org/W2970926924', 'https://openalex.org/W2971078337', 'https://openalex.org/W2971134989', 'https://openalex.org/W2984051011', 'https://openalex.org/W4300835687', 'https://openalex.org/W4385245566']","Neural Machine Translation (NMT) models have been proved strong when\ntranslating clean texts, but they are very sensitive to noise in the input.\nImproving NMT models robustness can be seen as a form of ""domain"" adaption to\nnoise. The recently created Machine Translation on Noisy Text task corpus\nprovides noisy-clean parallel data for a few language pairs, but this data is\nvery limited in size and diversity. The state-of-the-art approaches are heavily\ndependent on large volumes of back-translated data. This paper has two main\ncontributions: Firstly, we propose new data augmentation methods to extend\nlimited noisy data and further improve NMT robustness to noise while keeping\nthe models small. Secondly, we explore the effect of utilizing noise from\nexternal data in the form of speech transcripts and show that it could help\nrobustness.\n",0.9946524064171123
SKG_MT_1024,https://openalex.org/W2150066400,2011,26,"['https://openalex.org/W1631260214', 'https://openalex.org/W1767083593', 'https://openalex.org/W1895815892', 'https://openalex.org/W2006969979', 'https://openalex.org/W2020999234', 'https://openalex.org/W2038698865', 'https://openalex.org/W2045238294', 'https://openalex.org/W2049633694', 'https://openalex.org/W2095907708', 'https://openalex.org/W2099873701', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105577415', 'https://openalex.org/W2115526192', 'https://openalex.org/W2116330964', 'https://openalex.org/W2116594867', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126270798', 'https://openalex.org/W2151197196', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2161742089', 'https://openalex.org/W2162465526', 'https://openalex.org/W2164151151', 'https://openalex.org/W2165666205', 'https://openalex.org/W2169724380', 'https://openalex.org/W2172138510', 'https://openalex.org/W2437005631']","In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs.",1.0
SKG_MT_1026,https://openalex.org/W2897105542,2018,37,"['https://openalex.org/W46679369', 'https://openalex.org/W1522301498', 'https://openalex.org/W1539673959', 'https://openalex.org/W1591706642', 'https://openalex.org/W1843891098', 'https://openalex.org/W1895577753', 'https://openalex.org/W1902237438', 'https://openalex.org/W1991133427', 'https://openalex.org/W1997644175', 'https://openalex.org/W2025768430', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2104246439', 'https://openalex.org/W2121879602', 'https://openalex.org/W2525778437', 'https://openalex.org/W2594978815', 'https://openalex.org/W2613904329', 'https://openalex.org/W2626778328', 'https://openalex.org/W2725082186', 'https://openalex.org/W2765961751', 'https://openalex.org/W2767899794', 'https://openalex.org/W2950448199', 'https://openalex.org/W2952288254', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963201387', 'https://openalex.org/W2963207607', 'https://openalex.org/W2964308564']","Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.",0.9949748743718593
SKG_MT_1028,https://openalex.org/W2952317054,2019,63,"['https://openalex.org/W222053410', 'https://openalex.org/W832270446', 'https://openalex.org/W1647671624', 'https://openalex.org/W1872443190', 'https://openalex.org/W1916559533', 'https://openalex.org/W2001496424', 'https://openalex.org/W2097776316', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114556561', 'https://openalex.org/W2123962305', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126712675', 'https://openalex.org/W2133512280', 'https://openalex.org/W2141332150', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250585411', 'https://openalex.org/W2434153183', 'https://openalex.org/W2593543827', 'https://openalex.org/W2595715041', 'https://openalex.org/W2735676397', 'https://openalex.org/W2736671181', 'https://openalex.org/W2744031566', 'https://openalex.org/W2788330850', 'https://openalex.org/W2806156201', 'https://openalex.org/W2891713103', 'https://openalex.org/W2960479961', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963261349', 'https://openalex.org/W2963792777', 'https://openalex.org/W2963829526', 'https://openalex.org/W2964029788', 'https://openalex.org/W3167591218', 'https://openalex.org/W3172943095', 'https://openalex.org/W3197274356', 'https://openalex.org/W3208836290', 'https://openalex.org/W3211848854', 'https://openalex.org/W4298153606']","We present a simple yet powerful data augmentation method for boosting Neural Machine Translation (NMT) performance by leveraging information retrieved from a Translation Memory (TM). We propose and test two methods for augmenting NMT training data with fuzzy TM matches. Tests on the DGT-TM data set for two language pairs show consistent and substantial improvements over a range of baseline systems. The results suggest that this method is promising for any translation environment in which a sizeable TM is available and a certain amount of repetition across translations is to be expected, especially considering its ease of implementation.",0.9935483870967742
SKG_MT_1029,https://openalex.org/W2563931368,2016,1,"['https://openalex.org/W222053410', 'https://openalex.org/W1508382620', 'https://openalex.org/W1528441900', 'https://openalex.org/W1782878722', 'https://openalex.org/W2092654472', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116442091', 'https://openalex.org/W2121380975', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153508793', 'https://openalex.org/W2166905217', 'https://openalex.org/W2250338682', 'https://openalex.org/W2252049711', 'https://openalex.org/W2402833032', 'https://openalex.org/W2466752041', 'https://openalex.org/W2576482813', 'https://openalex.org/W2963171262', 'https://openalex.org/W2963872035']","Dependency tree-to-tree translation models are powerful because they can naturally handle long range reorderings which is important for distant language pairs.The translation process is easy if it can be accomplished only by replacing non-terminals in translation rules with other rules.However it is sometimes necessary to adjoin translation rules.Flexible non-terminals have been proposed as a promising solution for this problem.A flexible non-terminal provides several insertion position candidates for the rules to be adjoined, but it increases the computational cost of decoding.In this paper we propose a neural network based insertion position selection model to reduce the computational cost by selecting the appropriate insertion positions.The experimental results show the proposed model can select the appropriate insertion position with a high accuracy.It reduces the decoding time and improves the translation quality owing to reduced search space.",1.0
SKG_MT_1030,https://openalex.org/W2106429429,2014,8,"['https://openalex.org/W121569490', 'https://openalex.org/W1631260214', 'https://openalex.org/W1934041838', 'https://openalex.org/W1969974515', 'https://openalex.org/W1994303046', 'https://openalex.org/W2016856586', 'https://openalex.org/W2095755718', 'https://openalex.org/W2096384808', 'https://openalex.org/W2096557251', 'https://openalex.org/W2097927681', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101207453', 'https://openalex.org/W2110168585', 'https://openalex.org/W2117278770', 'https://openalex.org/W2123635983', 'https://openalex.org/W2134800885', 'https://openalex.org/W2141440284', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152423400', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2160448241', 'https://openalex.org/W2163255203', 'https://openalex.org/W2250956062', 'https://openalex.org/W2296338145', 'https://openalex.org/W2396575863', 'https://openalex.org/W2437005631', 'https://openalex.org/W4254408171']","This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the German→English translation task of the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014).Both hierarchical and phrase-based SMT systems are applied employing hierarchical phrase reordering and word class language models.For the phrase-based system, we run discriminative phrase training.In addition, we describe our preprocessing pipeline for German→English.",1.0
SKG_MT_1031,https://openalex.org/W2923622379,2019,305,"['https://openalex.org/W1540371141', 'https://openalex.org/W1753482797', 'https://openalex.org/W2007347635', 'https://openalex.org/W2105410942', 'https://openalex.org/W2118090838', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2151834591', 'https://openalex.org/W2161943765', 'https://openalex.org/W2171671120', 'https://openalex.org/W2296073425', 'https://openalex.org/W2523478734', 'https://openalex.org/W2525778437', 'https://openalex.org/W2537667581', 'https://openalex.org/W2584268338', 'https://openalex.org/W2741838462', 'https://openalex.org/W2760656271', 'https://openalex.org/W2760788470', 'https://openalex.org/W2767019613', 'https://openalex.org/W2785523195', 'https://openalex.org/W2796108585', 'https://openalex.org/W2888456631', 'https://openalex.org/W2898846200', 'https://openalex.org/W2962697716', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962911098', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963932569', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964308564', 'https://openalex.org/W3099907503', 'https://openalex.org/W4293569541', 'https://openalex.org/W4300428972', 'https://openalex.org/W4385245566']","Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, Tom Mitchell. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_1032,https://openalex.org/W2250316417,2015,36,"['https://openalex.org/W66690650', 'https://openalex.org/W808955830', 'https://openalex.org/W1519942606', 'https://openalex.org/W1539939880', 'https://openalex.org/W1965555277', 'https://openalex.org/W2010595692', 'https://openalex.org/W2057900969', 'https://openalex.org/W2066792529', 'https://openalex.org/W2096384330', 'https://openalex.org/W2101105183', 'https://openalex.org/W2109553965', 'https://openalex.org/W2116599427', 'https://openalex.org/W2117717100', 'https://openalex.org/W2120831529', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127610924', 'https://openalex.org/W2134131174', 'https://openalex.org/W2134800885', 'https://openalex.org/W2135739396', 'https://openalex.org/W2139671364', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150299430', 'https://openalex.org/W2151504427', 'https://openalex.org/W2156985047', 'https://openalex.org/W2251320341', 'https://openalex.org/W2595715041', 'https://openalex.org/W2953025830']","We use character-based statistical machine translation in order to correct user search queries in the e-commerce domain.The training data is automatically extracted from event logs where users re-issue their search queries with potentially corrected spelling within the same session.We show results on a test set which was annotated by humans and compare against online autocorrection capabilities of three additional web sites.Overall, the methods presented in this paper outperform fully productized spellchecking and autocorrection services in terms of accuracy and F1 score.We also propose novel evaluation steps based on retrieved search results of the corrected queries in terms of quantity and relevance.",1.0
SKG_MT_1034,https://openalex.org/W2760258467,2017,6,"['https://openalex.org/W1554944419', 'https://openalex.org/W1988790447', 'https://openalex.org/W2008652694', 'https://openalex.org/W2056132907', 'https://openalex.org/W2087735403', 'https://openalex.org/W2141440284', 'https://openalex.org/W2143426320', 'https://openalex.org/W2149327368', 'https://openalex.org/W2166446427', 'https://openalex.org/W2171033594', 'https://openalex.org/W2250342921', 'https://openalex.org/W2261755308', 'https://openalex.org/W2508355219', 'https://openalex.org/W2511164486', 'https://openalex.org/W2513945636', 'https://openalex.org/W2518299649', 'https://openalex.org/W2595715041', 'https://openalex.org/W2604377206', 'https://openalex.org/W2752289368', 'https://openalex.org/W2753057867']","Referential translation machines achieve top performance in both bilingual and monolingual settings without accessing any task or domain specific information or resource.RTMs achieve the 3rd system results for German to English sentence-level prediction of translation quality and the 2nd system results according to root mean squared error.In addition to the new features about substring distances, punctuation tokens, character n-grams, and alignment crossings, and additional learning models, we average prediction scores from different models using weights based on their training performance for improved results.",1.0
SKG_MT_1035,https://openalex.org/W2915751160,2015,3,"['https://openalex.org/W1614298861', 'https://openalex.org/W1631260214', 'https://openalex.org/W1644288249', 'https://openalex.org/W1995560154', 'https://openalex.org/W2000546550', 'https://openalex.org/W2016856586', 'https://openalex.org/W2054533749', 'https://openalex.org/W2083460949', 'https://openalex.org/W2095650036', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105245376', 'https://openalex.org/W2108862644', 'https://openalex.org/W2117278770', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2138706636', 'https://openalex.org/W2143564602', 'https://openalex.org/W2144725461', 'https://openalex.org/W2146574666', 'https://openalex.org/W2156985047', 'https://openalex.org/W2183405210', 'https://openalex.org/W2250375075', 'https://openalex.org/W2250905272', 'https://openalex.org/W2251682575', 'https://openalex.org/W2293139442', 'https://openalex.org/W2401082558', 'https://openalex.org/W2406079600', 'https://openalex.org/W2542068976', 'https://openalex.org/W2595715041', 'https://openalex.org/W2792120524', 'https://openalex.org/W3209717902']","In this paper, the KIT systems submitted to the Shared Translation Task are presented.We participated in two translation directions: from German to English and from English to German.Both translations are generated using phrase-based translation systems.The performance of the systems was boosted by using language models built based on different tokens such as word, part-of-speech, and automacally generated word clusters.The difference in word order between German and English is addressed by part-of-speech and syntactic tree-based reordering models.In addition to a discriminative word lexicon, we used hypothesis rescoring using the ListNet algorithm after generating the translation with the phrase-based system.We evaluated the rescoring using only the baseline features as well as using additional computational complex features.",1.0
SKG_MT_1036,https://openalex.org/W2118090838,2013,540,"['https://openalex.org/W36903255', 'https://openalex.org/W63749954', 'https://openalex.org/W71795751', 'https://openalex.org/W179875071', 'https://openalex.org/W1508001288', 'https://openalex.org/W1532325895', 'https://openalex.org/W1787856957', 'https://openalex.org/W2028330546', 'https://openalex.org/W2077428231', 'https://openalex.org/W2088911157', 'https://openalex.org/W2103305545', 'https://openalex.org/W2105577415', 'https://openalex.org/W2110310640', 'https://openalex.org/W2113459411', 'https://openalex.org/W2113958117', 'https://openalex.org/W2116316001', 'https://openalex.org/W2117130368', 'https://openalex.org/W2118020653', 'https://openalex.org/W2125595887', 'https://openalex.org/W2131462252', 'https://openalex.org/W2139989135', 'https://openalex.org/W2141678623', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152382718', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158139315', 'https://openalex.org/W2164019165', 'https://openalex.org/W2164973920', 'https://openalex.org/W2169724380', 'https://openalex.org/W2170741935', 'https://openalex.org/W2183748887', 'https://openalex.org/W2184045248', 'https://openalex.org/W2187089797', 'https://openalex.org/W2189048262', 'https://openalex.org/W2247368039', 'https://openalex.org/W2251012068', 'https://openalex.org/W2251033195', 'https://openalex.org/W2251098065', 'https://openalex.org/W2296073425', 'https://openalex.org/W2613634265', 'https://openalex.org/W2950577311', 'https://openalex.org/W4285719527']","We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.",1.0
SKG_MT_1037,https://openalex.org/W2758605572,2017,4,"['https://openalex.org/W1522301498', 'https://openalex.org/W1606347560', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106429429', 'https://openalex.org/W2110168585', 'https://openalex.org/W2113021982', 'https://openalex.org/W2116792345', 'https://openalex.org/W2122270629', 'https://openalex.org/W2123301721', 'https://openalex.org/W2133564696', 'https://openalex.org/W2141440284', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150634928', 'https://openalex.org/W2154988249', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2250831586', 'https://openalex.org/W2251367463', 'https://openalex.org/W2291126447', 'https://openalex.org/W2296354688', 'https://openalex.org/W2400065810', 'https://openalex.org/W2408504891', 'https://openalex.org/W2512848817', 'https://openalex.org/W2516649778', 'https://openalex.org/W2595715041', 'https://openalex.org/W2622799641', 'https://openalex.org/W2757154661', 'https://openalex.org/W2916946197', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963333747', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3203276480']","This paper describes the statistical machine translation system developed at RWTH Aachen University for the English→German and German→English translation tasks of the EMNLP 2017 Second Conference on Machine Translation (WMT 2017).We use ensembles of attention-based neural machine translation system for both directions.We use the provided parallel and synthetic data to train the models.In addition, we also create a phrasal system using joint translation and reordering models in decoding and neural models in rescoring.",1.0
SKG_MT_1038,https://openalex.org/W2891713103,2018,54,"['https://openalex.org/W832270446', 'https://openalex.org/W1522301498', 'https://openalex.org/W1924770834', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114556561', 'https://openalex.org/W2126712675', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131744502', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145790651', 'https://openalex.org/W2251469009', 'https://openalex.org/W2460354904', 'https://openalex.org/W2618463334', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964308564']","Translation memories (TM) facilitate human translators to reuse existing repetitive translation fragments. In this paper, we propose a novel method to combine the strengths of both TM and neural machine translation (NMT) for high-quality translation. We treat the target translation of a TM match as an additional reference input and encode it into NMT with an extra encoder. A gating mechanism is further used to balance the impact of the TM match on the NMT decoder. Experiment results on the UN corpus demonstrate that when fuzzy matches are higher than 50%, the quality of NMT translation can be significantly improved by over 10 BLEU points.",1.0
SKG_MT_1040,https://openalex.org/W3017454464,2020,210,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2251743902', 'https://openalex.org/W2531207078', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2807535859', 'https://openalex.org/W2809456172', 'https://openalex.org/W2888456631', 'https://openalex.org/W2888520903', 'https://openalex.org/W2899015110', 'https://openalex.org/W2912095972', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921280978', 'https://openalex.org/W2951451051', 'https://openalex.org/W2952153923', 'https://openalex.org/W2953190730', 'https://openalex.org/W2958953787', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962807144', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499433', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963714898', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963983698', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964073484', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970290486', 'https://openalex.org/W2970925270', 'https://openalex.org/W2970925677', 'https://openalex.org/W2972840215', 'https://openalex.org/W2985204668', 'https://openalex.org/W2995304149', 'https://openalex.org/W3101577648', 'https://openalex.org/W3101672304', 'https://openalex.org/W3211848854', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.",1.0
SKG_MT_1041,https://openalex.org/W3045587196,2020,1,"['https://openalex.org/W1522301498', 'https://openalex.org/W1815076433', 'https://openalex.org/W2095705004', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143177362', 'https://openalex.org/W2157331557', 'https://openalex.org/W2183341477', 'https://openalex.org/W2222235228', 'https://openalex.org/W2525778437', 'https://openalex.org/W2530647954', 'https://openalex.org/W2540404261', 'https://openalex.org/W2567571499', 'https://openalex.org/W2763421725', 'https://openalex.org/W2798542795', 'https://openalex.org/W2933138175', 'https://openalex.org/W2948210185', 'https://openalex.org/W2949555952', 'https://openalex.org/W2949803292', 'https://openalex.org/W2951873305', 'https://openalex.org/W2963112338', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963382396', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963406669', 'https://openalex.org/W2963697731', 'https://openalex.org/W2963748441', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2990194721', 'https://openalex.org/W2995025784', 'https://openalex.org/W2998320111', 'https://openalex.org/W3045622142', 'https://openalex.org/W3204406378', 'https://openalex.org/W4301368689', 'https://openalex.org/W4385245566']","In this paper, we introduce a system built for the Duolingo Simultaneous Translation And Paraphrase for Language Education (STAPLE) shared task at the 4th Workshop on Neural Generation and Translation (WNGT 2020). We participated in the English-to-Japanese track with a Transformer model pretrained on the JParaCrawl corpus and fine-tuned in two steps on the JESC corpus and then the (smaller) Duolingo training corpus. First, during training, we find it is essential to deliberately expose the model to higher-quality translations more often during training for optimal translation performance. For inference, encouraging a small amount of diversity with Diverse Beam Search to improve translation coverage yielded marginal improvement over regular Beam Search. Finally, using an auxiliary filtering model to filter out unlikely candidates from Beam Search improves performance further. We achieve a weighted F1 score of 27.56% on our own test set, outperforming the STAPLE AWS translations baseline score of 4.31%.",1.0
SKG_MT_1043,https://openalex.org/W2964345285,2019,208,"['https://openalex.org/W6908809', 'https://openalex.org/W648786980', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2154652894', 'https://openalex.org/W2157331557', 'https://openalex.org/W2162245945', 'https://openalex.org/W2176263492', 'https://openalex.org/W2268617045', 'https://openalex.org/W2296701362', 'https://openalex.org/W2525778437', 'https://openalex.org/W2613904329', 'https://openalex.org/W2741986820', 'https://openalex.org/W2890220768', 'https://openalex.org/W2904829696', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566']","Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way. Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. Experiment results on Chinese->English and WMT’14 English->German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets.",1.0
SKG_MT_1044,https://openalex.org/W3108989951,2020,4,"['https://openalex.org/W630532510', 'https://openalex.org/W832270446', 'https://openalex.org/W1614298861', 'https://openalex.org/W1967880962', 'https://openalex.org/W2101105183', 'https://openalex.org/W2153579005', 'https://openalex.org/W2250539671', 'https://openalex.org/W2467834614', 'https://openalex.org/W2605035112', 'https://openalex.org/W2757592053', 'https://openalex.org/W2789910672', 'https://openalex.org/W2951770285', 'https://openalex.org/W2952317054', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962780935', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2962987735', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963149635', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963672008', 'https://openalex.org/W2963841178', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964291396', 'https://openalex.org/W2971046749', 'https://openalex.org/W2988451549', 'https://openalex.org/W2998702515', 'https://openalex.org/W3005296017', 'https://openalex.org/W3032532958', 'https://openalex.org/W3035531963', 'https://openalex.org/W3088348768']",International audience,1.0
SKG_MT_1045,https://openalex.org/W2175275133,2015,9,"['https://openalex.org/W16967297', 'https://openalex.org/W66137051', 'https://openalex.org/W630532510', 'https://openalex.org/W2087735403', 'https://openalex.org/W2096765155', 'https://openalex.org/W2097927681', 'https://openalex.org/W2111856253', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126704587', 'https://openalex.org/W2134800885', 'https://openalex.org/W2144746247', 'https://openalex.org/W2147880316', 'https://openalex.org/W2149150290', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153635508', 'https://openalex.org/W2157512532', 'https://openalex.org/W2158195707', 'https://openalex.org/W2160001241', 'https://openalex.org/W2164984707', 'https://openalex.org/W2169939314', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2579915115', 'https://openalex.org/W2592627105', 'https://openalex.org/W2916566496', 'https://openalex.org/W3120421331', 'https://openalex.org/W3197057620', 'https://openalex.org/W4285719527']","This paper describes the submission of the UGENT-LT3 SCATE system to the WMT15 Shared Task on Quality Estimation (QE), viz.English-Spanish word and sentence-level QE.We conceived QE as a supervised Machine Learning (ML) problem and designed additional features and combined these with the baseline feature set to estimate quality.The sentence-level QE system re-uses the word level predictions of the word-level QE system.We experimented with different learning methods and observe improvements over the baseline system for wordlevel QE with the use of the new features and by combining learning methods into ensembles.For sentence-level QE we show that using a single feature based on word-level predictions can perform better than the baseline system and using this in combination with additional features led to further improvements in performance.",1.0
SKG_MT_1046,https://openalex.org/W2251550922,2014,12,"['https://openalex.org/W111475876', 'https://openalex.org/W222053410', 'https://openalex.org/W646531355', 'https://openalex.org/W1505646019', 'https://openalex.org/W1975879668', 'https://openalex.org/W1996430422', 'https://openalex.org/W2006969979', 'https://openalex.org/W2022204871', 'https://openalex.org/W2038698865', 'https://openalex.org/W2069736034', 'https://openalex.org/W2081795963', 'https://openalex.org/W2084046180', 'https://openalex.org/W2087735403', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2142262074', 'https://openalex.org/W2145604837', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2154124206', 'https://openalex.org/W2159755860', 'https://openalex.org/W2166706824', 'https://openalex.org/W2167660864', 'https://openalex.org/W2199803028', 'https://openalex.org/W2250974141', 'https://openalex.org/W2372057287', 'https://openalex.org/W2434901392', 'https://openalex.org/W3146306708', 'https://openalex.org/W3203276480']","In this paper, we explore bilingual sentiment knowledge for statistical machine translation (SMT). We propose to explicitly model the consistency of sentiment between the source and target side with a lexicon-based approach. The experiments show that the proposed model significantly improves Chinese-to-English NIST translation over a competitive baseline.",1.0
SKG_MT_1048,https://openalex.org/W2101816610,2013,20,"['https://openalex.org/W1592510853', 'https://openalex.org/W1631260214', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107878390', 'https://openalex.org/W2114539183', 'https://openalex.org/W2119856489', 'https://openalex.org/W2121155211', 'https://openalex.org/W2123318312', 'https://openalex.org/W2124807415', 'https://openalex.org/W2148392518', 'https://openalex.org/W2151976760', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158732077', 'https://openalex.org/W2270190199', 'https://openalex.org/W2604377206', 'https://openalex.org/W2752885492', 'https://openalex.org/W3104921985', 'https://openalex.org/W3145128584']",We use feature decay algorithms (FDA) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction. We develop parallel FDA for solving computational scalability problems caused by the abundance of training data for SMT models and LM models and still achieve SMT performance that is on par with using all of the training data or better. Parallel FDA runs separate FDA models on randomized subsets of the training data and combines the instance selections later. Parallel FDA can also be used for selecting the LM corpus based on the training set selected by parallel FDA. The high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing SMT systems. The relevancy of the selected LM corpus can reach up to 86% reduction in the number of OOV tokens and up to 74% reduction in the perplexity. We perform SMT experiments in all language pairs in the&#13;\nWMT13 translation task and obtain SMT performance close to the top systems using signiï¬cantly less resources for training and development.,1.0
SKG_MT_1049,https://openalex.org/W2798833929,2018,17,"['https://openalex.org/W6908809', 'https://openalex.org/W222053410', 'https://openalex.org/W1632114991', 'https://openalex.org/W1869752048', 'https://openalex.org/W1979495315', 'https://openalex.org/W2032370604', 'https://openalex.org/W2063123613', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110104386', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132382844', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134729743', 'https://openalex.org/W2157331557', 'https://openalex.org/W2586524454', 'https://openalex.org/W2586559132', 'https://openalex.org/W2594047108', 'https://openalex.org/W2594229957', 'https://openalex.org/W2604164736', 'https://openalex.org/W2739894144', 'https://openalex.org/W2963069010', 'https://openalex.org/W2963073938', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963520382', 'https://openalex.org/W2963593215', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963888305', 'https://openalex.org/W2963913268', 'https://openalex.org/W2964308564']","Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-sequence NMT model). The BLEU score of the proposed method is higher than that of the sequence-to-sequence NMT, tree-based NMT, and forest-based SMT systems.",1.0
SKG_MT_1051,https://openalex.org/W3101668578,2020,21,"['https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W1678356000', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2767989436', 'https://openalex.org/W2807880213', 'https://openalex.org/W2888519496', 'https://openalex.org/W2888520903', 'https://openalex.org/W2890964657', 'https://openalex.org/W2908336025', 'https://openalex.org/W2912934387', 'https://openalex.org/W2913637113', 'https://openalex.org/W2946462349', 'https://openalex.org/W2952339051', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962822108', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963823140', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970550739', 'https://openalex.org/W2970682957', 'https://openalex.org/W2984489779', 'https://openalex.org/W2989571009', 'https://openalex.org/W2997110224', 'https://openalex.org/W2998681865', 'https://openalex.org/W3004484424', 'https://openalex.org/W3080183383', 'https://openalex.org/W4293718192']","Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT’14 English-to-German and WMT’18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.",1.0
SKG_MT_1053,https://openalex.org/W2121457870,2014,85,"['https://openalex.org/W419376470', 'https://openalex.org/W808583520', 'https://openalex.org/W1996430422', 'https://openalex.org/W2089629691', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127863960', 'https://openalex.org/W2129336405', 'https://openalex.org/W2132959801', 'https://openalex.org/W2142112143', 'https://openalex.org/W2147262247', 'https://openalex.org/W2153508793', 'https://openalex.org/W2184135559', 'https://openalex.org/W2398009384', 'https://openalex.org/W2595715041', 'https://openalex.org/W3202218022', 'https://openalex.org/W3204130541']","In this paper, we propose new algorithms for learning segmentation strategies for simultaneous speech translation.In contrast to previously proposed heuristic methods, our method finds a segmentation that directly maximizes the performance of the machine translation system.We describe two methods based on greedy search and dynamic programming that search for the optimal segmentation strategy.An experimental evaluation finds that our algorithm is able to segment the input two to three times more frequently than conventional methods in terms of number of words, while maintaining the same score of automatic evaluation. 1",1.0
SKG_MT_1054,https://openalex.org/W2891713103,2018,54,"['https://openalex.org/W832270446', 'https://openalex.org/W1522301498', 'https://openalex.org/W1924770834', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114556561', 'https://openalex.org/W2126712675', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131744502', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145790651', 'https://openalex.org/W2251469009', 'https://openalex.org/W2460354904', 'https://openalex.org/W2618463334', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964298349', 'https://openalex.org/W2964308564']","Translation memories (TM) facilitate human translators to reuse existing repetitive translation fragments. In this paper, we propose a novel method to combine the strengths of both TM and neural machine translation (NMT) for high-quality translation. We treat the target translation of a TM match as an additional reference input and encode it into NMT with an extra encoder. A gating mechanism is further used to balance the impact of the TM match on the NMT decoder. Experiment results on the UN corpus demonstrate that when fuzzy matches are higher than 50%, the quality of NMT translation can be significantly improved by over 10 BLEU points.",1.0
SKG_MT_1056,https://openalex.org/W2529548870,2017,187,"['https://openalex.org/W419376470', 'https://openalex.org/W808583520', 'https://openalex.org/W1522301498', 'https://openalex.org/W2089629691', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108325777', 'https://openalex.org/W2119717200', 'https://openalex.org/W2121457870', 'https://openalex.org/W2130942839', 'https://openalex.org/W2132959801', 'https://openalex.org/W2133564696', 'https://openalex.org/W2178654303', 'https://openalex.org/W2251955814', 'https://openalex.org/W2395821692', 'https://openalex.org/W2419292002', 'https://openalex.org/W2952264928', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963158258', 'https://openalex.org/W2963729263', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964260331', 'https://openalex.org/W2964308564']","This research was reported in the trade magazine Slator: Language Industry Intelligence in Oct 2016 as “Significant progress in real-time machine translation.""",1.0
SKG_MT_1058,https://openalex.org/W2798637166,2018,2,"['https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251044602', 'https://openalex.org/W2737114849', 'https://openalex.org/W2756978580', 'https://openalex.org/W2760656271', 'https://openalex.org/W2800233718', 'https://openalex.org/W2949888546', 'https://openalex.org/W2964043796']","We empirically investigate learning from partial feedback in neural machine translation (NMT), when partial feedback is collected by asking users to highlight a correct chunk of a translation. We propose a simple and effective way of utilizing such feedback in NMT training. We demonstrate how the common machine translation problem of domain mismatch between training and deployment can be reduced solely based on chunk-level user feedback. We conduct a series of simulation experiments to test the effectiveness of the proposed method. Our results show that chunk-level feedback outperforms sentence based feedback by up to 2.61% BLEU absolute.",1.0
SKG_MT_1060,https://openalex.org/W2251507105,2014,34,"['https://openalex.org/W152958635', 'https://openalex.org/W1504046386', 'https://openalex.org/W1528941926', 'https://openalex.org/W1996430422', 'https://openalex.org/W2097606805', 'https://openalex.org/W2097955083', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113125821', 'https://openalex.org/W2123301721', 'https://openalex.org/W2126241965', 'https://openalex.org/W2143263475', 'https://openalex.org/W2152367404', 'https://openalex.org/W2153653739', 'https://openalex.org/W2162245945', 'https://openalex.org/W2169599995', 'https://openalex.org/W2189472871', 'https://openalex.org/W2250859223', 'https://openalex.org/W2251565792', 'https://openalex.org/W2407892396', 'https://openalex.org/W2595715041', 'https://openalex.org/W4241645538']","In this paper, we describe our English-Hindi and Hindi-English statistical systems submitted to the WMT14 shared task.The core components of our translation systems are phrase based (Hindi-English) and factored (English-Hindi) SMT systems.We show that the use of number, case and Tree Adjoining Grammar information as factors helps to improve English-Hindi translation, primarily by generating morphological inflections correctly.We show improvements to the translation systems using pre-procesing and post-processing components.To overcome the structural divergence between English and Hindi, we preorder the source side sentence to conform to the target language word order.Since parallel corpus is limited, many words are not translated.We translate out-of-vocabulary words and transliterate named entities in a post-processing stage.We also investigate ranking of translations from multiple systems to select the best translation.",1.0
SKG_MT_1062,https://openalex.org/W2138046680,2012,25,"['https://openalex.org/W224064951', 'https://openalex.org/W635530177', 'https://openalex.org/W1568793342', 'https://openalex.org/W1576520375', 'https://openalex.org/W1587871245', 'https://openalex.org/W1591659308', 'https://openalex.org/W1638000437', 'https://openalex.org/W1986330201', 'https://openalex.org/W2087735403', 'https://openalex.org/W2097606805', 'https://openalex.org/W2109943925', 'https://openalex.org/W2127713198', 'https://openalex.org/W2135231361', 'https://openalex.org/W2276605909', 'https://openalex.org/W2399720833', 'https://openalex.org/W2783135427']","This paper describes Uppsala University’s submissions to the Quality Estimation (QE) shared task at WMT 2012. We present a QE system based on Support Vector Machine regression, using a number of explicitly defined features extracted from the Machine Translation input, output and models in combination with tree kernels over constituency and dependency parse trees for the input and output sentences. We confirm earlier results suggesting that tree kernels can be a useful tool for QE system construction especially in the early stages of system design. 1",1.0
SKG_MT_1063,https://openalex.org/W2963792777,2017,52,"['https://openalex.org/W6908809', 'https://openalex.org/W1910131649', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134800885', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2407166119', 'https://openalex.org/W2525778437', 'https://openalex.org/W2530887700', 'https://openalex.org/W2550821151', 'https://openalex.org/W2577335011', 'https://openalex.org/W2595715041', 'https://openalex.org/W2950527759', 'https://openalex.org/W2951184134', 'https://openalex.org/W2962708992', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962954913', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963558617', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963699608', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4303633609']","Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by 9.0 and 2.7 BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.",1.0
SKG_MT_1065,https://openalex.org/W1519942606,2012,43,"['https://openalex.org/W171476473', 'https://openalex.org/W201346080', 'https://openalex.org/W214028658', 'https://openalex.org/W635530177', 'https://openalex.org/W1605282883', 'https://openalex.org/W1994581546', 'https://openalex.org/W2102443632', 'https://openalex.org/W2107221152', 'https://openalex.org/W2111666304', 'https://openalex.org/W2116576881', 'https://openalex.org/W2116599427', 'https://openalex.org/W2118947254', 'https://openalex.org/W2119727789', 'https://openalex.org/W2124807415', 'https://openalex.org/W2143927888', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2169273645', 'https://openalex.org/W2170204377', 'https://openalex.org/W2736671181', 'https://openalex.org/W2951166594']",In this paper we investigate the use of character-level translation models to support the translation from and to underresourced languages and textual domains via closely related pivot languages. Our experiments show that these low-level models can be successful even with tiny amounts of training data. We test the approach on movie subtitles for three language pairs and legal texts for another language pair in a domain adaptation task. Our pivot translations outperform the baselines by a large margin. 1,1.0
SKG_MT_1066,https://openalex.org/W2970409072,2019,17,"['https://openalex.org/W630532510', 'https://openalex.org/W1905522558', 'https://openalex.org/W1915251500', 'https://openalex.org/W2064675550', 'https://openalex.org/W2117278770', 'https://openalex.org/W2130942839', 'https://openalex.org/W2147262247', 'https://openalex.org/W2157331557', 'https://openalex.org/W2567571499', 'https://openalex.org/W2744813330', 'https://openalex.org/W2756566411', 'https://openalex.org/W2757592053', 'https://openalex.org/W2803241009', 'https://openalex.org/W2883904829', 'https://openalex.org/W2922349260', 'https://openalex.org/W2952573656', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963149635', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964265128', 'https://openalex.org/W3204406378']","The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.",1.0
SKG_MT_1068,https://openalex.org/W2251921265,2015,19,"['https://openalex.org/W71795751', 'https://openalex.org/W222053410', 'https://openalex.org/W1423339008', 'https://openalex.org/W1753482797', 'https://openalex.org/W1832693441', 'https://openalex.org/W2097998348', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103305545', 'https://openalex.org/W2118090838', 'https://openalex.org/W2118536060', 'https://openalex.org/W2120615054', 'https://openalex.org/W2121870595', 'https://openalex.org/W2122993045', 'https://openalex.org/W2123635983', 'https://openalex.org/W2124322414', 'https://openalex.org/W2129776742', 'https://openalex.org/W2133280805', 'https://openalex.org/W2136016850', 'https://openalex.org/W2142074148', 'https://openalex.org/W2144945507', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153579005', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158899491', 'https://openalex.org/W2166465139', 'https://openalex.org/W2250445771', 'https://openalex.org/W2250545560', 'https://openalex.org/W2250644439', 'https://openalex.org/W2250714477', 'https://openalex.org/W2250861254', 'https://openalex.org/W2251395256', 'https://openalex.org/W2251682575', 'https://openalex.org/W2251690405', 'https://openalex.org/W2251855842', 'https://openalex.org/W2251939518', 'https://openalex.org/W4285719527']",,1.0
SKG_MT_1069,https://openalex.org/W2250695194,2013,11,"['https://openalex.org/W16967297', 'https://openalex.org/W91928571', 'https://openalex.org/W635530177', 'https://openalex.org/W1631260214', 'https://openalex.org/W1848260265', 'https://openalex.org/W2000359198', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111142112', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132001515', 'https://openalex.org/W2134800885', 'https://openalex.org/W2137387514', 'https://openalex.org/W2137698233', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147192413', 'https://openalex.org/W2159755860', 'https://openalex.org/W2180952760', 'https://openalex.org/W2251202280', 'https://openalex.org/W2783135427', 'https://openalex.org/W3202971710']","In Statistical Machine Translation we often have to combine different sources of parallel training data to build a good system. One way of doing this is to build separate translation models from each data set and linearly inter-polate them, and to date the main method for optimising the interpolation weights is to min-imise the model perplexity on a heldout set. In this work, rather than optimising for this indi-rect measure, we directly optimise for BLEU on the tuning set and show improvements in average performance over two data sets and 8 language pairs. 1",1.0
SKG_MT_1070,https://openalex.org/W2970820321,2019,126,"['https://openalex.org/W1686946872', 'https://openalex.org/W2025341678', 'https://openalex.org/W2096516049', 'https://openalex.org/W2114771311', 'https://openalex.org/W2123442489', 'https://openalex.org/W2563574619', 'https://openalex.org/W2610120229', 'https://openalex.org/W2767204723', 'https://openalex.org/W2773621464', 'https://openalex.org/W2773956126', 'https://openalex.org/W2892205701', 'https://openalex.org/W2896457183', 'https://openalex.org/W2903193068', 'https://openalex.org/W2912351236', 'https://openalex.org/W2914120296', 'https://openalex.org/W2914924671', 'https://openalex.org/W2946417913', 'https://openalex.org/W2946794439', 'https://openalex.org/W2951984970', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962777840', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963261262', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963503967', 'https://openalex.org/W2963651521', 'https://openalex.org/W2963759780', 'https://openalex.org/W3015703505', 'https://openalex.org/W3099668342', 'https://openalex.org/W3102226577', 'https://openalex.org/W4237723258', 'https://openalex.org/W4288351520', 'https://openalex.org/W4294103325', 'https://openalex.org/W4297730150', 'https://openalex.org/W4297749952', 'https://openalex.org/W4385245566']","We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We chose the Transformers for our analysis as they have been shown effective with various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and observe that the choice of the objective determines this process. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.",0.996078431372549
SKG_MT_1074,https://openalex.org/W1736600331,2010,59,"['https://openalex.org/W30655990', 'https://openalex.org/W1498238796', 'https://openalex.org/W1631260214', 'https://openalex.org/W1969974515', 'https://openalex.org/W1989348531', 'https://openalex.org/W2005712418', 'https://openalex.org/W2040781285', 'https://openalex.org/W2061910127', 'https://openalex.org/W2098032362', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105891181', 'https://openalex.org/W2107895803', 'https://openalex.org/W2117745860', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125536435', 'https://openalex.org/W2133768083', 'https://openalex.org/W2140343992', 'https://openalex.org/W2143291137', 'https://openalex.org/W2144879357', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150378737', 'https://openalex.org/W2150903784', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158388102', 'https://openalex.org/W2165199647', 'https://openalex.org/W2169724380', 'https://openalex.org/W2171421863', 'https://openalex.org/W2242975712', 'https://openalex.org/W2437005631']","A principal weakness of conventional (i.e., non-hierarchical) phrase-based statistical machine translation is that it can only exploit continuous phrases. In this paper, we extend phrase-based decoding to allow both source and target phrasal discontinuities, which provide better generalization on unseen data and yield significant improvements to a standard phrase-based system (Moses). More interestingly, our discontinuous phrasebased system also outperforms a state-of-the-art hierarchical system (Joshua) by a very significant margin (+1.03 BLEU on average on five Chinese-English NIST test sets), even though both Joshua and our system support discontinuous phrases. Since the key difference between these two systems is that ours is not hierarchical—i.e., our system uses a string-based decoder instead of CKY, and it imposes no hard hierarchical reordering constraints during training and decoding—this paper sets out to challenge the commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses. 1",1.0
SKG_MT_1076,https://openalex.org/W3037879901,2020,5,"['https://openalex.org/W1524786316', 'https://openalex.org/W1667342666', 'https://openalex.org/W1986345088', 'https://openalex.org/W2128070455', 'https://openalex.org/W2134670378', 'https://openalex.org/W2250599277', 'https://openalex.org/W2251375842', 'https://openalex.org/W2739766884', 'https://openalex.org/W2774332521', 'https://openalex.org/W2799493995', 'https://openalex.org/W2920839365', 'https://openalex.org/W2923362077', 'https://openalex.org/W2942274440', 'https://openalex.org/W2970587490', 'https://openalex.org/W3034673518', 'https://openalex.org/W3097330605', 'https://openalex.org/W3102818226', 'https://openalex.org/W3185596593', 'https://openalex.org/W3186483169', 'https://openalex.org/W4288262060']","Nico Herbig, Santanu Pal, Tim Düwel, Kalliopi Meladaki, Mahsa Monshizadeh, Vladislav Hnatovskiy, Antonio Krüger, Josef van Genabith. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2020.",0.9959183673469387
SKG_MT_1077,https://openalex.org/W2137698233,2012,138,"['https://openalex.org/W22168010', 'https://openalex.org/W64777181', 'https://openalex.org/W145987047', 'https://openalex.org/W265531733', 'https://openalex.org/W635530177', 'https://openalex.org/W1491049622', 'https://openalex.org/W1596675947', 'https://openalex.org/W1625582487', 'https://openalex.org/W1631260214', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W1989658336', 'https://openalex.org/W1994581546', 'https://openalex.org/W2000359198', 'https://openalex.org/W2006969979', 'https://openalex.org/W2054399842', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115410424', 'https://openalex.org/W2116492146', 'https://openalex.org/W2117278770', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132001515', 'https://openalex.org/W2137387514', 'https://openalex.org/W2144600658', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2170204377', 'https://openalex.org/W2280403519', 'https://openalex.org/W2399188371', 'https://openalex.org/W3104029765', 'https://openalex.org/W3211848854']","We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT). While techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately. We also explore adapting multiple (4–10) data sets with no a priori distinction between in-domain and out-of-domain data except for an in-domain development set.",1.0
SKG_MT_1078,https://openalex.org/W3110945136,2020,7,"['https://openalex.org/W1902237438', 'https://openalex.org/W2118434577', 'https://openalex.org/W2531207078', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561274697', 'https://openalex.org/W2577335011', 'https://openalex.org/W2742141468', 'https://openalex.org/W2770394828', 'https://openalex.org/W2889326796', 'https://openalex.org/W2891924676', 'https://openalex.org/W2919290281', 'https://openalex.org/W2949888546', 'https://openalex.org/W2949973181', 'https://openalex.org/W2952153923', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970311224', 'https://openalex.org/W2982866705', 'https://openalex.org/W3035144493']","Prior works have demonstrated that a low-resource language pair can benefit from multilingual machine translation (MT) systems, which rely on many language pairs' joint training. This paper proposes two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese and English-Vietnamese. The first strategy is about dynamical learning word similarity of tokens in the shared space among source languages while another one attempts to augment the translation ability of rare words through updating their embeddings during the training. Besides, we leverage monolingual data for multilingual MT systems to increase the amount of synthetic parallel corpora while dealing with the data sparsity problem. We have shown significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and released our datasets for the research community.",0.9904761904761905
SKG_MT_1080,https://openalex.org/W2422843715,2019,96,"['https://openalex.org/W71795751', 'https://openalex.org/W1631260214', 'https://openalex.org/W1753482797', 'https://openalex.org/W1915251500', 'https://openalex.org/W2006969979', 'https://openalex.org/W2064675550', 'https://openalex.org/W2100664567', 'https://openalex.org/W2108501770', 'https://openalex.org/W2109926133', 'https://openalex.org/W2118434577', 'https://openalex.org/W2121745180', 'https://openalex.org/W2122270629', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145094598', 'https://openalex.org/W2145662801', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2251560973', 'https://openalex.org/W2284660317', 'https://openalex.org/W2533887179', 'https://openalex.org/W2949416428', 'https://openalex.org/W2949888546', 'https://openalex.org/W2963216553', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W6677129709', 'https://openalex.org/W6679436768', 'https://openalex.org/W6898505805']",,1.0
SKG_MT_1082,https://openalex.org/W2135598948,2013,39,"['https://openalex.org/W88081813', 'https://openalex.org/W94670513', 'https://openalex.org/W99445809', 'https://openalex.org/W192736094', 'https://openalex.org/W1586073462', 'https://openalex.org/W1604437366', 'https://openalex.org/W1631260214', 'https://openalex.org/W1868971014', 'https://openalex.org/W1997060558', 'https://openalex.org/W2053966956', 'https://openalex.org/W2097478551', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101200183', 'https://openalex.org/W2113538861', 'https://openalex.org/W2119759918', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133503566', 'https://openalex.org/W2146867136', 'https://openalex.org/W2147880316', 'https://openalex.org/W2152374779', 'https://openalex.org/W2153848201', 'https://openalex.org/W2155461687', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158823144', 'https://openalex.org/W2160637503', 'https://openalex.org/W2163942301', 'https://openalex.org/W2166116787', 'https://openalex.org/W2180952760', 'https://openalex.org/W2250307271', 'https://openalex.org/W2250434667', 'https://openalex.org/W2895810819', 'https://openalex.org/W2914314925', 'https://openalex.org/W2917807695', 'https://openalex.org/W3213193971']","Social media texts are written in an informal style, which hinders other natural language processing (NLP) applications such as machine translation. Text normalization is thus important for processing of social media text. Previous work mostly focused on normalizing words by replacing an informal word with its formal form. In this paper, to further improve other downstream NLP applications, we argue that other normalization operations should also be performed, e.g., missing word recovery and punctuation correction. A novel beam-search decoder is proposed to effectively integrate various normalization operations. Empirical results show that our system obtains statistically significant improvements over two strong baselines in both normalization and translation tasks, for both Chinese and English.",1.0
SKG_MT_1083,https://openalex.org/W2919188216,2019,66,"['https://openalex.org/W2145339207', 'https://openalex.org/W2147262247', 'https://openalex.org/W2296073425', 'https://openalex.org/W2567571499', 'https://openalex.org/W2740718109', 'https://openalex.org/W2741838462', 'https://openalex.org/W2750588180', 'https://openalex.org/W2756978580', 'https://openalex.org/W2891089320', 'https://openalex.org/W2898846200', 'https://openalex.org/W2899181341', 'https://openalex.org/W2923622379', 'https://openalex.org/W2928941594', 'https://openalex.org/W2946379889', 'https://openalex.org/W2962890089', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963962154', 'https://openalex.org/W2964005754', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964327384', 'https://openalex.org/W4298018319', 'https://openalex.org/W4303683898']","Gaurav Kumar, George Foster, Colin Cherry, Maxim Krikun. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_1085,https://openalex.org/W1573488949,2015,10,"['https://openalex.org/W36903255', 'https://openalex.org/W141304052', 'https://openalex.org/W932413789', 'https://openalex.org/W1965154800', 'https://openalex.org/W1970689298', 'https://openalex.org/W2060108852', 'https://openalex.org/W2083545877', 'https://openalex.org/W2089745520', 'https://openalex.org/W2121227244', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131462252', 'https://openalex.org/W2132339004', 'https://openalex.org/W2134800885', 'https://openalex.org/W2152808281', 'https://openalex.org/W2158049734', 'https://openalex.org/W2158195707', 'https://openalex.org/W2171928131', 'https://openalex.org/W2172140247', 'https://openalex.org/W2250489405', 'https://openalex.org/W2251071050', 'https://openalex.org/W2251682575', 'https://openalex.org/W2949679234', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950075229', 'https://openalex.org/W2950797609', 'https://openalex.org/W2951714314']","This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difficult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the trade-offs between neural models and back-off n-gram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.",1.0
SKG_MT_1087,https://openalex.org/W2888808532,2018,85,"['https://openalex.org/W1614298861', 'https://openalex.org/W1902237438', 'https://openalex.org/W1905522558', 'https://openalex.org/W2097927681', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136156618', 'https://openalex.org/W2517196708', 'https://openalex.org/W2597891111', 'https://openalex.org/W2740433069', 'https://openalex.org/W2760656271', 'https://openalex.org/W2770394828', 'https://openalex.org/W2797913374', 'https://openalex.org/W2903193068', 'https://openalex.org/W2916864519', 'https://openalex.org/W2950577311', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963708445', 'https://openalex.org/W2963780864', 'https://openalex.org/W2963919854', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W3098341425', 'https://openalex.org/W3203064768']","Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.",1.0
SKG_MT_1088,https://openalex.org/W2989476591,2019,4,"['https://openalex.org/W21006490', 'https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W1899794420', 'https://openalex.org/W1902237438', 'https://openalex.org/W1938755728', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2137807925', 'https://openalex.org/W2138621090', 'https://openalex.org/W2153579005', 'https://openalex.org/W2158899491', 'https://openalex.org/W2242874043', 'https://openalex.org/W2250646737', 'https://openalex.org/W2493916176', 'https://openalex.org/W2539338396', 'https://openalex.org/W2737608545', 'https://openalex.org/W2755957574', 'https://openalex.org/W2888844359', 'https://openalex.org/W2889574253', 'https://openalex.org/W2899682268', 'https://openalex.org/W2904683980', 'https://openalex.org/W2905927205', 'https://openalex.org/W2951559648', 'https://openalex.org/W2952230511', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962779279', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962902328', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963421945', 'https://openalex.org/W2963656735', 'https://openalex.org/W2963702081', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W3099206234', 'https://openalex.org/W4294170691', 'https://openalex.org/W4299579390']","Neural models that eliminate the softmax bottleneck by generating word embeddings (rather than multinomial distributions over a vocabulary) attain faster training with fewer learnable parameters. These models are currently trained by maximizing densities of pretrained target embeddings under von Mises-Fisher distributions parameterized by corresponding model-predicted embeddings. This work explores the utility of margin-based loss functions in optimizing such models. We present syn-margin loss, a novel margin-based loss that uses a synthetic negative sample constructed from only the predicted and target embeddings at every step. The loss is efficient to compute, and we use a geometric analysis to argue that it is more consistent and interpretable than other margin-based losses. Empirically, we find that syn-margin provides small but significant improvements over both vMF and standard margin-based losses in continuous-output neural machine translation.",1.0
SKG_MT_1090,https://openalex.org/W2250489405,2013,258,"['https://openalex.org/W179875071', 'https://openalex.org/W222053410', 'https://openalex.org/W1575384945', 'https://openalex.org/W1675954498', 'https://openalex.org/W1815076433', 'https://openalex.org/W1965154800', 'https://openalex.org/W1999965501', 'https://openalex.org/W2013540053', 'https://openalex.org/W2020382207', 'https://openalex.org/W2026383756', 'https://openalex.org/W2058695628', 'https://openalex.org/W2100714283', 'https://openalex.org/W2105891181', 'https://openalex.org/W2110415041', 'https://openalex.org/W2121227244', 'https://openalex.org/W2124807415', 'https://openalex.org/W2141599568', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147152072', 'https://openalex.org/W2153653739', 'https://openalex.org/W2171928131', 'https://openalex.org/W2176797192', 'https://openalex.org/W2250379827', 'https://openalex.org/W2251098065', 'https://openalex.org/W2296386938', 'https://openalex.org/W2296491785', 'https://openalex.org/W2766736793']","We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets.",1.0
SKG_MT_1094,https://openalex.org/W3103840770,2020,5,"['https://openalex.org/W66597527', 'https://openalex.org/W2101105183', 'https://openalex.org/W2133459682', 'https://openalex.org/W2250421192', 'https://openalex.org/W2250597803', 'https://openalex.org/W2251926178', 'https://openalex.org/W2252166243', 'https://openalex.org/W2257408573', 'https://openalex.org/W2260677151', 'https://openalex.org/W2621975677', 'https://openalex.org/W2903193068', 'https://openalex.org/W2962798274', 'https://openalex.org/W2963341956', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970641574', 'https://openalex.org/W2970791445', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971120958', 'https://openalex.org/W2971148473', 'https://openalex.org/W2983040767', 'https://openalex.org/W3035390927']","The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of translation quality as training data. Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations. In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.",1.0
SKG_MT_1096,https://openalex.org/W99956235,2014,19,"['https://openalex.org/W10376690', 'https://openalex.org/W651894412', 'https://openalex.org/W1631260214', 'https://openalex.org/W1632114991', 'https://openalex.org/W1748393397', 'https://openalex.org/W1865009630', 'https://openalex.org/W1892207274', 'https://openalex.org/W1964357740', 'https://openalex.org/W1975133852', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008652694', 'https://openalex.org/W2018869373', 'https://openalex.org/W2056132907', 'https://openalex.org/W2060786818', 'https://openalex.org/W2078861931', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115990601', 'https://openalex.org/W2118296893', 'https://openalex.org/W2123318312', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126400076', 'https://openalex.org/W2126747756', 'https://openalex.org/W2130765257', 'https://openalex.org/W2137894131', 'https://openalex.org/W2138302120', 'https://openalex.org/W2141799614', 'https://openalex.org/W2143426320', 'https://openalex.org/W2149327368', 'https://openalex.org/W2158732077', 'https://openalex.org/W2160001241', 'https://openalex.org/W2182468807', 'https://openalex.org/W2251869843', 'https://openalex.org/W2251994258', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2595715041', 'https://openalex.org/W2604377206']","We use referential translation machines (RTM) for quality estimation of translation outputs. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpre-tants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs achieve top performance in automatic, ac-curate, and language independent predic-tion of sentence-level and word-level sta-tistical machine translation (SMT) qual-ity. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations and achieve the top performance in WMT13 quality estimation task (QET13). We im-prove our RTM models with the Parallel FDA5 instance selection model, with ad-ditional features for predicting the trans-lation performance, and with improved learning models. We develop RTM mod-els for each WMT14 QET (QET14) sub-task, obtain improvements over QET13 re-sults, and rank 1st in all of the tasks and subtasks of QET14. 1",1.0
SKG_MT_1099,https://openalex.org/W2964003477,2017,14,"['https://openalex.org/W1514535095', 'https://openalex.org/W1779224757', 'https://openalex.org/W1843891098', 'https://openalex.org/W1861492603', 'https://openalex.org/W1895577753', 'https://openalex.org/W1902237438', 'https://openalex.org/W1947758080', 'https://openalex.org/W2070150502', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2139079654', 'https://openalex.org/W2149327368', 'https://openalex.org/W2194775991', 'https://openalex.org/W2247931231', 'https://openalex.org/W2574872930', 'https://openalex.org/W2579653291', 'https://openalex.org/W2949335953', 'https://openalex.org/W2951176429', 'https://openalex.org/W2952122856', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963909453', 'https://openalex.org/W2964049455']","This paper describes Oregon State University's submissions to the shared WMT'17 task ""multimodal translation task I"".In this task, all the sentence pairs are image captions in different languages.The key difference between this task and conventional machine translation is that we have corresponding images as additional information for each sentence pair.In this paper, we introduce a simple but effective system which takes an image shared between different languages, feeding it into the both encoding and decoding side.We report our system's performance for English-French and English-German with Flickr30K (in-domain) and MSCOCO (out-ofdomain) datasets.Our system achieves the best performance in TER for English-German for MSCOCO dataset.",1.0
SKG_MT_1101,https://openalex.org/W2251690405,2013,31,"['https://openalex.org/W91928571', 'https://openalex.org/W98255950', 'https://openalex.org/W179875071', 'https://openalex.org/W222053410', 'https://openalex.org/W1423339008', 'https://openalex.org/W1498238796', 'https://openalex.org/W1554663460', 'https://openalex.org/W1631260214', 'https://openalex.org/W1973923101', 'https://openalex.org/W2006969979', 'https://openalex.org/W2008769302', 'https://openalex.org/W2016518303', 'https://openalex.org/W2101044864', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102017903', 'https://openalex.org/W2104327354', 'https://openalex.org/W2111142112', 'https://openalex.org/W2117130368', 'https://openalex.org/W2118960083', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132339004', 'https://openalex.org/W2139671364', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2158139315', 'https://openalex.org/W2162430620', 'https://openalex.org/W2170686170', 'https://openalex.org/W2180952760', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251222643', 'https://openalex.org/W2437005631', 'https://openalex.org/W2916679805', 'https://openalex.org/W3171974722']","Most statistical machine translation (SMT) systems are modeled using a log-linear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embed-ding features on Chinese-to-English and Japanese-to-English translation tasks.",1.0
SKG_MT_1102,https://openalex.org/W2963223057,2018,53,"['https://openalex.org/W222053410', 'https://openalex.org/W1902237438', 'https://openalex.org/W2045767629', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250548645', 'https://openalex.org/W2251131401', 'https://openalex.org/W2525778437', 'https://openalex.org/W2576482813', 'https://openalex.org/W2731114144', 'https://openalex.org/W2767521466', 'https://openalex.org/W2774784811', 'https://openalex.org/W2800936852', 'https://openalex.org/W2807253849', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964296923', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964325863', 'https://openalex.org/W4385245566']","Recent neural machine translation (NMT) systems have been greatly improved by encoder-decoder models with attention mechanisms and sub-word units. However, important differences between languages with logographic and alphabetic writing systems have long been overlooked. This study focuses on these differences and uses a simple approach to improve the performance of NMT systems utilizing decomposed sub-character level information for logographic languages. Our results indicate that our approach not only improves the translation capabilities of NMT systems between Chinese and English, but also further improves NMT systems between Chinese and Japanese, because it utilizes the shared information brought by similar sub-character units.",1.0
SKG_MT_1103,https://openalex.org/W2740295203,2017,17,"['https://openalex.org/W16967297', 'https://openalex.org/W222053410', 'https://openalex.org/W1982895372', 'https://openalex.org/W2080373976', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113066332', 'https://openalex.org/W2113549939', 'https://openalex.org/W2115081467', 'https://openalex.org/W2124807415', 'https://openalex.org/W2141971526', 'https://openalex.org/W2144091461', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149327368', 'https://openalex.org/W2167375373', 'https://openalex.org/W2169423212', 'https://openalex.org/W2250872883', 'https://openalex.org/W2251798853', 'https://openalex.org/W2251816503', 'https://openalex.org/W2356613612', 'https://openalex.org/W2405897494', 'https://openalex.org/W2510492408', 'https://openalex.org/W2512924740', 'https://openalex.org/W2575583396', 'https://openalex.org/W2595715041', 'https://openalex.org/W2797922696', 'https://openalex.org/W2892357930', 'https://openalex.org/W2906926620', 'https://openalex.org/W2963816901', 'https://openalex.org/W3170166771', 'https://openalex.org/W3203021515', 'https://openalex.org/W3204864772', 'https://openalex.org/W3214125373', 'https://openalex.org/W4245882962', 'https://openalex.org/W4285719527']","Rajen Chatterjee, Gebremedhen Gebremelak, Matteo Negri, Marco Turchi. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. 2017.",1.0
SKG_MT_1104,https://openalex.org/W4245202262,2020,59,"['https://openalex.org/W630532510', 'https://openalex.org/W832270446', 'https://openalex.org/W1948141881', 'https://openalex.org/W2057235967', 'https://openalex.org/W2114556561', 'https://openalex.org/W2126712675', 'https://openalex.org/W2148708890', 'https://openalex.org/W2158874082', 'https://openalex.org/W2182922969', 'https://openalex.org/W2250585411', 'https://openalex.org/W2467834614', 'https://openalex.org/W2532807140', 'https://openalex.org/W2605035112', 'https://openalex.org/W2757592053', 'https://openalex.org/W2788330850', 'https://openalex.org/W2805394970', 'https://openalex.org/W2806156201', 'https://openalex.org/W2951770285', 'https://openalex.org/W2952317054', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963829526', 'https://openalex.org/W2963841178', 'https://openalex.org/W2998702515', 'https://openalex.org/W3088348768', 'https://openalex.org/W3208836290', 'https://openalex.org/W4288025890', 'https://openalex.org/W4385245566']","This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with ""copy"" information while translations based on embedding similarities tend to extend the translation ""context"". Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.",1.0
SKG_MT_1105,https://openalex.org/W2293344577,2016,95,"['https://openalex.org/W22168010', 'https://openalex.org/W68733909', 'https://openalex.org/W149643677', 'https://openalex.org/W1749884237', 'https://openalex.org/W1861492603', 'https://openalex.org/W1895577753', 'https://openalex.org/W1905882502', 'https://openalex.org/W1931639407', 'https://openalex.org/W1975517671', 'https://openalex.org/W2006186569', 'https://openalex.org/W2031489346', 'https://openalex.org/W2095569536', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102605133', 'https://openalex.org/W2117539524', 'https://openalex.org/W2119775030', 'https://openalex.org/W2132217790', 'https://openalex.org/W2134800885', 'https://openalex.org/W2141440284', 'https://openalex.org/W2144211451', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149557440', 'https://openalex.org/W2155607551', 'https://openalex.org/W2164766438', 'https://openalex.org/W2170716095', 'https://openalex.org/W2171421863', 'https://openalex.org/W2247931231', 'https://openalex.org/W2250742840', 'https://openalex.org/W2250976127', 'https://openalex.org/W2251017614', 'https://openalex.org/W2252200119', 'https://openalex.org/W2437005631', 'https://openalex.org/W2735359189', 'https://openalex.org/W2950178297', 'https://openalex.org/W2951548327', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963881583', 'https://openalex.org/W2964308564', 'https://openalex.org/W4285719527']","We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolingually captioned images, and on state-of-the-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.",1.0
SKG_MT_1106,https://openalex.org/W2899181341,2018,33,"['https://openalex.org/W1522301498', 'https://openalex.org/W1850531616', 'https://openalex.org/W1902237438', 'https://openalex.org/W1915251500', 'https://openalex.org/W2044493620', 'https://openalex.org/W2100664567', 'https://openalex.org/W2105410942', 'https://openalex.org/W2131429577', 'https://openalex.org/W2161943765', 'https://openalex.org/W2171671120', 'https://openalex.org/W2270364989', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2594021297', 'https://openalex.org/W2740275380', 'https://openalex.org/W2785324569', 'https://openalex.org/W2798587560', 'https://openalex.org/W2951025196', 'https://openalex.org/W2952190837', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2962955856', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963696295', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964121744', 'https://openalex.org/W4297664295', 'https://openalex.org/W4298393544']","Traditional active learning (AL) methods for machine translation (MT) rely on heuristics. However, these heuristics are limited when the characteristics of the MT problem change due to e.g. the language pair or the amount of the initial bitext. In this paper, we present a framework to learn sentence selection strategies for neural MT. We train the AL query strategy using a high-resource language-pair based on AL simulations, and then transfer it to the low-resource language-pair of interest. The learned query strategy capitalizes on the shared characteristics between the language pairs to make an effective use of the AL budget. Our experiments on three language-pairs confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions.",1.0
SKG_MT_1107,https://openalex.org/W2795429538,2018,18,"['https://openalex.org/W6908809', 'https://openalex.org/W1753482797', 'https://openalex.org/W1793121960', 'https://openalex.org/W1902237438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2087165009', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2178903040', 'https://openalex.org/W2194775991', 'https://openalex.org/W2257408573', 'https://openalex.org/W2267186426', 'https://openalex.org/W2342395274', 'https://openalex.org/W2345668077', 'https://openalex.org/W2384495648', 'https://openalex.org/W2394571815', 'https://openalex.org/W2409591106', 'https://openalex.org/W2415583245', 'https://openalex.org/W2460354904', 'https://openalex.org/W2512924740', 'https://openalex.org/W2564089970', 'https://openalex.org/W2595715041', 'https://openalex.org/W2597015119', 'https://openalex.org/W2597655663', 'https://openalex.org/W2613904329', 'https://openalex.org/W2618101654', 'https://openalex.org/W2737638662', 'https://openalex.org/W2754560064', 'https://openalex.org/W2755908707', 'https://openalex.org/W2755989362', 'https://openalex.org/W2949335953', 'https://openalex.org/W2949888546', 'https://openalex.org/W2951008357', 'https://openalex.org/W2952136670', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962954913', 'https://openalex.org/W2963077125', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963448850', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963770546', 'https://openalex.org/W2963899396', 'https://openalex.org/W2963917928', 'https://openalex.org/W2963983719', 'https://openalex.org/W2964084166', 'https://openalex.org/W2964125283', 'https://openalex.org/W2964308564', 'https://openalex.org/W4233432642', 'https://openalex.org/W4285719527', 'https://openalex.org/W4294555862', 'https://openalex.org/W4297692367', 'https://openalex.org/W4297779254', 'https://openalex.org/W4385245566']","Neural sequence-to-sequence networks with attention have achieved remarkable\nperformance for machine translation. One of the reasons for their effectiveness\nis their ability to capture relevant source-side contextual information at each\ntime-step prediction through an attention mechanism. However, the target-side\ncontext is solely based on the sequence model which, in practice, is prone to a\nrecency bias and lacks the ability to capture effectively non-sequential\ndependencies among words. To address this limitation, we propose a\ntarget-side-attentive residual recurrent network for decoding, where attention\nover previous words contributes directly to the prediction of the next word.\nThe residual learning facilitates the flow of information from the distant past\nand is able to emphasize any of the previously translated words, hence it gains\naccess to a wider context. The proposed model outperforms a neural MT baseline\nas well as a memory and self-attention network on three language pairs. The\nanalysis of the attention learned by the decoder confirms that it emphasizes a\nwider context, and that it captures syntactic-like structures.\n",1.0
SKG_MT_1108,https://openalex.org/W2181989146,2015,6,"['https://openalex.org/W11511616', 'https://openalex.org/W22168010', 'https://openalex.org/W42707549', 'https://openalex.org/W204945112', 'https://openalex.org/W222053410', 'https://openalex.org/W1606076566', 'https://openalex.org/W2018967998', 'https://openalex.org/W2033593667', 'https://openalex.org/W2096389047', 'https://openalex.org/W2109664771', 'https://openalex.org/W2113788796', 'https://openalex.org/W2115008667', 'https://openalex.org/W2144600658', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2159705183', 'https://openalex.org/W2159712545', 'https://openalex.org/W2184135559', 'https://openalex.org/W2250548645', 'https://openalex.org/W2437005631', 'https://openalex.org/W2626190081', 'https://openalex.org/W4241645538']","We propose a method for simultaneously translating from a single source language to multiple target languages T1, T2, etc.The motivation behind this method is that if we only have a weak language model for T1 and translations in T1 and T2 are associated, we can use the information from a strong language model over T2 to disambiguate the translations in T1, providing better translation results.As a specific framework to realize multi-target translation, we expand the formalism of synchronous context-free grammars to handle multiple targets, and describe methods for rule extraction, scoring, pruning, and search with these models.Experiments find that multi-target translation with a strong language model in a similar second target language can provide gains of up to 0.8-1.5 BLEU points. 1",1.0
SKG_MT_1109,https://openalex.org/W2807480793,2018,139,"['https://openalex.org/W102708294', 'https://openalex.org/W1426956448', 'https://openalex.org/W1522301498', 'https://openalex.org/W1614298861', 'https://openalex.org/W1686810756', 'https://openalex.org/W1854214752', 'https://openalex.org/W2094728533', 'https://openalex.org/W2095705004', 'https://openalex.org/W2108598243', 'https://openalex.org/W2123024445', 'https://openalex.org/W2125786288', 'https://openalex.org/W2127426251', 'https://openalex.org/W2127795553', 'https://openalex.org/W2184957013', 'https://openalex.org/W2250539671', 'https://openalex.org/W2283196293', 'https://openalex.org/W2470413457', 'https://openalex.org/W2526174222', 'https://openalex.org/W2568389463', 'https://openalex.org/W2759136286', 'https://openalex.org/W2950577311', 'https://openalex.org/W2963173190', 'https://openalex.org/W2963606136', 'https://openalex.org/W2964121744', 'https://openalex.org/W4239072543']","Current methods for knowledge graph (KG) representation learning focus solely on the structure of the KG and do not exploit any kind of external information, such as visual and linguistic information corresponding to the KG entities. In this paper, we propose a multimodal translation-based approach that defines the energy of a KG triple as the sum of sub-energy functions that leverage both multimodal (visual and linguistic) and structural KG representations. Next, a ranking-based loss is minimized using a simple neural network architecture. Moreover, we introduce a new large-scale dataset for multimodal KG representation learning. We compared the performance of our approach to other baselines on two standard tasks, namely knowledge graph completion and triple classification, using our as well as the WN9-IMG dataset. The results demonstrate that our approach outperforms all baselines on both tasks and datasets.",1.0
SKG_MT_1110,https://openalex.org/W2890501761,2018,79,"['https://openalex.org/W1522301498', 'https://openalex.org/W1821462560', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2194775991', 'https://openalex.org/W2251994258', 'https://openalex.org/W2311921240', 'https://openalex.org/W2402144811', 'https://openalex.org/W2519091744', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2613904329', 'https://openalex.org/W2740087922', 'https://openalex.org/W2767206889', 'https://openalex.org/W2769810959', 'https://openalex.org/W2789543585', 'https://openalex.org/W2949382160', 'https://openalex.org/W2953384591', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963069107', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964084720', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W4294619240', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Existing approaches to neural machine translation are typically autoregressive models. While these models attain state-of-the-art translation quality, they are suffering from low parallelizability and thus slow at decoding long sequences. In this paper, we propose a novel model for fast sequence generation — the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive property in global but relieves in local and thus are able to produce multiple successive words in parallel at each time step. Experiments conducted on English-German and Chinese-English translation tasks show that the SAT achieves a good balance between translation quality and decoding speed. On WMT’14 English-German translation, the SAT achieves 5.58× speedup while maintaining 88% translation quality, significantly better than the previous non-autoregressive methods. When produces two words at each time step, the SAT is almost lossless (only 1% degeneration in BLEU score).",1.0
SKG_MT_1111,https://openalex.org/W2165213242,2011,17,"['https://openalex.org/W1984417383', 'https://openalex.org/W2018869373', 'https://openalex.org/W2035720976', 'https://openalex.org/W2038721957', 'https://openalex.org/W2099607809', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116594867', 'https://openalex.org/W2129804798', 'https://openalex.org/W2143927888', 'https://openalex.org/W2145685230', 'https://openalex.org/W2156985047', 'https://openalex.org/W2162499915', 'https://openalex.org/W2164454850', 'https://openalex.org/W2169724380']",This paper describes the submission from the National University of Singapore to the WMT 2011 Shared Evaluation Task and the Tunable Metric Task. Our entry is TESLA in three different,0.9915966386554622
SKG_MT_1113,https://openalex.org/W2962697716,2018,49,"['https://openalex.org/W189413142', 'https://openalex.org/W222053410', 'https://openalex.org/W1902237438', 'https://openalex.org/W1924770834', 'https://openalex.org/W2003374509', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101210369', 'https://openalex.org/W2101293500', 'https://openalex.org/W2101454539', 'https://openalex.org/W2106565589', 'https://openalex.org/W2114912785', 'https://openalex.org/W2120615054', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157025692', 'https://openalex.org/W2219047839', 'https://openalex.org/W2247305181', 'https://openalex.org/W2250473257', 'https://openalex.org/W2250954350', 'https://openalex.org/W2251537235', 'https://openalex.org/W2258247613', 'https://openalex.org/W2316153475', 'https://openalex.org/W2436001372', 'https://openalex.org/W2550186622', 'https://openalex.org/W2594990650', 'https://openalex.org/W2595715041', 'https://openalex.org/W2737175496', 'https://openalex.org/W2759173152', 'https://openalex.org/W2949743947', 'https://openalex.org/W2951271661', 'https://openalex.org/W2951368271', 'https://openalex.org/W2962907349', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963277143', 'https://openalex.org/W2963641561', 'https://openalex.org/W2964189868', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4297771995']","Frederick Liu, Han Lu, Graham Neubig. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",1.0
SKG_MT_1118,https://openalex.org/W2294727331,2015,2,"['https://openalex.org/W1852713323', 'https://openalex.org/W1905522558', 'https://openalex.org/W2018869373', 'https://openalex.org/W2057235967', 'https://openalex.org/W2062346908', 'https://openalex.org/W2083545877', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2149327368', 'https://openalex.org/W2399981156', 'https://openalex.org/W2595715041', 'https://openalex.org/W2962978267', 'https://openalex.org/W3004726606', 'https://openalex.org/W6773166339', 'https://openalex.org/W6898505805']",© 2015 The Authors. Published by Association for Computational Linguistics . This is an open access article available under a Creative Commons licence. &#13;\nThe published version can be accessed at the following link on the publisher’s website: https://www.aclweb.org/anthology/N15-1103,1.0
SKG_MT_1120,https://openalex.org/W2251466960,2014,6,"['https://openalex.org/W45351999', 'https://openalex.org/W201532657', 'https://openalex.org/W1509213251', 'https://openalex.org/W1582482241', 'https://openalex.org/W1605282883', 'https://openalex.org/W1631260214', 'https://openalex.org/W1978161643', 'https://openalex.org/W2018482254', 'https://openalex.org/W2098603082', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105891181', 'https://openalex.org/W2111142112', 'https://openalex.org/W2113788796', 'https://openalex.org/W2117642127', 'https://openalex.org/W2117873652', 'https://openalex.org/W2118252968', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134285054', 'https://openalex.org/W2134729743', 'https://openalex.org/W2136094405', 'https://openalex.org/W2144600658', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152249239', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158468759', 'https://openalex.org/W2159755860', 'https://openalex.org/W2161227214', 'https://openalex.org/W2164529863', 'https://openalex.org/W2341118687']","Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation (SMT) involving morphologically complex languages. When translating into a segmented language, an extra step is required to desegment the output; previous studies have desegmented the 1-best output from the decoder. In this paper, we expand our translation options by desegmentingn-best lists or lattices. Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs, which allows for inclusion of features related to the desegmentation process, as well as an unsegmented language model (LM). We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation, showing significant improvements in translation quality over desegmentation of 1-best decoder outputs.",1.0
SKG_MT_1126,https://openalex.org/W3035376412,2020,18,"['https://openalex.org/W132913264', 'https://openalex.org/W2145094598', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2493916176', 'https://openalex.org/W2752172973', 'https://openalex.org/W2798931235', 'https://openalex.org/W2890007195', 'https://openalex.org/W2898785264', 'https://openalex.org/W2914120296', 'https://openalex.org/W2932618389', 'https://openalex.org/W2962735107', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963109507', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963443683', 'https://openalex.org/W2963469388', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963919854', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964266061', 'https://openalex.org/W4241645538', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390']","The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.",1.0
SKG_MT_1127,https://openalex.org/W2251926178,2014,66,"['https://openalex.org/W1966519405', 'https://openalex.org/W2019398887', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112163934', 'https://openalex.org/W2115259925', 'https://openalex.org/W2123301721', 'https://openalex.org/W2123724244', 'https://openalex.org/W2141766660', 'https://openalex.org/W2147192413', 'https://openalex.org/W2149327368', 'https://openalex.org/W2167007060', 'https://openalex.org/W2252166243', 'https://openalex.org/W2270190199', 'https://openalex.org/W2597815766']","Recent human evaluation of machine translation has focused on relative preference judgments of translation quality, making it difficult to track longitudinal improvements over time. We carry out a large-scale crowd-sourcing experiment to estimate the degree to which state-of-theart performance in machine translation has increased over the past five years. To facilitate longitudinal evaluation, we move away from relative preference judgments and instead ask human judges to provide direct estimates of the quality of individual translations in isolation from alternate outputs. For seven European language pairs, our evaluation estimates an average 10-point improvement to state-of-theart machine translation between 2007 and 2012, with Czech-to-English translation standing out as the language pair achieving most substantial gains. Our method of human evaluation offers an economically feasible and robust means of performing ongoing longitudinal evaluation of machine translation.",0.9894736842105263
SKG_MT_1128,https://openalex.org/W2984941342,2020,0,"['https://openalex.org/W22168010', 'https://openalex.org/W49707343', 'https://openalex.org/W1975879668', 'https://openalex.org/W2101105183', 'https://openalex.org/W2148365102', 'https://openalex.org/W2467834614', 'https://openalex.org/W2550821151', 'https://openalex.org/W2567571499', 'https://openalex.org/W2738588665', 'https://openalex.org/W2757920198', 'https://openalex.org/W2798362442', 'https://openalex.org/W2922158773', 'https://openalex.org/W2928941594', 'https://openalex.org/W2949938546', 'https://openalex.org/W2952103439', 'https://openalex.org/W2954218767', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963919854', 'https://openalex.org/W2967600676', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970038984', 'https://openalex.org/W2970328625', 'https://openalex.org/W2971043182', 'https://openalex.org/W2975153649', 'https://openalex.org/W2978824654', 'https://openalex.org/W2979202502', 'https://openalex.org/W2980367889', 'https://openalex.org/W3015810608', 'https://openalex.org/W3103450644', 'https://openalex.org/W3196290596', 'https://openalex.org/W3204406378']","Machine translation has an undesirable propensity to produce ""translationese"" artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train sentence-level classifiers to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model. Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency. Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality. We analyze these models using metrics to measure the degree of translationese in the output, and present an analysis of the capriciousness of heuristically-based train-data tagging.",1.0
SKG_MT_1129,https://openalex.org/W2252106864,2011,17,"['https://openalex.org/W21337280', 'https://openalex.org/W22168010', 'https://openalex.org/W1514971736', 'https://openalex.org/W1716250762', 'https://openalex.org/W1982816048', 'https://openalex.org/W2031535158', 'https://openalex.org/W2100271871', 'https://openalex.org/W2140671896', 'https://openalex.org/W2154124206', 'https://openalex.org/W2567948266']","State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework, where the knowledge of a human translator is combined with the MT system. We present a statistical IMT system able to learn from user feedback by means of the application of online learning techniques. These techniques allow the MT system to update the parameters of the underlying models in real time. According to empirical results, our system outperforms the results of conventional IMT systems. To the best of our knowledge, this online learning capability has never been provided by previous IMT systems. Our IMT system is implemented in C++, JavaScript, and ActionScript; and is publicly available on the Web. 1",1.0
SKG_MT_1132,https://openalex.org/W2165213242,2011,17,"['https://openalex.org/W1984417383', 'https://openalex.org/W2018869373', 'https://openalex.org/W2035720976', 'https://openalex.org/W2038721957', 'https://openalex.org/W2099607809', 'https://openalex.org/W2101105183', 'https://openalex.org/W2116594867', 'https://openalex.org/W2129804798', 'https://openalex.org/W2143927888', 'https://openalex.org/W2145685230', 'https://openalex.org/W2156985047', 'https://openalex.org/W2162499915', 'https://openalex.org/W2164454850', 'https://openalex.org/W2169724380']",This paper describes the submission from the National University of Singapore to the WMT 2011 Shared Evaluation Task and the Tunable Metric Task. Our entry is TESLA in three different,0.9915966386554622
SKG_MT_1134,https://openalex.org/W2945735543,2019,119,"['https://openalex.org/W2169610', 'https://openalex.org/W1965605789', 'https://openalex.org/W2079735306', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115081467', 'https://openalex.org/W2120735855', 'https://openalex.org/W2124807415', 'https://openalex.org/W2251044566', 'https://openalex.org/W2472403012', 'https://openalex.org/W2512924740', 'https://openalex.org/W2515295520', 'https://openalex.org/W2551396370', 'https://openalex.org/W2606974598', 'https://openalex.org/W2741049976', 'https://openalex.org/W2772735181', 'https://openalex.org/W2778814079', 'https://openalex.org/W2888519496', 'https://openalex.org/W2889326796', 'https://openalex.org/W2897820187', 'https://openalex.org/W2903193068', 'https://openalex.org/W2907630459', 'https://openalex.org/W2962718684', 'https://openalex.org/W2962721878', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963126845', 'https://openalex.org/W2963206148', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963241825', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963352809', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963545917', 'https://openalex.org/W2963655793', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963877622', 'https://openalex.org/W2963918774', 'https://openalex.org/W2964029788', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964165364', 'https://openalex.org/W2964212550', 'https://openalex.org/W2964213257', 'https://openalex.org/W3098341425', 'https://openalex.org/W3204406378']","J. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, Benjamin Van Durme. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_1135,https://openalex.org/W2915330159,2019,16,"['https://openalex.org/W2007780422', 'https://openalex.org/W2525032226', 'https://openalex.org/W2552839021', 'https://openalex.org/W2557436004', 'https://openalex.org/W2574790321', 'https://openalex.org/W2963109634', 'https://openalex.org/W2963318456', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963551569', 'https://openalex.org/W2964183327', 'https://openalex.org/W2964308564']","Reuben Cohn-Gordon, Noah Goodman. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",0.9915966386554622
SKG_MT_1136,https://openalex.org/W2949405462,2019,29,"['https://openalex.org/W8895266', 'https://openalex.org/W342285082', 'https://openalex.org/W658020064', 'https://openalex.org/W1542713999', 'https://openalex.org/W1828724394', 'https://openalex.org/W2105673178', 'https://openalex.org/W2126725946', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140903445', 'https://openalex.org/W2222512263', 'https://openalex.org/W2252212383', 'https://openalex.org/W2257408573', 'https://openalex.org/W2294774419', 'https://openalex.org/W2493916176', 'https://openalex.org/W2496235729', 'https://openalex.org/W2512924740', 'https://openalex.org/W2593864460', 'https://openalex.org/W2739740656', 'https://openalex.org/W2740534027', 'https://openalex.org/W2742155240', 'https://openalex.org/W2773493195', 'https://openalex.org/W2794365787', 'https://openalex.org/W2798389157', 'https://openalex.org/W2798931235', 'https://openalex.org/W2799245424', 'https://openalex.org/W2804625779', 'https://openalex.org/W2890007195', 'https://openalex.org/W2952037945', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963002901', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963472233', 'https://openalex.org/W2963602293', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964266061', 'https://openalex.org/W2964308564', 'https://openalex.org/W4292692470', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4299585995']","Mining parallel sentences from comparable corpora is important. Most previous work relies on supervised systems, which are trained on parallel data, thus their applicability is problematic in low-resource scenarios. Recent developments in building unsupervised bilingual word embeddings made it possible to mine parallel sentences based on cosine similarities of source and target language words. We show that relying only on this information is not enough, since sentences often have similar words but different meanings. We detect continuous parallel segments in sentence pair candidates and rely on them when mining parallel sentences. We show better mining accuracy on three language pairs in a standard shared task on artificial data. We also provide the first experiments showing that parallel sentences mined from real life sources improve unsupervised MT. Our code is available, we hope it will be used to support low-resource MT research.",1.0
SKG_MT_1138,https://openalex.org/W2339274804,2016,5,"['https://openalex.org/W2125055259', 'https://openalex.org/W2130942839', 'https://openalex.org/W2149837184', 'https://openalex.org/W2152446597', 'https://openalex.org/W2250375035', 'https://openalex.org/W2251171690', 'https://openalex.org/W2251609535', 'https://openalex.org/W2252123671', 'https://openalex.org/W2296308987', 'https://openalex.org/W2311921240', 'https://openalex.org/W2339995566', 'https://openalex.org/W2963292345', 'https://openalex.org/W2964308564']","Two extensions to the AMR smatch scoring script are presented. The first extension com-bines the smatch scoring script with the C6.0 rule-based classifier to produce a human-readable report on the error patterns frequency observed in the scored AMR graphs. This first extension results in 4% gain over the state-of-art CAMR baseline parser by adding to it a manually crafted wrapper fixing the identified CAMR parser errors. The second extension combines a per-sentence smatch with an en-semble method for selecting the best AMR graph among the set of AMR graphs for the same sentence. This second modification au-tomatically yields further 0.4% gain when ap-plied to outputs of two nondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level neural translation AMR parser. For AMR parsing task the character-level neural translation attains surprising 7% gain over the carefully optimized word-level neural translation. Overall, we achieve smatch F1=62% on the SemEval-2016 official scor-ing set and F1=67% on the LDC2015E86 test set.",0.9957805907172996
SKG_MT_1139,https://openalex.org/W2946379889,2019,122,"['https://openalex.org/W1522301498', 'https://openalex.org/W1848260265', 'https://openalex.org/W1902237438', 'https://openalex.org/W1905522558', 'https://openalex.org/W2115410424', 'https://openalex.org/W2116652448', 'https://openalex.org/W2117278770', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134800885', 'https://openalex.org/W2147262247', 'https://openalex.org/W2148708890', 'https://openalex.org/W2185701500', 'https://openalex.org/W2223152534', 'https://openalex.org/W2296073425', 'https://openalex.org/W2296338145', 'https://openalex.org/W2419539795', 'https://openalex.org/W2515631395', 'https://openalex.org/W2518037268', 'https://openalex.org/W2560647685', 'https://openalex.org/W2567571499', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740718109', 'https://openalex.org/W2741838462', 'https://openalex.org/W2744813330', 'https://openalex.org/W2751885040', 'https://openalex.org/W2756978580', 'https://openalex.org/W2760656271', 'https://openalex.org/W2773493195', 'https://openalex.org/W2778814079', 'https://openalex.org/W2785355717', 'https://openalex.org/W2794561587', 'https://openalex.org/W2805394970', 'https://openalex.org/W2886342729', 'https://openalex.org/W2898846200', 'https://openalex.org/W2903277914', 'https://openalex.org/W2919188216', 'https://openalex.org/W2923622379', 'https://openalex.org/W2939335894', 'https://openalex.org/W2945383715', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962890089', 'https://openalex.org/W2963366389', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963897095', 'https://openalex.org/W2964067969', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964122237', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964327384', 'https://openalex.org/W3082674894', 'https://openalex.org/W4298018319']","Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, Kevin Duh. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_1142,https://openalex.org/W3046089934,2020,2,"['https://openalex.org/W630532510', 'https://openalex.org/W2183341477', 'https://openalex.org/W2184135559', 'https://openalex.org/W2419539795', 'https://openalex.org/W2466062786', 'https://openalex.org/W2595715041', 'https://openalex.org/W2788277448', 'https://openalex.org/W2885514718', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949938546', 'https://openalex.org/W2952988558', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W3045622142', 'https://openalex.org/W4385245566']","We introduce our TMU system that is submitted to The 4th Workshop on Neural Generation and Translation (WNGT2020) to English-to-Japanese (En→Ja) track on Simultaneous Translation And Paraphrase for Language Education (STAPLE) shared task. In most cases machine translation systems generate a single output from the input sentence, however, in order to assist language learners in their journey with better and more diverse feedback, it is helpful to create a machine translation system that is able to produce diverse translations of each input sentence. However, creating such systems would require complex modifications in a model to ensure the diversity of outputs. In this paper, we investigated if it is possible to create such systems in a simple way and whether it can produce desired diverse outputs. In particular, we combined the outputs from forward and backward neural translation models (NMT). Our system achieved third place in En→Ja track, despite adopting only a simple approach.",1.0
SKG_MT_1143,https://openalex.org/W3106144205,2020,23,"['https://openalex.org/W630532510', 'https://openalex.org/W1522301498', 'https://openalex.org/W1915251500', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118434577', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153579005', 'https://openalex.org/W2220350356', 'https://openalex.org/W2294774419', 'https://openalex.org/W2567571499', 'https://openalex.org/W2576482813', 'https://openalex.org/W2744813330', 'https://openalex.org/W2756978580', 'https://openalex.org/W2758310181', 'https://openalex.org/W2760452458', 'https://openalex.org/W2786058094', 'https://openalex.org/W2886342729', 'https://openalex.org/W2933138175', 'https://openalex.org/W2949920209', 'https://openalex.org/W2949973181', 'https://openalex.org/W2950940239', 'https://openalex.org/W2954647460', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963697731', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970925270', 'https://openalex.org/W2985623695', 'https://openalex.org/W3035254119', 'https://openalex.org/W3104229680', 'https://openalex.org/W3204406378', 'https://openalex.org/W4294170691', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Neural network methods exhibit strong performance only in a few resource-rich domains. Practitioners therefore employ domain adaptation from resource-rich domains that are, in most cases, distant from the target domain. Domain adaptation between distant domains (e.g., movie subtitles and research papers), however, cannot be performed effectively due to mismatches in vocabulary; it will encounter many domain-specific words (e.g., “angstrom”) and words whose meanings shift across domains (e.g., “conductor”). In this study, aiming to solve these vocabulary mismatches in domain adaptation for neural machine translation (NMT), we propose vocabulary adaptation, a simple method for effective fine-tuning that adapts embedding layers in a given pretrained NMT model to the target domain. Prior to fine-tuning, our method replaces the embedding layers of the NMT model by projecting general word embeddings induced from monolingual data in a target domain onto a source-domain embedding space. Experimental results indicate that our method improves the performance of conventional fine-tuning by 3.86 and 3.28 BLEU points in En-Ja and De-En translation, respectively.",1.0
SKG_MT_1145,https://openalex.org/W3035072529,2020,66,"['https://openalex.org/W28749054', 'https://openalex.org/W1522301498', 'https://openalex.org/W1618905105', 'https://openalex.org/W1904365287', 'https://openalex.org/W1933798900', 'https://openalex.org/W2012942264', 'https://openalex.org/W2098824882', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107103101', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2149327368', 'https://openalex.org/W2159391028', 'https://openalex.org/W2176263492', 'https://openalex.org/W2183341477', 'https://openalex.org/W2188631158', 'https://openalex.org/W2254249950', 'https://openalex.org/W2626967530', 'https://openalex.org/W2790319220', 'https://openalex.org/W2889606145', 'https://openalex.org/W2918914336', 'https://openalex.org/W2948798935', 'https://openalex.org/W2950780363', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963551569', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964212410', 'https://openalex.org/W2964308564', 'https://openalex.org/W2968685800', 'https://openalex.org/W2971302374', 'https://openalex.org/W4232919069', 'https://openalex.org/W4288482469', 'https://openalex.org/W4297801368', 'https://openalex.org/W4385245566']","Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.",1.0
SKG_MT_1146,https://openalex.org/W3118324981,2020,9,"['https://openalex.org/W2101105183', 'https://openalex.org/W2117621558', 'https://openalex.org/W2152561112', 'https://openalex.org/W2250342921', 'https://openalex.org/W2496235729', 'https://openalex.org/W2508316494', 'https://openalex.org/W2508907594', 'https://openalex.org/W2740934986', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963014409', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963359165', 'https://openalex.org/W2964053711', 'https://openalex.org/W3006381853', 'https://openalex.org/W3029985558', 'https://openalex.org/W3082928416', 'https://openalex.org/W3089254180']","Translating to and from low-resource polysynthetic languages present numerous challenges for NMT. We present the results of our systems for the English--Inuktitut language pair for the WMT 2020 translation tasks. We investigated the importance of correct morphological segmentation, whether or not adding data from a related language (Greenlandic) helps, and whether using contextual word embeddings improves translation. While each method showed some promise, the results are mixed.",1.0
SKG_MT_1149,https://openalex.org/W3112593586,2020,7,"['https://openalex.org/W630532510', 'https://openalex.org/W1828724394', 'https://openalex.org/W2184135559', 'https://openalex.org/W2555745756', 'https://openalex.org/W2770394828', 'https://openalex.org/W2885950361', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921280978', 'https://openalex.org/W2952153923', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964007535', 'https://openalex.org/W3035207248', 'https://openalex.org/W3035464238']","Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions.&#13;\nHowever, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system.&#13;\nIn this paper, we investigate zero-shot performance of a multilingual EN&lt;-&gt;{FR,CS,DE,FI} system trained on WMT data.&#13;\nWe find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements.&#13;\nWe observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation.&#13;\nA recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single {\\em bridge} language, e.g.\\ English.&#13;\nWe find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions. &#13;\nWe show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.",1.0
SKG_MT_1150,https://openalex.org/W2970250638,2019,9,"['https://openalex.org/W560371024', 'https://openalex.org/W2101761627', 'https://openalex.org/W2574872930', 'https://openalex.org/W2579496717', 'https://openalex.org/W2744813330', 'https://openalex.org/W2756978580', 'https://openalex.org/W2757222607', 'https://openalex.org/W2760452458', 'https://openalex.org/W2767899794', 'https://openalex.org/W2798931235', 'https://openalex.org/W2803237843', 'https://openalex.org/W2805394970', 'https://openalex.org/W2888519496', 'https://openalex.org/W2896234185', 'https://openalex.org/W2916835973', 'https://openalex.org/W2951642234', 'https://openalex.org/W2962700074', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962897020', 'https://openalex.org/W2962934837', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963578424', 'https://openalex.org/W2963661177', 'https://openalex.org/W2963777589', 'https://openalex.org/W2963897095', 'https://openalex.org/W2963950336', 'https://openalex.org/W2964247056', 'https://openalex.org/W2970558573', 'https://openalex.org/W2972448360', 'https://openalex.org/W4385245566']","This paper describes the machine translation system developed jointly by Baidu Research and Oregon State University for WMT 2019 Machine Translation Robustness Shared Task. Translation of social media is a very challenging problem, since its style is very different from normal parallel corpora (e.g. News) and also include various types of noises. To make it worse, the amount of social media parallel corpora is extremely limited. In this paper, we use a domain sensitive training method which leverages a large amount of parallel data from popular domains together with a little amount of parallel data from social media. Furthermore, we generate a parallel dataset with pseudo noisy source sentences which are back-translated from monolingual data using a model trained by a similar domain sensitive way. In this way, we achieve more than 10 BLEU improvement in both En-Fr and Fr-En translation compared with the baseline methods.",0.99581589958159
SKG_MT_1151,https://openalex.org/W2159181679,2010,3,"['https://openalex.org/W1965660534', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115717119', 'https://openalex.org/W2116957398', 'https://openalex.org/W2123126659', 'https://openalex.org/W2139885235', 'https://openalex.org/W2146574666', 'https://openalex.org/W2146628926', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2154581043', 'https://openalex.org/W2165132531', 'https://openalex.org/W2165594795', 'https://openalex.org/W2166905217', 'https://openalex.org/W2437005631']","This paper presents a novel filtration criterion to restrict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed wellformed dependency restriction is used to filter out bad rules. Furthermore, a new feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed. Experimental results show that, the new criteria weeds out about 40 % rules while with translation performance improvement, and the new feature brings another improvement to the baseline system, especially on larger corpus. 1",1.0
SKG_MT_1153,https://openalex.org/W2610850660,2017,228,"['https://openalex.org/W2101105183', 'https://openalex.org/W2596164567']","The quality of a Neural Machine Translation system depends substantially on\nthe availability of sizable parallel corpora. For low-resource language pairs\nthis is not the case, resulting in poor translation quality. Inspired by work\nin computer vision, we propose a novel data augmentation approach that targets\nlow-frequency words by generating new sentence pairs containing rare words in\nnew, synthetically created contexts. Experimental results on simulated\nlow-resource settings show that our method improves translation quality by up\nto 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.\n",1.0
SKG_MT_1156,https://openalex.org/W2593341061,2017,185,"['https://openalex.org/W6908809', 'https://openalex.org/W22168010', 'https://openalex.org/W1514535095', 'https://openalex.org/W1686810756', 'https://openalex.org/W1753482797', 'https://openalex.org/W1895577753', 'https://openalex.org/W1902237438', 'https://openalex.org/W1934041838', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102605133', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2139501017', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2159243025', 'https://openalex.org/W2185175083', 'https://openalex.org/W2194775991', 'https://openalex.org/W2212703438', 'https://openalex.org/W2247931231', 'https://openalex.org/W2250342921', 'https://openalex.org/W2251743902', 'https://openalex.org/W2251994258', 'https://openalex.org/W2293344577', 'https://openalex.org/W2345720230', 'https://openalex.org/W2417549359', 'https://openalex.org/W2509282593', 'https://openalex.org/W2512381898', 'https://openalex.org/W2513263213', 'https://openalex.org/W2516756687', 'https://openalex.org/W2553522418', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740091278', 'https://openalex.org/W2740565324', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963344439', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963909453', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998704965', 'https://openalex.org/W4285719527', 'https://openalex.org/W4297780100']","We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.",1.0
SKG_MT_1158,https://openalex.org/W776270814,2014,19,"['https://openalex.org/W108437174', 'https://openalex.org/W1488557242', 'https://openalex.org/W1489181569', 'https://openalex.org/W1518867723', 'https://openalex.org/W1521376992', 'https://openalex.org/W1529616844', 'https://openalex.org/W1543107604', 'https://openalex.org/W1568793342', 'https://openalex.org/W1603508585', 'https://openalex.org/W1869906767', 'https://openalex.org/W1880085624', 'https://openalex.org/W1994057144', 'https://openalex.org/W2015333112', 'https://openalex.org/W2018869373', 'https://openalex.org/W2029097226', 'https://openalex.org/W2078861931', 'https://openalex.org/W2088781183', 'https://openalex.org/W2098507980', 'https://openalex.org/W2107170966', 'https://openalex.org/W2116492146', 'https://openalex.org/W2117652747', 'https://openalex.org/W2120779048', 'https://openalex.org/W2125712079', 'https://openalex.org/W2139621418', 'https://openalex.org/W2165894879', 'https://openalex.org/W2251610689', 'https://openalex.org/W2270190199', 'https://openalex.org/W2745750801', 'https://openalex.org/W2917452219']","This paper describes the UPC submissions to the WMT14 Metrics Shared Task: UPC-IPA and UPC-STOUT. These metrics use a collection of evaluation measures in-tegrated in ASIYA, a toolkit for machine translation evaluation. In addition to some standard metrics, the two submissions take advantage of novel metrics that consider linguistic structures, lexical relationships, and semantics to compare both source and reference translation against the candidate translation. The new metrics are available for several target languages other than En-glish. In the the official WMT14 evalua-tion, UPC-IPA and UPC-STOUT scored above the average in 7 out of 9 language pairs at the system level and 8 out of 9 at the segment level. 1",0.9948186528497409
SKG_MT_1160,https://openalex.org/W2963598809,2016,124,"['https://openalex.org/W6908809', 'https://openalex.org/W1513462981', 'https://openalex.org/W1902237438', 'https://openalex.org/W1973923101', 'https://openalex.org/W2100664567', 'https://openalex.org/W2117238933', 'https://openalex.org/W2126946601', 'https://openalex.org/W2133564696', 'https://openalex.org/W2145033586', 'https://openalex.org/W2148708890', 'https://openalex.org/W2162245945', 'https://openalex.org/W2163403121', 'https://openalex.org/W2169724380', 'https://openalex.org/W2180952760', 'https://openalex.org/W2212846646', 'https://openalex.org/W2251299219', 'https://openalex.org/W2291126447', 'https://openalex.org/W2361821140', 'https://openalex.org/W2561274697', 'https://openalex.org/W2577335011', 'https://openalex.org/W2963937700', 'https://openalex.org/W2964308564']","In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs.We simply compute the distance between the machine attentions and the ""true"" alignments, and minimize this cost in the training procedure.Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.",1.0
SKG_MT_1164,https://openalex.org/W3034906024,2020,37,"['https://openalex.org/W273093436', 'https://openalex.org/W1821462560', 'https://openalex.org/W2145094598', 'https://openalex.org/W2251743902', 'https://openalex.org/W2546938941', 'https://openalex.org/W2555745756', 'https://openalex.org/W2888456631', 'https://openalex.org/W2905933322', 'https://openalex.org/W2919290281', 'https://openalex.org/W2949911645', 'https://openalex.org/W2950428495', 'https://openalex.org/W2951065878', 'https://openalex.org/W2953190730', 'https://openalex.org/W2962696859', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962807144', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963499433', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963983698', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964508650', 'https://openalex.org/W2964707311', 'https://openalex.org/W2970925677', 'https://openalex.org/W2971254483', 'https://openalex.org/W3011824510', 'https://openalex.org/W3087183776']","Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.",1.0
SKG_MT_1165,https://openalex.org/W2107468211,2012,37,"['https://openalex.org/W22168010', 'https://openalex.org/W214028658', 'https://openalex.org/W222053410', 'https://openalex.org/W1481546552', 'https://openalex.org/W1978161643', 'https://openalex.org/W1980219648', 'https://openalex.org/W2006969979', 'https://openalex.org/W2015350341', 'https://openalex.org/W2017802499', 'https://openalex.org/W2038698865', 'https://openalex.org/W2093710484', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108847444', 'https://openalex.org/W2115526192', 'https://openalex.org/W2116211107', 'https://openalex.org/W2116492146', 'https://openalex.org/W2116599427', 'https://openalex.org/W2117873652', 'https://openalex.org/W2123641849', 'https://openalex.org/W2125799830', 'https://openalex.org/W2126270798', 'https://openalex.org/W2131988669', 'https://openalex.org/W2133444727', 'https://openalex.org/W2134283755', 'https://openalex.org/W2142100228', 'https://openalex.org/W2147925066', 'https://openalex.org/W2150028966', 'https://openalex.org/W2151197196', 'https://openalex.org/W2152213375', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157875692', 'https://openalex.org/W2158388102', 'https://openalex.org/W2161792612', 'https://openalex.org/W2162465526', 'https://openalex.org/W2163353449', 'https://openalex.org/W2164151151', 'https://openalex.org/W2164454850', 'https://openalex.org/W2166086342', 'https://openalex.org/W2169755331', 'https://openalex.org/W2170464899', 'https://openalex.org/W2407949352', 'https://openalex.org/W2570451785', 'https://openalex.org/W2913739034']","In this paper, we demonstrate that accurate machine translation is possible without the concept of “words, ” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs. 1",1.0
SKG_MT_1166,https://openalex.org/W2474396346,2016,1,"['https://openalex.org/W1527525292', 'https://openalex.org/W1682639223', 'https://openalex.org/W1746819321', 'https://openalex.org/W1783061888', 'https://openalex.org/W1933798900', 'https://openalex.org/W2018770010', 'https://openalex.org/W2042590947', 'https://openalex.org/W2061885628', 'https://openalex.org/W2079521622', 'https://openalex.org/W2108324828', 'https://openalex.org/W2133675239', 'https://openalex.org/W2134477639', 'https://openalex.org/W2137018331', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150582689', 'https://openalex.org/W2158471180', 'https://openalex.org/W2160001241', 'https://openalex.org/W2171305756', 'https://openalex.org/W2186839874', 'https://openalex.org/W2251150371', 'https://openalex.org/W2251311344', 'https://openalex.org/W2251557434', 'https://openalex.org/W2252083640', 'https://openalex.org/W2950177356', 'https://openalex.org/W2952677397', 'https://openalex.org/W3044695148']","Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows.",1.0
SKG_MT_1167,https://openalex.org/W141243353,2010,7,"['https://openalex.org/W132913264', 'https://openalex.org/W1494864219', 'https://openalex.org/W1631260214', 'https://openalex.org/W1970689298', 'https://openalex.org/W1973152633', 'https://openalex.org/W2006969979', 'https://openalex.org/W2061528982', 'https://openalex.org/W2080373976', 'https://openalex.org/W2101456909', 'https://openalex.org/W2115081467', 'https://openalex.org/W2116337133', 'https://openalex.org/W2124807415', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047']","This paper describes the development of French–English and English–French ma-chine translation systems for the 2010 WMT shared task evaluation. These sys-tems were standard phrase-based statisti-cal systems based on the Moses decoder, trained on the provided data only. Most of our efforts were devoted to the choice and extraction of bilingual data used for training. We filtered out some bilingual corpora and pruned the phrase table. We also investigated the impact of adding two types of additional bilingual texts, ex-tracted automatically from the available monolingual data. We first collected bilin-gual data by performing automatic trans-lations of monolingual texts. The second type of bilingual text was harvested from comparable corpora with Information Re-trieval techniques. 1",1.0
SKG_MT_1169,https://openalex.org/W2250714477,2013,60,"['https://openalex.org/W22861983', 'https://openalex.org/W23077562', 'https://openalex.org/W71795751', 'https://openalex.org/W1423339008', 'https://openalex.org/W1555286493', 'https://openalex.org/W1596986901', 'https://openalex.org/W1631260214', 'https://openalex.org/W1889268436', 'https://openalex.org/W1969974515', 'https://openalex.org/W1997420744', 'https://openalex.org/W2009156021', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097997328', 'https://openalex.org/W2097998348', 'https://openalex.org/W2099851297', 'https://openalex.org/W2099864888', 'https://openalex.org/W2103305545', 'https://openalex.org/W2104518905', 'https://openalex.org/W2117130368', 'https://openalex.org/W2118536060', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125573226', 'https://openalex.org/W2127218421', 'https://openalex.org/W2131367528', 'https://openalex.org/W2133280805', 'https://openalex.org/W2134729743', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158899491', 'https://openalex.org/W2161586008', 'https://openalex.org/W2164019165', 'https://openalex.org/W2171421863', 'https://openalex.org/W2247119764', 'https://openalex.org/W2250907725', 'https://openalex.org/W2399028049', 'https://openalex.org/W4285719527']","While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.",1.0
SKG_MT_1174,https://openalex.org/W2121155211,2012,5,"['https://openalex.org/W1631260214', 'https://openalex.org/W2006969979', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117051955', 'https://openalex.org/W2124807415', 'https://openalex.org/W2136657878', 'https://openalex.org/W2138934709', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158195707', 'https://openalex.org/W2164948578']","We describe the systems developed by the team of the Qatar Computing Research Institute for the WMT12 Shared Translation Task. We used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English.",0.9948186528497409
SKG_MT_1175,https://openalex.org/W2096435350,2012,7,"['https://openalex.org/W222053410', 'https://openalex.org/W1996430422', 'https://openalex.org/W2099607809', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115081467', 'https://openalex.org/W2123301721', 'https://openalex.org/W2129804798', 'https://openalex.org/W2146813944', 'https://openalex.org/W2149327368', 'https://openalex.org/W2159107349', 'https://openalex.org/W2162499915', 'https://openalex.org/W2165213242', 'https://openalex.org/W2181844119', 'https://openalex.org/W2467575451', 'https://openalex.org/W2785522575', 'https://openalex.org/W2895810819', 'https://openalex.org/W2915395439']","50th Annual Meeting of the Association for Computational Linguistics, ACL 2012 - Proceedings of the Conference",1.0
SKG_MT_1176,https://openalex.org/W2970410759,2019,3,"['https://openalex.org/W2101105183', 'https://openalex.org/W2149327368', 'https://openalex.org/W2157331557', 'https://openalex.org/W2212846646', 'https://openalex.org/W2251507105', 'https://openalex.org/W2550821151', 'https://openalex.org/W2595715041', 'https://openalex.org/W2786253471', 'https://openalex.org/W2805790316', 'https://openalex.org/W2890007195', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902466572', 'https://openalex.org/W2952360713', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W3106541240', 'https://openalex.org/W4385245566']","Riktim Mondal, Shankha Raj Nayek, Aditya Chowdhury, Santanu Pal, Sudip Kumar Naskar, Josef van Genabith. Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1). 2019.",1.0
SKG_MT_1177,https://openalex.org/W2099864888,2012,5,"['https://openalex.org/W13343785', 'https://openalex.org/W1525066359', 'https://openalex.org/W1969974515', 'https://openalex.org/W2008961349', 'https://openalex.org/W2018156128', 'https://openalex.org/W2048978997', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097997328', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105245376', 'https://openalex.org/W2111798208', 'https://openalex.org/W2121340590', 'https://openalex.org/W2121404172', 'https://openalex.org/W2123301721', 'https://openalex.org/W2124807415', 'https://openalex.org/W2129124601', 'https://openalex.org/W2131367528', 'https://openalex.org/W2143263475', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158065314', 'https://openalex.org/W2158388102', 'https://openalex.org/W2162245945', 'https://openalex.org/W2165828367', 'https://openalex.org/W2169724380', 'https://openalex.org/W2171421863', 'https://openalex.org/W2200091548', 'https://openalex.org/W2401082558', 'https://openalex.org/W2401304182', 'https://openalex.org/W2626190081', 'https://openalex.org/W3166311385', 'https://openalex.org/W3203149905']","This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks. © 2012 Association for computational Linguistics.",1.0
SKG_MT_1178,https://openalex.org/W3034332156,2020,16,"['https://openalex.org/W1855892484', 'https://openalex.org/W2466918907', 'https://openalex.org/W2493916176', 'https://openalex.org/W2594088761', 'https://openalex.org/W2750672897', 'https://openalex.org/W2904683980', 'https://openalex.org/W2936969148', 'https://openalex.org/W2949328740', 'https://openalex.org/W2950613790', 'https://openalex.org/W2952167535', 'https://openalex.org/W2962772361', 'https://openalex.org/W2963047628', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963419157', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963679377', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963834942', 'https://openalex.org/W2964172053', 'https://openalex.org/W3008549139', 'https://openalex.org/W4294367149', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390']","Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.",0.9955156950672646
SKG_MT_1179,https://openalex.org/W2971081268,2019,15,"['https://openalex.org/W2101105183', 'https://openalex.org/W2133564696', 'https://openalex.org/W2251290832', 'https://openalex.org/W2546938941', 'https://openalex.org/W2550821151', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740433069', 'https://openalex.org/W2752630748', 'https://openalex.org/W2797913374', 'https://openalex.org/W2887920589', 'https://openalex.org/W2889511806', 'https://openalex.org/W2902382260', 'https://openalex.org/W2912095972', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963571015', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963708445', 'https://openalex.org/W2963831310', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964308564', 'https://openalex.org/W2994475016', 'https://openalex.org/W3104652516', 'https://openalex.org/W4323355575', 'https://openalex.org/W4385245566']","This paper describes the University of Maryland's submission to the WMT 2019 Kazakh-English news translation task. We study the impact of transfer learning from another low-resource but related language. We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling. The submitted system improves over a Kazakh-only baseline by +5.45 BLEU on newstest2019.",1.0
SKG_MT_1180,https://openalex.org/W3037052270,2020,5,"['https://openalex.org/W1522301498', 'https://openalex.org/W1816313093', 'https://openalex.org/W1902237438', 'https://openalex.org/W1915022094', 'https://openalex.org/W1915251500', 'https://openalex.org/W1955369839', 'https://openalex.org/W2013556410', 'https://openalex.org/W2025401819', 'https://openalex.org/W2090955184', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251743902', 'https://openalex.org/W2295072214', 'https://openalex.org/W2467834614', 'https://openalex.org/W2526425061', 'https://openalex.org/W2550821151', 'https://openalex.org/W2554013724', 'https://openalex.org/W2555428947', 'https://openalex.org/W2556468274', 'https://openalex.org/W2561274697', 'https://openalex.org/W2756566411', 'https://openalex.org/W2757041753', 'https://openalex.org/W2758310181', 'https://openalex.org/W2778814079', 'https://openalex.org/W2807895655', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962731009', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962982474', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963877604', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964053711', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970101452', 'https://openalex.org/W2998280981', 'https://openalex.org/W3082674894', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","Neural machine translation (NMT) has achieved impressive performance recently by using large-scale parallel corpora. However, it struggles in the low-resource and morphologically-rich scenarios of agglutinative language translation task. Inspired by the finding that monolingual data can greatly improve the NMT performance, we propose a multi-task neural model that jointly learns to perform bi-directional translation and agglutinative language stemming. Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source-side sentence to specify the desired target outputs of the two different tasks. Experimental results on Turkish-English and Uyghur-Chinese show that our proposed approach can significantly improve the translation performance on agglutinative languages by using a small amount of monolingual data.",1.0
SKG_MT_1181,https://openalex.org/W3002201054,2020,3,"['https://openalex.org/W189530018', 'https://openalex.org/W1526763309', 'https://openalex.org/W1538023239', 'https://openalex.org/W1604065854', 'https://openalex.org/W1901129140', 'https://openalex.org/W1951724000', 'https://openalex.org/W2030780436', 'https://openalex.org/W2043186830', 'https://openalex.org/W2094069964', 'https://openalex.org/W2100398002', 'https://openalex.org/W2101763985', 'https://openalex.org/W2104029973', 'https://openalex.org/W2123924326', 'https://openalex.org/W2136530135', 'https://openalex.org/W2136545725', 'https://openalex.org/W2139647714', 'https://openalex.org/W2461047981', 'https://openalex.org/W2523573275', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2593116425', 'https://openalex.org/W2605131327', 'https://openalex.org/W2774707525', 'https://openalex.org/W2901796276', 'https://openalex.org/W2941649920', 'https://openalex.org/W2945700568', 'https://openalex.org/W2954386831', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963542120', 'https://openalex.org/W2972425358', 'https://openalex.org/W3082845185', 'https://openalex.org/W3087197566']","We present enhancements to a speech-to-speech translation pipeline in order to perform automatic dubbing. Our architecture features neural machine translation generating output of preferred length, prosodic alignment of the translation with the original speech segments, neural text-to-speech with fine tuning of the duration of each utterance, and, finally, audio rendering to enriches text-to-speech output with background noise and reverberation extracted from the original audio. We report on a subjective evaluation of automatic dubbing of excerpts of TED Talks from English into Italian, which measures the perceived naturalness of automatic dubbing and the relative importance of each proposed enhancement.",1.0
SKG_MT_1182,https://openalex.org/W3015919249,2020,7,"['https://openalex.org/W874657746', 'https://openalex.org/W1582482241', 'https://openalex.org/W2060277733', 'https://openalex.org/W2101105183', 'https://openalex.org/W2407166119', 'https://openalex.org/W2483215953', 'https://openalex.org/W2511234952', 'https://openalex.org/W2560647685', 'https://openalex.org/W2567571499', 'https://openalex.org/W2572474373', 'https://openalex.org/W2757592053', 'https://openalex.org/W2792115266', 'https://openalex.org/W2887768933', 'https://openalex.org/W2891525068', 'https://openalex.org/W2902200621', 'https://openalex.org/W2925188774', 'https://openalex.org/W2945383715', 'https://openalex.org/W2950760213', 'https://openalex.org/W2950866572', 'https://openalex.org/W2950888501', 'https://openalex.org/W2952328691', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962787423', 'https://openalex.org/W2962863357', 'https://openalex.org/W2962890089', 'https://openalex.org/W2962990575', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963418779', 'https://openalex.org/W2963457723', 'https://openalex.org/W2963526187', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963913356', 'https://openalex.org/W2970279348', 'https://openalex.org/W2972568911', 'https://openalex.org/W2972680990', 'https://openalex.org/W2972972637', 'https://openalex.org/W2978943549', 'https://openalex.org/W2981385260', 'https://openalex.org/W3037697022', 'https://openalex.org/W3041866211', 'https://openalex.org/W3215160418']","Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019). Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a `balanced' dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is `catastrophic forgetting', which we address both in adaptation and in inference. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. During inference we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al (2019) on WinoMT with no degradation of general test set BLEU, and we show this scheme can be applied to remove gender bias in the output of `black box` online commercial MT systems. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.",1.0
SKG_MT_1184,https://openalex.org/W2980969112,2019,2,"['https://openalex.org/W114517082', 'https://openalex.org/W1902237438', 'https://openalex.org/W2101105183', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144600658', 'https://openalex.org/W2150355110', 'https://openalex.org/W2184135559', 'https://openalex.org/W2220350356', 'https://openalex.org/W2525778437', 'https://openalex.org/W2896691342', 'https://openalex.org/W2898962441', 'https://openalex.org/W2899771611', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963631431', 'https://openalex.org/W2963887123', 'https://openalex.org/W2964053711', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964199361', 'https://openalex.org/W3005389111']","Neural Machine Translation (NMT) models generally perform translation using a fixed-size lexical vocabulary, which is an important bottleneck on their generalization capability and overall translation quality. The standard approach to overcome this limitation is to segment words into subword units, typically using some external tools with arbitrary heuristics, resulting in vocabulary units not optimized for the translation task. Recent studies have shown that the same approach can be extended to perform NMT directly at the level of characters, which can deliver translation accuracy on-par with subword-based models, on the other hand, this requires relatively deeper networks. In this paper, we propose a more computationally-efficient solution for character-level NMT which implements a hierarchical decoding architecture where translations are subsequently generated at the level of words and characters. We evaluate different methods for open-vocabulary NMT in the machine translation task from English into five languages with distinct morphological typology, and show that the hierarchical decoding model can reach higher translation accuracy than the subword-level NMT model using significantly fewer parameters, while demonstrating better capacity in learning longer-distance contextual and grammatical dependencies than the standard character-level NMT model.",1.0
SKG_MT_1185,https://openalex.org/W2953190730,2019,45,"['https://openalex.org/W1522301498', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1910131649', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121879602', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2251743902', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2561274697', 'https://openalex.org/W2610245951', 'https://openalex.org/W2613904329', 'https://openalex.org/W2739978843', 'https://openalex.org/W2772120246', 'https://openalex.org/W2807535859', 'https://openalex.org/W2809456172', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888456631', 'https://openalex.org/W2890964657', 'https://openalex.org/W2891924676', 'https://openalex.org/W2899113456', 'https://openalex.org/W2935811960', 'https://openalex.org/W2950635152', 'https://openalex.org/W2962778428', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962807144', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963331137', 'https://openalex.org/W2963347649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499433', 'https://openalex.org/W2963643655', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963842982', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4244167344', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973']","Multilingual neural machine translation (Multi-NMT) with one encoder-decoder model has made remarkable progress due to its simple deployment. However, this multilingual translation paradigm does not make full use of language commonality and parameter sharing between encoder and decoder. Furthermore, this kind of paradigm cannot outperform the individual models trained on bilingual corpus in most cases. In this paper, we propose a compact and language-sensitive method for multilingual translation. To maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. To make the representor sensitive for specific languages, we further introduce language-sensitive embedding, attention, and discriminator with the ability to enhance model performance. We verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets. Moreover, we find that our model is especially helpful in low-resource and zero-shot translation scenarios.",1.0
SKG_MT_1188,https://openalex.org/W3116912800,2020,10,"['https://openalex.org/W1902237438', 'https://openalex.org/W2093790824', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2153508793', 'https://openalex.org/W2250539671', 'https://openalex.org/W2516756687', 'https://openalex.org/W2561274697', 'https://openalex.org/W2581101319', 'https://openalex.org/W2593341061', 'https://openalex.org/W2809361596', 'https://openalex.org/W2956020289', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963877297', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970885024', 'https://openalex.org/W2984930179', 'https://openalex.org/W2985818450', 'https://openalex.org/W2997331536', 'https://openalex.org/W3017195079']","Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-the-art results in the task of machine translation because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nevertheless, NMT still suffers low translation quality for low resource languages. To encounter this challenge, the multi-modal concept comes in. The multi-modal concept combines textual and visual features to improve the translation quality of low resource languages. Moreover, the utilization of monolingual data in the pre-training step can improve the performance of the system for low resource language translations. Workshop on Asian Translation 2020 (WAT2020) organized a translation task for multimodal translation in English to Hindi. We have participated in the same in two-track submission, namely text-only and multi-modal translation with team name CNLP-NITS. The evaluated results are declared at the WAT2020 translation task, which reports that our multi-modal NMT system attained higher scores than our text-only NMT on both challenge and evaluation test set. For the challenge test data, our multi-modal neural machine translation system achieves Bilingual Evaluation Understudy (BLEU) score of 33.57, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively.",1.0
SKG_MT_1189,https://openalex.org/W2756726036,2017,1,"['https://openalex.org/W23077562', 'https://openalex.org/W179875071', 'https://openalex.org/W193080678', 'https://openalex.org/W1905522558', 'https://openalex.org/W1982498087', 'https://openalex.org/W2159755860', 'https://openalex.org/W2186961328', 'https://openalex.org/W2212703438', 'https://openalex.org/W2251682575', 'https://openalex.org/W2400065810', 'https://openalex.org/W2513202451', 'https://openalex.org/W2515631395', 'https://openalex.org/W2527845440', 'https://openalex.org/W2594229957', 'https://openalex.org/W2740718109', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962801832', 'https://openalex.org/W2962867687', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963463964']","We describe the machine translation systems developed at the National Research Council of Canada (NRC) for the Russian-English and Chinese-English news translation tasks of the Second Conference on Machine Translation (WMT 2017).We conducted several experiments to explore the best baseline settings for neural machine translation (NMT).In the Russian-English task, to our surprise, our bestperforming system is one that rescores phrase-based statistical machine translation outputs using NMT rescoring features.On the other hand, in the Chinese-English task, which has far more parallel training data, NMT is able to outperform SMT significantly.The NRC MT systems is the best constrained system in Russian-English (out of nine participants) and the fourth best constrained system in Chinese-English (out of twenty participants) in WMT 2017 human evaluation.",1.0
SKG_MT_1190,https://openalex.org/W2508824506,2016,2,"['https://openalex.org/W932413789', 'https://openalex.org/W1665921526', 'https://openalex.org/W2083545877', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101207453', 'https://openalex.org/W2103078213', 'https://openalex.org/W2110168585', 'https://openalex.org/W2118776487', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136016850', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2156985047', 'https://openalex.org/W2163255203', 'https://openalex.org/W2251222643', 'https://openalex.org/W2251682575', 'https://openalex.org/W2251771687', 'https://openalex.org/W2408727897', 'https://openalex.org/W2964138630', 'https://openalex.org/W2964308564', 'https://openalex.org/W3203276480']","Recently, neural network models have achieved consistent improvements in statistical machine translation.However, most networks only use one-hot encoded input vectors of words as their input.In this work, we investigated the exponentially decaying bag-of-words input features for feed-forward neural network translation models and proposed to train the decay rates along with other weight parameters.This novel bag-of-words model improved our phrase-based state-of-the-art system, which already includes a neural network translation model, by up to 0.5% BLEU and 0.6% TER on three different translation tasks and even achieved a similar performance to the bidirectional LSTM translation model.",1.0
SKG_MT_1191,https://openalex.org/W148573246,2012,10,"['https://openalex.org/W29814777', 'https://openalex.org/W61355150', 'https://openalex.org/W123309796', 'https://openalex.org/W1973923101', 'https://openalex.org/W2042918649', 'https://openalex.org/W2103362845', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125885330', 'https://openalex.org/W2153635508', 'https://openalex.org/W2158710545', 'https://openalex.org/W2159130414', 'https://openalex.org/W2250195817']","This paper overviews FBK’s participation in the Cross-Lingual Textual Entailment for Content Synchronization task organized within SemEval-2012. Our participation is characterized by using cross-lingual matching features extracted from lexical and semantic phrase tables and dependency relations. The features are used for multi-class and binary classification using SVMs. Using a combination of lexical, syntactic, and semantic features to create a cross-lingual textual entailment system, we report on experiments over the provided dataset. Our best run achieved an accuracy of 50.4 % on the Spanish-English dataset (with the average score and the median system respectively achieving 40.7 % and 34.6%), demonstrating the effectiveness of a “pure ” cross-lingual approach that avoids intermediate translations. 1",0.9911504424778761
SKG_MT_1192,https://openalex.org/W2953287808,2019,50,"['https://openalex.org/W155101268', 'https://openalex.org/W2028176545', 'https://openalex.org/W2096268847', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2127331160', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153579005', 'https://openalex.org/W2162499915', 'https://openalex.org/W2250234233', 'https://openalex.org/W2250342921', 'https://openalex.org/W2250597803', 'https://openalex.org/W2250875036', 'https://openalex.org/W2251994258', 'https://openalex.org/W2257408573', 'https://openalex.org/W2260677151', 'https://openalex.org/W2294699749', 'https://openalex.org/W2331726854', 'https://openalex.org/W2333897677', 'https://openalex.org/W2512848817', 'https://openalex.org/W2512924740', 'https://openalex.org/W2593833795', 'https://openalex.org/W2600463316', 'https://openalex.org/W2608787653', 'https://openalex.org/W2756675635', 'https://openalex.org/W2760656271', 'https://openalex.org/W2760738985', 'https://openalex.org/W2896457183', 'https://openalex.org/W2903193068', 'https://openalex.org/W2903376039', 'https://openalex.org/W2915756181', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962958286', 'https://openalex.org/W2963241825', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963341956', 'https://openalex.org/W4294170691']","Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and system-level tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset.",0.994475138121547
SKG_MT_1195,https://openalex.org/W2798938673,2018,41,"['https://openalex.org/W342285082', 'https://openalex.org/W1542713999', 'https://openalex.org/W1562955078', 'https://openalex.org/W1662133657', 'https://openalex.org/W1828724394', 'https://openalex.org/W2053186076', 'https://openalex.org/W2102749417', 'https://openalex.org/W2118090838', 'https://openalex.org/W2121415745', 'https://openalex.org/W2126725946', 'https://openalex.org/W2140406733', 'https://openalex.org/W2144945507', 'https://openalex.org/W2153579005', 'https://openalex.org/W2158199200', 'https://openalex.org/W2158899491', 'https://openalex.org/W2250385423', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250646737', 'https://openalex.org/W2251033195', 'https://openalex.org/W2251804196', 'https://openalex.org/W2252212383', 'https://openalex.org/W2270364989', 'https://openalex.org/W2293547632', 'https://openalex.org/W2294774419', 'https://openalex.org/W2295584157', 'https://openalex.org/W2295781714', 'https://openalex.org/W2402040300', 'https://openalex.org/W2493916176', 'https://openalex.org/W2508069829', 'https://openalex.org/W2561995736', 'https://openalex.org/W2594021297', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2757521750', 'https://openalex.org/W2759378924', 'https://openalex.org/W2952037945', 'https://openalex.org/W2952190837', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962795068', 'https://openalex.org/W2963061446', 'https://openalex.org/W2963118869', 'https://openalex.org/W2964222437', 'https://openalex.org/W4294170691', 'https://openalex.org/W4299579390']","We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.",1.0
SKG_MT_1196,https://openalex.org/W2970223278,2019,27,"['https://openalex.org/W1508577659', 'https://openalex.org/W2008961349', 'https://openalex.org/W2117278770', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136544838', 'https://openalex.org/W2147930801', 'https://openalex.org/W2153508793', 'https://openalex.org/W2155607551', 'https://openalex.org/W2156985047', 'https://openalex.org/W2162245945', 'https://openalex.org/W2250333994', 'https://openalex.org/W2546938941', 'https://openalex.org/W2576482813', 'https://openalex.org/W2623037479', 'https://openalex.org/W2756566411', 'https://openalex.org/W2806921177', 'https://openalex.org/W2887920589', 'https://openalex.org/W2888740011', 'https://openalex.org/W2889191148', 'https://openalex.org/W2890007195', 'https://openalex.org/W2903193068', 'https://openalex.org/W2913659301', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963667932', 'https://openalex.org/W2964259010', 'https://openalex.org/W3121623419', 'https://openalex.org/W3203149905', 'https://openalex.org/W4288601832', 'https://openalex.org/W4297801177', 'https://openalex.org/W4307459710', 'https://openalex.org/W4385245566']","Chunting Zhou, Xuezhe Ma, Junjie Hu, Graham Neubig. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_1197,https://openalex.org/W2962915948,2019,74,"['https://openalex.org/W1757796397', 'https://openalex.org/W1916559533', 'https://openalex.org/W2064675550', 'https://openalex.org/W2072128103', 'https://openalex.org/W2130942839', 'https://openalex.org/W2145339207', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2518578398', 'https://openalex.org/W2595715041', 'https://openalex.org/W2767206889', 'https://openalex.org/W2789543585', 'https://openalex.org/W2890501761', 'https://openalex.org/W2892213699', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962969034', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963536265', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964074409', 'https://openalex.org/W3211848854', 'https://openalex.org/W4231109964', 'https://openalex.org/W4241645538', 'https://openalex.org/W4298857966', 'https://openalex.org/W4385245566']","Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De.",1.0
SKG_MT_1198,https://openalex.org/W2951997403,2019,14,"['https://openalex.org/W808583520', 'https://openalex.org/W2089629691', 'https://openalex.org/W2101105183', 'https://openalex.org/W2121457870', 'https://openalex.org/W2183341477', 'https://openalex.org/W2251955814', 'https://openalex.org/W2529548870', 'https://openalex.org/W2890698823', 'https://openalex.org/W2896234185', 'https://openalex.org/W2899202004', 'https://openalex.org/W2946888380', 'https://openalex.org/W2951562371', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964078338', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964272710', 'https://openalex.org/W2964308564']","Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk's adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.",1.0
SKG_MT_1199,https://openalex.org/W2140785880,2010,3,"['https://openalex.org/W222053410', 'https://openalex.org/W1479758177', 'https://openalex.org/W1519201847', 'https://openalex.org/W1632114991', 'https://openalex.org/W1909398668', 'https://openalex.org/W2038721957', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113788796', 'https://openalex.org/W2119168550', 'https://openalex.org/W2121380975', 'https://openalex.org/W2123126659', 'https://openalex.org/W2123301721', 'https://openalex.org/W2135002837', 'https://openalex.org/W2144279206', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2166905217', 'https://openalex.org/W2340764283']","In modern machine translation practice, a statistical phrasal or hierarchical translation system usually relies on a huge set of translation rules extracted from bi-lingual training data. This approach not only results in space and efficiency issues, but also suffers from the sparse data problem. In this paper, we propose to use factorized grammars, an idea widely accepted in the field of linguistic grammar construction, to generalize translation rules, so as to solve these two problems. We designed a method to take advantage of the XTAG English Grammar to facilitate the extraction of factorized rules. We experimented on various setups of low-resource language translation, and showed consistent significant improvement in BLEU over state-ofthe-art string-to-dependency baseline systems with 200K words of bi-lingual training data.",1.0
SKG_MT_1200,https://openalex.org/W3102881428,2020,13,"['https://openalex.org/W99662281', 'https://openalex.org/W112847953', 'https://openalex.org/W1598796236', 'https://openalex.org/W1924770834', 'https://openalex.org/W2008937318', 'https://openalex.org/W2064675550', 'https://openalex.org/W2081580037', 'https://openalex.org/W2095705004', 'https://openalex.org/W2096721022', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115203492', 'https://openalex.org/W2122402213', 'https://openalex.org/W2123301721', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2159267415', 'https://openalex.org/W2163942301', 'https://openalex.org/W2175517206', 'https://openalex.org/W2250539671', 'https://openalex.org/W2396881363', 'https://openalex.org/W2525778437', 'https://openalex.org/W2531207078', 'https://openalex.org/W2534253848', 'https://openalex.org/W2563574619', 'https://openalex.org/W2592864539', 'https://openalex.org/W2613904329', 'https://openalex.org/W2776226510', 'https://openalex.org/W2784089855', 'https://openalex.org/W2805430026', 'https://openalex.org/W2806253224', 'https://openalex.org/W2896457183', 'https://openalex.org/W2911489562', 'https://openalex.org/W2912334285', 'https://openalex.org/W2937845937', 'https://openalex.org/W2941997429', 'https://openalex.org/W2949170474', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963631431', 'https://openalex.org/W2964098600', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2975059944', 'https://openalex.org/W2979250794', 'https://openalex.org/W2981852735', 'https://openalex.org/W2996428491', 'https://openalex.org/W3027766697', 'https://openalex.org/W4241645538', 'https://openalex.org/W4288089799', 'https://openalex.org/W4385245566', 'https://openalex.org/W4386506836']","The language used by physicians and health professionals in prescription\ndirections includes medical jargon and implicit directives and causes much\nconfusion among patients. Human intervention to simplify the language at the\npharmacies may introduce additional errors that can lead to potentially severe\nhealth outcomes. We propose a novel machine translation-based approach,\nPharmMT, to automatically and reliably simplify prescription directions into\npatient-friendly language, thereby significantly reducing pharmacist workload.\nWe evaluate the proposed approach over a dataset consisting of over 530K\nprescriptions obtained from a large mail-order pharmacy. The end-to-end system\nachieves a BLEU score of 60.27 against the reference directions generated by\npharmacists, a 39.6% relative improvement over the rule-based normalization.\nPharmacists judged 94.3% of the simplified directions as usable as-is or with\nminimal changes. This work demonstrates the feasibility of a machine\ntranslation-based tool for simplifying prescription directions in real-life.\n",0.9938650306748467
SKG_MT_1203,https://openalex.org/W2788575190,2018,34,"['https://openalex.org/W30845872', 'https://openalex.org/W1537859740', 'https://openalex.org/W2090861223', 'https://openalex.org/W2113106066', 'https://openalex.org/W2251408482', 'https://openalex.org/W2252212004', 'https://openalex.org/W2295179313', 'https://openalex.org/W2295894802', 'https://openalex.org/W2327501763', 'https://openalex.org/W2525778437', 'https://openalex.org/W2526425061', 'https://openalex.org/W2577255746', 'https://openalex.org/W2582956876', 'https://openalex.org/W2593011301', 'https://openalex.org/W2762715843', 'https://openalex.org/W2786891429', 'https://openalex.org/W2914746235', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963303028', 'https://openalex.org/W2963633299', 'https://openalex.org/W2963819008', 'https://openalex.org/W2964102148', 'https://openalex.org/W2964308564']","We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input.",1.0
SKG_MT_1206,https://openalex.org/W2167375373,2014,24,"['https://openalex.org/W77450199', 'https://openalex.org/W1514971736', 'https://openalex.org/W1716250762', 'https://openalex.org/W1982816048', 'https://openalex.org/W2100271871', 'https://openalex.org/W2124807415', 'https://openalex.org/W2153653739', 'https://openalex.org/W2567948266', 'https://openalex.org/W2725430220', 'https://openalex.org/W2914003022', 'https://openalex.org/W3098375631', 'https://openalex.org/W3197744324']","We present the new THOT toolkit for fully-automatic and interactive statistical ma-chine translation (SMT). Initial public ver-sions of THOT date back to 2005 and did only include estimation of phrase-based models. By contrast, the new version of-fers several new features that had not been previously incorporated. The key innova-tions provided by the toolkit are computer-aided translation, including post-editing and interactive SMT, incremental learn-ing and robust generation of alignments at phrase level. In addition to this, the toolkit also provides standard SMT fea-tures such as fully-automatic translation, scalable and parallel algorithms for model training, client-server implementation of the translation functionality, etc. The toolkit can be compiled in Unix-like and Windows platforms and it is released un-der the GNU Lesser General Public Li-cense (LGPL). 1",1.0
SKG_MT_1209,https://openalex.org/W2164628107,2012,102,"['https://openalex.org/W22168010', 'https://openalex.org/W39090792', 'https://openalex.org/W49707343', 'https://openalex.org/W103390042', 'https://openalex.org/W222053410', 'https://openalex.org/W417454032', 'https://openalex.org/W570535444', 'https://openalex.org/W1494910745', 'https://openalex.org/W1499498905', 'https://openalex.org/W1553669113', 'https://openalex.org/W1579838312', 'https://openalex.org/W1631260214', 'https://openalex.org/W1966812932', 'https://openalex.org/W1972421427', 'https://openalex.org/W1982008367', 'https://openalex.org/W2003458432', 'https://openalex.org/W2006832571', 'https://openalex.org/W2015099947', 'https://openalex.org/W2044916741', 'https://openalex.org/W2045675590', 'https://openalex.org/W2049790815', 'https://openalex.org/W2058373514', 'https://openalex.org/W2077779631', 'https://openalex.org/W2096765155', 'https://openalex.org/W2097927681', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113788796', 'https://openalex.org/W2116492146', 'https://openalex.org/W2123082355', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127849236', 'https://openalex.org/W2137387514', 'https://openalex.org/W2146574666', 'https://openalex.org/W2148365102', 'https://openalex.org/W2148959489', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2155974709', 'https://openalex.org/W2158195707', 'https://openalex.org/W2550419310', 'https://openalex.org/W2560656125', 'https://openalex.org/W2615826748', 'https://openalex.org/W2912150279', 'https://openalex.org/W2954684707', 'https://openalex.org/W3140453591', 'https://openalex.org/W3141884538', 'https://openalex.org/W3196290596', 'https://openalex.org/W4230225957', 'https://openalex.org/W4232870099', 'https://openalex.org/W4285719527', 'https://openalex.org/W4302806199']","We investigate the differences between language models compiled from original target-language texts and those compiled from texts manually translated to the target language. Corroborating established observations of Translation Studies, we demonstrate that the latter are significantly better predictors of translated sentences than the former, and hence fit the reference set better. Furthermore, translated texts yield better language models for statistical machine translation than original texts.",0.9928057553956835
SKG_MT_1211,https://openalex.org/W2153252986,2013,32,"['https://openalex.org/W111475876', 'https://openalex.org/W204260652', 'https://openalex.org/W1551202288', 'https://openalex.org/W1563567452', 'https://openalex.org/W2007291639', 'https://openalex.org/W2039217078', 'https://openalex.org/W2061397531', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105891181', 'https://openalex.org/W2130988241', 'https://openalex.org/W2134612861', 'https://openalex.org/W2139621418', 'https://openalex.org/W2144804812', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158847908', 'https://openalex.org/W2159156933', 'https://openalex.org/W2167393476', 'https://openalex.org/W2167447786', 'https://openalex.org/W2434901392', 'https://openalex.org/W2437005631', 'https://openalex.org/W2885050925']","We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations. 1",1.0
SKG_MT_1212,https://openalex.org/W3087204350,2020,7,"['https://openalex.org/W113676117', 'https://openalex.org/W170711724', 'https://openalex.org/W630532510', 'https://openalex.org/W1508577659', 'https://openalex.org/W1551436493', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105410942', 'https://openalex.org/W2105666200', 'https://openalex.org/W2148334774', 'https://openalex.org/W2152367404', 'https://openalex.org/W2153653739', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251654371', 'https://openalex.org/W2496235729', 'https://openalex.org/W2525778437', 'https://openalex.org/W2787646045', 'https://openalex.org/W2796578018', 'https://openalex.org/W2805035444', 'https://openalex.org/W2949303037', 'https://openalex.org/W2960374072', 'https://openalex.org/W2962735107', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963216505', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963626623', 'https://openalex.org/W2963919854', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963993537', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970858854', 'https://openalex.org/W3011207117', 'https://openalex.org/W3197651372']","Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources.Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them.In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering.With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before.Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation.We also evaluate on a new test set of 1000 pairs made with extensive quality control.We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status.To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation.We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages.Our data and code are available at https: //github.com/csebuetnlp/banglanmt.",0.9957805907172996
SKG_MT_1213,https://openalex.org/W2168677000,2014,36,"['https://openalex.org/W23077562', 'https://openalex.org/W137989762', 'https://openalex.org/W162697342', 'https://openalex.org/W222053410', 'https://openalex.org/W1544567521', 'https://openalex.org/W1631260214', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105891181', 'https://openalex.org/W2109704865', 'https://openalex.org/W2110660056', 'https://openalex.org/W2122609803', 'https://openalex.org/W2124807415', 'https://openalex.org/W2126784811', 'https://openalex.org/W2133990480', 'https://openalex.org/W2137498753', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147272182', 'https://openalex.org/W2156554947', 'https://openalex.org/W2156985047', 'https://openalex.org/W2160538511', 'https://openalex.org/W2162010692', 'https://openalex.org/W2164503643', 'https://openalex.org/W2171802951', 'https://openalex.org/W2250732891', 'https://openalex.org/W2250822480', 'https://openalex.org/W2251986002', 'https://openalex.org/W2394840413', 'https://openalex.org/W2595715041', 'https://openalex.org/W2786062171', 'https://openalex.org/W3170253630']","In this paper we study the use of sentence-level dialect identification in optimizing machine translation system selection when translating mixed dialect input. We test our approach on Arabic, a prototypical diglossic language; and we optimize the combination of four different machine translation systems. Our best result im-proves over the best single MT system baseline by 1.0 % BLEU and over a strong system selection baseline by 0.6 % BLEU on a blind test set. 1",1.0
SKG_MT_1216,https://openalex.org/W2970064096,2019,8,"['https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2149327368', 'https://openalex.org/W2472373455', 'https://openalex.org/W2574872930', 'https://openalex.org/W2896457183', 'https://openalex.org/W2902608666', 'https://openalex.org/W2914120296', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963816901', 'https://openalex.org/W2963824830', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970871182', 'https://openalex.org/W3006530332', 'https://openalex.org/W4385245566']","Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of post-editing, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in QE and adopt a bi-directional translation-like strategy. The strategy is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our strategy, which makes the pre-training slightly harder, can bring improvements for QE. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.",0.9941520467836257
SKG_MT_1217,https://openalex.org/W2951685898,2019,5,"['https://openalex.org/W2086792987', 'https://openalex.org/W2101105183', 'https://openalex.org/W2130942839', 'https://openalex.org/W2157331557', 'https://openalex.org/W2187644741', 'https://openalex.org/W2289351056', 'https://openalex.org/W2467834614', 'https://openalex.org/W2536985093', 'https://openalex.org/W2758334418', 'https://openalex.org/W2796108585', 'https://openalex.org/W2797328513', 'https://openalex.org/W2806412155', 'https://openalex.org/W2888539709', 'https://openalex.org/W2888790544', 'https://openalex.org/W2903193068', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963418779', 'https://openalex.org/W2963499433', 'https://openalex.org/W2963506925', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378']","In this work, we conduct a study on Neural Machine Translation (NMT) for English-Indonesian (EN-ID) and Indonesian-English (ID-EN). We focus on spoken language domains, namely colloquial and speech languages. We build NMT systems using the Transformer model for both translation directions and implement domain adaptation, in which we train our pre-trained NMT systems on speech language (in-domain) data. Moreover, we conduct an evaluation on how the domain-adaptation method in our EN-ID system can result in more formal translation outputs.",1.0
SKG_MT_1219,https://openalex.org/W2963463964,2016,402,"['https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1915251500', 'https://openalex.org/W2006969979', 'https://openalex.org/W2078861931', 'https://openalex.org/W2096557251', 'https://openalex.org/W2100664567', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113788796', 'https://openalex.org/W2118434577', 'https://openalex.org/W2119717200', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2137143056', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154652894', 'https://openalex.org/W2157331557', 'https://openalex.org/W2176263492', 'https://openalex.org/W2250445771', 'https://openalex.org/W2284660317', 'https://openalex.org/W2341718173', 'https://openalex.org/W2963216553', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538']","We propose minimum risk training for end-to-end neural machine translation.Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable.Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs.Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.",1.0
SKG_MT_1220,https://openalex.org/W2126228600,2011,2,"['https://openalex.org/W13657349', 'https://openalex.org/W222053410', 'https://openalex.org/W2095683820', 'https://openalex.org/W2100238596', 'https://openalex.org/W2100281225', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117164663', 'https://openalex.org/W2118536060', 'https://openalex.org/W2119168550', 'https://openalex.org/W2122609803', 'https://openalex.org/W2130959832', 'https://openalex.org/W2136657878', 'https://openalex.org/W2138414624', 'https://openalex.org/W2145033586', 'https://openalex.org/W2146574666', 'https://openalex.org/W2151893110', 'https://openalex.org/W2152263452', 'https://openalex.org/W2158388102', 'https://openalex.org/W2159358338', 'https://openalex.org/W2165666205', 'https://openalex.org/W2166905217', 'https://openalex.org/W2170852307', 'https://openalex.org/W2171802951']","This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypo-theses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes indepen-dently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1",1.0
SKG_MT_1222,https://openalex.org/W2963603518,2017,0,"['https://openalex.org/W6908809', 'https://openalex.org/W91928571', 'https://openalex.org/W144133692', 'https://openalex.org/W1973435495', 'https://openalex.org/W2058475745', 'https://openalex.org/W2067802667', 'https://openalex.org/W2091158010', 'https://openalex.org/W2100119435', 'https://openalex.org/W2108862644', 'https://openalex.org/W2111142112', 'https://openalex.org/W2115056464', 'https://openalex.org/W2116316001', 'https://openalex.org/W2121127625', 'https://openalex.org/W2125595887', 'https://openalex.org/W2142112143', 'https://openalex.org/W2143331230', 'https://openalex.org/W2144600658', 'https://openalex.org/W2146574666', 'https://openalex.org/W2152263452', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2159755860', 'https://openalex.org/W2163582407', 'https://openalex.org/W2250847409', 'https://openalex.org/W2250905272', 'https://openalex.org/W2250993060', 'https://openalex.org/W2294378299', 'https://openalex.org/W2343245819', 'https://openalex.org/W2571626454', 'https://openalex.org/W4241645538']","Pairwise ranking methods are the most widely used discriminative training approaches for structure prediction problems in natural language processing (NLP). Decomposing the problem of ranking hypotheses into pairwise comparisons enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder learning. We propose a listwise learning framework for structure prediction problems such as machine translation. Our framework directly models the entire translation list’s ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.",1.0
SKG_MT_1223,https://openalex.org/W2888520903,2018,143,"['https://openalex.org/W1522301498', 'https://openalex.org/W1899504021', 'https://openalex.org/W2107878631', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2253400648', 'https://openalex.org/W2331143823', 'https://openalex.org/W2402302915', 'https://openalex.org/W2410678056', 'https://openalex.org/W2525778437', 'https://openalex.org/W2565538933', 'https://openalex.org/W2585476045', 'https://openalex.org/W2594990650', 'https://openalex.org/W2613904329', 'https://openalex.org/W2681861910', 'https://openalex.org/W2772798650', 'https://openalex.org/W2896060389', 'https://openalex.org/W2952574409', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962742960', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962906592', 'https://openalex.org/W2962990163', 'https://openalex.org/W2963101856', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963599677', 'https://openalex.org/W2963631431', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W4385245566']","While current state-of-the-art NMT models, such as RNN seq2seq and Transformers, possess a large number of parameters, they are still shallow in comparison to convolutional models used for both text and vision applications. In this work we attempt to train significantly (2-3x) deeper Transformer and Bi-RNN encoders for machine translation. We propose a simple modification to the attention mechanism that eases the optimization of deeper models, and results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT’14 English-German and WMT’15 Czech-English tasks for both architectures.",1.0
SKG_MT_1225,https://openalex.org/W2251797508,2013,6,"['https://openalex.org/W1767012588', 'https://openalex.org/W1889220380', 'https://openalex.org/W1968084137', 'https://openalex.org/W2019614587', 'https://openalex.org/W2041232209', 'https://openalex.org/W2050502527', 'https://openalex.org/W2051167396', 'https://openalex.org/W2066308426', 'https://openalex.org/W2075344734', 'https://openalex.org/W2096648470', 'https://openalex.org/W2096765155', 'https://openalex.org/W2108220507', 'https://openalex.org/W2125500315', 'https://openalex.org/W2127361019', 'https://openalex.org/W2154558620', 'https://openalex.org/W2913739034']","This paper studies named entity translation and proposes “selective temporality” as a new feature, as using temporal features may be harmful for translating “atemporal ” entities. Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1%. 1",1.0
SKG_MT_1227,https://openalex.org/W2158762489,2011,56,"['https://openalex.org/W28548568', 'https://openalex.org/W214816566', 'https://openalex.org/W566945405', 'https://openalex.org/W1500281234', 'https://openalex.org/W1560760997', 'https://openalex.org/W1562458182', 'https://openalex.org/W1885135473', 'https://openalex.org/W2012327518', 'https://openalex.org/W2044563557', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117141075', 'https://openalex.org/W2143927888', 'https://openalex.org/W2147192413', 'https://openalex.org/W2149327368', 'https://openalex.org/W2463396630']","We present BLAST, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part of MT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation. 1",0.991869918699187
SKG_MT_1228,https://openalex.org/W1594128446,2012,15,"['https://openalex.org/W75781568', 'https://openalex.org/W108437174', 'https://openalex.org/W165935821', 'https://openalex.org/W222053410', 'https://openalex.org/W1535015163', 'https://openalex.org/W1588242179', 'https://openalex.org/W1697844855', 'https://openalex.org/W1920193847', 'https://openalex.org/W1973923101', 'https://openalex.org/W2037894654', 'https://openalex.org/W2092654472', 'https://openalex.org/W2095690342', 'https://openalex.org/W2100565866', 'https://openalex.org/W2107038437', 'https://openalex.org/W2119168550', 'https://openalex.org/W2123126659', 'https://openalex.org/W2139183784', 'https://openalex.org/W2146418175', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2168966090', 'https://openalex.org/W2437005631']","Chiang’s hierarchical phrase-based (HPB) translation model advances the state-of-the-art in statistical machine translation by expanding conventional phrases to hierarchical phrases – phrases that contain sub-phrases. However, the original HPB model is prone to overgeneration due to lack of linguistic knowledge: the grammar may suggest more derivations than appropriate, many of which may lead to ungrammatical translations. On the other hand, limitations of glue grammar rules in the original HPB model may actually prevent systems from considering some reasonable derivations. This paper presents a simple but effective translation model, called the Head-Driven HPB (HD-HPB) model, which incorporates head information in translation rules to better capture syntax-driven information in a derivation. In addition, unlike the original glue rules, the HD-HPB model allows improved reordering between any two neighboring non-terminals to explore a larger reordering search space. An extensive set of experiments on Chinese-English translation on four NIST MT test sets, using both a small and a large training set, show that our HD-HPB model consistently and statistically significantly outperforms Chiang’s model as well as a source side SAMT-style model. 1",1.0
SKG_MT_1229,https://openalex.org/W2515116787,2016,0,"['https://openalex.org/W222053410', 'https://openalex.org/W932413789', 'https://openalex.org/W1479669738', 'https://openalex.org/W1551202288', 'https://openalex.org/W1902237438', 'https://openalex.org/W2100664567', 'https://openalex.org/W2110104386', 'https://openalex.org/W2118434577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140133598', 'https://openalex.org/W2146574666', 'https://openalex.org/W2153999629', 'https://openalex.org/W2154581043', 'https://openalex.org/W2156985047', 'https://openalex.org/W2165666205', 'https://openalex.org/W2169954957', 'https://openalex.org/W2250445771', 'https://openalex.org/W2250548645', 'https://openalex.org/W2251222643', 'https://openalex.org/W2251269336', 'https://openalex.org/W2251682575', 'https://openalex.org/W2251855842', 'https://openalex.org/W2252264945', 'https://openalex.org/W2293111166', 'https://openalex.org/W2962997665', 'https://openalex.org/W2963636883', 'https://openalex.org/W2963661253', 'https://openalex.org/W2964308564']","One of the major challenges for statistical machine translation (SMT) is to choose the appropriate translation rules based on the sentence context.This paper proposes a continuous space rule selection (CSRS) model for syntax-based SMT to perform this context-dependent rule selection.In contrast to existing maximum entropy based rule selection (MERS) models, which use discrete representations of words as features, the CSRS model is learned by a feed-forward neural network and uses real-valued vector representations of words, allowing for better generalization.In addition, we propose a method to train the rule selection models only on minimal rules, which are more frequent and have richer training data compared to non-minimal rules.We tested our model on different translation tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively.",1.0
SKG_MT_1230,https://openalex.org/W2473895758,2016,25,"['https://openalex.org/W1531245479', 'https://openalex.org/W1571735237', 'https://openalex.org/W2046384065', 'https://openalex.org/W2057235967', 'https://openalex.org/W2100271871', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110417778', 'https://openalex.org/W2112815206', 'https://openalex.org/W2140671896', 'https://openalex.org/W2153579005', 'https://openalex.org/W2153635508', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158195707', 'https://openalex.org/W2164788644', 'https://openalex.org/W2181607856', 'https://openalex.org/W2251044602', 'https://openalex.org/W2251171258', 'https://openalex.org/W2251347599', 'https://openalex.org/W2725430220', 'https://openalex.org/W2797922696', 'https://openalex.org/W2998704965', 'https://openalex.org/W3120421331', 'https://openalex.org/W4241645538', 'https://openalex.org/W4285719527', 'https://openalex.org/W4294170691']","Shanbo Cheng, Shujian Huang, Huadong Chen, Xin-Yu Dai, Jiajun Chen. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",0.9923664122137404
SKG_MT_1231,https://openalex.org/W203061456,2010,4,"['https://openalex.org/W201532657', 'https://openalex.org/W1584789643', 'https://openalex.org/W2042783153', 'https://openalex.org/W2053306448', 'https://openalex.org/W2069712814', 'https://openalex.org/W2100397666', 'https://openalex.org/W2113788796', 'https://openalex.org/W2117621558', 'https://openalex.org/W2118972857', 'https://openalex.org/W2124807415', 'https://openalex.org/W2136657878', 'https://openalex.org/W2141182089', 'https://openalex.org/W2143134347', 'https://openalex.org/W3183153947']",This paper describes the Aalto submission for the German-to-English and the Czechto-English translation tasks of the ACL,1.0
SKG_MT_1232,https://openalex.org/W1635500704,2010,14,"['https://openalex.org/W22168010', 'https://openalex.org/W1539833865', 'https://openalex.org/W1541981365', 'https://openalex.org/W2010588484', 'https://openalex.org/W2051434435', 'https://openalex.org/W2105410942', 'https://openalex.org/W2144452292', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153653739', 'https://openalex.org/W2168822971', 'https://openalex.org/W2269188183', 'https://openalex.org/W2402118343', 'https://openalex.org/W2570764145']","Production of parallel training corpora for the development of statistical machine translation (SMT) systems for resource-poor languages usually requires extensive manual effort. Active sample selection aims to reduce the labor, time, and expense incurred in producing such resources, attaining a given performance benchmark with the smallest possible training corpus by choosing informative, nonredundant source sentences from an available candidate pool for manual translation. We present a novel, discriminative sample selection strategy that preferentially selects batches of candidate sentences with constructs that lead to erroneous translations on a held-out development set. The proposed strategy supports a built-in diversity mechanism that reduces redundancy in the selected batches. Simulation experiments on English-to-Pashto and Spanish-to-English translation tasks demonstrate the superiority of the proposed approach to a number of competing techniques, such as random selection, dissimilarity-based selection, as well as a recently proposed semisupervised active learning strategy. 1",1.0
SKG_MT_1233,https://openalex.org/W2407166119,2016,75,"['https://openalex.org/W10656737', 'https://openalex.org/W932413789', 'https://openalex.org/W1582482241', 'https://openalex.org/W1606347560', 'https://openalex.org/W1753482797', 'https://openalex.org/W1818782312', 'https://openalex.org/W2021568158', 'https://openalex.org/W2046932483', 'https://openalex.org/W2100664567', 'https://openalex.org/W2105891181', 'https://openalex.org/W2113021982', 'https://openalex.org/W2114550122', 'https://openalex.org/W2114912785', 'https://openalex.org/W2117130368', 'https://openalex.org/W2118434577', 'https://openalex.org/W2121338597', 'https://openalex.org/W2126399065', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140343992', 'https://openalex.org/W2141440284', 'https://openalex.org/W2145631726', 'https://openalex.org/W2150378737', 'https://openalex.org/W2157331557', 'https://openalex.org/W2250303366', 'https://openalex.org/W2252272516', 'https://openalex.org/W2291126447', 'https://openalex.org/W2294059674', 'https://openalex.org/W2410539690', 'https://openalex.org/W2437005631', 'https://openalex.org/W2949888546', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962907349', 'https://openalex.org/W2964308564', 'https://openalex.org/W3037950864', 'https://openalex.org/W4300988640']","We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.",1.0
SKG_MT_1234,https://openalex.org/W2945719503,2019,17,"['https://openalex.org/W169664677', 'https://openalex.org/W222053410', 'https://openalex.org/W255975419', 'https://openalex.org/W1601432161', 'https://openalex.org/W1710422233', 'https://openalex.org/W1738081185', 'https://openalex.org/W2101566153', 'https://openalex.org/W2115030595', 'https://openalex.org/W2124807415', 'https://openalex.org/W2136353104', 'https://openalex.org/W2151996595', 'https://openalex.org/W2163038970', 'https://openalex.org/W2166545452', 'https://openalex.org/W2171421863', 'https://openalex.org/W2296696605', 'https://openalex.org/W2340762547', 'https://openalex.org/W2577914266', 'https://openalex.org/W2608029998', 'https://openalex.org/W2739563583', 'https://openalex.org/W2752047430', 'https://openalex.org/W2759511005', 'https://openalex.org/W2796108585', 'https://openalex.org/W2799051177', 'https://openalex.org/W2806412155', 'https://openalex.org/W2808508619', 'https://openalex.org/W2888159079', 'https://openalex.org/W2891534142', 'https://openalex.org/W2900654821', 'https://openalex.org/W2962712961', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963366552', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964093087', 'https://openalex.org/W2964120396', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964289193', 'https://openalex.org/W2964291396']","Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware NMT systems is hampered by several problems. Firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. Secondly, previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. To address the first issue, we perform a human study on an English-Russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. We then create test sets targeting these phenomena. To address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.",0.9961089494163424
SKG_MT_1236,https://openalex.org/W2169836073,2012,26,"['https://openalex.org/W1489181569', 'https://openalex.org/W2057235967', 'https://openalex.org/W2082058602', 'https://openalex.org/W2109664771', 'https://openalex.org/W2124807415', 'https://openalex.org/W2171458318', 'https://openalex.org/W2177801600', 'https://openalex.org/W2496235729', 'https://openalex.org/W2607303097', 'https://openalex.org/W3104922922', 'https://openalex.org/W3196540194']","To facilitate the creation and usage of custom SMT systems we have created a cloud-based platform for do-it-yourself MT. The platform is developed in the EU collaboration project LetsMT!. This system demonstration paper presents the motivation in developing the LetsMT! platform, its main features, architecture, and an evaluation in a practical use case.",0.9925925925925926
SKG_MT_1238,https://openalex.org/W3037003839,2020,1,"['https://openalex.org/W1522301498', 'https://openalex.org/W1902237438', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2525778437', 'https://openalex.org/W2561274697', 'https://openalex.org/W2613904329', 'https://openalex.org/W2740743644', 'https://openalex.org/W2760452458', 'https://openalex.org/W2803985397', 'https://openalex.org/W2805394970', 'https://openalex.org/W2841102378', 'https://openalex.org/W2885250264', 'https://openalex.org/W2888539709', 'https://openalex.org/W2889326796', 'https://openalex.org/W2896457183', 'https://openalex.org/W2897507397', 'https://openalex.org/W2908336025', 'https://openalex.org/W2933138175', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962700074', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963897095', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970409072', 'https://openalex.org/W2977665134', 'https://openalex.org/W2997936605', 'https://openalex.org/W3037465386', 'https://openalex.org/W4288253296', 'https://openalex.org/W4385245566']",This paper describes the CASIA’s system for the IWSLT 2020 open domain translation task. This year we participate in both Chinese→Japanese and Japanese→Chinese translation tasks. Our system is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques.,1.0
SKG_MT_1239,https://openalex.org/W2743229121,2017,10,"['https://openalex.org/W1522301498', 'https://openalex.org/W2100664567', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2146574666', 'https://openalex.org/W2154124206', 'https://openalex.org/W2250831586', 'https://openalex.org/W2251682575', 'https://openalex.org/W2400065810', 'https://openalex.org/W2408504891', 'https://openalex.org/W2410217169', 'https://openalex.org/W2410539690', 'https://openalex.org/W2413436069', 'https://openalex.org/W2525778437', 'https://openalex.org/W2566564022', 'https://openalex.org/W2566623769', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963266340']","In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German-&gt;English news domain and English-&gt;Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong NMT baseline.",1.0
SKG_MT_1240,https://openalex.org/W3037217258,2020,142,"['https://openalex.org/W1524333225', 'https://openalex.org/W1970987322', 'https://openalex.org/W2101105183', 'https://openalex.org/W2113106066', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136530135', 'https://openalex.org/W2184135559', 'https://openalex.org/W2252212004', 'https://openalex.org/W2327501763', 'https://openalex.org/W2406343628', 'https://openalex.org/W2519091744', 'https://openalex.org/W2595715041', 'https://openalex.org/W2762715843', 'https://openalex.org/W2763421725', 'https://openalex.org/W2766219058', 'https://openalex.org/W2785350307', 'https://openalex.org/W2796108585', 'https://openalex.org/W2799923439', 'https://openalex.org/W2804704270', 'https://openalex.org/W2890785942', 'https://openalex.org/W2899274165', 'https://openalex.org/W2903739847', 'https://openalex.org/W2928941594', 'https://openalex.org/W2936848022', 'https://openalex.org/W2936969148', 'https://openalex.org/W2946200149', 'https://openalex.org/W2949328740', 'https://openalex.org/W2949382160', 'https://openalex.org/W2951418500', 'https://openalex.org/W2953190524', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963240019', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964243274', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965198191', 'https://openalex.org/W2970730223', 'https://openalex.org/W2970971581', 'https://openalex.org/W2972448360', 'https://openalex.org/W2972780808', 'https://openalex.org/W2972818416', 'https://openalex.org/W2973048981', 'https://openalex.org/W2974231335', 'https://openalex.org/W2997436923', 'https://openalex.org/W3007142233', 'https://openalex.org/W3008549139', 'https://openalex.org/W3012492057', 'https://openalex.org/W3015338123', 'https://openalex.org/W3015703505', 'https://openalex.org/W3016160783', 'https://openalex.org/W4288400010', 'https://openalex.org/W4295312788', 'https://openalex.org/W4297747548', 'https://openalex.org/W4300558631']","Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, Shinji Watanabe. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2020.",0.9894736842105263
SKG_MT_1241,https://openalex.org/W2970705013,2019,2,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101207453', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2149327368', 'https://openalex.org/W2158195707', 'https://openalex.org/W2186615578', 'https://openalex.org/W2250342921', 'https://openalex.org/W2512848817', 'https://openalex.org/W2595715041', 'https://openalex.org/W2778814079', 'https://openalex.org/W2902319873', 'https://openalex.org/W2902698002', 'https://openalex.org/W2902786662', 'https://openalex.org/W2945534329', 'https://openalex.org/W2952614664', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963118869', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964121744', 'https://openalex.org/W3082674894', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","Jan Rosendahl, Christian Herold, Yunsu Kim, Miguel Graça, Weiyue Wang, Parnia Bahar, Yingbo Gao, Hermann Ney. Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1). 2019.",1.0
SKG_MT_1242,https://openalex.org/W2133622676,2013,39,"['https://openalex.org/W298172948', 'https://openalex.org/W635530177', 'https://openalex.org/W1205016396', 'https://openalex.org/W1625582487', 'https://openalex.org/W1631260214', 'https://openalex.org/W1716250762', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115410424', 'https://openalex.org/W2124807415', 'https://openalex.org/W2132001515', 'https://openalex.org/W2137698233', 'https://openalex.org/W2144600658', 'https://openalex.org/W2151594415', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2250238837', 'https://openalex.org/W2886079201', 'https://openalex.org/W2892587090']","While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 BLEU over unadapted systems and single-domain adaptation.",1.0
SKG_MT_1243,https://openalex.org/W3093818688,2020,19,"['https://openalex.org/W22168010', 'https://openalex.org/W151377110', 'https://openalex.org/W222053410', 'https://openalex.org/W630532510', 'https://openalex.org/W1753482797', 'https://openalex.org/W1821462560', 'https://openalex.org/W2101105183', 'https://openalex.org/W2127218421', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2184135559', 'https://openalex.org/W2257408573', 'https://openalex.org/W2294370754', 'https://openalex.org/W2537667581', 'https://openalex.org/W2567571499', 'https://openalex.org/W2581863816', 'https://openalex.org/W2744813330', 'https://openalex.org/W2750588180', 'https://openalex.org/W2757592053', 'https://openalex.org/W2760260440', 'https://openalex.org/W2760452458', 'https://openalex.org/W2767206889', 'https://openalex.org/W2778814079', 'https://openalex.org/W2805394970', 'https://openalex.org/W2896457183', 'https://openalex.org/W2905933322', 'https://openalex.org/W2945289329', 'https://openalex.org/W2948210185', 'https://openalex.org/W2952650870', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963122608', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963897095', 'https://openalex.org/W2963913356', 'https://openalex.org/W2964247056', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970015022', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970758702', 'https://openalex.org/W2983981554', 'https://openalex.org/W2991786320', 'https://openalex.org/W2995999067', 'https://openalex.org/W3010250255', 'https://openalex.org/W3016610080', 'https://openalex.org/W3046158644', 'https://openalex.org/W3082674894', 'https://openalex.org/W3089067043', 'https://openalex.org/W3204406378', 'https://openalex.org/W4297817411', 'https://openalex.org/W4385245566']","Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity). In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage. We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines. In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.",1.0
SKG_MT_1244,https://openalex.org/W2251084562,2013,4,"['https://openalex.org/W91928571', 'https://openalex.org/W144133692', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111142112', 'https://openalex.org/W2120459453', 'https://openalex.org/W2124807415', 'https://openalex.org/W2140343992', 'https://openalex.org/W2140967626', 'https://openalex.org/W2142112143', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146574666', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156953672', 'https://openalex.org/W2158614781', 'https://openalex.org/W2159755860', 'https://openalex.org/W2160218441', 'https://openalex.org/W2164641162', 'https://openalex.org/W2429914308', 'https://openalex.org/W2437005631']","MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations.",1.0
SKG_MT_1248,https://openalex.org/W2098447295,2013,22,"['https://openalex.org/W37508832', 'https://openalex.org/W165283731', 'https://openalex.org/W1483236033', 'https://openalex.org/W1553669113', 'https://openalex.org/W1631260214', 'https://openalex.org/W1996430422', 'https://openalex.org/W1999187286', 'https://openalex.org/W2022166150', 'https://openalex.org/W2024181699', 'https://openalex.org/W2032175749', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105174485', 'https://openalex.org/W2114584278', 'https://openalex.org/W2117278770', 'https://openalex.org/W2118020653', 'https://openalex.org/W2129629757', 'https://openalex.org/W2130122925', 'https://openalex.org/W2145251161', 'https://openalex.org/W2147694185', 'https://openalex.org/W2148922863', 'https://openalex.org/W2148959489', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153635508', 'https://openalex.org/W2155011048', 'https://openalex.org/W2156909104', 'https://openalex.org/W2162211144', 'https://openalex.org/W2167187514', 'https://openalex.org/W2250929565', 'https://openalex.org/W2758884106', 'https://openalex.org/W3196290596', 'https://openalex.org/W3197138970']","We propose a method for automatically detecting low-quality Web-text translated by statistical machine translation (SMT) systems. We focus on the phrase salad phenomenon that is observed in existing SMT results and propose a set of computationally inexpensive features to effectively detect such machine-translated sentences from a large-scale Web-mined text. Unlike previous approaches that require bilingual data, our method uses only monolingual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8 % for sentences and 80.6 % for text in noisy Web pages. 1",1.0
SKG_MT_1251,https://openalex.org/W3119152910,2020,4,"['https://openalex.org/W2561995736', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963341956', 'https://openalex.org/W2970791445', 'https://openalex.org/W2970986500', 'https://openalex.org/W2971120958', 'https://openalex.org/W2986143601', 'https://openalex.org/W2996403597', 'https://openalex.org/W3029985558', 'https://openalex.org/W3035390927', 'https://openalex.org/W3035459196', 'https://openalex.org/W3120179416']","We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with contextual embed- dings extracted from different layers of two different pretrained models, multilingual BERT and XLM-RoBERTa. We also experiment with learning bilingual mappings that trans- form the vector subspace of the source language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2’s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the tar- get embedding space using a cross-lingual lin- ear projection (CLP) matrix learnt from a small development set.",1.0
SKG_MT_1252,https://openalex.org/W2131296021,2014,39,"['https://openalex.org/W91670896', 'https://openalex.org/W835791623', 'https://openalex.org/W1551202288', 'https://openalex.org/W1563792215', 'https://openalex.org/W1632114991', 'https://openalex.org/W2009856297', 'https://openalex.org/W2042262613', 'https://openalex.org/W2075179010', 'https://openalex.org/W2081266320', 'https://openalex.org/W2095751497', 'https://openalex.org/W2119954850', 'https://openalex.org/W2135161317', 'https://openalex.org/W2136094405', 'https://openalex.org/W2136925175', 'https://openalex.org/W2139183784', 'https://openalex.org/W2149327368', 'https://openalex.org/W2162245945', 'https://openalex.org/W2166957049', 'https://openalex.org/W2167072947', 'https://openalex.org/W2170496269', 'https://openalex.org/W2786305482']","We present a study of aspects of discourse structure -specifically discourse devices used to organize information in a sentence -that significantly impact the quality of machine translation.Our analysis is based on manual evaluations of translations of news from Chinese and Arabic to English.We find that there is a particularly strong mismatch in the notion of what constitutes a sentence in Chinese and English, which occurs often and is associated with significant degradation in translation quality.Also related to lower translation quality is the need to employ multiple explicit discourse connectives (because, but, etc.), as well as the presence of ambiguous discourse connectives in the English translation.Furthermore, the mismatches between discourse expressions across languages significantly impact translation quality.",1.0
SKG_MT_1253,https://openalex.org/W2251855842,2014,123,"['https://openalex.org/W71795751', 'https://openalex.org/W100623710', 'https://openalex.org/W132913264', 'https://openalex.org/W179875071', 'https://openalex.org/W222053410', 'https://openalex.org/W932413789', 'https://openalex.org/W1675954498', 'https://openalex.org/W1725104462', 'https://openalex.org/W1753482797', 'https://openalex.org/W2104518905', 'https://openalex.org/W2117130368', 'https://openalex.org/W2118090838', 'https://openalex.org/W2118536060', 'https://openalex.org/W2133280805', 'https://openalex.org/W2143269898', 'https://openalex.org/W2147262247', 'https://openalex.org/W2147768505', 'https://openalex.org/W2153579005', 'https://openalex.org/W2158388102', 'https://openalex.org/W2158899491', 'https://openalex.org/W2163605009', 'https://openalex.org/W2169488311', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250714477', 'https://openalex.org/W2251395256', 'https://openalex.org/W2251690405', 'https://openalex.org/W2251939518', 'https://openalex.org/W2952230511', 'https://openalex.org/W2998704965', 'https://openalex.org/W3197700997', 'https://openalex.org/W3203497103', 'https://openalex.org/W4233906699', 'https://openalex.org/W4285719527', 'https://openalex.org/W4294170691']","We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings.The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of nontranslation pairs simultaneously.After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other.We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates.Extensive experiments show that the BRAE is remarkably effective in these two tasks.",1.0
SKG_MT_1254,https://openalex.org/W2251324170,2015,4,"['https://openalex.org/W1502293651', 'https://openalex.org/W1577544661', 'https://openalex.org/W1585340632', 'https://openalex.org/W1631260214', 'https://openalex.org/W1829822087', 'https://openalex.org/W1970961429', 'https://openalex.org/W2001064229', 'https://openalex.org/W2060127787', 'https://openalex.org/W2095690342', 'https://openalex.org/W2097927681', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102258849', 'https://openalex.org/W2106310992', 'https://openalex.org/W2113541941', 'https://openalex.org/W2116492146', 'https://openalex.org/W2119168550', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131988669', 'https://openalex.org/W2133956246', 'https://openalex.org/W2145790651', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2152263452', 'https://openalex.org/W2152752344', 'https://openalex.org/W2152907450', 'https://openalex.org/W2153800732', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047', 'https://openalex.org/W2158953777', 'https://openalex.org/W2160382364', 'https://openalex.org/W2161795601', 'https://openalex.org/W2164415172', 'https://openalex.org/W2251053286', 'https://openalex.org/W2251844685', 'https://openalex.org/W2595715041', 'https://openalex.org/W2620000475', 'https://openalex.org/W2950186769']","Compared to tree grammars, graph grammars have stronger generative capacity over structures.Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation.The graph we use is directly converted from a dependency tree by labelling edges.We build our translation model in the log-linear framework with standard features.Large-scale experiments on Chinese-English and German-English tasks show that our model is significantly better than the state-of-the-art hierarchical phrase-based (HPB) model and a recently improved dependency tree-to-string model on BLEU, METEOR and TER scores.Experiments also suggest that our model has better capability to perform long-distance reordering and is more suitable for translating long sentences.",1.0
SKG_MT_1257,https://openalex.org/W2461086874,2016,8,"['https://openalex.org/W4707553', 'https://openalex.org/W38126138', 'https://openalex.org/W92412080', 'https://openalex.org/W143516883', 'https://openalex.org/W1819903106', 'https://openalex.org/W1973152633', 'https://openalex.org/W1981053739', 'https://openalex.org/W1990190154', 'https://openalex.org/W2000026602', 'https://openalex.org/W2012833704', 'https://openalex.org/W2033593667', 'https://openalex.org/W2101096097', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105673178', 'https://openalex.org/W2107695330', 'https://openalex.org/W2124807415', 'https://openalex.org/W2127876534', 'https://openalex.org/W2152925220', 'https://openalex.org/W2163364265', 'https://openalex.org/W2165558283', 'https://openalex.org/W2165599843', 'https://openalex.org/W2251764313', 'https://openalex.org/W2407127455']",,1.0
SKG_MT_1258,https://openalex.org/W3105825505,2020,32,"['https://openalex.org/W1494198834', 'https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2112796928', 'https://openalex.org/W2113106066', 'https://openalex.org/W2257408573', 'https://openalex.org/W2466918907', 'https://openalex.org/W2563850823', 'https://openalex.org/W2593011301', 'https://openalex.org/W2745785989', 'https://openalex.org/W2785350307', 'https://openalex.org/W2921280978', 'https://openalex.org/W2936774411', 'https://openalex.org/W2936969148', 'https://openalex.org/W2945700568', 'https://openalex.org/W2949328740', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963767893', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963834942', 'https://openalex.org/W2964102148', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964172053', 'https://openalex.org/W2971152344', 'https://openalex.org/W2973048981', 'https://openalex.org/W2982129078', 'https://openalex.org/W3007142233', 'https://openalex.org/W3008549139', 'https://openalex.org/W3015440307', 'https://openalex.org/W3017535695', 'https://openalex.org/W3135348217', 'https://openalex.org/W4298393544', 'https://openalex.org/W4300558631', 'https://openalex.org/W4385245566']","Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.",1.0
SKG_MT_1259,https://openalex.org/W3032317609,2020,2,"['https://openalex.org/W1959608418', 'https://openalex.org/W2409550820', 'https://openalex.org/W2552839021', 'https://openalex.org/W2626778328', 'https://openalex.org/W2782704147', 'https://openalex.org/W2883384628', 'https://openalex.org/W2889326796', 'https://openalex.org/W2904751523', 'https://openalex.org/W2908336025', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963090522', 'https://openalex.org/W2963145887', 'https://openalex.org/W2963170102', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963650605', 'https://openalex.org/W2963713328', 'https://openalex.org/W2963736842', 'https://openalex.org/W2964076537', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2973501257', 'https://openalex.org/W2987820424', 'https://openalex.org/W3009437003']","Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables. The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy. Unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training time. Previous works impose strong assumptions on the distribution of the latent code and limit the choice of the NMT architecture. In this paper, we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows. We demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions, significantly outperforming strong baselines.",1.0
SKG_MT_1261,https://openalex.org/W2947465697,2019,2,"['https://openalex.org/W27508602', 'https://openalex.org/W131533222', 'https://openalex.org/W1521058074', 'https://openalex.org/W1980776243', 'https://openalex.org/W2028175314', 'https://openalex.org/W2051593977', 'https://openalex.org/W2082980366', 'https://openalex.org/W2086039194', 'https://openalex.org/W2101105183', 'https://openalex.org/W2107130271', 'https://openalex.org/W2118021410', 'https://openalex.org/W2129468719', 'https://openalex.org/W2143927888', 'https://openalex.org/W2145094598', 'https://openalex.org/W2154359981', 'https://openalex.org/W2154652894', 'https://openalex.org/W2161374612', 'https://openalex.org/W2186267129', 'https://openalex.org/W2251939518', 'https://openalex.org/W2252001469', 'https://openalex.org/W2626778328', 'https://openalex.org/W2741049976', 'https://openalex.org/W2753738274', 'https://openalex.org/W2760656271', 'https://openalex.org/W2766462485', 'https://openalex.org/W2789543585', 'https://openalex.org/W2796032388', 'https://openalex.org/W2798858969', 'https://openalex.org/W2798931235', 'https://openalex.org/W2804145368', 'https://openalex.org/W2896457183', 'https://openalex.org/W2951714314', 'https://openalex.org/W2951813108', 'https://openalex.org/W2952463445', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963012544', 'https://openalex.org/W2963126845', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963418779', 'https://openalex.org/W2963463583', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963799213', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964212550', 'https://openalex.org/W2964308564', 'https://openalex.org/W3104033643']","Paraphrasing exemplifies the ability to abstract semantic content from surface forms. Recent work on automatic paraphrasing is dominated by methods leveraging Machine Translation (MT) as an intermediate step. This contrasts with humans, who can paraphrase without being bilingual. This work proposes to learn paraphrasing models from an unlabeled monolingual corpus only. To that end, we propose a residual variant of vector-quantized variational auto-encoder. We compare with MT-based approaches on paraphrase identification, generation, and training augmentation. Monolingual paraphrasing outperforms unsupervised translation in all settings. Comparisons with supervised translation are more mixed: monolingual paraphrasing is interesting for identification and augmentation; supervised translation is superior for generation.",1.0
SKG_MT_1262,https://openalex.org/W3034214887,2020,32,"['https://openalex.org/W1522301498', 'https://openalex.org/W1905522558', 'https://openalex.org/W2016043834', 'https://openalex.org/W2097998348', 'https://openalex.org/W2117278770', 'https://openalex.org/W2192203593', 'https://openalex.org/W2296073425', 'https://openalex.org/W2515631395', 'https://openalex.org/W2525778437', 'https://openalex.org/W2567571499', 'https://openalex.org/W2740718109', 'https://openalex.org/W2741838462', 'https://openalex.org/W2750588180', 'https://openalex.org/W2756978580', 'https://openalex.org/W2757592053', 'https://openalex.org/W2760452458', 'https://openalex.org/W2801179766', 'https://openalex.org/W2802153702', 'https://openalex.org/W2886342729', 'https://openalex.org/W2896457183', 'https://openalex.org/W2898846200', 'https://openalex.org/W2902918014', 'https://openalex.org/W2919188216', 'https://openalex.org/W2923622379', 'https://openalex.org/W2931612954', 'https://openalex.org/W2946379889', 'https://openalex.org/W2947187520', 'https://openalex.org/W2950760213', 'https://openalex.org/W2951665052', 'https://openalex.org/W2952474700', 'https://openalex.org/W2953355233', 'https://openalex.org/W2962890089', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963281280', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963949210', 'https://openalex.org/W2963962154', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964067969', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964125718', 'https://openalex.org/W2964327384', 'https://openalex.org/W2970066656', 'https://openalex.org/W2970858854', 'https://openalex.org/W3035019713', 'https://openalex.org/W3035608860', 'https://openalex.org/W3099907503', 'https://openalex.org/W3104229680', 'https://openalex.org/W3124229194', 'https://openalex.org/W3208011254', 'https://openalex.org/W4297782088', 'https://openalex.org/W4298018319', 'https://openalex.org/W4298116016', 'https://openalex.org/W4300223101', 'https://openalex.org/W4385245566']","Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.",1.0
SKG_MT_1264,https://openalex.org/W2512273476,2016,30,"['https://openalex.org/W1533861849', 'https://openalex.org/W1966443646', 'https://openalex.org/W2065240770', 'https://openalex.org/W2070150502', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105051853', 'https://openalex.org/W2119788759', 'https://openalex.org/W2133280805', 'https://openalex.org/W2138692180', 'https://openalex.org/W2141599568', 'https://openalex.org/W2146502635', 'https://openalex.org/W2149327368', 'https://openalex.org/W2153045205', 'https://openalex.org/W2155712036', 'https://openalex.org/W2162355876', 'https://openalex.org/W2173361515', 'https://openalex.org/W2177512083', 'https://openalex.org/W2250175451', 'https://openalex.org/W2250421192', 'https://openalex.org/W2251202616', 'https://openalex.org/W2251506003', 'https://openalex.org/W2252143362', 'https://openalex.org/W2431106835', 'https://openalex.org/W2464096189', 'https://openalex.org/W2467646401', 'https://openalex.org/W2468484304', 'https://openalex.org/W2468672598', 'https://openalex.org/W2469197239', 'https://openalex.org/W2963001778', 'https://openalex.org/W2963481404', 'https://openalex.org/W2964154091', 'https://openalex.org/W4301290611']","We explore the applicability of machine translation evaluation (MTE) methods to a very different problem: answer ranking in community Question Answering.In particular, we adopt a pairwise neural network (NN) architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings, and which efficiently models complex non-linear interactions.The evaluation results show state-of-the-art performance, with sizeable contribution from both the MTE features and from the pairwise NN architecture.",1.0
SKG_MT_1267,https://openalex.org/W2989523826,2019,1,"['https://openalex.org/W1959608418', 'https://openalex.org/W2025768430', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108501770', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153579005', 'https://openalex.org/W2176263492', 'https://openalex.org/W2525778437', 'https://openalex.org/W2561995736', 'https://openalex.org/W2580192806', 'https://openalex.org/W2612675303', 'https://openalex.org/W2613904329', 'https://openalex.org/W2740132093', 'https://openalex.org/W2741602058', 'https://openalex.org/W2751527518', 'https://openalex.org/W2762484717', 'https://openalex.org/W2794714381', 'https://openalex.org/W2798931235', 'https://openalex.org/W2890007195', 'https://openalex.org/W2914120296', 'https://openalex.org/W2932618389', 'https://openalex.org/W2944815030', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962762898', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962808855', 'https://openalex.org/W2962824887', 'https://openalex.org/W2963084599', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963248296', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963413917', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963602293', 'https://openalex.org/W2963768805', 'https://openalex.org/W2963804993', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964032708', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964352247', 'https://openalex.org/W4294170691', 'https://openalex.org/W4298393544', 'https://openalex.org/W4299579390', 'https://openalex.org/W4385245566']","In this paper, we alleviate the local optimality of back-translation by learning a policy (takes the form of an encoder-decoder and is defined by its parameters) with future rewarding under the reinforcement learning framework, which aims to optimize the global word predictions for unsupervised neural machine translation. To this end, we design a novel reward function to characterize high-quality translations from two aspects: n-gram matching and semantic adequacy. The n-gram matching is defined as an alternative for the discrete BLEU metric, and the semantic adequacy is used to measure the adequacy of conveying the meaning of the source sentence to the target. During training, our model strives for earning higher rewards by learning to produce grammatically more accurate and semantically more adequate translations. Besides, a variational inference network (VIN) is proposed to constrain the corresponding sentences in two languages have the same or similar latent semantic code. On the widely used WMT’14 English-French, WMT’16 English-German and NIST Chinese-to-English benchmarks, our models respectively obtain 27.59/27.15, 19.65/23.42 and 22.40 BLEU points without using any labeled data, demonstrating consistent improvements over previous unsupervised NMT models.",1.0
SKG_MT_1269,https://openalex.org/W3012638462,2020,8,"['https://openalex.org/W1724438581', 'https://openalex.org/W1753482797', 'https://openalex.org/W1821462560', 'https://openalex.org/W1902237438', 'https://openalex.org/W1935978687', 'https://openalex.org/W1992348535', 'https://openalex.org/W2002016471', 'https://openalex.org/W2064675550', 'https://openalex.org/W2114766824', 'https://openalex.org/W2119144962', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2140660536', 'https://openalex.org/W2141155619', 'https://openalex.org/W2147800946', 'https://openalex.org/W2157331557', 'https://openalex.org/W2163605009', 'https://openalex.org/W2198190323', 'https://openalex.org/W2319920447', 'https://openalex.org/W2405920868', 'https://openalex.org/W2460130460', 'https://openalex.org/W2469490737', 'https://openalex.org/W2512629640', 'https://openalex.org/W2525778437', 'https://openalex.org/W2557257847', 'https://openalex.org/W2597655663', 'https://openalex.org/W2608554408', 'https://openalex.org/W2613332842', 'https://openalex.org/W2626778328', 'https://openalex.org/W2748428003', 'https://openalex.org/W2754526845', 'https://openalex.org/W2767785892', 'https://openalex.org/W2767989436', 'https://openalex.org/W2777406049', 'https://openalex.org/W2787752464', 'https://openalex.org/W2806311723', 'https://openalex.org/W2889847962', 'https://openalex.org/W2892090442', 'https://openalex.org/W2914526845', 'https://openalex.org/W2947946877', 'https://openalex.org/W2949961122', 'https://openalex.org/W2950458216', 'https://openalex.org/W2950894517', 'https://openalex.org/W2951978180', 'https://openalex.org/W2952191002', 'https://openalex.org/W2952344559', 'https://openalex.org/W2952369090', 'https://openalex.org/W2952444318', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963689957', 'https://openalex.org/W2964199361', 'https://openalex.org/W2979314664', 'https://openalex.org/W3133056632']","State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to full-precision, our 8-bit models score greater or equal BLEU on most tasks. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results.",1.0
SKG_MT_1271,https://openalex.org/W2889756799,2018,13,"['https://openalex.org/W46679369', 'https://openalex.org/W1522301498', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2101105183', 'https://openalex.org/W2127863960', 'https://openalex.org/W2130903752', 'https://openalex.org/W2133564696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2229833550', 'https://openalex.org/W2251743902', 'https://openalex.org/W2525778437', 'https://openalex.org/W2555745756', 'https://openalex.org/W2605717780', 'https://openalex.org/W2743555600', 'https://openalex.org/W2786790428', 'https://openalex.org/W2798465082', 'https://openalex.org/W2799920282', 'https://openalex.org/W2899771611', 'https://openalex.org/W2913340405', 'https://openalex.org/W2949335953', 'https://openalex.org/W2949888546', 'https://openalex.org/W2950300355', 'https://openalex.org/W2952230511', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962964385', 'https://openalex.org/W2963088995', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964085268']","In multilingual neural machine translation, it has been shown that sharing a single translation model between multiple languages can achieve competitive performance, sometimes even leading to performance gains over bilingually trained models. However, these improvements are not uniform; often multilingual parameter sharing results in a decrease in accuracy due to translation models not being able to accommodate different languages in their limited parameter space. In this work, we examine parameter sharing techniques that strike a happy medium between full sharing and individual training, specifically focusing on the self-attentional Transformer model. We find that the full parameter sharing approach leads to increases in BLEU scores mainly when the target languages are from a similar language family. However, even in the case where target languages are from different families where full parameter sharing leads to a noticeable drop in BLEU scores, our proposed methods for partial sharing of parameters can lead to substantial improvements in translation accuracy.",1.0
SKG_MT_1272,https://openalex.org/W3092100598,2020,2,"['https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123301721', 'https://openalex.org/W2144746247', 'https://openalex.org/W2148365102', 'https://openalex.org/W2149327368', 'https://openalex.org/W2154652894', 'https://openalex.org/W2156985047', 'https://openalex.org/W2340762547', 'https://openalex.org/W2512848817', 'https://openalex.org/W2770840533', 'https://openalex.org/W2902463012', 'https://openalex.org/W2903376039', 'https://openalex.org/W2952103439', 'https://openalex.org/W2953072129', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963824830', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963959336', 'https://openalex.org/W2967707653', 'https://openalex.org/W2970986500', 'https://openalex.org/W2996403597', 'https://openalex.org/W3006801027', 'https://openalex.org/W3034238904', 'https://openalex.org/W3034716087', 'https://openalex.org/W3034998639', 'https://openalex.org/W3035252911', 'https://openalex.org/W3099757670', 'https://openalex.org/W3119881489']","This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the \textit{mismatching issue} when directly adopting BERTScore to QE. Specifically, there exist lots of mismatching errors between the source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross-lingual patterns, \textit{e.g.} word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.",1.0
SKG_MT_1274,https://openalex.org/W3034955736,2020,53,"['https://openalex.org/W854541894', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2519091744', 'https://openalex.org/W2525778437', 'https://openalex.org/W2540404261', 'https://openalex.org/W2767206889', 'https://openalex.org/W2885588803', 'https://openalex.org/W2888539709', 'https://openalex.org/W2908336025', 'https://openalex.org/W2934842096', 'https://openalex.org/W2946794439', 'https://openalex.org/W2948197522', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962801832', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963631431', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964302946', 'https://openalex.org/W2964308564', 'https://openalex.org/W2970045405', 'https://openalex.org/W2970565456', 'https://openalex.org/W2970777192', 'https://openalex.org/W2970971581', 'https://openalex.org/W2977162702', 'https://openalex.org/W2977721932', 'https://openalex.org/W2988309730', 'https://openalex.org/W2990555152', 'https://openalex.org/W3103729510', 'https://openalex.org/W3106104873', 'https://openalex.org/W4252337780', 'https://openalex.org/W4288103164', 'https://openalex.org/W4288347855', 'https://openalex.org/W4295312788', 'https://openalex.org/W4301368689', 'https://openalex.org/W4385245566']","Recent work has questioned the importance of the Transformer's multi-headed attention for achieving high translation quality. We push further in this direction by developing a ""hard-coded"" attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.",1.0
SKG_MT_1276,https://openalex.org/W1594128446,2012,15,"['https://openalex.org/W75781568', 'https://openalex.org/W108437174', 'https://openalex.org/W165935821', 'https://openalex.org/W222053410', 'https://openalex.org/W1535015163', 'https://openalex.org/W1588242179', 'https://openalex.org/W1697844855', 'https://openalex.org/W1920193847', 'https://openalex.org/W1973923101', 'https://openalex.org/W2037894654', 'https://openalex.org/W2092654472', 'https://openalex.org/W2095690342', 'https://openalex.org/W2100565866', 'https://openalex.org/W2107038437', 'https://openalex.org/W2119168550', 'https://openalex.org/W2123126659', 'https://openalex.org/W2139183784', 'https://openalex.org/W2146418175', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154124206', 'https://openalex.org/W2168966090', 'https://openalex.org/W2437005631']","Chiang’s hierarchical phrase-based (HPB) translation model advances the state-of-the-art in statistical machine translation by expanding conventional phrases to hierarchical phrases – phrases that contain sub-phrases. However, the original HPB model is prone to overgeneration due to lack of linguistic knowledge: the grammar may suggest more derivations than appropriate, many of which may lead to ungrammatical translations. On the other hand, limitations of glue grammar rules in the original HPB model may actually prevent systems from considering some reasonable derivations. This paper presents a simple but effective translation model, called the Head-Driven HPB (HD-HPB) model, which incorporates head information in translation rules to better capture syntax-driven information in a derivation. In addition, unlike the original glue rules, the HD-HPB model allows improved reordering between any two neighboring non-terminals to explore a larger reordering search space. An extensive set of experiments on Chinese-English translation on four NIST MT test sets, using both a small and a large training set, show that our HD-HPB model consistently and statistically significantly outperforms Chiang’s model as well as a source side SAMT-style model. 1",1.0
SKG_MT_1277,https://openalex.org/W2117607680,2011,10,"['https://openalex.org/W222053410', 'https://openalex.org/W1554237613', 'https://openalex.org/W1561155402', 'https://openalex.org/W1566209275', 'https://openalex.org/W1631260214', 'https://openalex.org/W1662133657', 'https://openalex.org/W1966907789', 'https://openalex.org/W1973923101', 'https://openalex.org/W1980776243', 'https://openalex.org/W2001810881', 'https://openalex.org/W2006617528', 'https://openalex.org/W2009987829', 'https://openalex.org/W2045767629', 'https://openalex.org/W2051593977', 'https://openalex.org/W2052806359', 'https://openalex.org/W2063596712', 'https://openalex.org/W2086039194', 'https://openalex.org/W2090915937', 'https://openalex.org/W2091384152', 'https://openalex.org/W2095684787', 'https://openalex.org/W2097726431', 'https://openalex.org/W2098603082', 'https://openalex.org/W2100976324', 'https://openalex.org/W2103081392', 'https://openalex.org/W2103864529', 'https://openalex.org/W2107130271', 'https://openalex.org/W2108701407', 'https://openalex.org/W2119465309', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130359236', 'https://openalex.org/W2132446289', 'https://openalex.org/W2138974820', 'https://openalex.org/W2139828769', 'https://openalex.org/W2145685230', 'https://openalex.org/W2146574666', 'https://openalex.org/W2149327368', 'https://openalex.org/W2150836210', 'https://openalex.org/W2154124206', 'https://openalex.org/W2159495802', 'https://openalex.org/W2160052288', 'https://openalex.org/W2162507901', 'https://openalex.org/W2167170026', 'https://openalex.org/W2467575451', 'https://openalex.org/W2468684362', 'https://openalex.org/W3133994440']","Paraphrases are useful for statistical machine translation (SMT) and natural language processing tasks. Distributional paraphrase generation is independent of parallel texts and syntactic parses, and hence is suitable also for resource-poor languages, but tends to erroneously rank antonyms, trend-contrasting, and polarity-dissimilar candidates as good paraphrases. We present here a novel method for improving distributional paraphrasing by filtering out such candidates. We evaluate it in simulated low and mid-resourced SMT tasks, translating from English to two quite different languages. We show statistically significant gains in English-to-Chinese translation quality, up to 1 BLEU from nonfiltered paraphrase-augmented models (1.6 BLEU from baseline). We also show that yielding gains in translation to Arabic, a morphologically rich language, is not straightforward. 1",1.0
SKG_MT_1278,https://openalex.org/W2898972706,2018,59,"['https://openalex.org/W1515851193', 'https://openalex.org/W1753482797', 'https://openalex.org/W1810943226', 'https://openalex.org/W1902237438', 'https://openalex.org/W2006969979', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2119717200', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133564696', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2532807140', 'https://openalex.org/W2547875792', 'https://openalex.org/W2575583396', 'https://openalex.org/W2595715041', 'https://openalex.org/W2740759433', 'https://openalex.org/W2752047430', 'https://openalex.org/W2885765547', 'https://openalex.org/W2962700074', 'https://openalex.org/W2962780935', 'https://openalex.org/W2963091079', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4245882962']","Although end-to-end neural machine translation (NMT) has achieved remarkable progress in the recent years, the idea of adopting multi-pass decoding mechanism into conventional NMT is not well explored. In this paper, we propose a novel architecture called adaptive multi-pass decoder, which introduces a flexible multi-pass polishing mechanism to extend the capacity of NMT via reinforcement learning. More specifically, we adopt an extra policy network to automatically choose a suitable and effective number of decoding passes, according to the complexity of source sentences and the quality of the generated translations. Extensive experiments on Chinese-English translation demonstrate the effectiveness of our proposed adaptive multi-pass decoder upon the conventional NMT with a significant improvement about 1.55 BLEU.",1.0
SKG_MT_1280,https://openalex.org/W2141895568,2013,46,"['https://openalex.org/W44695385', 'https://openalex.org/W120531462', 'https://openalex.org/W198017583', 'https://openalex.org/W255975419', 'https://openalex.org/W569521333', 'https://openalex.org/W1531087451', 'https://openalex.org/W1901714926', 'https://openalex.org/W2030987872', 'https://openalex.org/W2067815623', 'https://openalex.org/W2101105183', 'https://openalex.org/W2124807415', 'https://openalex.org/W2134800885', 'https://openalex.org/W2140646431', 'https://openalex.org/W2146574666', 'https://openalex.org/W2151996595', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154006786', 'https://openalex.org/W2163038970', 'https://openalex.org/W2164649209', 'https://openalex.org/W2180952760', 'https://openalex.org/W3210965875']","We describe Docent, an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units. By taking translation to the document level, our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware SMT models. 1",0.9937888198757764
SKG_MT_1282,https://openalex.org/W2250974141,2013,23,"['https://openalex.org/W76590478', 'https://openalex.org/W108437174', 'https://openalex.org/W111475876', 'https://openalex.org/W222053410', 'https://openalex.org/W1961521608', 'https://openalex.org/W1973923101', 'https://openalex.org/W2060127787', 'https://openalex.org/W2101190066', 'https://openalex.org/W2110104386', 'https://openalex.org/W2115557995', 'https://openalex.org/W2130988241', 'https://openalex.org/W2142632103', 'https://openalex.org/W2144002870', 'https://openalex.org/W2144279206', 'https://openalex.org/W2146574666', 'https://openalex.org/W2150378737', 'https://openalex.org/W2151170651', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2158431714', 'https://openalex.org/W2158847908', 'https://openalex.org/W2164196204', 'https://openalex.org/W2165666205', 'https://openalex.org/W2434901392', 'https://openalex.org/W2437005631']","Incorporating semantic structure into a linguistics-free translation model is challenging, since semantic structures are closely tied to syntax. In this paper, we propose a two-level approach to exploiting predicate-argument structure reordering in a hierarchical phrase-based translation model. First, we introduce linguistically motivated constraints into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy. 1",1.0
SKG_MT_1285,https://openalex.org/W2971296520,2019,55,"['https://openalex.org/W854541894', 'https://openalex.org/W1498990157', 'https://openalex.org/W1601924930', 'https://openalex.org/W1787224781', 'https://openalex.org/W1902237438', 'https://openalex.org/W2006969979', 'https://openalex.org/W2123045220', 'https://openalex.org/W2133564696', 'https://openalex.org/W2148708890', 'https://openalex.org/W2153653739', 'https://openalex.org/W2156985047', 'https://openalex.org/W2185733961', 'https://openalex.org/W2327501763', 'https://openalex.org/W2413794162', 'https://openalex.org/W2606974598', 'https://openalex.org/W2613904329', 'https://openalex.org/W2626639386', 'https://openalex.org/W2657631929', 'https://openalex.org/W2741040846', 'https://openalex.org/W2760327630', 'https://openalex.org/W2798474427', 'https://openalex.org/W2888539709', 'https://openalex.org/W2897507397', 'https://openalex.org/W2912070261', 'https://openalex.org/W2912351236', 'https://openalex.org/W2962687637', 'https://openalex.org/W2962714778', 'https://openalex.org/W2962834107', 'https://openalex.org/W2962851944', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963382180', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963499882', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963598809', 'https://openalex.org/W2964159778', 'https://openalex.org/W2964174820', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W4293861706', 'https://openalex.org/W4298170715', 'https://openalex.org/W4385245566']","Despite their original goal to jointly learn to align and translate, Neural Machine Translation (NMT) models, especially Transformer, are often perceived as not learning interpretable word alignments. In this paper, we show that NMT models do learn interpretable word alignments, which could only be revealed with proper interpretation methods. We propose a series of such methods that are model-agnostic, are able to be applied either offline or online, and do not require parameter update or architectural change. We show that under the force decoding setup, the alignments induced by our interpretation method are of better quality than fast-align for some systems, and when performing free decoding, they agree well with the alignments induced by automatic alignment tools.",1.0
SKG_MT_1287,https://openalex.org/W2741787148,2017,28,"['https://openalex.org/W635530177', 'https://openalex.org/W1902237438', 'https://openalex.org/W1905522558', 'https://openalex.org/W2101105183', 'https://openalex.org/W2117278770', 'https://openalex.org/W2118434577', 'https://openalex.org/W2124807415', 'https://openalex.org/W2135873687', 'https://openalex.org/W2147262247', 'https://openalex.org/W2180952760', 'https://openalex.org/W2184135559', 'https://openalex.org/W2218630638', 'https://openalex.org/W2251453970', 'https://openalex.org/W2337363174', 'https://openalex.org/W2402118343', 'https://openalex.org/W2512924740', 'https://openalex.org/W2515631395', 'https://openalex.org/W2525778437', 'https://openalex.org/W2540404261', 'https://openalex.org/W2567571499', 'https://openalex.org/W2577152659', 'https://openalex.org/W2579496717', 'https://openalex.org/W2584268338', 'https://openalex.org/W2736422900', 'https://openalex.org/W2740553716', 'https://openalex.org/W2773549337', 'https://openalex.org/W2949888546', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963506925', 'https://openalex.org/W2964308564', 'https://openalex.org/W3098341425', 'https://openalex.org/W3204406378']","Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.",1.0
SKG_MT_1288,https://openalex.org/W2964007535,2016,257,"['https://openalex.org/W179875071', 'https://openalex.org/W1522301498', 'https://openalex.org/W1571227886', 'https://openalex.org/W1606058447', 'https://openalex.org/W1753482797', 'https://openalex.org/W1815076433', 'https://openalex.org/W1902237438', 'https://openalex.org/W2006617528', 'https://openalex.org/W2111666304', 'https://openalex.org/W2119727789', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131774270', 'https://openalex.org/W2133564696', 'https://openalex.org/W2157331557', 'https://openalex.org/W2158364201', 'https://openalex.org/W2220350356', 'https://openalex.org/W2251743902', 'https://openalex.org/W2252272516', 'https://openalex.org/W2339995566', 'https://openalex.org/W2417549359', 'https://openalex.org/W2595715041', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963324947', 'https://openalex.org/W2963842982', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564']","In this paper, we propose a novel finetuning algorithm for the recently introduced multiway, multilingual neural machine translate that enables zero-resource machine translation.When used together with novel manyto-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters.",1.0
SKG_MT_1289,https://openalex.org/W3035636774,2020,29,"['https://openalex.org/W1522301498', 'https://openalex.org/W2101105183', 'https://openalex.org/W2123442489', 'https://openalex.org/W2124807415', 'https://openalex.org/W2133564696', 'https://openalex.org/W2466062786', 'https://openalex.org/W2525778437', 'https://openalex.org/W2532807140', 'https://openalex.org/W2575583396', 'https://openalex.org/W2576482813', 'https://openalex.org/W2595715041', 'https://openalex.org/W2613904329', 'https://openalex.org/W2739894144', 'https://openalex.org/W2752047430', 'https://openalex.org/W2757978590', 'https://openalex.org/W2798463315', 'https://openalex.org/W2798542795', 'https://openalex.org/W2889009749', 'https://openalex.org/W2889404673', 'https://openalex.org/W2890908793', 'https://openalex.org/W2898972706', 'https://openalex.org/W2908336025', 'https://openalex.org/W2920538220', 'https://openalex.org/W2935811960', 'https://openalex.org/W2948197522', 'https://openalex.org/W2949644922', 'https://openalex.org/W2950692458', 'https://openalex.org/W2952809536', 'https://openalex.org/W2962780935', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963018920', 'https://openalex.org/W2963091079', 'https://openalex.org/W2963357517', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2965785213', 'https://openalex.org/W2970287057', 'https://openalex.org/W2988975212', 'https://openalex.org/W4245882962', 'https://openalex.org/W4385245566']","Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation. Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure. In order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates.",1.0
SKG_MT_1290,https://openalex.org/W2125595887,2013,36,"['https://openalex.org/W30655990', 'https://openalex.org/W91928571', 'https://openalex.org/W144133692', 'https://openalex.org/W232191560', 'https://openalex.org/W1554447113', 'https://openalex.org/W1565703351', 'https://openalex.org/W1601269729', 'https://openalex.org/W1631260214', 'https://openalex.org/W1632114991', 'https://openalex.org/W1707559977', 'https://openalex.org/W1966771059', 'https://openalex.org/W2004915807', 'https://openalex.org/W2008225289', 'https://openalex.org/W2058475745', 'https://openalex.org/W2095755718', 'https://openalex.org/W2096204319', 'https://openalex.org/W2096557251', 'https://openalex.org/W2097606805', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105891181', 'https://openalex.org/W2111142112', 'https://openalex.org/W2113651538', 'https://openalex.org/W2119168550', 'https://openalex.org/W2123825474', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131988669', 'https://openalex.org/W2133233009', 'https://openalex.org/W2138302120', 'https://openalex.org/W2140343992', 'https://openalex.org/W2140967626', 'https://openalex.org/W2142112143', 'https://openalex.org/W2143564602', 'https://openalex.org/W2146502635', 'https://openalex.org/W2146662964', 'https://openalex.org/W2154124206', 'https://openalex.org/W2158614781', 'https://openalex.org/W2159755860', 'https://openalex.org/W2160218441', 'https://openalex.org/W2160697141', 'https://openalex.org/W2163238067', 'https://openalex.org/W2164301055', 'https://openalex.org/W2169724380', 'https://openalex.org/W2169755259', 'https://openalex.org/W2171421863', 'https://openalex.org/W2180952760', 'https://openalex.org/W2182252474', 'https://openalex.org/W2183072638', 'https://openalex.org/W2251202280', 'https://openalex.org/W2293596968', 'https://openalex.org/W2949198759', 'https://openalex.org/W3202207277']","We present a fast and scalable online method for tuning statistical machine trans-lation models with large feature sets. The standard tuning algorithm—MERT—only scales to tens of features. Recent discrimi-native algorithms that accommodate sparse features have produced smaller than ex-pected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant trans-lation quality gains by exploiting sparse fea-tures. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and ap-plies to other recent discriminative methods for machine translation. 1",1.0
SKG_MT_1291,https://openalex.org/W2892213699,2018,133,"['https://openalex.org/W2005708641', 'https://openalex.org/W2101105183', 'https://openalex.org/W2125838338', 'https://openalex.org/W2127141656', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2143177362', 'https://openalex.org/W2251994258', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2512924740', 'https://openalex.org/W2525778437', 'https://openalex.org/W2550821151', 'https://openalex.org/W2606134370', 'https://openalex.org/W2613904329', 'https://openalex.org/W2760656271', 'https://openalex.org/W2767206889', 'https://openalex.org/W2778814079', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963434219', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W3082674894', 'https://openalex.org/W4253573210', 'https://openalex.org/W4297747548', 'https://openalex.org/W4385245566']","Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel non-autoregressive architecture based on connectionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the translation quality comparable to other non-autoregressive models.",1.0
SKG_MT_1293,https://openalex.org/W2250771471,2015,18,"['https://openalex.org/W181529752', 'https://openalex.org/W1970689298', 'https://openalex.org/W2101234009', 'https://openalex.org/W2115990601', 'https://openalex.org/W2117278770', 'https://openalex.org/W2126725946', 'https://openalex.org/W2141599568', 'https://openalex.org/W2143337266', 'https://openalex.org/W2147880316', 'https://openalex.org/W2156985047', 'https://openalex.org/W2164984707', 'https://openalex.org/W2184325618', 'https://openalex.org/W2251150371', 'https://openalex.org/W2251222643', 'https://openalex.org/W2437096199', 'https://openalex.org/W2998704965', 'https://openalex.org/W3044695148', 'https://openalex.org/W4285719527']",© 2015 The Authors. Published by Association for Computational Linguistics. This is an open access article available under a Creative Commons licence. &#13;\nThe published version can be accessed at the following link on the publisher’s website: https://www.aclweb.org/anthology/W15-3041,0.9915966386554622
SKG_MT_1295,https://openalex.org/W2250215576,2015,5,[],"In this paper, we enhance the traditional confusion network system\ncombination approach with an additional model trained by a neural network. This\nwork is motivated by the fact that the commonly used binary system voting\nmodels only assign each input system a global weight which is responsible for\nthe global impact of each input system on all translations. This prevents\nindividual systems with low system weights from having influence on the system\ncombination output, although in some situations this could be helpful. Further,\nwords which have only been seen by one or few systems rarely have a chance of\nbeing present in the combined output. We train a local system voting model by a\nneural network which is based on the words themselves and the combinatorial\noccurrences of the different system outputs. This gives system combination the\noption to prefer other systems at different word positions even for the same\nsentence.\n",1.0
SKG_MT_1296,https://openalex.org/W2945059185,2019,61,"['https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W1816079941', 'https://openalex.org/W1879966306', 'https://openalex.org/W1902237438', 'https://openalex.org/W2095690342', 'https://openalex.org/W2101105183', 'https://openalex.org/W2128815520', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2144002870', 'https://openalex.org/W2150378737', 'https://openalex.org/W2157331557', 'https://openalex.org/W2182833076', 'https://openalex.org/W2250974141', 'https://openalex.org/W2252272516', 'https://openalex.org/W2525778437', 'https://openalex.org/W2549259847', 'https://openalex.org/W2552110825', 'https://openalex.org/W2563574619', 'https://openalex.org/W2586559132', 'https://openalex.org/W2619269479', 'https://openalex.org/W2739894144', 'https://openalex.org/W2741239863', 'https://openalex.org/W2758137671', 'https://openalex.org/W2765312172', 'https://openalex.org/W2769298630', 'https://openalex.org/W2962700074', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962739703', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962820991', 'https://openalex.org/W2963011474', 'https://openalex.org/W2963069107', 'https://openalex.org/W2963355447', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963463964', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963571341', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963653811', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963699608', 'https://openalex.org/W2963876447', 'https://openalex.org/W2963888305', 'https://openalex.org/W2963913268', 'https://openalex.org/W2963956654', 'https://openalex.org/W2964048171', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964167098', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964289395', 'https://openalex.org/W2964308564', 'https://openalex.org/W3204406378', 'https://openalex.org/W4385245566']","Meishan Zhang, Zhenghua Li, Guohong Fu, Min Zhang. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",1.0
SKG_MT_1298,https://openalex.org/W2515631395,2016,37,"['https://openalex.org/W179875071', 'https://openalex.org/W222053410', 'https://openalex.org/W1538131130', 'https://openalex.org/W1614298861', 'https://openalex.org/W1848260265', 'https://openalex.org/W1905522558', 'https://openalex.org/W1989658336', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103042430', 'https://openalex.org/W2111856253', 'https://openalex.org/W2115056464', 'https://openalex.org/W2115410424', 'https://openalex.org/W2117278770', 'https://openalex.org/W2120615054', 'https://openalex.org/W2122270629', 'https://openalex.org/W2136477195', 'https://openalex.org/W2137698233', 'https://openalex.org/W2145662801', 'https://openalex.org/W2147262247', 'https://openalex.org/W2155454737', 'https://openalex.org/W2163605009', 'https://openalex.org/W2172268343', 'https://openalex.org/W2189102307', 'https://openalex.org/W2250521169', 'https://openalex.org/W2250539671', 'https://openalex.org/W2250575108', 'https://openalex.org/W2250999640', 'https://openalex.org/W2251103205', 'https://openalex.org/W2251287417', 'https://openalex.org/W2251453970', 'https://openalex.org/W2252053087', 'https://openalex.org/W2252083640', 'https://openalex.org/W2294860948', 'https://openalex.org/W2949541494', 'https://openalex.org/W2951701153', 'https://openalex.org/W2963921497', 'https://openalex.org/W3202006521', 'https://openalex.org/W4285719527']","In this paper, we propose a method which uses semi-supervised convolutional neural networks (CNNs) to select in-domain training data for statistical machine translation. This approach is particularly effective when only tiny amounts of in-domain data are available. The in-domain data and randomly sampled general-domain data are used to train a data selection model with semi-supervised CNN, then this model computes domain relevance scores for all the sentences in the generaldomain data set. The sentence pairs with top scores are selected to train the system. We carry out experiments on 4 language directions with three test domains. Compared with strong baseline systems trained with large amount of data, this method can improve the performance up to 3.1 BLEU. Its performances are significant better than three state-of-the-art language model based data selection methods. We also show that the in-domain data used to train the selection model could be as fewas 100sentences, whichmakesfinegrained topic-dependent translation adaptation possible.",1.0
SKG_MT_1301,https://openalex.org/W2970074184,2019,77,"['https://openalex.org/W808583520', 'https://openalex.org/W2089629691', 'https://openalex.org/W2101105183', 'https://openalex.org/W2132959801', 'https://openalex.org/W2143612262', 'https://openalex.org/W2178654303', 'https://openalex.org/W2251530174', 'https://openalex.org/W2251955814', 'https://openalex.org/W2419292002', 'https://openalex.org/W2529548870', 'https://openalex.org/W2890698823', 'https://openalex.org/W2951456627', 'https://openalex.org/W2951642234', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964078338', 'https://openalex.org/W2970084653', 'https://openalex.org/W4289730217', 'https://openalex.org/W4385245566']","Baigong Zheng, Renjie Zheng, Mingbo Ma, Liang Huang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",1.0
SKG_MT_1304,https://openalex.org/W2563850823,2016,14,"['https://openalex.org/W191292882', 'https://openalex.org/W229329885', 'https://openalex.org/W1483594422', 'https://openalex.org/W1537859740', 'https://openalex.org/W1539673959', 'https://openalex.org/W1557247526', 'https://openalex.org/W1969608442', 'https://openalex.org/W1990856848', 'https://openalex.org/W2011783148', 'https://openalex.org/W2039938580', 'https://openalex.org/W2044138293', 'https://openalex.org/W2060090514', 'https://openalex.org/W2063655091', 'https://openalex.org/W2076405165', 'https://openalex.org/W2078234404', 'https://openalex.org/W2101281673', 'https://openalex.org/W2121169486', 'https://openalex.org/W2123955499', 'https://openalex.org/W2127863960', 'https://openalex.org/W2131988669', 'https://openalex.org/W2132187905', 'https://openalex.org/W2142390309', 'https://openalex.org/W2155359448', 'https://openalex.org/W2162465526', 'https://openalex.org/W2164766438', 'https://openalex.org/W2250677019', 'https://openalex.org/W2251408482', 'https://openalex.org/W2345799635', 'https://openalex.org/W2400297987', 'https://openalex.org/W2466918907', 'https://openalex.org/W2513440303', 'https://openalex.org/W2513832136', 'https://openalex.org/W2574132677', 'https://openalex.org/W3040954139']","Language documentation begins by gathering speech.Manual or automatic transcription at the word level is typically not possible because of the absence of an orthography or prior lexicon, and though manual phonemic transcription is possible, it is prohibitively slow.On the other hand, translations of the minority language into a major language are more easily acquired.We propose a method to harness such translations to improve automatic phoneme recognition.The method assumes no prior lexicon or translation model, instead learning them from phoneme lattices and translations of the speech being transcribed.Experiments demonstrate phoneme error rate improvements against two baselines and the model's ability to learn useful bilingual lexical entries.",1.0
SKG_MT_1306,https://openalex.org/W2108740414,2011,18,"['https://openalex.org/W76590478', 'https://openalex.org/W222053410', 'https://openalex.org/W1479669738', 'https://openalex.org/W1510052640', 'https://openalex.org/W1551202288', 'https://openalex.org/W1964388487', 'https://openalex.org/W1979495315', 'https://openalex.org/W2018116550', 'https://openalex.org/W2020477116', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103156738', 'https://openalex.org/W2105891181', 'https://openalex.org/W2110020502', 'https://openalex.org/W2110104386', 'https://openalex.org/W2112777616', 'https://openalex.org/W2112900913', 'https://openalex.org/W2115289978', 'https://openalex.org/W2119168550', 'https://openalex.org/W2134275125', 'https://openalex.org/W2134729743', 'https://openalex.org/W2139885235', 'https://openalex.org/W2143008661', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2158065314', 'https://openalex.org/W2159107349', 'https://openalex.org/W2159358338', 'https://openalex.org/W2165666205', 'https://openalex.org/W2166781037', 'https://openalex.org/W2166905217', 'https://openalex.org/W2168966090', 'https://openalex.org/W2437005631', 'https://openalex.org/W2911246871']","Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks. 1",1.0
SKG_MT_1310,https://openalex.org/W2252065493,2013,83,"['https://openalex.org/W23077562', 'https://openalex.org/W45604574', 'https://openalex.org/W125693536', 'https://openalex.org/W179314280', 'https://openalex.org/W300322770', 'https://openalex.org/W1631260214', 'https://openalex.org/W1846689784', 'https://openalex.org/W1868901511', 'https://openalex.org/W1968447186', 'https://openalex.org/W1972567251', 'https://openalex.org/W1996430422', 'https://openalex.org/W2031248101', 'https://openalex.org/W2080514141', 'https://openalex.org/W2083195487', 'https://openalex.org/W2087735403', 'https://openalex.org/W2097341304', 'https://openalex.org/W2100238596', 'https://openalex.org/W2101105183', 'https://openalex.org/W2105842272', 'https://openalex.org/W2105891181', 'https://openalex.org/W2106317217', 'https://openalex.org/W2111491614', 'https://openalex.org/W2113641473', 'https://openalex.org/W2116089997', 'https://openalex.org/W2118439278', 'https://openalex.org/W2121127625', 'https://openalex.org/W2121227244', 'https://openalex.org/W2123068277', 'https://openalex.org/W2124807415', 'https://openalex.org/W2125712079', 'https://openalex.org/W2126704587', 'https://openalex.org/W2131988669', 'https://openalex.org/W2134729743', 'https://openalex.org/W2134800885', 'https://openalex.org/W2136657878', 'https://openalex.org/W2138414624', 'https://openalex.org/W2138556193', 'https://openalex.org/W2138934709', 'https://openalex.org/W2142112143', 'https://openalex.org/W2151222319', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154090186', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157512532', 'https://openalex.org/W2158049734', 'https://openalex.org/W2158195707', 'https://openalex.org/W2158917268', 'https://openalex.org/W2159358338', 'https://openalex.org/W2159506050', 'https://openalex.org/W2164766438', 'https://openalex.org/W2168733445', 'https://openalex.org/W2180952760', 'https://openalex.org/W2186839874', 'https://openalex.org/W2251631319', 'https://openalex.org/W2270190199', 'https://openalex.org/W2437005631', 'https://openalex.org/W2895810819', 'https://openalex.org/W3021452258', 'https://openalex.org/W3196998944', 'https://openalex.org/W3203035525']","This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT.",1.0
SKG_MT_1311,https://openalex.org/W2337826259,2016,2,"['https://openalex.org/W60686164', 'https://openalex.org/W1560218263', 'https://openalex.org/W1569788011', 'https://openalex.org/W1746819321', 'https://openalex.org/W1879130152', 'https://openalex.org/W1994352665', 'https://openalex.org/W2095755718', 'https://openalex.org/W2097998348', 'https://openalex.org/W2099201756', 'https://openalex.org/W2100183594', 'https://openalex.org/W2100238596', 'https://openalex.org/W2101105183', 'https://openalex.org/W2102712342', 'https://openalex.org/W2106411961', 'https://openalex.org/W2113145584', 'https://openalex.org/W2119168550', 'https://openalex.org/W2131241448', 'https://openalex.org/W2131792768', 'https://openalex.org/W2134477639', 'https://openalex.org/W2153653739', 'https://openalex.org/W2169003314', 'https://openalex.org/W2192203593', 'https://openalex.org/W2615441104', 'https://openalex.org/W2952519754', 'https://openalex.org/W2962687950', 'https://openalex.org/W2962996334', 'https://openalex.org/W2963258372', 'https://openalex.org/W2964096186']","Daniel Beck, Adrià de Gispert, Gonzalo Iglesias, Aurelien Waite, Bill Byrne. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",1.0
SKG_MT_1313,https://openalex.org/W1591201057,2010,7,"['https://openalex.org/W1631260214', 'https://openalex.org/W1773803948', 'https://openalex.org/W1973200086', 'https://openalex.org/W2015191286', 'https://openalex.org/W2074546930', 'https://openalex.org/W2095907708', 'https://openalex.org/W2096175520', 'https://openalex.org/W2101105183', 'https://openalex.org/W2122228338', 'https://openalex.org/W2131988669', 'https://openalex.org/W2133512280', 'https://openalex.org/W2141182089', 'https://openalex.org/W2144783305', 'https://openalex.org/W2147192413', 'https://openalex.org/W2151197196', 'https://openalex.org/W2156985047', 'https://openalex.org/W2157435188', 'https://openalex.org/W2157875692', 'https://openalex.org/W2160842254', 'https://openalex.org/W2161742089', 'https://openalex.org/W2161952424', 'https://openalex.org/W2164766438', 'https://openalex.org/W2405762604', 'https://openalex.org/W2952343510']","This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. The method can be applied to any language pair where the source language is unsegmented and the target language segmentation is known. First, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating five Asian languages into English revealed that the method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available state-ofthe-art monolingually-built segmentation tools. 1",1.0
SKG_MT_1314,https://openalex.org/W2963661253,2016,253,"['https://openalex.org/W581956982', 'https://openalex.org/W1753482797', 'https://openalex.org/W1902237438', 'https://openalex.org/W1965893653', 'https://openalex.org/W2064675550', 'https://openalex.org/W2101105183', 'https://openalex.org/W2114912785', 'https://openalex.org/W2116261113', 'https://openalex.org/W2116316001', 'https://openalex.org/W2118434577', 'https://openalex.org/W2127863960', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133459682', 'https://openalex.org/W2133564696', 'https://openalex.org/W2136848157', 'https://openalex.org/W2138204974', 'https://openalex.org/W2153508793', 'https://openalex.org/W2153579005', 'https://openalex.org/W2157331557', 'https://openalex.org/W2167723982', 'https://openalex.org/W2175585630', 'https://openalex.org/W2212846646', 'https://openalex.org/W2251678492', 'https://openalex.org/W2402302915', 'https://openalex.org/W2756871636', 'https://openalex.org/W2759607775', 'https://openalex.org/W2952360713', 'https://openalex.org/W2962819663', 'https://openalex.org/W2962907349', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963355447', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964199361', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964335437', 'https://openalex.org/W4294170691']","Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information.We propose a novel end-to-end syntactic NMT model, extending a sequenceto-sequence model with the source-side phrase structure.Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence.Experimental results on the WAT'15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.",1.0
SKG_MT_1316,https://openalex.org/W2117198860,2014,28,"['https://openalex.org/W38126138', 'https://openalex.org/W645927007', 'https://openalex.org/W1508577659', 'https://openalex.org/W1693107767', 'https://openalex.org/W2041232209', 'https://openalex.org/W2086039194', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103042430', 'https://openalex.org/W2116042738', 'https://openalex.org/W2117642127', 'https://openalex.org/W2120679544', 'https://openalex.org/W2121415745', 'https://openalex.org/W2121745180', 'https://openalex.org/W2139812240', 'https://openalex.org/W2139823104', 'https://openalex.org/W2140406733', 'https://openalex.org/W2145662801', 'https://openalex.org/W2145685230', 'https://openalex.org/W2146574666', 'https://openalex.org/W2151075664', 'https://openalex.org/W2153653739', 'https://openalex.org/W2163568299', 'https://openalex.org/W2250229103', 'https://openalex.org/W2251302843', 'https://openalex.org/W2257408573', 'https://openalex.org/W2270190199', 'https://openalex.org/W2437005631', 'https://openalex.org/W4241645538', 'https://openalex.org/W4302780227']","Statistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates.In this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data.The proposed technique first constructs phrase graphs using both source and target language monolingual corpora.Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations.We report results on a large Arabic-English system and a medium-sized Urdu-English system.Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets.Source!Target! el gato! los gatos!un gato! cat! the cat! the cats! a cat! Target!Prob.! the cat! 0.7! cat! 0.15! …! …! felino!canino!el perro!Target!Prob.! canine!0.6!dog! 0.3!…! …! Target!Prob.! the cats!0.8! cats! 0.1!…! …! Target!Prob.! the dog! 0.9! dog! 0.05",1.0
SKG_MT_1320,https://openalex.org/W2168275463,2010,2,"['https://openalex.org/W1489525520', 'https://openalex.org/W1535152236', 'https://openalex.org/W1984417383', 'https://openalex.org/W2101105183', 'https://openalex.org/W2106818711', 'https://openalex.org/W2123301721', 'https://openalex.org/W2127793178', 'https://openalex.org/W2128859735', 'https://openalex.org/W2132019450', 'https://openalex.org/W2135625884', 'https://openalex.org/W2149343034', 'https://openalex.org/W2169279899', 'https://openalex.org/W2172253183']","This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English. 1",1.0
SKG_MT_1321,https://openalex.org/W2951535825,2019,37,"['https://openalex.org/W1522301498', 'https://openalex.org/W1980776243', 'https://openalex.org/W2064675550', 'https://openalex.org/W2095705004', 'https://openalex.org/W2126400076', 'https://openalex.org/W2133458109', 'https://openalex.org/W2138238299', 'https://openalex.org/W2145685230', 'https://openalex.org/W2152180407', 'https://openalex.org/W2251044566', 'https://openalex.org/W2251047310', 'https://openalex.org/W2251861449', 'https://openalex.org/W2419539795', 'https://openalex.org/W2462305634', 'https://openalex.org/W2463895987', 'https://openalex.org/W2515295520', 'https://openalex.org/W2605035112', 'https://openalex.org/W2609278920', 'https://openalex.org/W2739967986', 'https://openalex.org/W2798389157', 'https://openalex.org/W2798762751', 'https://openalex.org/W2807849360', 'https://openalex.org/W2886198413', 'https://openalex.org/W2962696263', 'https://openalex.org/W2962735107', 'https://openalex.org/W2963001247', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963499246', 'https://openalex.org/W2963508788', 'https://openalex.org/W2963672008', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963804993', 'https://openalex.org/W2963899155', 'https://openalex.org/W2963918774', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964212550', 'https://openalex.org/W2964213257', 'https://openalex.org/W2970618241', 'https://openalex.org/W2973088264', 'https://openalex.org/W3104033643', 'https://openalex.org/W4206856021', 'https://openalex.org/W4295803813', 'https://openalex.org/W4302343710', 'https://openalex.org/W4313908941']","We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating para-phrase corpora. Further, we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines.",1.0
SKG_MT_1323,https://openalex.org/W2986265153,2020,6,"['https://openalex.org/W158861739', 'https://openalex.org/W1517853909', 'https://openalex.org/W1522301498', 'https://openalex.org/W1528941926', 'https://openalex.org/W1605282883', 'https://openalex.org/W1829822087', 'https://openalex.org/W1951216520', 'https://openalex.org/W1964922004', 'https://openalex.org/W2016856586', 'https://openalex.org/W2060127787', 'https://openalex.org/W2060377713', 'https://openalex.org/W2064675550', 'https://openalex.org/W2070536585', 'https://openalex.org/W2074729706', 'https://openalex.org/W2087946919', 'https://openalex.org/W2095755718', 'https://openalex.org/W2101105183', 'https://openalex.org/W2108540317', 'https://openalex.org/W2113788796', 'https://openalex.org/W2119202242', 'https://openalex.org/W2120770606', 'https://openalex.org/W2124807415', 'https://openalex.org/W2130942839', 'https://openalex.org/W2130988241', 'https://openalex.org/W2134800885', 'https://openalex.org/W2139604620', 'https://openalex.org/W2140343992', 'https://openalex.org/W2142708806', 'https://openalex.org/W2143541936', 'https://openalex.org/W2144995019', 'https://openalex.org/W2145910665', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2154462503', 'https://openalex.org/W2158716484', 'https://openalex.org/W2159636675', 'https://openalex.org/W2165666205', 'https://openalex.org/W2182833076', 'https://openalex.org/W2220350356', 'https://openalex.org/W2250489794', 'https://openalex.org/W2250618788', 'https://openalex.org/W2250974141', 'https://openalex.org/W2251369387', 'https://openalex.org/W2251400573', 'https://openalex.org/W2251678492', 'https://openalex.org/W2264869955', 'https://openalex.org/W2292919134', 'https://openalex.org/W2339995566', 'https://openalex.org/W2407166119', 'https://openalex.org/W2407799926', 'https://openalex.org/W2428172939', 'https://openalex.org/W2471147443', 'https://openalex.org/W2511550932', 'https://openalex.org/W2512924740', 'https://openalex.org/W2518157461', 'https://openalex.org/W2525778437', 'https://openalex.org/W2525907473', 'https://openalex.org/W2531207078', 'https://openalex.org/W2550821151', 'https://openalex.org/W2560864221', 'https://openalex.org/W2563574619', 'https://openalex.org/W2572474373', 'https://openalex.org/W2573119710', 'https://openalex.org/W2579599705', 'https://openalex.org/W2594470997', 'https://openalex.org/W2601836666', 'https://openalex.org/W2605069615', 'https://openalex.org/W2605717780', 'https://openalex.org/W2609281178', 'https://openalex.org/W2737638662', 'https://openalex.org/W2739894144', 'https://openalex.org/W2740433069', 'https://openalex.org/W2741026152', 'https://openalex.org/W2756888147', 'https://openalex.org/W2758137671', 'https://openalex.org/W2758723264', 'https://openalex.org/W2759173152', 'https://openalex.org/W2760656271', 'https://openalex.org/W2767019613', 'https://openalex.org/W2773621464', 'https://openalex.org/W2799124508', 'https://openalex.org/W2885588803', 'https://openalex.org/W2888329843', 'https://openalex.org/W2890623477', 'https://openalex.org/W2892205701', 'https://openalex.org/W2893141505', 'https://openalex.org/W2896457183', 'https://openalex.org/W2896667998', 'https://openalex.org/W2906152891', 'https://openalex.org/W2907630459', 'https://openalex.org/W2912206855', 'https://openalex.org/W2912351236', 'https://openalex.org/W2921280978', 'https://openalex.org/W2921848006', 'https://openalex.org/W2922523190', 'https://openalex.org/W2949643137', 'https://openalex.org/W2951335443', 'https://openalex.org/W2951559648', 'https://openalex.org/W2951715670', 'https://openalex.org/W2953369973', 'https://openalex.org/W2953830716', 'https://openalex.org/W2962697716', 'https://openalex.org/W2962732637', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962776659', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962790223', 'https://openalex.org/W2962799131', 'https://openalex.org/W2962911926', 'https://openalex.org/W2963251942', 'https://openalex.org/W2963400886', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963430224', 'https://openalex.org/W2963503967', 'https://openalex.org/W2963641561', 'https://openalex.org/W2963648186', 'https://openalex.org/W2963661253', 'https://openalex.org/W2963756346', 'https://openalex.org/W2963991316', 'https://openalex.org/W2964045208', 'https://openalex.org/W2964060510', 'https://openalex.org/W2964089333', 'https://openalex.org/W2964204621', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964291396', 'https://openalex.org/W2964303116', 'https://openalex.org/W2964308564', 'https://openalex.org/W2966280323', 'https://openalex.org/W3089109144', 'https://openalex.org/W3202429746', 'https://openalex.org/W4241645538', 'https://openalex.org/W4241738801']","Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.",1.0
SKG_MT_1324,https://openalex.org/W2102028293,2013,17,"['https://openalex.org/W38126138', 'https://openalex.org/W635530177', 'https://openalex.org/W1523199077', 'https://openalex.org/W1570013475', 'https://openalex.org/W1570490112', 'https://openalex.org/W2012833704', 'https://openalex.org/W2049633694', 'https://openalex.org/W2099873701', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103042430', 'https://openalex.org/W2115367993', 'https://openalex.org/W2118365837', 'https://openalex.org/W2121745180', 'https://openalex.org/W2122056984', 'https://openalex.org/W2124807415', 'https://openalex.org/W2135391077', 'https://openalex.org/W2138247936', 'https://openalex.org/W2139812240', 'https://openalex.org/W2140406733', 'https://openalex.org/W2169360026']","In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocabulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus). 1",1.0
SKG_MT_1325,https://openalex.org/W2130959832,2010,34,"['https://openalex.org/W1768132882', 'https://openalex.org/W1998301456', 'https://openalex.org/W2100238596', 'https://openalex.org/W2101105183', 'https://openalex.org/W2103149536', 'https://openalex.org/W2111491614', 'https://openalex.org/W2117164663', 'https://openalex.org/W2118439278', 'https://openalex.org/W2119168550', 'https://openalex.org/W2122489459', 'https://openalex.org/W2136657878', 'https://openalex.org/W2138414624', 'https://openalex.org/W2141182089', 'https://openalex.org/W2145033586', 'https://openalex.org/W2150986244', 'https://openalex.org/W2157512532', 'https://openalex.org/W2159358338', 'https://openalex.org/W2168828735', 'https://openalex.org/W2168966090', 'https://openalex.org/W2171802951', 'https://openalex.org/W2437005631']","Machine translation benefits from two types of decoding techniques: consensus decoding over multiple hypotheses under a single model and system combination over hypotheses from different models. We present model combination, a method that integrates consensus decoding and system combination into a unified, forest-based technique. Our approach makes few assumptions about the underlying component models, enabling us to combine systems with heterogenous structure. Unlike most system combination techniques, we reuse the search space of component models, which entirely avoids the need to align translation hypotheses. Despite its relative simplicity, model combination improves translation quality over a pipelined approach of first applying consensus decoding to individual systems, and then applying system combination to their output. We demonstrate BLEU improvements across data sets and language pairs in large-scale experiments. 1",1.0
SKG_MT_1326,https://openalex.org/W2250319048,2013,15,"['https://openalex.org/W30655990', 'https://openalex.org/W1736600331', 'https://openalex.org/W1969974515', 'https://openalex.org/W2095755718', 'https://openalex.org/W2111142112', 'https://openalex.org/W2121404172', 'https://openalex.org/W2124807415', 'https://openalex.org/W2135002837', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157817599', 'https://openalex.org/W2168966090', 'https://openalex.org/W2186598481', 'https://openalex.org/W2437005631']","Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called Phrasal-Hiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding discontinuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for Arabic-English, Chinese-English and German-English translation.",1.0
SKG_MT_1327,https://openalex.org/W2156641381,2012,5,"['https://openalex.org/W230880734', 'https://openalex.org/W1479669738', 'https://openalex.org/W1510052640', 'https://openalex.org/W1551202288', 'https://openalex.org/W1632114991', 'https://openalex.org/W1973923101', 'https://openalex.org/W2097790277', 'https://openalex.org/W2101105183', 'https://openalex.org/W2110104386', 'https://openalex.org/W2112900913', 'https://openalex.org/W2116316001', 'https://openalex.org/W2119168550', 'https://openalex.org/W2142632103', 'https://openalex.org/W2143008661', 'https://openalex.org/W2146574666', 'https://openalex.org/W2147880316', 'https://openalex.org/W2150378737', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2153800732', 'https://openalex.org/W2154124206', 'https://openalex.org/W2156515921', 'https://openalex.org/W2158388102', 'https://openalex.org/W2160133551', 'https://openalex.org/W2160382364', 'https://openalex.org/W2161586008', 'https://openalex.org/W2162245945', 'https://openalex.org/W2166210475', 'https://openalex.org/W2437005631', 'https://openalex.org/W2962735828', 'https://openalex.org/W2994982620']","We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks. 1",1.0
SKG_MT_1330,https://openalex.org/W2105174485,2011,17,"['https://openalex.org/W105964407', 'https://openalex.org/W1544826511', 'https://openalex.org/W1682095442', 'https://openalex.org/W1966954239', 'https://openalex.org/W1972819093', 'https://openalex.org/W1996161790', 'https://openalex.org/W1999187286', 'https://openalex.org/W2002664886', 'https://openalex.org/W2018869373', 'https://openalex.org/W2034106047', 'https://openalex.org/W2035720976', 'https://openalex.org/W2060833990', 'https://openalex.org/W2070150502', 'https://openalex.org/W2101105183', 'https://openalex.org/W2101778115', 'https://openalex.org/W2102074916', 'https://openalex.org/W2104822358', 'https://openalex.org/W2114584278', 'https://openalex.org/W2134566648', 'https://openalex.org/W2154486363', 'https://openalex.org/W2156584656', 'https://openalex.org/W2158240052', 'https://openalex.org/W2159107349', 'https://openalex.org/W2162211144', 'https://openalex.org/W2163582407', 'https://openalex.org/W2463396630']","We describe our submissions to the WMT11 shared MT evaluation task: MTeRater and MTeRater-Plus. Both are machine-learned metrics that use features from e-rater ®, an automated essay scoring engine designed to assess writing proﬁciency. Despite using only features from e-rater and without comparing to translations, MTeRater achieves a sentencelevel correlation with human rankings equivalent to BLEU. Since MTeRater only assesses ﬂuency, we build a meta-metric, MTeRaterPlus, that incorporates adequacy by combining MTeRater with other MT evaluation metrics and heuristics. This meta-metric has a higher correlation with human rankings than either MTeRater or individual MT metrics alone. However, we also ﬁnd that e rater features may not have signiﬁcant impact on correlation in every case.",1.0
SKG_MT_1331,https://openalex.org/W3035548285,2020,18,"['https://openalex.org/W222053410', 'https://openalex.org/W2006969979', 'https://openalex.org/W2064675550', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133280805', 'https://openalex.org/W2133564696', 'https://openalex.org/W2152263452', 'https://openalex.org/W2153653739', 'https://openalex.org/W2157331557', 'https://openalex.org/W2539201987', 'https://openalex.org/W2566564022', 'https://openalex.org/W2613904329', 'https://openalex.org/W2804044248', 'https://openalex.org/W2817535134', 'https://openalex.org/W2885588803', 'https://openalex.org/W2888520903', 'https://openalex.org/W2888539709', 'https://openalex.org/W2921311659', 'https://openalex.org/W2962739339', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962931466', 'https://openalex.org/W2962945603', 'https://openalex.org/W2963260202', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964334713', 'https://openalex.org/W2965575120', 'https://openalex.org/W2970550739', 'https://openalex.org/W2970682957', 'https://openalex.org/W4241645538', 'https://openalex.org/W4385245566']","The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018). Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (“phrases”) and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.",1.0
SKG_MT_1333,https://openalex.org/W2178149891,2015,12,"['https://openalex.org/W1631260214', 'https://openalex.org/W2060786818', 'https://openalex.org/W2101816610', 'https://openalex.org/W2124807415', 'https://openalex.org/W2137894131', 'https://openalex.org/W2148392518', 'https://openalex.org/W2156985047', 'https://openalex.org/W2251994258', 'https://openalex.org/W2257408573', 'https://openalex.org/W2595715041', 'https://openalex.org/W2604377206']","We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and obtain results close to the top with an average of 3.176 BLEU points difference using significantly less resources for building SMT systems.ParFDA is a parallel implementation of feature decay algorithms (FDA) developed for fast deployment of accurate SMT systems (Bic ¸ici, 2013;Bic ¸ici et al., 2014;Bic ¸ici and Yuret, 2015).ParFDA Moses SMT system we built is able to obtain the top TER performance in French to English translation.We make the data for building ParFDA Moses SMT systems for WMT15 available:",1.0
SKG_MT_1334,https://openalex.org/W3120951354,2020,23,"['https://openalex.org/W1904365287', 'https://openalex.org/W2149327368', 'https://openalex.org/W2953072129', 'https://openalex.org/W2962678612', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963212250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964121744', 'https://openalex.org/W2971120958', 'https://openalex.org/W2983040767']","In this paper, we describe the Bering Lab’s submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.",1.0
SKG_MT_1342,https://openalex.org/W3094522744,2020,2,"['https://openalex.org/W2006617528', 'https://openalex.org/W2101105183', 'https://openalex.org/W2111666304', 'https://openalex.org/W2119727789', 'https://openalex.org/W2170204377', 'https://openalex.org/W2251743902', 'https://openalex.org/W2270190199', 'https://openalex.org/W2531207078', 'https://openalex.org/W2550821151', 'https://openalex.org/W2555745756', 'https://openalex.org/W2566926700', 'https://openalex.org/W2567571499', 'https://openalex.org/W2610245951', 'https://openalex.org/W2919290281', 'https://openalex.org/W2921280978', 'https://openalex.org/W2952153923', 'https://openalex.org/W2962889503', 'https://openalex.org/W2962982474', 'https://openalex.org/W2963247703', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963549838', 'https://openalex.org/W2964007535', 'https://openalex.org/W2964034111', 'https://openalex.org/W2964073484', 'https://openalex.org/W2972909479', 'https://openalex.org/W2985301125', 'https://openalex.org/W3017454464', 'https://openalex.org/W3029318821', 'https://openalex.org/W3035019713']","Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as the source or target language). While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora (WMT), and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples (the same sentence in more than two languages). We set out to study the use of multi-way aligned examples to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation (cMNMT) and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 111*112=12,432 language pairs that provides competitive translation quality for all language pairs.",1.0
SKG_MT_1345,https://openalex.org/W3034571331,2020,99,"['https://openalex.org/W1494198834', 'https://openalex.org/W1537859740', 'https://openalex.org/W1902237438', 'https://openalex.org/W2113106066', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134546430', 'https://openalex.org/W2251321385', 'https://openalex.org/W2296073425', 'https://openalex.org/W2327501763', 'https://openalex.org/W2345837149', 'https://openalex.org/W2402577742', 'https://openalex.org/W2466918907', 'https://openalex.org/W2605131327', 'https://openalex.org/W2620757702', 'https://openalex.org/W2747874407', 'https://openalex.org/W2785350307', 'https://openalex.org/W2936774411', 'https://openalex.org/W2936969148', 'https://openalex.org/W2949328740', 'https://openalex.org/W2955541912', 'https://openalex.org/W2962680099', 'https://openalex.org/W2962780374', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963834942', 'https://openalex.org/W2964102148', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964327384', 'https://openalex.org/W2972448360', 'https://openalex.org/W2981991061', 'https://openalex.org/W2989819126', 'https://openalex.org/W2992632249', 'https://openalex.org/W2997436923', 'https://openalex.org/W3007328579', 'https://openalex.org/W3008549139', 'https://openalex.org/W3015703505', 'https://openalex.org/W3095189764', 'https://openalex.org/W4288021071', 'https://openalex.org/W4298018319', 'https://openalex.org/W4300558631', 'https://openalex.org/W4385245566']","End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.",1.0
SKG_MT_1346,https://openalex.org/W2250911766,2015,96,"['https://openalex.org/W145476170', 'https://openalex.org/W1760025803', 'https://openalex.org/W2101802482', 'https://openalex.org/W2101848544', 'https://openalex.org/W2127426251', 'https://openalex.org/W2127795553', 'https://openalex.org/W2184957013', 'https://openalex.org/W2283196293', 'https://openalex.org/W2952155763']",International audience,1.0
SKG_MT_1347,https://openalex.org/W2251682575,2014,489,"['https://openalex.org/W125693536', 'https://openalex.org/W179875071', 'https://openalex.org/W932413789', 'https://openalex.org/W1528441900', 'https://openalex.org/W1753482797', 'https://openalex.org/W1815076433', 'https://openalex.org/W1880262756', 'https://openalex.org/W1909398668', 'https://openalex.org/W1934041838', 'https://openalex.org/W1996903695', 'https://openalex.org/W2013540053', 'https://openalex.org/W2060127787', 'https://openalex.org/W2083545877', 'https://openalex.org/W2100183594', 'https://openalex.org/W2116042738', 'https://openalex.org/W2118090838', 'https://openalex.org/W2140343992', 'https://openalex.org/W2141599568', 'https://openalex.org/W2144879357', 'https://openalex.org/W2156985047', 'https://openalex.org/W2250489405', 'https://openalex.org/W2250732891', 'https://openalex.org/W2251098065', 'https://openalex.org/W2251222643', 'https://openalex.org/W2251246760', 'https://openalex.org/W2252177599', 'https://openalex.org/W2437005631', 'https://openalex.org/W2914484425', 'https://openalex.org/W4285719527']","Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, John Makhoul. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2014.",1.0
